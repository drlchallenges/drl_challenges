,Id,PostTypeId,AcceptedAnswerId,ParentId,CreationDate,DeletionDate,Score,ViewCount,Body,OwnerUserId,OwnerDisplayName,LastEditorUserId,LastEditorDisplayName,LastEditDate,LastActivityDate,Title,Tags,AnswerCount,CommentCount,FavoriteCount,ClosedDate,CommunityOwnedDate,ContentLicense,
14,56447589,1,56453315,,2019-06-04 16:07:23,,0,803,"<p>According to these guys (<a href=""https://nihit.github.io/resources/spaceinvaders.pdf"" rel=""nofollow noreferrer"">https://nihit.github.io/resources/spaceinvaders.pdf</a>) it is possible to perform Early Stopping with Deep Reinforcement Learning. I used that before with Deep Learning on Keras, but, how to do that on keras-rl? in the same fit() function or before sending the model to the agent?</p>
",2163392,,8893595,,2019-06-05 02:51:16,2019-06-05 02:51:16,Deep Reinforcement Learning (keras-rl) Early stopping,<machine-learning><keras><deep-learning><reinforcement-learning><keras-rl>,1,0,,,,CC BY-SA 4.0,
15,56650573,1,56859618,,2019-06-18 13:41:19,,1,82,"<p>I am currently trying to reproduce some results on my installation of flow from your previous papers. I ran over the following questions, where I am not clear about the exact parameters used in the experiments, and the results given in the papers.</p>

<p>For [1], I expected to be able to reproduce the results by running stabilizing_highway.py from your repo. (with commit ""bc44b21"", although I tried to run the current version, but could not find differences related to my questions).
I expected the merge scenario used being the same in [2].</p>

<p>Where I already found differences in the papers/code was:</p>

<p>1) the reward function in [2] (2) is different than in [1] (6): the first uses a max and normalizing in the first part of the sum. Why this difference?
Looking at the code, I interpret it the following: 
Depending on the evaluate flag, you either compute  (a) the reward as average speed over all vehicles in the simulation or (b) as the function given in [2] (without the normalizing term on the speed), but with a value of alpha (eta2 in the code) = 0.1 (see merge.py, line 167, compute_reward). I could not find the alpha parameter given in the papers, so I assume the code version was used?</p>

<p>2) I further read the code as if you were calculating it by iterating over ALL vehicles in the simulation, not just the observed ones? This seems counterintuitive to me, using a reward function in a partially observed environment to train the agent by using information from the fully observed state information...!? </p>

<p>3) This leads to the next question: you eventually want to evaluate the reward as given when the evaluate flag is set, namely the average speed of all vehicles in the simulation, as given in Table 1 of [1]. Are these values calculated by averaging over the ""speed"" column in the emissions.csv file you can produce running the visualizer tool?</p>

<p>4) The next question is regarding the cumulative return in the Figures of [1] and [2]. In [1], FIgure 3, in the merge scenarios, the cum. returns are max of around 500, while the max. values of [2], Figure 5 are around 200000.  Why this difference? The different reward functions used? Please, could you provide the alpha values for both and verify which version is correct (paper or code)?</p>

<p>5) What I also observe looking at [1] Table 1, Merge1&amp;2: ES has clearly the highest values of average speed, but TRPO and PPO have a better cumulative return. Does this suggest that the 40 rollouts for evaluation where not enough to get a representative mean value? Or that maximizing the training reward function does not necessarily give good evaluation results? </p>

<p>6) Some other parameters are unclear to me:
In [1] Fig3, 50 rollouts are mentioned, while N_ROLLOUTS=20. What do you recommend using?
In [1] A.2 Merge, T=400, while HORIZON=600, and [2] C. Simulations talks about 3600s. Looking at a replay in Sumo produced when running visualizer_rllib.py, Simulation terminates at time 120.40, which would match the HORIZON of 600 with time steps of 0.2s (this information is given in [2].)
So I assume, that for this scenario, the horizon should be set much higher than both in 1 and the code, and rather set to 18.000?</p>

<p>Thanks for any hints!
KR
M</p>

<p>[1] Vinitsky, E., Kreidieh, A., Le Flem, L., Kheterpal, N., Jang, K., Wu, F., ... &amp; Bayen, A. M. (2018, October). Benchmarks for reinforcement learning in mixed-autonomy traffic. In Conference on Robot Learning (pp. 399-409)</p>

<p>[2] Kreidieh, Abdul Rahman, Cathy Wu, and Alexandre M. Bayen. ""Dissipating stop-and-go waves in closed and open networks via deep reinforcement learning."" In 2018 21st International Conference on Intelligent Transportation Systems (ITSC), pp. 1475-1480. IEEE, 2018.</p>
",11664844,,6279687,,2019-06-18 14:11:33,2019-07-02 20:26:19,"Simulation parameters and reward calculation in benchmark scenario ""Merge""",<flow-project>,1,1,,,,CC BY-SA 4.0,
24,70562317,1,70568519,,2022-01-03 06:28:15,,0,143,"<p>I am working on an Actor-Critic model in Pytorch. The model first receives the input in an RNN and then the policy net comes into play. The code for Policy net is:</p>
<pre><code>class Policy(nn.Module):
    &quot;&quot;&quot;
    implements both actor and critic in one model
    &quot;&quot;&quot;
    def __init__(self):
        super(Policy, self).__init__()
        self.fc1 = nn.Linear(state_size+1, 128)

        self.fc2 = nn.Linear(128, 64)

        # actor's layer
        self.action_head = nn.Linear(64, action_size)
        self.mu = nn.Sigmoid()
        self.var = nn.Softplus()

        # critic's layer
        self.value_head = nn.Linear(64, 1)


    def forward(self, x):
        &quot;&quot;&quot;
        forward of both actor and critic
        &quot;&quot;&quot;
        x = F.relu(self.fc1(x))
        x = F.relu(self.fc2(x))

        # actor: choses action to take from state s_t 
        # by returning probability of each action
        action_prob = self.action_head(x)
        mu = self.mu(action_prob)
        var = self.var(action_prob)

        # critic: evaluates being in the state s_t
        state_values = self.value_head(x)

        return mu, var, state_values
policy = Policy()
</code></pre>
<p>In model class, we are calling this policy after the rnn. And in agent classâ€™s act method, we are calling the model to get the action like this:</p>
<pre><code> def act(self, some_input, state):
      mu, var, state_value = self.model(some_input, state)
      mu = mu.data.cpu().numpy()
      sigma = torch.sqrt(var).data.cpu().numpy()
      action = np.random.normal(mu, sigma)
      action = np.clip(action, 0, 1)
      action = torch.from_numpy(action/1000)
      return action, state_value
</code></pre>
<p>I must mention that in optimizer, we are calling the model.parameters. When we print all the trainable parameters in each epoch, we see that everything else is changing except for the policy.action_head. Any idea why this is happening? I must also mention how the losses are calculated now:</p>
<pre><code>       advantage = reward - Value
       Lp = -math.log(pdf_prob_now)*advantage
       policy_losses.append(Lp)
       #similar for value_losses
#after all the runs in the epoch is done
loss = torch.stack(policy_losses).sum() + alpha*torch.stack(value_losses).sum()
loss.backward()
</code></pre>
<p>Here Value is the state_value (the 2nd output from agent.act) and the pdf_prob_now is the probability of the action from all possible actions which is calculated like this:</p>
<pre><code>def find_pdf(policy, action, rnn_output):
    mu, var, _ = policy(rnn_output)
    mu = mu.data.cpu().numpy()
    sigma = torch.sqrt(var).data.cpu().numpy()
    pdf_probability = stats.norm.pdf(action.cpu(), loc=mu, scale=sigma)
    return pdf_probability
</code></pre>
<p>Is there some logical error here?</p>
",14608622,,,,,2022-01-03 16:13:10,Some weights of Actor Critic model not updating,<pytorch><reinforcement-learning>,1,0,,,,CC BY-SA 4.0,
25,54870581,1,54873357,,2019-02-25 16:23:42,,1,568,"<p>My question is not related to the implementation of Reinforcement Learning, but to understand the concept of RL when every state is a terminal state.</p>

<p>I'll give an example: A robot is learning to play soccer, just shooting. The reward is the distance between the ball and the goalpost after it shoots for the goal. The state is an array of multiple features, and the action is an array with the three-dimensional force.</p>

<p>If we considered episodic RL, I feel like the approach doesn't make sense. Indeed, the robot shoots and the reward is given: each episode is a terminal episode. It doesn't make sense to pass the next state to the system since the algorithm doesn't care about it to optimize the reward - in this case, I would use an Actor-Critic approach to handle continuous state and action space. Someone might argue that a different supervised learning approach, such as a Deep Neural Network, might work better. But I am not sure since in that case, the algorithm would not be able to achieve good results with input far from the training set. As far as I saw, RL is able to generalize better for this context.</p>

<p>The question is: is RL a valid methodology for this problem, and how are terminal states managed in this case? Are you aware of similar examples in literature?</p>
",9177422,,,,,2019-03-04 08:34:07,Reinforcement Learning where every state is terminal,<machine-learning><reinforcement-learning>,3,3,0,,,CC BY-SA 4.0,
27,56312962,1,56318256,,2019-05-26 11:09:35,,1,3103,"<p>Hello StackOverflow Community!</p>

<p>I have a question about Actor-Critic Models in Reinforcement Learning.</p>

<p>While listening policy gradient methods classes of Berkeley University, it is said in the lecture that in the actor-critic algorithms where we both optimize our policy with some policy parameters and our value functions with some value function parameters, we use same parameters in both optimization problems(i.e. policy parameters = value function parameters) in some algorithms (e.g. A2C/A3C)</p>

<p>I could not understand how this works. I was thinking that we should optimize them separately. How does this shared parameter solution helps us?</p>

<p>Thanks in advance :)</p>
",5481706,,,,,2019-05-27 12:04:29,How do shared parameters in actor-critic models work?,<reinforcement-learning>,1,0,,,,CC BY-SA 4.0,
30,56730118,1,56765796,,2019-06-24 05:05:08,,0,326,"<p>I am working on project in which I need to find best optimised path from 1 point to another in continuous space in multi agent scenario.  I am looking for best algorithm which suits this problem using Reinforcement learning. I have tried ""Multi-agent actor-critic for mixed cooperative-competitive environment"" but it does not seems to reach goals in 10000 epesidoes.  How can I improve this algorithm or is there any other algorithm that can help me with this. </p>
",11503948,,,,,2019-06-26 05:28:26,Best algorithm for multi agent continuous space path finding using Reinforcement learning,<deep-learning><artificial-intelligence><pytorch><reinforcement-learning><multi-agent>,1,0,,,,CC BY-SA 4.0,
34,61708729,1,61747498,,2020-05-10 07:44:43,,0,2784,"<p>I'm using the Soft Actor-Critic implementation available <a href=""https://github.com/pranz24/pytorch-soft-actor-critic"" rel=""nofollow noreferrer"">here</a> for one of my projects. But when I try to run it, I get the following error:</p>

<p><code>RuntimeError: one of the variables needed for gradient computation has been modified by an inplace operation: [torch.FloatTensor [256, 1]], which is output 0 of TBackward, is at version 3; expected version 2 instead. Hint: enable anomaly detection to find the operation that failed to compute its gradient, with torch.autograd.set_detect_anomaly(True).</code></p>

<p>The error arises from the gradient computation in the <code>sac.py</code> file. I can't see the operation that might be inplace. Any help?</p>

<p>The traceback:</p>

<pre><code>Traceback (most recent call last)
&lt;ipython-input-10-c124add9a61d&gt; in &lt;module&gt;()
     22             for i in range(updates_per_step):
     23                 # Update parameters of all the networks
---&gt; 24                 critic_1_loss, critic_2_loss, policy_loss, ent_loss, alpha = agent.update_parameters(memory, batch_size, updates)
     25                 updates += 1
     26 

2 frames
&lt;ipython-input-7-a2432c4c3767&gt; in update_parameters(self, memory, batch_size, updates)
     87 
     88         self.policy_optim.zero_grad()
---&gt; 89         policy_loss.backward()
     90         self.policy_optim.step()
     91 

/usr/local/lib/python3.6/dist-packages/torch/tensor.py in backward(self, gradient, retain_graph, create_graph)
    196                 products. Defaults to ``False``.
    197         """"""
--&gt; 198         torch.autograd.backward(self, gradient, retain_graph, create_graph)
    199 
    200     def register_hook(self, hook):

/usr/local/lib/python3.6/dist-packages/torch/autograd/__init__.py in backward(tensors, grad_tensors, retain_graph, create_graph, grad_variables)
     98     Variable._execution_engine.run_backward(
     99         tensors, grad_tensors, retain_graph, create_graph,
--&gt; 100         allow_unreachable=True)  # allow_unreachable flag
    101 
    102 
</code></pre>
",9427451,,,,,2020-05-12 08:49:49,Error: one of the variables needed for gradient computation has been modified by an inplace operation,<python><pytorch><reinforcement-learning>,1,0,,,,CC BY-SA 4.0,
35,65029828,1,65108767,,2020-11-26 22:13:15,,1,218,"<p>I implemented a simple actor-critic model in Tensorflow==2.3.1 to learn Cartpole environment. But it is not learning at all. The average scores of every 50 episodes is below 20. Can someone please point out why the model isn't learning?</p>
<p>I based my algorithm on the following pseudocode:</p>
<pre><code>for every episode do
    S &lt;- starting state
    for every step do
        choose action A based on actor_policy P
        take an action A
        observe the reward R and new state S_new
        calculate error E based on TD(0) schema V:
        if new state S_new is not terminal then
            E &lt;- R + discount_factor * V(S_new) - V(S)
        else
            E &lt;- R - V(S))
        end if
        calculate the loss for the critic: Lc &lt;- E^2
        calculate the loss for the actor:  La &lt;- -E * ln(P(A, S))
        calculate overall loss: L &lt;- Lc + La
        update the weights with the gradient: w &lt;- w + alpha * grad(w) * L
    end for
end for
</code></pre>
<p><code>Discount factor</code> and <code>alpha</code> are constant. Here are the requirements:</p>
<pre><code>box2d==2.3.10
gym==0.17.3
keras==2.4.3
matplotlib==3.3.3
numpy==1.19.4
scikit-learn==0.23.2
tensorflow==2.3.1
tensorflow-probability==0.11.1
tqdm==4.53.0
</code></pre>
<p>And finally, my code is:</p>
<pre><code>from typing import Optional
import math

import gym
import matplotlib.pyplot as plt
import numpy as np
import tensorflow as tf
import tensorflow_probability as tfp
from tqdm import tqdm

CART_POLE_ACTIONS = [0, 1]
CART_POLE_OUTPUT_LEN = len(CART_POLE_ACTIONS)
CART_POLE_INPUTS = 4


class ActorCriticController:
    def __init__(self, environment, learning_rate: float, discount_factor: float, input_size: int,
                 output_size: int, h1_size: int, h2_size: int, actions: list):
        self.environment = environment
        self.discount_factor: float = discount_factor
        self.input_size = input_size
        self.output_size = output_size
        self.h1_size = h1_size
        self.h2_size = h2_size
        self.actions = actions

        self.optimizer: tf.keras.optimizers.Adam = tf.keras.optimizers.Adam(
            learning_rate=learning_rate)
        self.last_error_squared: float = 0.0
        self.model: tf.keras.Model = self.create_actor_critic_model()
        self.log_action_probability: Optional[tf.Tensor] = None
        self.tape: Optional[tf.GradientTape] = None

    def create_actor_critic_model(self) -&gt; tf.keras.Model:
        inputs = tf.keras.Input(shape=(self.input_size,))
        hidden1 = tf.keras.layers.Dense(self.h1_size, activation='relu')(inputs)
        hidden2 = tf.keras.layers.Dense(self.h2_size, activation='relu')(hidden1)
        outputs_actor = tf.keras.layers.Dense(self.output_size, activation='softmax')(hidden2)
        outputs_critic = tf.keras.layers.Dense(1, activation='linear')(hidden2)

        model = tf.keras.Model(inputs=inputs, outputs=[outputs_actor, outputs_critic])

        return model

    def choose_action(self, state: np.ndarray) -&gt; int:
        state = self.format_state(state)

        self.tape = tf.GradientTape()
        with self.tape:
            probs, _ = self.model(state)
            action = tfp.distributions.Categorical(probs=probs).sample(1)
            index = self.actions.index(int(action))
            self.log_action_probability = math.log(probs[0][index], math.e)
        return int(action)

    def learn(self, state: np.ndarray, reward: float, new_state: np.ndarray, terminal: bool):
        state = self.format_state(state)
        new_state = self.format_state(new_state)

        with self.tape:
            _, critic_value = self.model(state)
            _, new_critic_value = self.model(new_state)

            error = reward - critic_value
            if not terminal:
                error += self.discount_factor * new_critic_value

            self.last_error_squared = float(error) ** 2
            loss = self.last_error_squared - error * self.log_action_probability

        gradients = self.tape.gradient(loss, self.model.trainable_weights)
        self.optimizer.apply_gradients(zip(gradients, self.model.trainable_weights))

    @staticmethod
    def format_state(state: np.ndarray) -&gt; np.ndarray:
        return np.reshape(state, (1, state.size))


def main() -&gt; None:
    environment = gym.make('CartPole-v1')
    controller = ActorCriticController(environment=environment,
                                       learning_rate=0.00001,
                                       discount_factor=0.99,
                                       input_size=CART_POLE_INPUTS,  # 4
                                       output_size=CART_POLE_OUTPUT_LEN,  # 2
                                       h1_size=128, # 1024, 128
                                       h2_size=32,  # 256, 32
                                       actions = CART_POLE_ACTIONS)

    past_rewards = []
    past_errors = []
    for i_episode in tqdm(range(2000)):
        done = False
        state = environment.reset()
        reward_sum = 0.0
        errors_history = []

        while not done:
            environment.render()

            action = controller.choose_action(state)
            new_state, reward, done, info = environment.step(action)
            controller.learn(state, reward, new_state, done)
            state = new_state
            reward_sum += reward
            errors_history.append(controller.last_error_squared)
        print(f&quot;reward_sum = {reward_sum}\n\n\n&quot;)
        past_rewards.append(reward_sum)
        past_errors.append(np.mean(errors_history))

        window_size = 50
        if i_episode % 25 == 0:
            if len(past_rewards) &gt;= window_size:
                fig, axs = plt.subplots(2)

                axs[0].plot(
                    [np.mean(past_errors[i:i + window_size]) for i in range(len(past_errors) - window_size)],
                    'tab:red',
                )
                axs[0].set_title('mean squared error')

                axs[1].plot(
                    [np.mean(past_rewards[i:i+window_size]) for i in range(len(past_rewards) - window_size)],
                    'tab:green',
                )
                axs[1].set_title('sum of rewards')
            plt.savefig(f'learning_{i_episode}.png')
            plt.clf()

    environment.close()
    controller.model.save(&quot;final.model&quot;)


if __name__ == '__main__':
    main()
</code></pre>
<p>Thank you in advance for your help. I hope someone will advise me :)</p>
",14716149,,14716149,,2020-11-27 13:36:51,2020-12-02 13:11:15,Problem with implementing temporal difference based on actor-critic,<python><python-3.x><tensorflow><tensorflow2.0><reinforcement-learning>,1,0,,,,CC BY-SA 4.0,
45,71419306,1,71488678,,2022-03-10 05:37:00,,0,78,"<p>In Actor-Critic methods the Actor and Critic are assigned two complimentary, but different goals. I'm trying to understand whether the differences between these goals (updating a policy and updating a value function) are large enough to warrant different models for the Actor and Critic, or if they are of similar enough complexity that the same model should be reused for simplicity. I realize that this could be very situational, but not in what way. For example, does the balance shift as the model complexity grows?</p>
<p>Please let me know if there are any rules of thumb for this, or if you know of a specific publication that addresses the issue.</p>
",14345989,,14345989,,2022-03-12 20:15:39,2022-03-15 20:32:34,Are there benefits to having Actor and Critic use significantly different models?,<tensorflow><keras><pytorch><reinforcement-learning><q-learning>,1,0,,,,CC BY-SA 4.0,
46,55673412,1,55673559,,2019-04-14 08:34:04,,2,1080,"<p>I am trying to implement <strong>Actor-Critic learning atuomation algorithm</strong> that is not same as basic actor-critic algorithm, it's little bit changed.</p>

<p>Anyway, I used Adam optimizer and implemented with pytorch</p>

<p>when i backward TD-error for Critic first, there's no error.
However, i backward loss for Actor, the error occured.</p>

<blockquote>
  <p>--------------------------------------------------------------------------- RuntimeError                              Traceback (most recent call
  last)  in 
       46             # update Actor Func
       47             optimizer_M.zero_grad()
  ---> 48             loss.backward()
       49             optimizer_M.step()
       50 </p>
  
  <p>~\Anaconda3\lib\site-packages\torch\tensor.py in backward(self,
  gradient, retain_graph, create_graph)
      100                 products. Defaults to <code>False</code>.
      101         """"""
  --> 102         torch.autograd.backward(self, gradient, retain_graph, create_graph)
      103 
      104     def register_hook(self, hook):</p>
  
  <p>~\Anaconda3\lib\site-packages\torch\autograd__init__.py in
  backward(tensors, grad_tensors, retain_graph, create_graph,
  grad_variables)
       88     Variable._execution_engine.run_backward(
       89         tensors, grad_tensors, retain_graph, create_graph,
  ---> 90         allow_unreachable=True)  # allow_unreachable flag
       91 
       92 </p>
  
  <p>RuntimeError: one of the variables needed for gradient computation has
  been modified by an inplace operation</p>
</blockquote>

<p>above is the content of error</p>

<p>I tried to find inplace operation, but I haven't found in my written code.
I think i don't know how to handle optimizer.</p>

<p>Here is main code:</p>

<pre><code>        for cur_step in range(1):   
        action = M_Agent(state, flag)  
        next_state, r = env.step(action)   

        # calculate TD Error
        TD_error = M_Agent.cal_td_error(r, next_state)

        # calculate Target
        target = torch.FloatTensor([M_Agent.cal_target(TD_error)])
        logit = M_Agent.cal_logit()
        loss = criterion(logit, target)

        # update value Func
        optimizer_M.zero_grad()
        TD_error.backward()
        optimizer_M.step()

        # update Actor Func
        loss.backward()
        optimizer_M.step()
</code></pre>

<p>Here is the agent network</p>

<pre><code>    # Actor-Critic Agent
    self.act_pipe = nn.Sequential(nn.Linear(state, 128),
                            nn.ReLU(),
                            nn.Dropout(0.5),
                            nn.Linear(128, 256),
                            nn.ReLU(),
                            nn.Dropout(0.5),
                            nn.Linear(256, num_action),
                            nn.Softmax()
                            )

     self.val_pipe = nn.Sequential(nn.Linear(state, 128),
                            nn.ReLU(),
                            nn.Dropout(0.5),
                            nn.Linear(128, 256),
                            nn.ReLU(),
                            nn.Dropout(0.5),
                            nn.Linear(256, 1)
                            )


      def forward(self, state, flag, test=None):

          temp_action_prob = self.act_pipe(state)
          self.action_prob = self.cal_prob(temp_action_prob, flag)
          self.action = self.get_action(self.action_prob)
          self.value = self.val_pipe(state)

          return self.action
</code></pre>

<p>I wanna update each network respectively.</p>

<p>and I wanna know that Basic <strong>TD Actor-Critic</strong> method uses TD error for loss??
or squared error between r+V(s') and V(s) ?</p>
",10143396,,2781816,,2019-04-14 08:42:06,2019-04-14 09:03:53,Adam optimizer error: one of the variables needed for gradient computation has been modified by an inplace operation,<optimization><error-handling><deep-learning><pytorch><reinforcement-learning>,1,0,,,,CC BY-SA 4.0,
49,44926583,1,45047565,,2017-07-05 12:41:52,,0,1045,"<p>I have recently tried to implement my own version of the <strong>Asynchronous Advantage Actor-Critic (A3C)</strong> method for deep reinforcement learning since I couldn't get other A3C implementations found in the web to work properly. The problem is that my version isn't converging either....So, I would really appreciate any help to identify the problem. The code is located in here: <a href=""https://github.com/MatheusMRFM/A3C-LSTM-with-Tensorflow"" rel=""nofollow noreferrer"">https://github.com/MatheusMRFM/A3C-LSTM-with-Tensorflow</a>. I am training the method using the Pong game from the Open AI gym environment. Here's what I did:</p>

<ul>
<li>My implementation is heavily based in the following A3C implementations: Arthur Juliani's version, Open AI's A3C and <a href=""https://github.com/andreimuntean/a3c"" rel=""nofollow noreferrer"">andreimuntean's version</a>. I've chosen these implementations due to their clarity and because everything seemed correct according the the original A3C paper;</li>
<li>I'm using a network as follows: a set of convolutional layers, a fully connected layer, an LSTM layer, and again, two fully connected layers (one for the policy and the other for the value function). I already tested several other architectures (changing the concolutional layers, removing the first hidden layer, change the output of the hidden and LSTM layers, etc. None of these configurations worked....</li>
<li>I tried 3 different optimizers: <strong>RMSPropOptimizer</strong>, <strong>AdadeltaOptimizer</strong>, and <strong>AdamOptimizer</strong>. I also tried different learning rates for each one. No luck;</li>
<li>I already tried several parameters based on the implementations that I looked.</li>
</ul>

<p>My code always ends up converging to a policy where the paddle always moves up or always moves down (not both). There must be some stupid detail that I missed, but I can't find it. Any help is welcome!</p>
",7215963,,,,,2017-07-12 02:54:24,Can't get my A3C with LSTM layer using Tensorflow to work,<asynchronous><tensorflow><deep-learning><reinforcement-learning>,1,2,,,,CC BY-SA 3.0,
61,72629938,1,72636494,,2022-06-15 10:39:54,,0,89,"<p>I am trying to plot the overestimation bias of the critics in DDPG and TD3 models.
So essentially there is a critic_target and a critic network.
I want to understand how does one go about finding the overestimation bias of the critic with the true Q value? and also how to find the true Q value?</p>
<p>I see in the original TD3 paper (<a href=""https://arxiv.org/pdf/1802.09477.pdf"" rel=""nofollow noreferrer"">https://arxiv.org/pdf/1802.09477.pdf</a>) that the author measures the overestimation bias of the value networks. Can someone guide me in plotting the same during the training phase of my actor-critic model?</p>
",19343941,,,,,2022-06-15 18:52:05,How to find the true Q-value and overestimation bias in actor critic,<deep-learning><reinforcement-learning><function-approximation>,1,0,,,,CC BY-SA 4.0,
64,66930752,1,66934727,,2021-04-03 11:43:03,,2,528,"<p>I am building an Actor-Critic neural network model in pytorch in order to train an agent to play the game of Quoridor (hopefully). For this reason, I have a neural network with two heads, one for the actor output which does a softmax on all the possible moves and one for the critic output which is just one neuron (for regressing the value of the input state).</p>
<p>Now, in quoridor, most of the times not all moves will be legal and as such I am wondering if I can exclude output neurons on the actor's head that correspond to illegal moves for the input state e.g. by passing a list of indices of all the neurons that correspond to legal moves. Thus, I want to not sum these outputs on the denominator of softmax.</p>
<p>Is there a functionality like this on pytorch (because I cannot find one)? Should I attempt to implement such a Softmax myself (kinda scared to, pytorch probably knows best, I ve been adviced to use LogSoftmax as well)?</p>
<p>Furthermore, do you think this approach of dealing with illegal moves is good? Or should I just let him guess illegal moves and penalize him (negative reward) for it in the hopes that eventually it will not pick illegal moves?</p>
<p>Or should I let the softmax be over all the outputs and then just set illegal ones to zero? The rest won't sum to 1 but maybe I can solve that by plain normalization (i.e. dividing by the L2 norm)?</p>
",11832749,,11832749,,2021-04-03 12:18:38,2022-11-07 20:32:11,Can I apply softmax only on specific output neurons?,<pytorch><reinforcement-learning><softmax>,2,2,,,,CC BY-SA 4.0,
76,68269032,1,68357354,,2021-07-06 10:38:51,,1,315,"<p>I'm trying to find out if DQN can solve the shortest path algorithm
so I have this Dataframe which contains a <code>source</code> which has <code>nodes id</code> ,<code>end</code> which represents the destination and also has <code>nodes id</code>, and the weights which represent the distance of the edge and then I converted the data frame into a graph theory as following</p>
<pre><code>DataFrame

    source  end weight
0   688615041   208456626   15.653688122127072
1   688615041   1799221665  10.092266065922756
2   1799221657  1799221660  8.673942902872051
3   1799221660  1799221665  15.282152665774992
4   1799221660  2003461246  25.85307821157314
5   1799221660  299832604   75.99884525624508
6   299832606   2003461227  4.510148061854331
7   299832606   2003461246  10.954119220974723
8   299832606   2364408910  4.903114362426424
9   1731824802  2003461235  6.812335798968233
10  1799221677  208456626   8.308567154008992
11  208456626   2003461246  14.56512909988425
12  208456626   1250468692  16.416527267975034
13  1011881546  1250468696  12.209773608913697
14  1011881546  2003461246  7.477102764665149
15  2364408910  1130166767  9.780352545373274
16  2364408910  2003461246  6.660771089602594
17  2364408910  2003461237  3.125301826317477
18  2364408911  2003461240  3.836966849565568
19  2364408911  2003461246  6.137847950353395
20  2364408911  2003461247  7.399469477211698
21  2364408911  2003461237  3.90876793066916
22  1250468692  1250468696  8.474825189804282
23  1250468701  2003461247  4.539111170687284
24  2003461235  2003461246  12.400601105777394
25  2003461246  2003461247  12.437602668573737
</code></pre>
<p>and the graph looks like this</p>
<pre><code>pos = nx.spring_layout(g)
edge_labels = nx.get_edge_attributes(g, 'weight')
nx.draw(g, pos, node_size=100)
nx.draw_networkx_edge_labels(g, pos, edge_labels, font_size=8)
nx.draw_networkx_labels(g, pos, font_size=10)
plt.title(&quot;Syntethic representation of the City&quot;)
plt.show()
print('Total number of Nodes: '+str(len(g.nodes)))
</code></pre>
<p><a href=""https://i.stack.imgur.com/FOZ7O.png"" rel=""nofollow noreferrer"">graph</a></p>
<p>Now I used DQN in a fixed state from node number 1130166767 as a start to node number 1731824802 as a goal.</p>
<p>this is the whole code of mine</p>
<pre><code>class Network(nn.Module):
  def __init__(self,input_dim,n_action):
    super(Network,self).__init__()
    self.f1=nn.Linear(input_dim,128)
    self.f2=nn.Linear(128,64)
    self.f3=nn.Linear(64,32)
    self.f4=nn.Linear(32,n_action)
    #self.optimizer=optim.Adam(self.parameters(),lr=lr)
    #self.loss=nn.MSELoss()
    self.device=T.device('cuda' if T.cuda.is_available() else 'cpu')
    self.to(self.device)

    
      def forward(self,x):
        x=F.relu(self.f1(x))
        x=F.relu(self.f2(x))
        x=F.relu(self.f3(x))
        x=self.f4(x)
        return x
    
      def act(self,obs):
        #state=T.tensor(obs).to(device)
        state=obs.to(self.device)
        actions=self.forward(state)
        action=T.argmax(actions).item()
    
        return action

device=T.device('cuda' if T.cuda.is_available() else 'cpu')
print(device)

num_states = len(g.nodes)*1
### if we need to train a specific set of nodes for ex 10 we *10
num_actions = len(g.nodes)
print(&quot;Expected number of States are: &quot;+str(num_states))
print(&quot;Expected number of action are: &quot;+str(num_actions))

#num_action*2=when we would like to convert the state into onehotvector we need to concatinate the two vector 22+22
online=Network(num_actions*2,num_actions)
target=Network(num_actions*2,num_actions)
target.load_state_dict(online.state_dict())
optimizer=T.optim.Adam(online.parameters(),lr=5e-4)

#create a dictionary that have encoded index for each node
#to solve this isssu
#reset()=476562122273
#number of state &lt; 476562122273
enc_node={}
dec_node={}
for index,nd in enumerate(g.nodes):
  enc_node[nd]=index
  dec_node[index]=nd

def wayenc(current_node,new_node,type=1):
  #encoded
  if type==1: #distance
    if new_node in g[current_node]:
      rw=g[current_node][new_node]['weight']*-1
      return rw,True
    rw=-5000
    return rw,False

def rw_function(current,action):
  #current_node
  #new_node
  beta=1 #between 1 and 0
  current=dec_node[current]
  new_node=dec_node[action]
  rw0,link=wayenc(current,new_node)
  rw1=0
  frw=rw0*beta+(1-beta)*rw1


  return frw,link

def state_enc(dst, end,n=len(g.nodes)):
    return dst+n*end

def state_dec(state,n=len(g.nodes)):
    dst = state%n
    end = (state-dst)/n
    return dst, int(end)

def step(state,action):
    done=False    
    current_node , end = state_dec(state)

    new_state = state_enc(action,end)


    rw,link=rw_function(current_node,action)

    if not link:
        new_state = state
        return new_state,rw,False  

    elif action == end:
        rw = 10000 #500*12
        done=True
      
    return new_state,rw,done

def reset():
  state=state_enc(enc_node[1130166767],enc_node[1731824802])
  return state

def state_to_vector(current_node,end_node):
  n=len(g.nodes)
  source_state_zeros=[0.]*n
  source_state_zeros[current_node]=1

  end_state_zeros=[0.]*n
  end_state_zeros[end_node]=1.
  vector=source_state_zeros+end_state_zeros
  return vector
    

#return a list of list converted from state to vectors
def list_of_vecotrs(new_obses_t):
  list_new_obss_t=new_obses_t.tolist()
  #convert to integer
  list_new_obss_t=[int(v) for v in list_new_obss_t]
  vector_list=[]
  for state in list_new_obss_t:
    s,f=state_dec(state)
    vector=state_to_vector(s,f)
    vector_list.append(vector)
  return vector_list

#fill the replay buffer
#replay_buffer=[]
rew_buffer=[0]
penalties=[]
episode_reward=0.0
batch_size=num_actions*2
buffer_size=100000
min_replay_size=int(buffer_size*0.20)
target_update_freq=1000
flag=0
action_list=np.arange(0,len(g.nodes)).tolist()
replay_buffer=deque(maxlen=buffer_size)


#populate the experience network 
obs=reset()
#obs,end=state_dec(start,len(g.nodes))
for _ in tqdm(range(min_replay_size)):
  action=np.random.choice(action_list)
  new_obs,rew,done=step(obs,action)
  transition=(obs,action,rew,done,new_obs)
  replay_buffer.append(transition)
  obs=new_obs
  if done:
    obs=reset()

#main training loop
obs=reset()
episodes=100000
start=1
end=0.1
decay=episodes
gamma=0.99
epsilon=0.5



gamma_list=[]
mean_reward=[]
done_location=[]
loss_list=[]
number_of_episodes=[]
stat_dict={'episodes':[],'epsilon':[],'explore_exploit':[],'time':[]}


for i in tqdm(range(episodes)):
  itr=0
  #epsilon=np.interp(i,[0,decay],[start,end])
  #gamma=np.interp(i,[0,decay],[start,end])
  epsilon=np.exp(-i/(episodes/3))
  rnd_sample=random.random()

  stat_dict['episodes'].append(i)
  stat_dict['epsilon'].append(epsilon)

  #choose an action
  if rnd_sample &lt;=epsilon:
    action=np.random.choice(action_list)
    stat_dict['explore_exploit'].append('explore')

  else:
    source,end=state_dec(obs)
    v_obs=state_to_vector(source,end)
    t_obs=T.tensor(v_obs)
    action=online.act(t_obs)
    stat_dict['explore_exploit'].append('exploit')

  #fill transition and append to replay buffer

  
  new_obs,rew,done=step(obs,action)

  transition=(obs,action,rew,done,new_obs)
  replay_buffer.append(transition)
  obs=new_obs
  episode_reward+=rew


  if done:
    obs=reset()
    rew_buffer.append(episode_reward)
    episode_reward=0.0
    done_location.append(i)


  #start gradient step

  transitions=random.sample(replay_buffer,batch_size)

  obses=np.asarray([t[0] for t in transitions])
  actions=np.asarray([t[1] for t in transitions])
  rews=np.asarray([t[2] for t in transitions])
  dones=np.asarray([t[3] for t in transitions])
  new_obses=np.asarray([t[4] for t in transitions])


  obses_t=T.as_tensor(obses,dtype=T.float32).to(device)
  actions_t=T.as_tensor(actions,dtype=T.int64).to(device).unsqueeze(-1)
  rews_t=T.as_tensor(rews,dtype=T.float32).to(device)
  dones_t=T.as_tensor(dones,dtype=T.float32).to(device)
  new_obses_t=T.as_tensor(new_obses,dtype=T.float32).to(device)

  
  list_new_obses_t=T.tensor(list_of_vecotrs(new_obses_t)).to(device)
  target_q_values=target(list_new_obses_t)##


  max_target_q_values=target_q_values.max(dim=1,keepdim=False)[0]
  targets=rews_t+gamma*(1-dones_t)*max_target_q_values

  
  list_obses_t=T.tensor(list_of_vecotrs(obses_t)).to(device)
  q_values=online(list_obses_t)
  action_q_values=T.gather(input=q_values,dim=1,index=actions_t)


  
  #warning UserWarning: Using a target size (torch.Size([24, 24])) that is different to the input size (torch.Size([24, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  targets=targets.unsqueeze(-1)
  loss=nn.functional.mse_loss(action_q_values,targets)
  #loss=rmsle(action_q_values,targets)
  loss_list.append(loss.item())

  optimizer.zero_grad()
  loss.backward()
  optimizer.step()

  #plot
  mean_reward.append(np.mean(rew_buffer))
  number_of_episodes.append(i)
  gamma_list.append(gamma)
  dec = {'number_of_episodes':number_of_episodes,'mean_reward':mean_reward,'gamma':gamma_list}
  #clear_output(wait=True)
  #sns.lineplot(data=dec, x=&quot;number_of_episodes&quot;, y=&quot;mean_reward&quot;)
  #plt.show()

  

  if i % target_update_freq==0:
    target.load_state_dict(online.state_dict())
  if i % 1000 ==0:
    print('step',i,'avg rew',round(np.mean(rew_buffer),2))
    pass
</code></pre>
<p>now if you can see from the<a href=""https://i.stack.imgur.com/tWGjx.png"" rel=""nofollow noreferrer"">photos</a></p>
<p>nither the rewards are increasing or the loss is decreasing, I tried the following</p>
<ol>
<li><p>increasing and decreasing the learning rate</p>
</li>
<li><p>changing target_update_freq from 100 1000  1000</p>
</li>
<li><p>I tried changing the state representation from Onehotvector to [state, end] and sent it as pair.</p>
</li>
<li><p>i tried to change the loss function from mse_loss,smooth_l1,... etc</p>
</li>
<li><p>i tried to increase the number of episodes</p>
</li>
<li><p>adding another layer to NN network
7.changing how the decay of epsilon works linear ,exponential</p>
</li>
</ol>
<p>most of these solutions are from questions on Stacked, but nothing works for me</p>
<p>How can I improve the performance? or in another ward? How can I increase the rewards?</p>
",1771445,,1771445,,2021-07-06 11:02:11,2022-09-14 08:21:12,using DQN to solve shortest path,<python><pytorch><reinforcement-learning><dqn>,1,6,,,,CC BY-SA 4.0,
79,61370115,1,74148819,,2020-04-22 16:24:26,,1,603,"<p>I have been playing around with the OpenAI Gym LunarLander testing a DQN neural network. I had gotten to a point where it was slowly learning. Since I had started with the CartPole problem which was solved in a couple of minutes/episodes, I initially thought that the LunarLander would function at a similar speed but it turns out that I was wrong. After having a decent code that could run correctly and having the neural network learn correctly for about an hour, I thought it's be a good idea to set up a save system for the model after some time of training so I could come back to it later.</p>

<p>I set up all the elements I wanted to have to make sure I could correctly keep track of how the neural network was doing but after getting it all to work, when I launched it with <code>env.render()</code> active, the first couple of steps were executing at a decent speed but then, after a specific point, the whole rendering slows right down as if something in the code is taking a long time to process (I haven't managed to pinpoint the exact moment this happens).</p>

<p>Since I've recently started playing around more in depth with keras and Machine Learning in python, I'm still not familiar with how the components behave within a system and which ones take a big hit on computing power.</p>

<p>Here are the two parts of code necessary to run what I have:</p>

<p><code>LunarLanderConfig.py</code></p>

<pre><code>ENVIRONMENT = 'LunarLander-v2'
EPISODES = 10000
POINTS_TO_SOLVE = 200
CONSECUTIVE_EPISODES_TO_SOLVE = 100
MAX_TICKS = 3000

GAMMA = 0.99
ALPHA = 0.001
MEMORY_SIZE = 1000000
EPSILON = 1.0
EPSILON_MIN = 0.01
EPSILON_DECAY = 0.995
BATCH_SIZE = 64
TRAINING_FREQUENCY = 4
</code></pre>

<p><code>LunarLander_AI.py</code></p>

<pre><code># IMPORTS
import gym
import random
import sys
import os
from collections import deque

from keras import Sequential
from keras.layers import Dense
from keras.optimizers import Adam
from keras.activations import relu, linear
from keras.callbacks import CSVLogger

import numpy as np

from LunarLanderConfig import *

# Save the state of the network to a .h5 file
import h5py
import argparse
import csv


class LunarLanderDQNAgent:
    # Initialise Agent
    def __init__(self):
        # render, test_model = self._args()
        test_model = None
        self.env = gym.make(ENVIRONMENT)
        self.render = True
        self.number_of_actions = self.env.action_space.n
        self.number_of_observations = self.env.observation_space.shape[0]

        self.epsilon = EPSILON # Exploration rate
        self.epsilon_min = EPSILON_MIN
        self.epsilon_decay = EPSILON_DECAY
        self.gamma = GAMMA # Discount factor
        self.alpha = ALPHA # Learning rate
        self.batch_size = BATCH_SIZE
        self.training_frequency = TRAINING_FREQUENCY
        self.memory = deque(maxlen=MEMORY_SIZE)
        self.model = self.build_model()

        self.save_name = ENVIRONMENT+'/'+ENVIRONMENT
        self.history = [('Episode', 'Score', 'Average score', 'Steps', 'Total steps')]
        self.csv_loss_logger = CSVLogger(ENVIRONMENT + '/' + ENVIRONMENT + '_loss.csv', append=True, separator=',')

        if test_model:
            self.load_model(test_model)
            self.test_agent()
        else:
            try:
                os.mkdir(ENVIRONMENT)
            except FileExistsError:
                pass
            self.train_agent()

    # Initialise Neural Network model 
    def build_model(self):
        model = Sequential()
        model.add(Dense(512, input_dim=self.number_of_observations, activation=relu))
        model.add(Dense(256, activation=relu))
        model.add(Dense(128, activation=relu))
        model.add(Dense(self.number_of_actions, activation=linear))
        model.compile(loss=""mse"", optimizer=Adam(learning_rate=self.alpha), metrics=['accuracy'])
        return model

    # Append the properties of a given state and step in the future
    def remember(self, state, action, reward, next_state, done):
        self.memory.append((state, action, reward, next_state, done))

    # Choose an action based on the exploration rate and current state of the network
    def choose_action(self, state):
        if np.random.rand() &lt;= self.epsilon:
            return random.randrange(self.number_of_actions)
        q_values = self.model.predict(state)
        return np.argmax(q_values[0])

    def replay(self, total_steps):
        if len(self.memory) &lt; self.batch_size:
            return

        if total_steps % self.training_frequency == 0:
            # Take a random sample of events from memory
            minibatch = random.sample(self.memory, self.batch_size)

            # Calculate Q values for each event and train the model
            for state, action, reward, next_state, done in minibatch:
                target = reward  

                if not done:
                    # Predict future reward
                    target = reward + self.gamma * np.amax(self.model.predict(next_state)[0])

                # Map state to future reward
                target_f = self.model.predict(state)
                target_f[0][action] = target

                # Train model
                self.model.fit(state, target_f, epochs=1, verbose=0, callbacks=[self.csv_loss_logger])

        self.epsilon *= self.epsilon_decay
        self.epsilon = max(self.epsilon_min, self.epsilon)   

    def preprocess_state(self, state):
        return np.reshape(state, (1, self.number_of_observations))

    # Will probably add the train function here
    def train_agent(self):
        try:
            score_history = deque(maxlen=CONSECUTIVE_EPISODES_TO_SOLVE)
            total_steps = 0

            for episode in range(EPISODES):
                # Reset state at the beginning of game
                state = self.preprocess_state(self.env.reset())
                steps = 0
                score = 0

                while True: # Could also iterate to a maximum number of steps/ticks/frames in LunarLanderConfig
                    # Increment step count at each frame
                    steps += 1
                    total_steps += 1

                    # Render or not
                    if self.render:
                        self.env.render()

                    # Choose an action
                    action = self.choose_action(state)

                    # Take action and move to next step
                    next_state, reward, done, _ = self.env.step(action)
                    next_state = self.preprocess_state(next_state)

                    # Adjust score
                    score += reward

                    # Add to memory
                    self.remember(state, action, reward, next_state, done)

                    # Train with the experience
                    self.replay(total_steps)

                    if done:
                        score_history.append(score)
                        average_score = np.mean(score_history)

                        text = ""[Episode {} of {}] - Score time this episode was {} with epsilon = {}"".format(episode, EPISODES, score, self.epsilon)
                        text2 = ""- Over last {} episodes: Min = {:.2f}, Mean = {:.2f}, Max = {:.2f}"".format(CONSECUTIVE_EPISODES_TO_SOLVE, min(score_history), average_score, max(score_history))
                        text3 = ""- Steps this episode: {}, Total steps: {}"".format(steps, total_steps)
                        print(text + ""\n"" + (15 + len(str(episode)) + len(str(EPISODES)))*' '+ text2 + ""\n"" + (15 + len(str(episode)) + len(str(EPISODES)))*' '+ text3)

                        # Check if the goal has been reached
                        if average_score &gt;= POINTS_TO_SOLVE:
                            print(""Lunar Lander solved in {} episodes with an average of {} points"".format((episode-CONSECUTIVE_EPISODES_TO_SOLVE), average_score))
                            filename = self.save_name + '_final.h5'
                            print(""Saving model to {}"".format(filename))
                            self.save_model(filename)
                            sys.exit()
                        break

                    # If not done, advance to the next state for the following iteration
                    state = next_state

                # Save weights every 100 episodes
                if episode % 100 == 0:
                    filename = self.save_name + '_' + str(episode) + '.h5'
                    self.save_model(filename)

            sys.exit()

        except KeyboardInterrupt:
            # Catch Ctrl+C and end the game correctly
            filename = self.save_name + '_final.h5'
            print(""Saving model to {}"".format(filename))
            self.save_model(filename)
            self.exit()
        except:
            self.env.close()
            sys.exit()


    def test_agent(self):
        try:
            score_history = deque(maxlen=CONSECUTIVE_EPISODES_TO_SOLVE)
            total_steps = 0

            for episode in range(CONSECUTIVE_EPISODES_TO_SOLVE):
                # Reset state at the beginning of game
                state = self.preprocess_state(self.env.reset())
                steps = 0
                score = 0

                while True: # Could also iterate to a maximum number of steps/ticks/frames in LunarLanderConfig
                    # Increment step count at each frame
                    steps += 1
                    total_steps += 1

                    # Render or not
                    if self.render:
                        self.env.render()

                    # Choose an action
                    action = self.choose_action(state)

                    # Take action and move to next step
                    next_state, reward, done, _ = self.env.step(action)
                    next_state = self.preprocess_state(next_state)

                    # Adjust score
                    score += reward

                    if done:
                        score_history.append(score)
                        average_score = np.mean(score_history)

                        text = ""[Episode {} of 99] - Score time this episode was {} with epsilon = {}"".format(episode, score, self.epsilon)
                        text2 = ""- Over last {} episodes: Min = {:.2f}, Mean = {:.2f}, Max = {:.2f}"".format(CONSECUTIVE_EPISODES_TO_SOLVE, min(score_history), average_score, max(score_history))
                        text3 = ""- Steps this episode: {}, Total steps: {}"".format(steps, total_steps)
                        print(text + ""\n"" + (17 + len(str(episode)))*' '+ text2 + ""\n"" + (17 + len(str(episode)))*' '+ text3)
                        break

                    # If not done, advance to the next state for the following iteration
                    state = next_state

            self.env.close()

        except :
            print(""Killing game"")
            self.env.close()
            sys.exit()

    def exit(self):
        filename = self.save_name + '_history.csv'
        print(""Saving training history to {}"".format(filename))
        with open(filename, ""w"") as out:
            csv_out = csv.writer(out)
            for row in self.history:
                csv_out.writerow(row)

        print(""Killing game"")
        self.env.close()
        sys.exit()

    def save_model(self, filename):
        self.model.save_weights(filename)

    def load_model(self, filename):
        self.model.load_weights(filename)

    # Argument parser for agent options
    def _args(self):
        parser = argparse.ArgumentParser()
        parser.add_argument('-r', '--render', help=""Render the game or not"", default=True, type=bool)
        parser.add_argument('-tm', '--test_model', help=""Filename of model of weights to test the performance of"", default=None)
        args = parser.parse_args()
        render = args.render
        test_model = args.test_model

        return render, test_model

if __name__ == ""__main__"":
    LunarLanderDQNAgent()
</code></pre>

<p><strong>SUMMARY</strong></p>

<p>I have the above code running smoothly for a couple of seconds then the rendering acts like a slideshow and I dont currently have the knowledge regarding the tools I used to identify the cause. I'd like to know if anyone can clearly see parts of code that are redundant and causing the execution to slow down or if some parts are just greedy and should be left out to increase performance and execution speed.</p>

<p>I am running 16Gb of RAM, an i7-9700K and and RTX 2070 Super for this if this is of any use</p>
",9545114,,,,,2022-10-21 04:47:20,OpenAI Gym LunarLander execution considerably slowed down for an unknown reason,<python><keras><neural-network><q-learning><dqn>,1,0,,,,CC BY-SA 4.0,
85,73614535,1,73614644,,2022-09-05 20:48:18,,0,26,"<p>Even though tf.agents initialize() require no input variables, this line</p>
<pre><code>agent.initialize()
</code></pre>
<p>produces this error</p>
<pre><code>TypeError: initialize() missing 1 required positional argument: 'self'
</code></pre>
<p>Ive tried agent.initialize(agent) because it apparently wanted self passing in... obviously that didnt work XD</p>
<p>I suspect the problem might be that this line</p>
<pre><code>print(type(agent))   
</code></pre>
<p>Produces</p>
<pre><code>&lt;class 'abc.ABCMeta'&gt;
</code></pre>
<p>But that might be normal...</p>
<p>##################################</p>
<p>My whole script below is reproducable</p>
<pre><code>###  for 9 by 9 connect 4 board 
#
import tensorflow as tf
from tf_agents.networks import q_network
from tf_agents.agents.dqn import dqn_agent
import tf_agents
import numpy as np

print(tf.__version__)
print(tf_agents.__version__)

import tensorflow.keras


observation_spec  = tf.TensorSpec(   #   observation tensor = the whole board , ideally 0's, 1's , 2's for empty, occupied by player 1 , occupied by player 2
    [9,9],
    dtype=tf.dtypes.float32,
    name=None
)

action_spec  = tf_agents.specs.BoundedArraySpec(    
    [1],                         ### tf_agents.networks.q_network only seems to take an action of size 1  
    dtype= type(1) ,     #tf.dtypes.float64,
    name=None, 
    minimum=0,
    maximum=2
)
#######################################

def make_tut_layer(size):
    return tf.keras.layers.Dense(
        units= size,
        activation= tf.keras.activations.relu,
        kernel_initializer=tf.keras.initializers.RandomNormal(mean=0., stddev=1.)
                                )

def make_q_layer(num_actions):
    q_values_layer = tf.keras.layers.Dense (        # last layer gives probability distribution over all actions so we can pick best action 
        num_actions ,
        activation = tf.keras.activations.relu , 
        kernel_initializer = tf.keras.initializers.RandomUniform( minval = 0.03 , maxval = 0.03),
        bias_initializer = tf.keras.initializers.Constant(-0.2)
                                        ) 
    return q_values_layer;



############################## stick together layers below

normal_layers = []

for i in range(3):
    normal_layers.append(make_tut_layer(81))
q_layer = make_q_layer(9)

q_net = keras.Sequential(normal_layers + [q_layer])

######################################

agent = dqn_agent.DqnAgent
(
    observation_spec,    ### bonus question, why do i get syntax errors when i try to label variables like ---&gt; time_step_spec = observation_spec, gives me SyntaxError: invalid syntax   on the = symbol       
    action_spec,
    q_net,
    tf.keras.optimizers.Adam(learning_rate= 0.001 )

)
eval1 = agent.policy
print(eval1)
eval2= agent.collect_policy
print(eval2)
print(type(agent))  
agent.initialize()
print(&quot; done &quot;)
</code></pre>
<p>And produces the output.</p>
<pre><code>2.9.2
0.13.0
&lt;property object at 0x000001A13268DA90&gt;
&lt;property object at 0x000001A13268DAE0&gt;
&lt;class 'abc.ABCMeta'&gt;
---------------------------------------------------------------------------
TypeError                                 Traceback (most recent call last)
Input In [53], in &lt;cell line: 73&gt;()
     71 print(eval2)
     72 print(type(agent))
---&gt; 73 agent.initialize()
     74 print(&quot; done &quot;)

TypeError: initialize() missing 1 required positional argument: 'self'
</code></pre>
<p>Is my agents type ok? should it be &lt;class 'abc.ABCMeta'&gt;</p>
<p>Why does my agent fail to initialize?</p>
",17560920,,17560920,,2022-09-05 21:02:04,2022-09-05 21:02:41,tf_agents dqn fails to initialize,<python><tensorflow><dqn><tf-agent>,1,0,,,,CC BY-SA 4.0,
104,73986812,1,74043115,,2022-10-07 11:56:15,,1,28,"<p>I am training a RL model using the DQN algorithm. At every iteration, I save the model as follows:</p>
<pre><code>agent = dqn.DQNTrainer(env=CustomEnv,config=config)
for n in range(100):
    result = agent.train()    
    agent.save()
</code></pre>
<p>I want to evluate the trained RL model using on a different environment. I am not sure how to load the checkpoint and evaluate in a different environment.</p>
<p>I try to load the trained model (the last checkpoint) but it throws me an error. I do the following:</p>
<pre><code>agent.restore('./RL_saved/checkpoint-100.tune_metadata')

</code></pre>
<p>It throws me an error saying</p>
<pre><code>unsupported pickle protocol: 5
</code></pre>
<p>and when I do</p>
<pre><code>agent.restore('./RL_saved/checkpoint-100.tune_metadata')

</code></pre>
<p>It throws me an error saying</p>
<pre><code>Invalid magic number; corrupt file?
</code></pre>
<p>Am I loading the model in the right way? And how do I pass the environment to the loaded model?</p>
",14698158,,14698158,,2022-10-13 06:42:24,2022-10-21 23:17:16,Saving and Loading RL model - DQN algorithm,<machine-learning><reinforcement-learning><dqn>,1,1,,,,CC BY-SA 4.0,
105,74001586,1,74107001,,2022-10-09 01:33:28,,0,41,"<p>Can't figure out how to make the gym.Env put out two separate arrays. It just seems to combine them into 1 array containing 2 arrays. But fitting to DQN NN expects two arrays.
I'm hoping to put the two arrays into the NN separately.</p>
<p>I've tried to show as much code as i can, but there's a lot.</p>
<p>I've tried playing around with the observation space a bit, tried box and Tuple,
can't seem to figure out where i'm going wrong.</p>
<pre><code>class GoEnv(gym.Env):

    def __init__(self):
        self.action_space = spaces.Discrete(3)
        self.observation_space = spaces.Tuple([spaces.Box(low=-np.inf, high=np.inf, shape=(2, 11), dtype=np.float32),
                                               spaces.Box(low=-np.inf, high=np.inf, shape=(1, 11), dtype=np.float32)])

    def step(self, action):
        state = [np.array(self.data), np.array(self.account)]
        return state, reward, self.done, info

envi = env.GoEnv()

def data_model():
    data_input = layers.Input(shape=(500, 2, 11))
    acc_input = layers.Input(shape=(500, 1, 11))

    dat_model = layers.Conv2D(filters=32, activation='swish', kernel_size=(500, 1),
                              padding='valid', strides=(500, 1))(data_input)
    dat_model = layers.Dense(3, activation='swish')(dat_model)
    dat_model = layers.Dense(3, activation='softmax')(dat_model)
    dat_model = layers.Flatten()(dat_model)
    dat_model = keras.Model(inputs=data_input, outputs=dat_model)

    acc_model = layers.Dense(3, activation='swish')(acc_input)
    acc_model = layers.Dense(3, activation='softmax')(acc_model)
    acc_model = layers.Flatten()(acc_model)
    acc_model = keras.Model(inputs=acc_input, outputs=acc_model)

    combined = layers.concatenate([dat_model.output, acc_model.output])

    z = layers.Flatten()(combined)
    z = layers.Dense(64, activation='swish')(z)
    z = layers.Dense(3, activation='softmax')(z)

    model = keras.Model(inputs=[dat_model.input, acc_model.input], outputs=z)

    return model

model = data_model()
model.summary()
actions = 3

def build_agent(model, actions):
    policy = BoltzmannQPolicy()
    memory = SequentialMemory(limit=50000, window_length=500)
    dqn = DQNAgent(model=model,
                   memory=memory,
                   policy=policy,
                   nb_actions=actions,
                   nb_steps_warmup=600,
                   target_model_update=1e-2)
    return dqn
dqn = build_agent(model, actions)
dqn.fit(envi, nb_steps=6000, visualize=False, verbose=1)
</code></pre>
<blockquote>
<blockquote>
<blockquote>
</blockquote>
</blockquote>
</blockquote>
<pre><code>Traceback (most recent call last):
  File &quot;C:/Users/Worrall/PycharmProjects/Prject/main.py&quot;, line 46, in &lt;module&gt;
    dqn.fit(envi, nb_steps=6000, visualize=False, verbose=1)
  File &quot;C:\Users\Worrall\PycharmProjects\DocumentRecog\venv\lib\site-packages\rl\core.py&quot;, line 168, in fit
    action = self.forward(observation)
  File &quot;C:\Users\Worrall\PycharmProjects\DocumentRecog\venv\lib\site-packages\rl\agents\dqn.py&quot;, line 224, in forward
    q_values = self.compute_q_values(state)
  File &quot;C:\Users\Worrall\PycharmProjects\DocumentRecog\venv\lib\site-packages\rl\agents\dqn.py&quot;, line 68, in compute_q_values
    q_values = self.compute_batch_q_values([state]).flatten()
  File &quot;C:\Users\Worrall\PycharmProjects\DocumentRecog\venv\lib\site-packages\rl\agents\dqn.py&quot;, line 63, in compute_batch_q_values
    q_values = self.model.predict_on_batch(batch)
  File &quot;C:\Users\Worrall\PycharmProjects\DocumentRecog\venv\lib\site-packages\tensorflow\python\keras\engine\training_v1.py&quot;, line 1200, in predict_on_batch
    inputs, _, _ = self._standardize_user_data(
  File &quot;C:\Users\Worrall\PycharmProjects\DocumentRecog\venv\lib\site-packages\tensorflow\python\keras\engine\training_v1.py&quot;, line 2328, in _standardize_user_data
    return self._standardize_tensors(
  File &quot;C:\Users\Worrall\PycharmProjects\DocumentRecog\venv\lib\site-packages\tensorflow\python\keras\engine\training_v1.py&quot;, line 2356, in _standardize_tensors
    x = training_utils.standardize_input_data(
  File &quot;C:\Users\Worrall\PycharmProjects\DocumentRecog\venv\lib\site-packages\tensorflow\python\keras\engine\training_utils.py&quot;, line 533, in standardize_input_data
    raise ValueError('Error when checking model ' + exception_prefix +
ValueError: Error when checking model input: the list of Numpy arrays that you are passing to your model is not the size the model expected. Expected to see 2 array(s), for inputs ['input_1', 'input_2'] but instead got the following list of 1 arrays: [array([[[array([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]]),
         array([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])],
        [array([[...
</code></pre>
",6439978,,6439978,,2022-10-10 08:56:28,2022-10-18 07:08:07,How to take two arrays as output from Gym.Env to fit to DQN NN,<python><tensorflow><keras><openai-gym><dqn>,1,1,,,,CC BY-SA 4.0,
110,64956763,1,64961297,,2020-11-22 16:42:11,,0,422,"<p>Have built a Reinforcement Learning DQN with variable length sequences as inputs, and positive and negative rewards calculated for actions. Some problem with my DQN model in Keras means that although the model runs, average rewards over time decrease, over single and multiple cycles of epsilon. This does not change even after significant period of training.
<a href=""https://i.stack.imgur.com/CvXWl.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/CvXWl.png"" alt=""Single cycle of epsilon, average rewards decreasing"" /></a></p>
<p><a href=""https://i.stack.imgur.com/1rstB.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/1rstB.png"" alt=""Multiple cycles of epsilon, average rewards decreasing"" /></a></p>
<p>My thinking is that this is due to using MeanSquareError in Keras as the Loss function (minimising error). So I am trying to implement gradient ascent (to maximise reward). How to do this in Keras? My current model is:</p>
<pre><code>model = Sequential()
inp = (env.NUM_TIMEPERIODS, env.NUM_FEATURES)
model.add(Input(shape=inp))  # 'a shape tuple(integers), not including batch-size
model.add(Masking(mask_value=0., input_shape=inp))

model.add(LSTM(env.NUM_FEATURES, input_shape=inp, return_sequences=True))
model.add(LSTM(env.NUM_FEATURES))
model.add(Dense(env.NUM_FEATURES))
model.add(Dense(4))

model.compile(loss='mse,
              optimizer=Adam(lr=LEARNING_RATE, decay=DECAY),
              metrics=[tf.keras.losses.MeanSquaredError()])
</code></pre>
<p>In trying to implement gradient ascent, by 'flipping' the gradient (as negative or inverse loss?), I have tried various loss definitions:</p>
<pre><code>loss=-'mse'    
loss=-tf.keras.losses.MeanSquaredError()    
loss=1/tf.keras.losses.MeanSquaredError()
</code></pre>
<p>but these all generate bad operand [for unary] errors.</p>
<p>How to adapt current Keras model to maximise rewards ?
Or is this gradient ascent not even the problem? Could it be some issue with the action policy?</p>
",10227573,,,,,2020-11-23 00:56:26,How to implement gradient ascent in a Keras DQN,<python><tensorflow><keras><deep-learning><dqn>,1,0,,,,CC BY-SA 4.0,
125,64866833,1,64875762,,2020-11-16 22:47:54,,0,1461,"<p>I have recently started learning about Deep Learning and Reinforcement Learning, and I am trying to figure out how to code a Convolutional Neural Network using Keras for a matrix of 0s and 1s with 10 rows and 3 columns.</p>
<p>The input matrix would look like this for example</p>
<pre><code>[
 [1, 0, 0], 
 [0, 1, 0], 
 [0, 0, 0], 
 ...
]
</code></pre>
<p>The output should be another matrix of 0s and 1s, different from the aforementioned input matrix and with a different number of rows and columns.</p>
<p>The location of 0s and 1s in the output matrix is dependent on the location of the 0s and 1s in the input matrix.</p>
<p>There is also a second output, an array where the values are dependent on the location of the 1 in the input matrix.</p>
<p>I have searched the internet for code examples but couldn't find anything useful.</p>
<p>Edit:</p>
<p>The input to the neural network is a 2D array with 10 rows and each row has 3 columns.
The output (for now at least) is a 2D array with 12 rows and each row has 10 columns (the same as the number of rows in the input 2D array).</p>
<p>This is what I came up with so far and I have no idea if it's correct or not.</p>
<pre><code>nbre_users = 10 # number of rows in the input 2D matrix
nbre_subchannels = 12 # number of rows in the output 2D matrix

model = Sequential()
model.add(Dense(50, input_shape=(nbre_users, 3), kernel_initializer=&quot;he_normal&quot; ,activation=&quot;relu&quot;))
model.add(Dense(20, kernel_initializer=&quot;he_normal&quot;, activation=&quot;relu&quot;))
model.add(Dense(5, kernel_initializer=&quot;he_normal&quot;, activation=&quot;relu&quot;))
model.add(Flatten())
model.add(Dense(nbre_subchannels))
model.add(Dense(nbre_users, activation = 'softmax'))
model.compile(optimizer=Adam(learning_rate=1e-4), loss='mean_squared_error')
</code></pre>
<p>Here is the model summary:</p>
<p><a href=""https://i.stack.imgur.com/7uP88.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/7uP88.png"" alt=""Model Summary"" /></a></p>
",8163377,,8163377,,2020-11-17 12:00:09,2020-11-17 13:10:10,Keras CNN for non-image matrix,<python><tensorflow><keras><conv-neural-network><dqn>,2,9,,,,CC BY-SA 4.0,
126,65087009,1,65113100,,2020-12-01 08:28:27,,2,941,"<p>I have a question about the input and output (layer) of a DQN.</p>
<p>e.g</p>
<p>Two points: P1(x1, y1) and P2(x2, y2)</p>
<p>P1 has to walk towards P2</p>
<p>I have the following information:</p>
<ul>
<li>Current position P1 (x/y)</li>
<li>Current position P2 (x/y)</li>
<li>Distance to P1-P2 (x/y)</li>
<li>Direction to P1-P2 (x/y)</li>
</ul>
<p>P1 has 4 possible actions:</p>
<ul>
<li>Up</li>
<li>Down</li>
<li>Left</li>
<li>Right</li>
</ul>
<p>How do I have to setup the input and output layer?</p>
<ul>
<li>4 input nodes</li>
<li>4 output nodes</li>
</ul>
<p>Is that correct?
What do I have to do with the output?
I got 4 arrays with 4 values each as output.
Is doing argmax on the output correct?</p>
<p>Edit:</p>
<p>Input / State:</p>
<pre><code># Current position P1
state_pos = [x_POS, y_POS]
state_pos = np.asarray(state_pos, dtype=np.float32)
# Current position P2
state_wp = [wp_x, wp_y]
state_wp = np.asarray(state_wp, dtype=np.float32)
# Distance P1 - P2 
state_dist_wp = [wp_x - x_POS, wp_y - y_POS]
state_dist_wp = np.asarray(state_dist_wp, dtype=np.float32)
# Direction P1 - P2
distance = [wp_x - x_POS, wp_y - y_POS]
norm = math.sqrt(distance[0] ** 2 + distance[1] ** 2)
state_direction_wp = [distance[0] / norm, distance[1] / norm]
state_direction_wp = np.asarray(state_direction_wp, dtype=np.float32)
state = [state_pos, state_wp, state_dist_wp, state_direction_wp]
state = np.array(state)
</code></pre>
<p>Network:</p>
<pre><code>def __init__(self):
    self.q_net = self._build_dqn_model()
    self.epsilon = 1 

def _build_dqn_model(self):
    q_net = Sequential()
    q_net.add(Dense(4, input_shape=(4,2), activation='relu', kernel_initializer='he_uniform'))
    q_net.add(Dense(128, activation='relu', kernel_initializer='he_uniform'))
    q_net.add(Dense(128, activation='relu', kernel_initializer='he_uniform'))
    q_net.add(Dense(4, activation='linear', kernel_initializer='he_uniform'))
    rms = tf.optimizers.RMSprop(lr = 1e-4)
    q_net.compile(optimizer=rms, loss='mse')
    return q_net

def random_policy(self, state):
    return np.random.randint(0, 4)

def collect_policy(self, state):
    if np.random.random() &lt; self.epsilon:
        return self.random_policy(state)
    return self.policy(state)

def policy(self, state):
    # Here I get 4 arrays with 4 values each as output
    action_q = self.q_net(state)
</code></pre>
",9314272,,9314272,,2020-12-01 11:18:44,2020-12-03 01:25:21,DQN understanding input and output (layer),<python><deep-learning><reinforcement-learning><q-learning><dqn>,2,0,,,,CC BY-SA 4.0,
127,61178239,1,61178262,,2020-04-12 20:54:10,,2,3098,"<p>I have made a code to solve Atari Breakout. I am facing a little problem, but I can't say what it is. </p>

<p>Here is the <a href=""https://github.com/JeremieGauthier/AI_Exercices/blob/master/Atari_Breakout/DQN_Breakout_with_ptan.py"" rel=""nofollow noreferrer"">code</a> </p>

<p>It is a problem with the replay memory. </p>

<pre><code>try:
    next_states = torch.tensor(batch[3], dtype=torch.float32) 
except:
    import ipdb; ipdb.set_trace()
</code></pre>

<p>The problem is located where are those lines.  <code>set_trace()</code> is use to pop-up an interactive shell. From there, if I run <code>for i in range(batch_size): print(batch[3][i].shape)</code>, I obtained this output</p>

<pre><code>ipdb&gt; for i in range(32): print(batch[3][i].shape)
(4, 84, 84)
(4, 84, 84)
(4, 84, 84)
(4, 84, 84)
(4, 84, 84)
(4, 84, 84)
(4, 84, 84)
(4, 84, 84)
(4, 84, 84)
(4, 84, 84)
(4, 84, 84)
(4, 84, 84)
(4, 84, 84)
(4, 84, 84)
(4, 84, 84)
(4, 84, 84)
(4, 84, 84)
(4, 84, 84)
(4, 84, 84)
(4, 84, 84)
(4, 84, 84)
(4, 84, 84)
(4, 84, 84)
(4, 84, 84)
(4, 84, 84)
(4, 84, 84)
(4, 84, 84)
*** AttributeError: 'NoneType' object has no attribute 'shape'
</code></pre>

<p>How can I improve that code to avoid such error?</p>
",13144280,jgauth,13144280,,2020-04-13 15:02:40,2020-04-14 14:17:57,"Getting the error ""AttributeError: 'NoneType' object has no attribute 'shape'"" when implementing Atari Breakout",<python-3.x><reinforcement-learning><dqn>,1,1,,,,CC BY-SA 4.0,
144,65152599,1,65157482,,2020-12-05 00:36:47,,0,151,"<p>I create an initial network model with the following acrchitecture.</p>
<pre><code>def create_model(env):

    dropout_prob = 0.8 #aggresive dropout regularization
    num_units = 256 #number of neurons in the hidden units
  
    model = Sequential()
    model.add(Flatten(input_shape=(1,) + env.input_shape))
    model.add(Dense(num_units))
    model.add(Activation('relu'))

    model.add(Dense(num_units))
    model.add(Dropout(dropout_prob))
    model.add(Activation('relu'))

    model.add(Dense(env.action_size))
    model.add(Activation('softmax'))
    print(model.summary())
    return model
</code></pre>
<p>Then I call the DQNAgent that updates the network architecture</p>
<pre><code>dqn = DQNAgent(model=model, nb_actions=env.action_size, memory=memory,
               nb_steps_warmup=settings['train']['warm_up'], 
               target_model_update=settings['train']['update_rate'], policy=policy, enable_dueling_network=True)
dqn.compile(Adam(lr=settings['train']['learning_rate']), metrics=['mse'])
</code></pre>
<p>Doing this results in an updated network architecture - as expected. The issue now is that when I try to call this fitted new network, the original create model function can't accept the saved model weights because the layer architecture doesn't fit at all.</p>
<pre><code>print(model.summary())

Layer (type)                 Output Shape              Param #   
=================================================================
flatten_49 (Flatten)         (None, 106)               0         
_________________________________________________________________
dense_147 (Dense)            (None, 128)               13696     
_________________________________________________________________
activation_145 (Activation)  (None, 128)               0         
_________________________________________________________________
dense_148 (Dense)            (None, 64)                8256      
_________________________________________________________________
dropout_49 (Dropout)         (None, 64)                0         
_________________________________________________________________
activation_146 (Activation)  (None, 64)                0         
_________________________________________________________________
dense_149 (Dense)            (None, 3)                 195       
_________________________________________________________________
activation_147 (Activation)  (None, 3)                 0         
=================================================================
Total params: 22,147
Trainable params: 22,147
Non-trainable params: 0

print(dqn.model.summary())
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
flatten_49_input (InputLayer (None, 1, 1, 106)         0         
_________________________________________________________________
flatten_49 (Flatten)         (None, 106)               0         
_________________________________________________________________
dense_147 (Dense)            (None, 128)               13696     
_________________________________________________________________
activation_145 (Activation)  (None, 128)               0         
_________________________________________________________________
dense_148 (Dense)            (None, 64)                8256      
_________________________________________________________________
dropout_49 (Dropout)         (None, 64)                0         
_________________________________________________________________
activation_146 (Activation)  (None, 64)                0         
_________________________________________________________________
dense_149 (Dense)            (None, 3)                 195       
_________________________________________________________________
dense_150 (Dense)            (None, 4)                 16        
_________________________________________________________________
lambda_3 (Lambda)            (None, 3)                 0         
=================================================================
Total params: 22,163
Trainable params: 22,163
Non-trainable params: 0
_________________________________________________________________
</code></pre>
<p>So without training a new dqn, I need to find a way to create a network architecture that is created off the original architecture but applies dqn model changes.</p>
",7756491,,,,,2020-12-05 13:37:11,Dueling DQN updates model architecture and causes issues,<reinforcement-learning><dqn><keras-rl>,1,0,,,,CC BY-SA 4.0,
147,64566325,1,65263689,,2020-10-28 04:09:15,,1,376,"<p>I have set up a python environment that is wrapped in a tensor flow class to make it a tensor flow environment. Then I set up the learning as per the collab notebooks listed <a href=""https://github.com/tensorflow/agents/tree/master/docs/tutorials"" rel=""nofollow noreferrer"">here</a>. Currently, I am using the dqn and REINFORCE agent.</p>
<p>The setup works well and the results are somewhat as expected. Now I want to go into the tuning of the hyperparameters like decaying epsilon greedy, weights etc.</p>
<p>I need some pointers on how to use the documentation on how to access these hyperparameters.</p>
",3656142,,10396469,,2020-10-28 17:13:33,2020-12-12 10:06:11,How to tune hyperparameters of tf-agents and policies in Tensor Flow?,<python><tensorflow><reinforcement-learning><dqn>,1,0,,,,CC BY-SA 4.0,
148,59350679,1,59443119,,2019-12-16 04:46:30,,1,169,"<p>Why dqn algorithm performs only one gradient descent step, i.e. trains for only one epoch? Would not it benefit from more epochs, wonâ€™t its accuracy improve with more epochs?</p>
",10437878,,,,,2019-12-22 09:42:28,Why Deep Q networks algorithm performs only one gradient descent step?,<reinforcement-learning><dqn>,1,3,,,,CC BY-SA 4.0,
150,60406081,1,60407336,,2020-02-26 02:54:34,,0,106,"<p>Based on my understanding, CNN output size for 1D is </p>

<p><code>output_size = (input_size - kernel_size + 2*padding)//stride + 1</code></p>

<p>Refer to <a href=""https://pytorch.org/tutorials/intermediate/reinforcement_q_learning.html"" rel=""nofollow noreferrer"">PyTorch DQN Tutorial</a>. In the tutorial, it uses 0 padding, which is fine. However, it computes the output size as follows:</p>

<pre><code>        def conv2d_size_out(size, kernel_size = 5, stride = 2):
            return (size - (kernel_size - 1) - 1) // stride  + 1
</code></pre>

<p>It the above a mistake or is there something I missed?</p>
",9286461,,,,,2020-02-26 05:28:38,Why is CNN convolution output size in PyTorch DQN tutorial computed with `kernel_size -1`?,<machine-learning><deep-learning><pytorch><conv-neural-network><dqn>,1,0,,,,CC BY-SA 4.0,
154,59848545,1,59869307,,2020-01-21 20:14:00,,1,1032,"<p>Why use 2 networks, train once every episode and update target network every <strong>N</strong> episode, when we can use 1 network and train it ONCE every <strong>N</strong> episode! there is literally no difference!</p>
",,user12592480,,,,2020-01-22 22:44:34,DOUBLE DQN doesn't make any sense,<reinforcement-learning><dqn>,1,0,,,,CC BY-SA 4.0,
155,61169488,1,61169904,,2020-04-12 09:43:36,,1,419,"<p>The problem i want to solve is actually not this simple, but this is kind of a toy game to help me solve the greater problem.</p>

<p>so i have a 5x5 matrix with values all equal to 0 :</p>

<pre><code>structure = np.zeros(25).reshape(5, 5)
</code></pre>

<p>and the goal is for the agent to turn all values into 1, so i have:</p>

<pre><code>goal_structure = np.ones(25).reshape(5, 5)
</code></pre>

<p>i created a class Player with 5 actions to go either left, right, up, down or flip (turn the value 0 to 1 or 1 to 0). For the reward, if the agent changes the value 0 into 1, it gets a +1 reward. if it turns a 1 into 0 in gets a negative reward (i tried many values from -1 to 0 or even -0.1). and if it just goes left, right, up or down, it gets a reward 0.</p>

<p>Because i want to feed the state to my neural net, i reshaped the state as below:</p>

<pre><code>reshaped_structure = np.reshape(structure, (1, 25))
</code></pre>

<p>and then i add the normalized position of the agent to the end of this array (because i suppose the agent should have a sense of where it is):</p>

<pre><code>reshaped_state = np.append(reshaped_structure, (np.float64(self.x/4), np.float64(self.y/4)))
state = reshaped_state
</code></pre>

<p>but i dont get any good results! it just like its random!i tried different reward functions, different optimizing algorithms, such as Exeperience replay, target net, Double DQN, duelling, but non of them seem to work! and i guess the problem is with defining the state. Can any one maybe helping me with defining a good state?</p>

<p>Thanks a lot!</p>

<p>ps: this is my step function:</p>

<pre><code>class Player:

def __init__(self):
    self.x = 0
    self.y = 0

    self.max_time_step = 50
    self.time_step = 0
    self.reward_list = []
    self.sum_reward_list = []
    self.sum_rewards = []

    self.gather_positions = []
    # self.dict = {}

    self.action_space = spaces.Discrete(5)
    self.observation_space = 27

def get_done(self, time_step):

    if time_step == self.max_time_step:
        done = True

    else:
        done = False

    return done

def flip_pixel(self):

    if structure[self.x][self.y] == 1:
        structure[self.x][self.y] = 0.0

    elif structure[self.x][self.y] == 0:
        structure[self.x][self.y] = 1

def step(self, action, time_step):

    reward = 0

    if action == right:

        if self.y &lt; y_threshold:
            self.y = self.y + 1
        else:
            self.y = y_threshold

    if action == left:

        if self.y &gt; y_min:
            self.y = self.y - 1
        else:
            self.y = y_min

    if action == up:

        if self.x &gt; x_min:
            self.x = self.x - 1
        else:
            self.x = x_min

    if action == down:

        if self.x &lt; x_threshold:
            self.x = self.x + 1
        else:
            self.x = x_threshold

    if action == flip:
        self.flip_pixel()

        if structure[self.x][self.y] == 1:
            reward = 1
        else:
            reward = -0.1



    self.reward_list.append(reward)

    done = self.get_done(time_step)

    reshaped_structure = np.reshape(structure, (1, 25))
    reshaped_state = np.append(reshaped_structure, (np.float64(self.x/4), np.float64(self.y/4)))
    state = reshaped_state

    return state, reward, done

def reset(self):

    structure = np.zeros(25).reshape(5, 5)

    reset_reshaped_structure = np.reshape(structure, (1, 25))
    reset_reshaped_state = np.append(reset_reshaped_structure, (0, 0))
    state = reset_reshaped_state

    self.x = 0
    self.y = 0
    self.reward_list = []

    self.gather_positions = []
    # self.dict.clear()

    return state
</code></pre>
",13088262,,,,,2020-04-13 14:56:18,how should i define the state for my gridworld like environment?,<python><machine-learning><deep-learning><reinforcement-learning><dqn>,1,0,,,,CC BY-SA 4.0,
156,64848622,1,64854430,,2020-11-15 19:04:37,,0,1459,"<p>I have a DQN agent, that receives a state composed of a numerical value indicating its position and a 2D array denoting the requests from a number of users.</p>
<p>My attempt of architecting the neural network was as described <a href=""https://ai.stackexchange.com/questions/24593/keras-dqn-model-with-multiple-inputs-and-multiple-outputs?noredirect=1#comment38863_24593"">here</a>.</p>
<p>Model Summary
<a href=""https://i.stack.imgur.com/1hyVb.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/1hyVb.png"" alt=""Model Summary"" /></a></p>
<p>The issue now is in the model.predict() method, it is written like this:</p>
<pre><code>target_f = self.model.predict(state)
</code></pre>
<p>in the method:</p>
<pre><code>def replay(self, batch_size): # method that trains NN with experiences sampled from memory
        minibatch = sample(self.memory, batch_size) 
        for state, action, reward, next_state, done in minibatch: 
            target = reward 
            if not done: 
                target = (reward + self.gamma * np.amax(self.model.predict(next_state)[0]))
            target_f = self.model.predict(state) 
            target_f[0][action] = target
            self.model.fit(state, target_f, epochs=1, verbose=0) 
        if self.epsilon &gt; self.epsilon_min:
            self.epsilon *= self.epsilon_decay
</code></pre>
<p>Where the state can be (agentPosition, [[1, 0, 0], [0, 0, 0], [0, 1, 0], ...])</p>
<p>The shape of the state is (2, (11, 3)) if I have 11 users for example (which equals the number of rows in the 2D array of requests).</p>
<p>The error says:</p>
<pre><code>ValueError: Failed to find data adapter that can handle input: (&lt;class 'list'&gt; containing values of types {&quot;&lt;class 'numpy.int32'&gt;&quot;, '(&lt;class \'list\'&gt; containing values of types {&quot;&lt;class \'numpy.ndarray\'&gt;&quot;})'}), &lt;class 'NoneType'&gt;
</code></pre>
<p>If instead I write it like this:</p>
<pre><code>target_f = target_f = self.model.predict(np.array([state[0], state[1]]))
</code></pre>
<p>The error is then different:</p>
<pre><code>ValueError: Error when checking model input: the list of Numpy arrays that you are passing to your model is not the size the model expected. Expected to see 2 array(s), for inputs ['dense_63_input', 'input_20'] but instead got the following list of 1 arrays: [array([[54],
       [((1, 0, 0), (0, 0, 0), (0, 0, 1), (0, 0, 1), (0, 0, 0), (0, 1, 0), (0, 0, 1), (0, 0, 1), (1, 0, 0), (1, 0, 0), (0, 0, 1))]],
      dtype=object)]...
</code></pre>
<p>Edit:
I did as indicated in the accepted solution, and I get this error:</p>
<pre><code>IndexError                                Traceback (most recent call last)
&lt;ipython-input-25-e99a46675915&gt; in &lt;module&gt;
     28 
     29     if len(agent.memory) &gt; batch_size:
---&gt; 30         agent.replay(batch_size) # train the agent by replaying the experiences of the episode
     31 
     32     if e % 100 == 0:

&lt;ipython-input-23-91f0ef2e2650&gt; in replay(self, batch_size)
    129                                             np.array(next_state[1]) ])[0])) # (maximum target Q based on future action a')
    130             target_f = self.model.predict( [ np.array(state[0]), \
--&gt; 131                                             np.array(state[1]) ] ) # approximately map current state to future discounted reward
    132             target_f[0][action] = target
    133             self.model.fit(state, target_f, epochs=1, verbose=0)

C:\Anaconda\envs\tensorflow\lib\site-packages\tensorflow_core\python\keras\engine\training.py in predict(self, x, batch_size, verbose, steps, callbacks, max_queue_size, workers, use_multiprocessing)
   1011         max_queue_size=max_queue_size,
   1012         workers=workers,
-&gt; 1013         use_multiprocessing=use_multiprocessing)
   1014 
   1015   def reset_metrics(self):

C:\Anaconda\envs\tensorflow\lib\site-packages\tensorflow_core\python\keras\engine\training_v2.py in predict(self, model, x, batch_size, verbose, steps, callbacks, max_queue_size, workers, use_multiprocessing, **kwargs)
    496         model, ModeKeys.PREDICT, x=x, batch_size=batch_size, verbose=verbose,
    497         steps=steps, callbacks=callbacks, max_queue_size=max_queue_size,
--&gt; 498         workers=workers, use_multiprocessing=use_multiprocessing, **kwargs)
    499 
    500 

C:\Anaconda\envs\tensorflow\lib\site-packages\tensorflow_core\python\keras\engine\training_v2.py in _model_iteration(self, model, mode, x, y, batch_size, verbose, sample_weight, steps, callbacks, max_queue_size, workers, use_multiprocessing, **kwargs)
    408     strategy = _get_distribution_strategy(model)
    409     batch_size, steps = dist_utils.process_batch_and_step_size(
--&gt; 410         strategy, x, batch_size, steps, mode)
    411     dist_utils.validate_callbacks(input_callbacks=callbacks,
    412                                   optimizer=model.optimizer)

C:\Anaconda\envs\tensorflow\lib\site-packages\tensorflow_core\python\keras\distribute\distributed_training_utils.py in process_batch_and_step_size(strategy, inputs, batch_size, steps_per_epoch, mode, validation_split)
    460   first_x_value = nest.flatten(inputs)[0]
    461   if isinstance(first_x_value, np.ndarray):
--&gt; 462     num_samples = first_x_value.shape[0]
    463     if validation_split and 0. &lt; validation_split &lt; 1.:
    464       num_samples = int(num_samples * (1 - validation_split))

IndexError: tuple index out of range
</code></pre>
<p>The <code>state[1]</code> is a tuple like this ((1, 0, 0), (0, 1, 0), ...)</p>
<p>The shape of <code>np.array(state[0])</code> is ().
The shape of <code>np.array(state[1])</code> is (11, 3).</p>
<p>If I write:</p>
<pre><code>self.model.predict( [ np.array(state[0]).reshape(-1,1), np.array(state[1]) ] ) 
</code></pre>
<p>It gives an error:</p>
<pre><code>ValueError                                Traceback (most recent call last)
&lt;ipython-input-28-e99a46675915&gt; in &lt;module&gt;
     28 
     29     if len(agent.memory) &gt; batch_size:
---&gt; 30         agent.replay(batch_size) # train the agent by replaying the experiences of the episode
     31 
     32     if e % 100 == 0:

&lt;ipython-input-26-5df3ba3feb8f&gt; in replay(self, batch_size)
    129                                             np.array(next_state[1]) ])[0])) # (maximum target Q based on future action a')
    130             target_f = self.model.predict( [ np.array(state[0]).reshape(-1, 1), \
--&gt; 131                                             np.array(state[1]) ] ) # approximately map current state to future discounted reward
    132             target_f[0][action] = target
    133             self.model.fit(state, target_f, epochs=1, verbose=0)

C:\Anaconda\envs\tensorflow\lib\site-packages\tensorflow_core\python\keras\engine\training.py in predict(self, x, batch_size, verbose, steps, callbacks, max_queue_size, workers, use_multiprocessing)
   1011         max_queue_size=max_queue_size,
   1012         workers=workers,
-&gt; 1013         use_multiprocessing=use_multiprocessing)
   1014 
   1015   def reset_metrics(self):

C:\Anaconda\envs\tensorflow\lib\site-packages\tensorflow_core\python\keras\engine\training_v2.py in predict(self, model, x, batch_size, verbose, steps, callbacks, max_queue_size, workers, use_multiprocessing, **kwargs)
    496         model, ModeKeys.PREDICT, x=x, batch_size=batch_size, verbose=verbose,
    497         steps=steps, callbacks=callbacks, max_queue_size=max_queue_size,
--&gt; 498         workers=workers, use_multiprocessing=use_multiprocessing, **kwargs)
    499 
    500 

C:\Anaconda\envs\tensorflow\lib\site-packages\tensorflow_core\python\keras\engine\training_v2.py in _model_iteration(self, model, mode, x, y, batch_size, verbose, sample_weight, steps, callbacks, max_queue_size, workers, use_multiprocessing, **kwargs)
    424           max_queue_size=max_queue_size,
    425           workers=workers,
--&gt; 426           use_multiprocessing=use_multiprocessing)
    427       total_samples = _get_total_number_of_samples(adapter)
    428       use_sample = total_samples is not None

C:\Anaconda\envs\tensorflow\lib\site-packages\tensorflow_core\python\keras\engine\training_v2.py in _process_inputs(model, mode, x, y, batch_size, epochs, sample_weights, class_weights, shuffle, steps, distribution_strategy, max_queue_size, workers, use_multiprocessing)
    644     standardize_function = None
    645     x, y, sample_weights = standardize(
--&gt; 646         x, y, sample_weight=sample_weights)
    647   elif adapter_cls is data_adapter.ListsOfScalarsDataAdapter:
    648     standardize_function = standardize

C:\Anaconda\envs\tensorflow\lib\site-packages\tensorflow_core\python\keras\engine\training.py in _standardize_user_data(self, x, y, sample_weight, class_weight, batch_size, check_steps, steps_name, steps, validation_split, shuffle, extract_tensors_from_dataset)
   2381         is_dataset=is_dataset,
   2382         class_weight=class_weight,
-&gt; 2383         batch_size=batch_size)
   2384 
   2385   def _standardize_tensors(self, x, y, sample_weight, run_eagerly, dict_inputs,

C:\Anaconda\envs\tensorflow\lib\site-packages\tensorflow_core\python\keras\engine\training.py in _standardize_tensors(self, x, y, sample_weight, run_eagerly, dict_inputs, is_dataset, class_weight, batch_size)
   2408           feed_input_shapes,
   2409           check_batch_axis=False,  # Don't enforce the batch size.
-&gt; 2410           exception_prefix='input')
   2411 
   2412     # Get typespecs for the input data and sanitize it if necessary.

C:\Anaconda\envs\tensorflow\lib\site-packages\tensorflow_core\python\keras\engine\training_utils.py in standardize_input_data(data, names, shapes, check_batch_axis, exception_prefix)
    571                            ': expected ' + names[i] + ' to have ' +
    572                            str(len(shape)) + ' dimensions, but got array '
--&gt; 573                            'with shape ' + str(data_shape))
    574         if not check_batch_axis:
    575           data_shape = data_shape[1:]

ValueError: Error when checking input: expected dense_42_input to have 3 dimensions, but got array with shape (1, 1)
</code></pre>
",8163377,,8163377,,2020-11-16 12:55:42,2020-11-16 12:55:42,Problem having the right input for model.predict() in Keras model,<python><keras><conv-neural-network><mlp><dqn>,1,0,,,,CC BY-SA 4.0,
158,65198501,1,65742611,,2020-12-08 11:59:13,,0,156,"<p>I am trying to implement a DRL (Deep Reinforcement Learning) Agent for self-driving vehicles. I am currently teaching my agent not to bump on other cars, <strong>using a simple camera</strong>. There are many ways to speed up the training, but currently, I am focusing on <strong>adding the sense of motion in my observation</strong>.</p>
<p>Everyone on the Internet (Including Google's article about Atari games) mentions that in order to add motion in observations is to <strong>capture 3-4 frames, instead of 1</strong> and feed them to the QNetwork as one observation. However, This isn't much practical when using camera data, because It requires a lot of computational power to train an agent. For example:</p>
<p>Suppose You use a grayscale camera of resolution 256x256 and we use a simple uniform replay memory that holds up to 20000 observations. Then, the number of pixels stored in the memory are:</p>
<pre><code>20000 (Samples) * 4 (Frames) * 256 (Width) * 256 (Height) = 5.2 GB of Physical RAM.
</code></pre>
<p>Also, suppose that You use a batch size of 64 observations to feed the agent, which contains a CNN of 32 filters in the 1st layer, then You need:</p>
<pre><code>64 (Batch Size) * (4 Frames) * 256 (Width) * 256 (Height) * 32 (Filters) = 0.5 GB of GPU.
</code></pre>
<p><strong>This is an insane amount of data that needs to be proccessed by the agent for 1 simple grayscale camera, just to add the sense of motion.</strong></p>
<p>I was thinking of an alternative way of adding the sense of motion, however, I can't find anything about it on the internet. Since we already know the speed of the vehicle, then we could feed the agent:</p>
<ul>
<li>1 Frame that contains the camera data.</li>
<li>1 Frame that contains the normalized value of the vehicle's speed in the center of the image (e.g. reserve a 32x32 window in the center of the image that contains the normalized speed of the vehicle (0.0-1.0) and the rest pixels have the value of 0.</li>
</ul>
<p><strong>In that way, we reduce the size of the data by half. Do You think this could be a good approach?</strong></p>
",6115152,,11341120,,2020-12-10 23:23:12,2022-07-31 22:19:02,Deep Reinforcement Learning Motion in Observation,<tensorflow><openai-gym><motion><dqn>,2,0,,,,CC BY-SA 4.0,
163,64799299,1,65263488,,2020-11-12 07:01:01,,1,1003,"<p>I'm quite new to RL and currently teaching myself how to implement different algorithms and hyper-parameters using tf_agents library.</p>
<p>I've been playing around with the code provided from this tutorial <a href=""https://colab.research.google.com/github/tensorflow/agents/blob/master/docs/tutorials/1_dqn_tutorial.ipynb"" rel=""nofollow noreferrer"">https://colab.research.google.com/github/tensorflow/agents/blob/master/docs/tutorials/1_dqn_tutorial.ipynb</a>.</p>
<p>After learning how to use TensorBoard I've come to wonder how I can visualize a graph from tf_agents library. Every TensorBoard tutorials/posts seems to implement its own model or define tf.function to log graph. However, I just can't apply such methods to the tutorial above.</p>
<p>If someone can help me visualize a model graph using tf_agents in TensorBoard, it will be very much appreciated. Thanks!</p>
",14622788,,,,,2020-12-12 09:41:22,Use TensorBoard to visualize graph from tf_agents,<tensorflow><tensorboard><reinforcement-learning><dqn>,1,0,,,,CC BY-SA 4.0,
164,67185851,1,67236329,,2021-04-20 20:23:11,,0,3682,"<p>I am starting to learn about <code>DQN</code>, and I am trying to solve the <code>FrozenLake-v0</code> problem from scratch by my self and using <code>Pytorch</code> so I will put the whole code since it's connected.</p>
<pre><code>class LinearDeepQNetwork(nn.Module):
  def __init__(self,lr,n_action,input_dim):
    super(LinearDeepQNetwork,self).__init__()
    self.f1=nn.Linear(input_dim,128)
    self.f2=nn.Linear(128,n_action)
    self.optimizer=optim.Adam(self.parameters(),lr=lr)
    self.loss=nn.MSELoss()
    self.device=T.device('cuda' if T.cuda.is_available() else 'cpu')
    self.to(self.device)

  def forward(self,state):
    layer1=F.relu(self.f1(state))
    actions=self.f2(layer1)

    return actions
</code></pre>
<p>the second class is the agent, and the problem is in the learning function</p>
<p>nd class is the agent, and the problem is in the learning function</p>
<pre><code>class Agent():
  def __init__(self,input_dim,n_action,lr,gamma=0.99,
               epslion=1.0,eps_dec=1e-5,eps_min=0.01):
    self.input_dim=input_dim
    self.n_action=n_action
    self.lr=lr
    self.gamma=gamma
    self.epslion=epslion
    self.eps_dec=eps_dec
    self.eps_min=eps_min
    self.action_space=[i for i in range(self.n_action)]

    self.Q=LinearDeepQNetwork(self.lr,self.n_action,self.input_dim)
  
  def choose_action(self,observation):
    if np.random.random()&gt;self.epslion:
      #conveate the state into tensor
      state=T.tensor(observation).to(self.Q.device)
      actions=self.Q.forward(state)
      action=T.argmax(actions).item()
    else:
      action=np.random.choice(self.action_space)

    return action
  
  def decrement_epsilon(self):
    self.epslion=self.epslion-self.eps_dec \
                  if self.epslion &gt; self.eps_min else self.eps_min
                  
  def OH(self,x,l):
    x = T.LongTensor([[x]])
    one_hot = T.FloatTensor(1,l)
    return one_hot.zero_().scatter_(1,x,1)

  def learn(self,state,action,reward,state_):
    self.Q.optimizer.zero_grad()
    states=Variable(self.OH(state,16)).to(self.Q.device)
    actions=T.tensor(action).to(self.Q.device)
    rewards=T.tensor(reward).to(self.Q.device)
    state_s=Variable(self.OH(state_,16)).to(self.Q.device)

    q_pred=self.Q.forward(states)[actions]
    
    q_next=self.Q.forward(state_s).max()

    q_target=reward+self.gamma*q_next
    loss=self.Q.loss(q_target,q_pred).to(self.Q.device)
    loss.backward()
    self.Q.optimizer.step()
    self.decrement_epsilon()
</code></pre>
<p>now the problem when I run the following code it gives me an error in the learning phase, and it gives me this error <code>index 1 is out of bounds for dimension 0 with size 1.</code></p>
<pre><code>env=gym.make('FrozenLake-v0')    
n_games=5000
scores=[]
eps_history=[]


agent=Agent(env.observation_space.n,env.action_space.n,0.0001)

for i in tqdm(range(n_games)):
  score=0
  done=False
  obs=env.reset()

  while not done:
    action=agent.choose_action(obs)
    obs_,reward,done,_=env.step(action)
    score+=reward
    
    agent.learn(obs,action,reward,obs_)
    obs=obs_
  scores.append(score)
  eps_history.append(agent.epslion)
  if i % 100 ==0:
    avg_score=np.mean(scores[-100:])
    print(f'score={score}  avg_score={avg_score} epsilon={agent.epslion} i={i}')
</code></pre>
<p>I think the problem is in the shape of the values between the NN and the agent class, but I can't figure out the problem.</p>
<p>Error traceback:</p>
<pre><code>IndexError                                Traceback (most recent call last)
&lt;ipython-input-10-2e279f658721&gt; in &lt;module&gt;()
     17     score+=reward
     18 
---&gt; 19     agent.learn(obs,action,reward,obs_)
     20     obs=obs_
     21   scores.append(score)

&lt;ipython-input-8-5359b19ec4fa&gt; in learn(self, state, action, reward, state_)
     39     state_s=Variable(self.OH(state_,16)).to(self.Q.device)
     40 
---&gt; 41     q_pred=self.Q.forward(states)[actions]
     42 
     43     q_next=self.Q.forward(state_s).max()

IndexError: index 1 is out of bounds for dimension 0 with size 1
</code></pre>
",1771445,,4685471,,2021-04-21 11:05:13,2021-04-23 20:14:29,index 1 is out of bounds for dimension 0 with size 1,<python><deep-learning><pytorch><dqn>,1,8,,,,CC BY-SA 4.0,
165,62039376,1,62041066,,2020-05-27 09:06:17,,0,166,"<p>I have created a DQN with a max memory size of 100000. I have a function that removes the oldest element in the memory if its size is greater than the max size. When I ran it doing 200 episodes, I noticed that the memory was already full at the 125th episode. Is it okay that my DQN will delete its oldest experience for the remaining episodes?</p>
",10665516,,,,,2020-05-27 10:35:14,Is it okay to remove most oldest experiences of DQN,<deep-learning><reinforcement-learning><q-learning><dqn>,1,0,,,,CC BY-SA 4.0,
166,65635342,1,65650938,,2021-01-08 19:33:03,,1,416,"<p>devs,</p>
<p>I found a bunch of examples of DQN implementations, but because I'm no TensorFlow expert, I'm a little bit confused.</p>
<p>Let's see <a href=""https://dumpz.org/c77HNAA4XxGF"" rel=""nofollow noreferrer"">here</a> is one one of them.</p>
<p>I can understand, on the 73rd line, we slice some batch of stored data <code>[{state, action, reward, newState, done}]</code> exactly, then we get <code>currentStates</code> which is <code>[[s1, s2, ...]]</code>, then on 75 we use the model to get <code>currentQs</code> which should be, how I understand, <code>[[act1, act2, ...]]</code>, because our model is used to get action from env's state. The same happens to <code>newCurrentStates</code> and <code>futureQs</code>.</p>
<p>But then on 88, we see <code>let maxFutureQ = Math.max(futureQs);</code>. What happened here? <code>futureQs</code> is an array of arrays with actions probabilities for each futureState? And then <code>maxFutureQ</code> should be an action probability, why then we add this to reward? This part is confusing me.</p>
<p>Also I cannot understand why we need to do <code>currentQ[action] = newQ;</code> on 94.</p>
<p>Please, could someone help me to understand what is going on here and leave comments for lines, maybe?</p>
<p>Thanks in advance.</p>
<p>edit:</p>
<p>discussed code:
<a href=""https://i.stack.imgur.com/yfgZu.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/yfgZu.png"" alt=""discussed code"" /></a></p>
",4369921,,4369921,,2021-01-11 07:38:26,2021-01-11 07:38:26,How does work this implementation of DQN algorithm on TensorFlowJs?,<javascript><tensorflow><tensorflow.js><q-learning><dqn>,1,0,0,,,CC BY-SA 4.0,
170,59943167,1,59978507,,2020-01-28 06:24:16,,0,46,"<p>I have read the DQN thesis.</p>

<p>While reading the DQN paper, I found that randomly selecting and learning samples reduced divergence in RL using a non-linier function approximator.</p>

<p>If so, why is the learning of RL using a non-linier function approximator divergent when the input data are strongly correlated?</p>
",7961575,,,,,2020-01-30 03:00:14,Why does randomizing samples of reinforcement learning model with a non-linear function approximator reduce variance?,<deep-learning><reinforcement-learning><nonlinear-functions><dqn>,1,4,,,,CC BY-SA 4.0,
171,63328885,1,66834414,,2020-08-09 16:39:15,,0,721,"<p>I know there are many similar topics discussed on StackOverflow, but I have done quite a lot research both in StackOverflow and on the Internet and I couldn't find a solution.
I am trying to implement the classic Deep Q Learning Algorithm to solve the openAI gym's cartpole game:
<a href=""https://gym.openai.com/envs/CartPole-v0/"" rel=""nofollow noreferrer"">OpenAI Gym Cartpole</a></p>
<p>Firstly, I created an agent that generates random weights. The results are shown in the graph below:
<a href=""https://i.stack.imgur.com/EkBJV.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/EkBJV.png"" alt=""Agent uses Random Search to beat cartpole"" /></a></p>
<p>Amazingly, the agent managed to reach 200 steps (which is the max) in many episodes by simply generating 4 random uniform weights [w1, w2, w3, w4] from (-1.0 to 1.0) in each episode.</p>
<p>So, i decided to implement a simple DQN with only 4 weights and 2 biases and to make the agent learn this game over the time. The weights will be initialized randomly in the beginning and Back-Propagation will be used to update them as the agent makes steps.</p>
<p>I used the Epsilon Greedy strategy to make the agent explore at the beginning and exploit the Q values later on. However, The results are disappointing compared to the random agent:</p>
<p><a href=""https://i.stack.imgur.com/FGs3Q.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/FGs3Q.png"" alt=""enter image description here"" /></a></p>
<p>I have tried to tune a lot of parameters and different architectures and the result doesn't change as much. So, my question is the following:</p>
<p><strong>Question:</strong>
Did i make a wrong implementation of DQN or a simple DQN cannot beat the cartpole? What's your experience? It does reduces the loss (Error), but it doesn't guarantee a good solution.
Thanks in advance.</p>
<pre><code>import tensorflow as tf
import gym
import numpy as np
import random as rand
import matplotlib.pyplot as plt

# Cartpole's Observation:
#   4 Inputs
#   2 Actions (LEFT | RIGHT)
input_size = 4
output_size = 2

# Deep Q Network Class
class DQN:
    def __init__(self, var_names):
        self.var_names = var_names

        self._define_placeholders()
        self._add_layers()
        self._define_loss()
        self._choose_optimizer()
        self._initialize()

    # Placeholders:
    # Inputs: The place where we feed the Observations (States).
    # Targets: Q_target = R + gamma*Q(s', a*).
    def _define_placeholders(self):
        self.inputs = tf.placeholder(tf.float32, shape=(None, input_size), name='inputs')
        self.targets = tf.placeholder( tf.float32, shape=(None, output_size), name='targets')

    # Layers:
    # 4 Input Weights.
    # 2 Biases.
    # output = softmax(inputs*weights + biases).
    # Weights and biases are initialized randomly.
    def _add_layers(self):
        w = tf.get_variable(name=self.var_names[0], shape=(input_size, output_size),
                                initializer=tf.initializers.random_uniform(minval=-1.0, maxval=1.0) )
        b = tf.get_variable(name=self.var_names[1], shape=(output_size),
                                initializer=tf.initializers.random_uniform(minval=-1.0, maxval=1.0) )
        self.outputs = tf.nn.softmax(tf.matmul(self.inputs, w) + b)
        self.prediction = tf.argmax(self.outputs, 1)

    # Loss = MSE.
    def _define_loss(self):
        self.mean_loss = tf.losses.mean_squared_error(labels=self.targets, predictions=self.outputs) / 2

    # AdamOptimizer with starting learning rate: a = 0.005.
    def _choose_optimizer(self):
        self.optimizer = tf.train.AdamOptimizer(learning_rate=0.005).minimize(loss=self.mean_loss)

    # Initializes the dqn's weights.
    def _initialize(self):
        initializer = tf.global_variables_initializer()
        self.sess = tf.InteractiveSession()
        self.sess.run(initializer)

    # Get's current's DQN weights.
    def get_weights(self):
        return [ self.sess.run( tf.trainable_variables(var) )[0] for var in self.var_names ]
        
    # Updates the weights of DQN.
    def update_weights(self, new_weights):
        variables = [tf.trainable_variables(name)[0] for name in self.var_names]
        update = [ tf.assign(var, weight) for (var, weight) in zip(variables, new_weights) ]
        self.sess.run(update)

    # Predicts the best possible action from a state s.
    # a* = argmax( Q(s) )
    # Returns from Q(s), a*
    def predict(self, states):
        Q, actions = self.sess.run( [self.outputs, self.prediction],
                                    feed_dict={self.inputs: states} )
        return Q, actions

    # It partially fits the given observations and the targets into the network.
    def partial_fit(self, states, targets):
        _, loss = self.sess.run( [self.optimizer, self.mean_loss],
                                    feed_dict={self.inputs: states, self.targets: targets} )
        return loss

# Replay Memory Buffer
# It stores experiences as (s,a,r,s') --&gt; (State, Action, Reward, Next_Action).
# It generates random mini-batches of experiences from the memory.
# If the memory is full, then it deletes the oldest experiences. Experience is an step.
class ReplayMemory:
    def __init__(self, mem_size):
        self.mem_size = mem_size
        self.experiences = []

    def add_experience(self, xp):
        self.experiences.append(xp)
        if len(self.experiences) &gt; self.mem_size:
            self.experiences.pop(0)

    def random_batch(self, batch_size):
        if len(self.experiences) &lt; batch_size:
            return self.experiences
        else:
            return rand.sample(self.experiences, batch_size)

# The agent's class.
# It contains 2 DQNs: Online DQN for Predictions and Target DQN for the targets.
class Agent:
    def __init__(self, epsilon, epsilon_decay, min_epsilon, gamma, mem_size):
        self.epsilon = epsilon
        self.epsilon_decay = epsilon_decay
        self.min_epsilon = min_epsilon
        self.gamma = gamma
        self.replay_mem = ReplayMemory(mem_size)
        self.online_dqn = DQN( var_names=['online_w', 'online_b'] )
        self.target_dqn = DQN( var_names=['target_w', 'target_b'] )
        self.state = None

    def set_epsilon(self, epsilon):
        self.epsilon = epsilon

    def reduce_epsilon(self):
        if self.epsilon &gt; self.min_epsilon:
            self.epsilon -= self.epsilon_decay
    
    def update_state(self, state):
        self.state = state

    def update_memory(self, state, action, reward, next_state):
       experience = (state, action, reward, next_state)
        self.replay_mem.add_experience(experience)

    # It updates the target network after N steps.
    def update_network(self):
        self.target_dqn.update_weights( self.online_dqn.get_weights() )

    # Randomly chooses an action from the enviroment.
    def explore(self, env):
        action = env.action_space.sample()
        return action

    # Predicts and chooses the best possible moves from the current state.
    def exploit(self):
        _, action = self.online_dqn.predict(self.state)
        return action[0]

    # Uses Epsilon-Greedy to decide whether to explore or exploit.
    # Epsilon starts with 1 and is reduced over the time.
    # After the agent makes a move, he returns: state, action, reward, next_state.
    def take_action(self, env):
        action = None
        p = rand.uniform(0.0, 1.0)
        if p &lt; self.epsilon:
            action = self.explore(env)
        else:
            action = self.exploit()
        next_state, reward, done, _ = env.step(action)
        if done:
            next_state = None
        else:
            next_state = np.reshape( next_state, (1, input_size) )
        return self.state, action, reward, next_state, done

    # Trains the agent.
    # A random mini-batch is generated from the memory.
    # We feed each experience into the DQN.
    # For each 
    # Q(s) = Qtarget(s)
    # Q(s'), a* = Qtarget(s'), argmax Q(s')
    # We set targets = Q(s')

    # For each action (a), reward (r), next_state (s') in the batch:
    # If s' is None the GameOver. So, we set target[i] = Reward
    # If s' != None, then target[i][a] = r + gamma*Q(s', 'a')

    # Then, the online DQN calculates the mean squared difference of r + gamma*Q(s', 'a') - Q(s, a)
    # and uses Back-Propagation to update the weights.
    def train(self):
        mini_batch = self.replay_mem.random_batch(batch_size=256)
        batch_size = len(mini_batch)
        states = np.zeros( shape=(batch_size, input_size) )
        next_states = np.zeros( shape=(batch_size, input_size) )
        for i in range(batch_size):
            states[i] = mini_batch[i][0]
            next_states[i] = mini_batch[i][3]

        Q, _ = self.target_dqn.predict(states)
        next_Q, next_actions = self.target_dqn.predict(next_states)
        targets = Q
        for i in range(batch_size):
            action = mini_batch[i][1]
            reward = mini_batch[i][2]
            next_state = mini_batch[i][3]
            if next_state is None:
                targets[i][action] = reward
            else:
                targets[i][action] = reward + self.gamma * next_Q[i][ next_actions[i] ]
        loss = self.online_dqn.partial_fit(states, targets)
        return loss
    
def play(agent, env, episodes, N, render=False, train=True):
    ep = 0
    episode_steps = []
    steps = 0
    total_steps = 0
    loss = 0

    # Sets the current state as the initial.
    # Cartpole spawns the agent in a random state.
    agent.update_state( np.reshape( env.reset(), (1, input_size) ) )
    agent.update_network()

    while ep &lt; episodes:
        if render:
            env.render()
    
        # The target DQN's weights are frozen.
        # The agent Updates the Target DQN's Weights after 100 steps.
        if train and total_steps % N == 0:
            agent.update_network()
            print('---Target network updated---')

        # Takes action.
        state, action, reward, next_state, done = agent.take_action(env)

        # Updates the memory and the current state.
        agent.update_memory(state, action, reward, next_state)
        agent.update_state(next_state)
        steps += 1
        total_steps += 1

        if train:
            loss = agent.train()

        if done:
            agent.update_state( np.reshape( env.reset(), (1, input_size) ) )
            episode_steps.append(steps)
            ep += 1
            if train:
                agent.reduce_epsilon()
                print('End of episode', ep, 'Training loss =', loss, 'Steps =', steps)
            steps = 0

    if render:
        env.close()

    return episode_steps

env = gym.make('CartPole-v0')

# Training the agent.
agent = Agent(epsilon=1, epsilon_decay = 0.01, min_epsilon = 0.05, gamma=0.9, mem_size=50000)
episodes = 1000
N = 100
episode_steps = play(agent, env, episodes, N)

# Plotting the results.
# After the training is done, the steps should be maximized (up to 200)
plt.plot(episode_steps)
plt.show()

# Testing the agent.
agent.set_epsilon(0)
episodes = 1
steps = play(agent, env, episodes, N, render=True, train=False)[0]
print('\nSteps =', steps)
</code></pre>
",6115152,,,,,2021-03-27 17:49:59,Deep Q - Learning for Cartpole with Tensorflow in Python,<python><tensorflow><reinforcement-learning><dqn>,1,0,,,,CC BY-SA 4.0,
172,65636703,1,65639193,,2021-01-08 21:33:30,,2,248,"<p>I have been trying to implement the Reinforcement learning algorithm on Python using different variants like <code>Q-learning</code>, <code>Deep Q-Network</code>, <code>Double DQN</code> and <code>Dueling Double DQN</code>. Consider a cart-pole example and to evaluate the performance of each of these variants, I can think of plotting <code>sum of rewards</code> to <code>number of episodes</code> <a href=""https://i.stack.imgur.com/kpV8I.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/kpV8I.png"" alt=""evaluating a Reinforcement learning model"" /></a> (attaching a picture of the plot) and the actual graphical output where how well the pole is stable while the cart is moving.</p>
<p>But these two evaluations are not really of interest in terms to explain the better variants quantitatively. I am new to the Reinforcement learning and trying to understand if any other ways to compare different variants of RL models on the same problem.</p>
<p>I am referring to the colab link <a href=""https://colab.research.google.com/github/ageron/handson-ml2/blob/master/18_reinforcement_learning.ipynb#scrollTo=MR0z7tfo3k9C"" rel=""nofollow noreferrer"">https://colab.research.google.com/github/ageron/handson-ml2/blob/master/18_reinforcement_learning.ipynb#scrollTo=MR0z7tfo3k9C</a> for the code on all the variants of cart pole example.</p>
",9531937,,,,,2021-01-09 03:53:41,Understanding and Evaluating different methods in Reinforcement Learning,<python><reinforcement-learning><openai-gym><dqn>,1,0,,,,CC BY-SA 4.0,
174,60447432,1,60504590,,2020-02-28 07:55:01,,1,893,"<p>I'm trying to implement my own Dueling DQN using tensorflow 2 based on <a href=""https://arxiv.org/pdf/1511.06581.pdf"" rel=""nofollow noreferrer"">https://arxiv.org/pdf/1511.06581.pdf</a>. I'm actually training it on the Atlantis environment but I can't get good results (<a href=""https://i.stack.imgur.com/r4toH.png"" rel=""nofollow noreferrer"">Mean reward per game</a> keeps decreasing while <a href=""https://i.stack.imgur.com/fDoK2.png"" rel=""nofollow noreferrer"">TD loss</a> increases). Though I believe I got the logic from the paper, I don't know if it comes from the direct implementation of the network or the chosen parameters. </p>

<p>EDIT : Using tf.keras.utils.plot_model gives me <a href=""https://i.stack.imgur.com/UMCvH.png"" rel=""nofollow noreferrer"">this</a>.</p>

<pre><code>class DQNAgent:
  def __init__(self, state_shape, n_actions, epsilon=0):
    self.state_input = Input(shape=state_shape, name='State')
    self.x = Conv2D(16, (3, 3), strides=2, activation='relu')(self.state_input)
    self.x = Conv2D(32, (3, 3), strides=2, activation='relu')(self.x)
    self.x = Conv2D(64, (3, 3), strides=2, activation='relu')(self.x)
    self.x = Flatten()(self.x)
    self.x = Dense(256, activation='relu')(self.x)

    self.head_v = Dense(256,activation='relu')(self.x)
    self.head_v = Dense(1, activation='linear',name=""Value"")(self.head_v)
    self.head_v = RepeatVector(n_actions)(self.head_v)
    self.head_v = Flatten()(self.head_v)

    self.head_a = Dense(256,activation='relu')(self.x)
    self.head_a = Dense(n_actions, activation='linear',name='Activation')(self.head_a)

    self.m_head_a = RepeatVector(n_actions)(tf.keras.backend.mean(self.head_a,axis=1,keepdims=True))
    self.m_head_a = Flatten(name='meanActivation')(self.m_head_a)

    self.head_a = Subtract()([self.head_a,self.m_head_a])

    self.head_q = Add(name = ""Q-value"")([self.head_v,self.head_a])


    self.network = tf.keras.Model(inputs=[self.state_input], outputs=[self.head_q])
    self.weights = self.network.trainable_variables
    self.epsilon = epsilon
    self.optimizer = tf.keras.optimizers.Adam(1e-3)

  def get_qvalues(self, state_t):
    return self.network(state_t)

  def train(self, exp_replay, batch_size=64):
    states, actions, rewards, next_states, is_done = exp_replay.sample(batch_size)
    is_not_done = 1 - is_done

    with tf.GradientTape() as t:
      current_qvalues = agent.get_qvalues(states)
      current_action_qvalues = tf.reduce_sum(tf.one_hot(actions, n_actions) * current_qvalues, axis=-1)
      next_qvalues_target = target_network.get_qvalues(next_states)
      next_state_values_target = tf.reduce_max(next_qvalues_target, axis=-1)
      reference_qvalues = rewards + gamma*next_state_values_target*is_not_done
      td_loss = (current_action_qvalues - reference_qvalues)**2
      td_loss = tf.math.reduce_mean(td_loss)

    var_list = agent.weights
    grads = t.gradient(td_loss,var_list)
    self.optimizer.apply_gradients(zip(grads, var_list))
    return td_loss


  def sample_actions(self, qvalues):
    batch_size, n_actions = qvalues.shape
    random_actions = np.random.choice(n_actions, size=batch_size)
    best_actions = tf.math.argmax(qvalues, axis=-1)
    should_explore = np.random.choice([0, 1], batch_size, p = [1-self.epsilon, self.epsilon])
    return np.where(should_explore, random_actions, best_actions)


def load_weights_into_target_network(agent, target_network):
  for t, e in zip(target_network.network.trainable_variables, agent.network.trainable_variables):
    t.assign(e)

env = make_env() # Apply frame buffer on ""AtlantisDeterministic-V4"" env
env.reset()
n_actions = env.action_space.n
state_dim = env.observation_space.shape

agent = DQNAgent(state_dim, n_actions, epsilon=0.5)    
target_network = DQNAgent(state_dim, n_actions)

exp_replay = ReplayBuffer(10**5) # Random experience replay buffer
play_and_record(agent, env, exp_replay, n_steps=10000) # Plays exactly n_steps and records each transition in the ReplayBuffer
gamma = 0.99

for i in trange(10**5):
  play_and_record(agent, env, exp_replay, 10)

  td_loss = agent.train(exp_replay, 64)

  # adjust agent parameters
  if i % 500 == 0:
    load_weights_into_target_network(agent, target_network)
    agent.epsilon = max(agent.epsilon * 0.99, 0.01)
</code></pre>
",12971481,,12971481,,2020-02-28 08:33:08,2020-03-03 09:58:23,Implementing Dueling DQN on TensorFlow 2.0,<python><deep-learning><tensorflow2.0><reinforcement-learning><dqn>,1,4,,,,CC BY-SA 4.0,
176,62001967,1,62026073,,2020-05-25 11:57:45,,-1,187,"<p>should the values of the state in DQN need to be only 0 to 1 for example
state = [0, 0, 0, 1, 1, 1, 1, 0, 1, 0]</p>

<p>or it can have a state with values greater than 1 eh
state = [6, 5, 4, 1, 1, 1, 2, 3, 15, 10]</p>
",10665516,,,,,2020-05-26 15:45:56,should dqn state values need to be 0 to 1 only,<python><deep-learning><reinforcement-learning><q-learning><dqn>,1,1,,,,CC BY-SA 4.0,
181,63408505,1,65742877,,2020-08-14 07:37:39,,2,1054,"<p>For reinforcement learning I have read that tensorboard isn't ideal since it gives the input of per episode and/or step. Since in reinforcement learning there are thousands of steps, it doesn't give us an overview of the content. I saw this modified tensorboard class here: <a href=""https://pythonprogramming.net/deep-q-learning-dqn-reinforcement-learning-python-tutorial/"" rel=""nofollow noreferrer"">https://pythonprogramming.net/deep-q-learning-dqn-reinforcement-learning-python-tutorial/</a></p>
<p>the class:</p>
<pre><code>class ModifiedTensorBoard(TensorBoard):
    # Overriding init to set initial step and writer (we want one log file for all .fit() calls)
    def __init__(self, name, **kwargs):
        super().__init__(**kwargs)
        self.step = 1
        self.writer = tf.summary.create_file_writer(self.log_dir)
        self._log_write_dir = os.path.join(self.log_dir, name)

    # Overriding this method to stop creating default log writer
    def set_model(self, model):
        pass

    # Overrided, saves logs with our step number
    # (otherwise every .fit() will start writing from 0th step)
    def on_epoch_end(self, epoch, logs=None):
        self.update_stats(**logs)

    # Overrided
    # We train for one batch only, no need to save anything at epoch end
    def on_batch_end(self, batch, logs=None):
        pass

    # Overrided, so won't close writer
    def on_train_end(self, _):
        pass

    def on_train_batch_end(self, batch, logs=None):
        pass

    # Custom method for saving own metrics
    # Creates writer, writes custom metrics and closes writer
    def update_stats(self, **stats):
        self._write_logs(stats, self.step)

    def _write_logs(self, logs, index):
        with self.writer.as_default():
            for name, value in logs.items():
                tf.summary.scalar(name, value, step=index)
                self.step += 1
                self.writer.flush()
</code></pre>
<p>and I would like to make it work with this layer:</p>
<pre><code>n_actions = env.action_space.n
input_dim = env.observation_space.n
model = tf.keras.Sequential()
model.add(tf.keras.layers.Dense(20, input_dim = input_dim , activation = 'relu'))#32
model.add(tf.keras.layers.Dense(10, activation = 'relu'))#10
model.add(tf.keras.layers.Dense(n_actions, activation = 'linear'))
model.compile(optimizer=tf.keras.optimizers.Adam(), loss = 'mse')
</code></pre>
<p>But I have yet to get it to work. Anyone who has worked with tensorboard before, do you know how to setup this up? Any insight is greatly appreciated.</p>
",8313547,,,,,2021-01-15 20:05:23,Using tensorboard with a DQN algorithm,<tensorflow><tensorboard><reinforcement-learning><dqn>,1,0,,,,CC BY-SA 4.0,
182,67773479,1,67775247,,2021-05-31 12:11:23,,1,269,"<p>I have a concern in understanding the Cartpole code as an example for Deep Q Learning. The DQL Agent part of the code as follow:</p>
<pre><code>class DQLAgent:
def __init__(self, env):
    # parameter / hyperparameter
    self.state_size = env.observation_space.shape[0]
    self.action_size = env.action_space.n
    
    self.gamma = 0.95
    self.learning_rate = 0.001 
    
    self.epsilon = 1  # explore
    self.epsilon_decay = 0.995
    self.epsilon_min = 0.01
    
    self.memory = deque(maxlen = 1000)
    
    self.model = self.build_model()
    
    
def build_model(self):
    # neural network for deep q learning
    model = Sequential()
    model.add(Dense(48, input_dim = self.state_size, activation = &quot;tanh&quot;))
    model.add(Dense(self.action_size,activation = &quot;linear&quot;))
    model.compile(loss = &quot;mse&quot;, optimizer = Adam(lr = self.learning_rate))
    return model

def remember(self, state, action, reward, next_state, done):
    # storage
    self.memory.append((state, action, reward, next_state, done))

def act(self, state):
    # acting: explore or exploit
    if random.uniform(0,1) &lt;= self.epsilon:
        return env.action_space.sample()
    else:
        act_values = self.model.predict(state)
        return np.argmax(act_values[0])

def replay(self, batch_size):
    # training
    if len(self.memory) &lt; batch_size:
        return
    minibatch = random.sample(self.memory,batch_size)
    for state, action, reward, next_state, done in minibatch:
        if done:
            target = reward 
        else:
            target = reward + self.gamma*np.amax(self.model.predict(next_state)[0])
        train_target = self.model.predict(state)
        train_target[0][action] = target
        self.model.fit(state,train_target, verbose = 0)
        
def adaptiveEGreedy(self):
    if self.epsilon &gt; self.epsilon_min:
        self.epsilon *= self.epsilon_decay
</code></pre>
<p>In the training section, we found our target and train_target. So why did we set <code>train_target[0][action] = target</code> here?</p>
<p>Every predict made while learning is not correct, but thanks to error calculation and backpropagation, the predict made at the end of the network will get closer and closer, but when we make <code>train_target[0][action] = target</code> here the error becomes 0, and in this case, how will the learning be?</p>
",12109594,,12109594,,2021-05-31 12:17:59,2021-05-31 22:21:42,Deep Q Learning - Cartpole Environment,<reinforcement-learning><dqn>,1,0,,,,CC BY-SA 4.0,
184,64690471,1,64755558,,2020-11-05 02:41:12,,2,695,"<p>Here is my implementation of DQN and DDQN for CartPole-v0 which I think is correct.</p>
<pre><code>import numpy as np
import torch
import torch.nn as nn
import torch.nn.functional as F
import gym
import torch.optim as optim
import random
import os
import time


class NETWORK(torch.nn.Module):
    def __init__(self, input_dim: int, output_dim: int, hidden_dim: int) -&gt; None:

        super(NETWORK, self).__init__()

        self.layer1 = torch.nn.Sequential(
            torch.nn.Linear(input_dim, hidden_dim),
            torch.nn.ReLU()
        )

        self.layer2 = torch.nn.Sequential(
            torch.nn.Linear(hidden_dim, hidden_dim),
            torch.nn.ReLU()
        )

        self.final = torch.nn.Linear(hidden_dim, output_dim)

    def forward(self, x: torch.Tensor) -&gt; torch.Tensor:
        x = self.layer1(x)
        x = self.layer2(x)
        x = self.final(x)

        return x

class ReplayBuffer(object):
    def __init__(self, capacity=50000):
        self.capacity = capacity
        self.memory = []
        self.position = 0

    def push(self, s0, a0, r, s1):
        if len(self.memory) &lt; self.capacity:
            self.memory.append(None)
        self.memory[self.position] = (s0, a0, r, s1)
        self.position = (self.position + 1) % self.capacity

    def sample(self, batch_size=64):
        return random.sample(self.memory, batch_size)

    def __len__(self):
        return len(self.memory)

class DQN(object):
    def __init__(self):
      self.state_dim = 4
      self.action_dim = 2
      self.lr = 0.001
      self.discount_factor = 0.99
      self.epsilon = 1
      self.epsilon_decay = 0.95
      self.num_train = 0
      self.num_train_episodes = 0
      self.batch_size = 64

      self.predict_network = NETWORK(input_dim=4, output_dim=2, hidden_dim=16).double()

      self.memory = ReplayBuffer(capacity=50000)
      self.optimizer = torch.optim.Adam(self.predict_network.parameters(), lr=self.lr)
      self.loss = 0

    def select_action(self, states: np.ndarray) -&gt; int:
      if np.random.uniform(0, 1) &lt; self.epsilon:
        return np.random.choice(self.action_dim)
      else:
        states = torch.from_numpy(states).unsqueeze_(dim=0)
        with torch.no_grad():
          Q_values = self.predict_network(states)
          action = torch.argmax(Q_values).item()
        return action

    def policy(self, states: np.ndarray) -&gt; int:
      states = torch.from_numpy(states).unsqueeze_(dim=0)
      with torch.no_grad():
        Q_values = self.predict_network(states)
        action = torch.argmax(Q_values).item()
      return action

    def train(self, s0, a0, r, s1, sign):
      if sign == 1:
        self.num_train_episodes += 1
        if self.epsilon &gt; 0.01:
          self.epsilon = max(self.epsilon * self.epsilon_decay, 0.01)
        return

      self.num_train += 1
      self.memory.push(s0, a0, r, s1)
      if len(self.memory) &lt; self.batch_size:
        return
      
      batch = self.memory.sample(self.batch_size)
      state_batch = torch.from_numpy(np.stack([b[0] for b in batch]))
      action_batch = torch.from_numpy(np.stack([b[1] for b in batch]))
      reward_batch = torch.from_numpy(np.stack([b[2] for b in batch]))
      next_state_batch = torch.from_numpy(np.stack([b[3] for b in batch]))

      Q_values = self.predict_network(state_batch)[torch.arange(self.batch_size), action_batch]
      
      next_state_Q_values = self.predict_network(next_state_batch).max(dim=1)[0]
      
      Q_targets = self.discount_factor * next_state_Q_values + reward_batch
      
      loss = F.mse_loss(Q_values, Q_targets.detach())
      
      self.optimizer.zero_grad()
      loss.backward()
      self.optimizer.step()

      self.loss = loss.data.item()

class DDQN(object):
    def __init__(self):
      self.state_dim = 4
      self.action_dim = 2
      self.lr = 0.001
      self.discount_factor = 0.9
      self.epsilon = 1
      self.epsilon_decay = 0.95
      self.num_train = 0
      self.num_train_episodes = 0
      self.batch_size = 64

      self.predict_network = NETWORK(input_dim=4, output_dim=2, hidden_dim=16).double()
      self.target_network = NETWORK(input_dim=4, output_dim=2, hidden_dim=16).double()
      self.target_network.load_state_dict(self.predict_network.state_dict())
      self.target_network.eval()

      self.memory = ReplayBuffer(capacity=50000)
      self.optimizer = torch.optim.Adam(self.predict_network.parameters(), lr=self.lr)

      self.loss = 0

    def select_action(self, states: np.ndarray) -&gt; int:
      if np.random.uniform(0, 1) &lt; self.epsilon:
        return np.random.choice(self.action_dim)
      else:
        states = torch.from_numpy(states).unsqueeze_(dim=0)
        with torch.no_grad():
          Q_values = self.predict_network(states)
        action = torch.argmax(Q_values).item()
        return action

    def policy(self, states: np.ndarray) -&gt; int:
      states = torch.from_numpy(states).unsqueeze_(dim=0)
      with torch.no_grad():
        Q_values = self.predict_network(states)
        action = torch.argmax(Q_values).item()
      return action

    def train(self, s0, a0, r, s1, sign):
      if sign == 1:
        self.num_train_episodes += 1
        if self.num_train_episodes % 2 == 0:
          self.target_network.load_state_dict(self.predict_network.state_dict())
          self.target_network.eval()
          
          if self.epsilon &gt; 0.01:
            self.epsilon = max(self.epsilon * self.epsilon_decay, 0.01)
        return

      self.num_train += 1
      self.memory.push(s0, a0, r, s1)
      if len(self.memory) &lt; self.batch_size:
        return
      batch = self.memory.sample(self.batch_size)
      state_batch = torch.from_numpy(np.stack([b[0] for b in batch]))
      action_batch = torch.from_numpy(np.stack([b[1] for b in batch]))
      reward_batch = torch.from_numpy(np.stack([b[2] for b in batch]))
      next_state_batch = torch.from_numpy(np.stack([b[3] for b in batch]))
      
      Q_values = self.predict_network(state_batch)[torch.arange(self.batch_size), action_batch]
      
      next_state_action_batch = torch.argmax(self.predict_network(next_state_batch), dim=1)
      
      next_state_Q_values = self.target_network(next_state_batch)[torch.arange(self.batch_size), next_state_action_batch]
      
      Q_targets = self.discount_factor * next_state_Q_values + reward_batch
      
      loss = F.smooth_l1_loss(Q_values, Q_targets.detach())
      self.optimizer.zero_grad()
      loss.backward()
      self.optimizer.step()

      self.loss = loss.data.item()
</code></pre>
<p>I used following to evaluate and train my DQN and DDQN.</p>
<pre><code>def eval_policy(agent, env_name, eval_episodes=10):
    eval_env = gym.make(env_name)
    avg_reward = 0.
    for _ in range(eval_episodes):
        state, done = eval_env.reset(), False
        while not done:
            action = agent.policy(state)
            state, reward, done, _ = eval_env.step(action)
            avg_reward += reward
    avg_reward /= eval_episodes
    print(&quot;---------------------------------------&quot;)
    print(f&quot;Evaluation over {eval_episodes} episodes: {avg_reward:.3f}&quot;)
    print(&quot;---------------------------------------&quot;)
    return avg_reward


env_name = 'CartPole-v0'
env = gym.make(env_name)
    
agent = DQN() # agent = DDQN()

for i in range(1000):
    state, done = env.reset(), False
    episodic_reward = 0
    while not done:
        action = agent.select_action(np.squeeze(state))
        next_state, reward, done, info = env.step(action)
        episodic_reward += reward      
        sign = 1 if done else 0
        agent.train(state, action, reward, next_state, sign)
        state = next_state        
    print(f'episode: {i}, reward: {episodic_reward}')  
    if i % 20 == 0:
        eval_reward = eval_policy(agent, env_name, eval_episodes=50)
        if eval_reward &gt;= 195:
            print(&quot;Problem solved in {} episodes&quot;.format(i + 1))
            break
</code></pre>
<p>The thing is that my DQN networks do not train and the loss grow exponentially using target.detach() in loss calculation. If I do not use .detach(), the DQN object would train but I believe that is not the correct way. For DDQN, my networks always do not train. Can anyone give some advice on where might be wrong?</p>
",12695435,,,,,2020-11-09 16:36:07,"Pytorch DQN, DDQN using .detach() caused very wield loss (increases exponentially) and do not learn at all",<pytorch><reinforcement-learning><q-learning><dqn>,1,0,,,,CC BY-SA 4.0,
190,66326130,1,66391403,,2021-02-23 02:19:24,,2,477,"<p>I am building a DQN for an Open Gym environment. My observation space is only 1 discrete value but my actions are:</p>
<pre><code>self.action_space = (Discrete(3), Box(-100, 100, (1,)))
</code></pre>
<p>ex: [1,56], [0,24], [2,-78]...</p>
<p>My current neural network is:</p>
<pre><code>model = Sequential()
model.add(Dense(24, activation='relu', input_shape=states)) # (1,)
model.add(Dense(24, activation='relu'))
model.add(Dense(2, activation='linear'))
</code></pre>
<p>(I copied it from a tutorial that only outputs 1 discrete value in the range [0,1]}</p>
<p>I understand that I need to change the last layer of my neural network but what would it be in my case?</p>
<p>My guess is that the last layer should have 3 binary outputs and 1 continuous output but I don't know
if it is possible to have different natures of outputs within the same layer.</p>
",1012297,,,,,2021-02-26 19:02:07,How to build a DQN that outputs 1 discrete and 1 continuous value as a pair?,<python><tensorflow><reinforcement-learning><openai-gym><dqn>,1,2,,,,CC BY-SA 4.0,
195,66008062,1,66030093,,2021-02-02 10:41:21,,0,62,"<p>I am wondering how best to feed back the changes my DQN agent makes on its environment, back to itself.</p>
<p>I have a battery model whereby an agent can observe a time-series forecast of 17 steps, and 5 features. It then makes a decision on whether to charge or discharge.</p>
<p>I want to includes its current state of charge (empty, half full, full etc) in its observation space (i.e. somewhere within the (17,5) dataframes I am feeding it).</p>
<p>I have several options, I can either set a whole column to the state of charge value, a whole row, or I can flatten the whole dataframe and set one value to the state of charge value.</p>
<p>Is any of these unwise? It seem a little rudimentary to me to set a whole columns to a single value, but should it actually impact performance? I am wary of flattening the whole thing as I plan to use either conv or lstm layers (although the current model is just dense layers).</p>
",11664078,,,,,2021-02-03 14:59:11,Reinforcement learning DQN environment structure,<python><deep-learning><reinforcement-learning><dqn>,1,0,,,,CC BY-SA 4.0,
199,70861260,1,70927326,,2022-01-26 09:34:26,,1,1237,"<p>I would like to train a DQN Agent with Keras-rl. My environment has both multi-discrete action and observation spaces. I am adapting the code of this video: <a href=""https://www.youtube.com/watch?v=bD6V3rcr_54&amp;t=5s"" rel=""nofollow noreferrer"">https://www.youtube.com/watch?v=bD6V3rcr_54&amp;t=5s</a></p>
<p>Then, I am sharing my code</p>
<pre><code>class ShowerEnv(Env):
    def __init__(self, max_machine_states_vec, production_rates_vec, production_threshold, scheduling_horizon, operations_horizon = 100):
        &quot;&quot;&quot;
        Returns:
        self.action_space is a vector with the maximum production rate fro each machine, a binary call-to-maintenance and a binary call-to-schedule
        &quot;&quot;&quot;

        num_machines = len(max_machine_states_vec)
        assert len(max_machine_states_vec) == len(production_rates_vec), &quot;Machine states and production rates have different cardinality&quot;
        # Actions we can take, down, stay, up
        self.action_space = MultiDiscrete(production_rates_vec + num_machines*[2] + [2]) ### Action space is the production rate from 0 to N and the choice of scheduling
        # Temperature array
        self.observation_space = MultiDiscrete(max_machine_states_vec + [scheduling_horizon+2]) ### Observation space is the 0,...,L for each machine + the scheduling state including &quot;ns&quot; (None = &quot;ns&quot;)
        # Set start temp
        Code going on...
.
.
.
.
def build_model(states, actions):
    actions_number = reduce(lambda a,b: a*b, env.action_space.nvec)
    model = Sequential()    
    model.add(Dense(24, activation='relu', input_shape= (1, states[0]) ))
    model.add(Dense(24, activation='relu'))
    model.add(Dense(actions_number, activation='linear'))
    return model

def build_agent(model, actions):
    policy = BoltzmannQPolicy()
    memory = SequentialMemory(limit=50000, window_length=1)
    dqn = DQNAgent(model=model, memory=memory, policy=policy, 
                nb_actions=actions, nb_steps_warmup=10, target_model_update=1e-2)
    return dqn
.
.
.
.
states = env.observation_space.shape
actions_number = reduce(lambda a,b: a*b, env.action_space.nvec)

model = build_model(states, actions)
model.summary()

dqn = build_agent(model, actions)
dqn.compile(Adam(lr=1e-3), metrics=['mae'])
dqn.fit(env, nb_steps=50000, visualize=False, verbose=1)
</code></pre>
<p>After initializing with 2 elements, so 5 actions, I get the following error:</p>
<p><code>ValueError: Model output &quot;Tensor(&quot;dense_2/BiasAdd:0&quot;, shape=(None, 1, 32), dtype=float32)&quot; has invalid shape. DQN expects a model that has one dimension for each action, in this case [2 2 2 2 2]</code></p>
<p>How can I solve this. I am quite sure because I do not fully understand how to adapt the code in the video to a MultiDiscrete action space.
Thanks :)</p>
",12202087,,,,,2022-01-31 17:54:30,Training DQN Agent with Multidiscrete action space in gym,<openai-gym><dqn><keras-rl>,1,0,0,,,CC BY-SA 4.0,
203,72554263,1,72625097,,2022-06-09 03:00:54,,0,41,"<p>everyone!When I was doing dqn programming, I encountered some problems. This error says<br />
â€œ Userwarning: Using a target size (torch.Size([32,32])) that is different to the input size (torch.Size([32,1])).This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.</p>
<p>return F.mse_loss(input,target,reduction=self.reduction)&quot;</p>
<p>And  I don't know where the mistake is because I am new to RL . And some of these codes are borrowed from other people's codes, so I don't understand some places.</p>
<p>here are codes:</p>
<pre><code># hyperparameters
gamma = 0.9
TARGET_REPLACE_ITER = 20
memory_capability = 100    
batch_size = 32
learning_rate = 0.001
n_state = 5
n_action = 32 
</code></pre>
<p>neural network code:</p>
<pre><code>class NN(nn.Module):

def __init__(self, ):
    super(NN,self).__init__()
    self.fc1 = nn.Linear(n_state, 32)
    self.fc1.weight.data.normal_(0, 0.1)
    self.fc2 = nn.Linear(32,64)
    self.out = nn.Linear(64, n_action)
    self.out.weight.data.normal_(0, 0.1)

def forward(self, x):
    x = self.fc1(x)
    x = F.relu(x)
    x = self.fc2(x)
    x = F.relu(x)
    action_value = self.out(x)
    return action_value
</code></pre>
<p>agent code:</p>
<pre><code>class Agent(object):
    def __init__(self,):
        self.learn_step_counter = 0
        self.memory = np.zeros((memory_capability, n_state * 2 + 2))
        self.memory_cntr = 0
        self.eval_net, self.target_net = NN(), NN()
        self.loss_func = nn.MSELoss()
        self.optimizer = torch.optim.Adam(self.eval_net.parameters(), lr=learning_rate)


    def choose_action(self, state):       
        state = torch.unsqueeze(torch.FloatTensor(state),0)   # state is 1-Dim np.array,shape = (5,)
        if random.random() &lt; epsilon:         
            action = random.randint(0,len(stringlist) - 1)
        
        else:
            action_value = self.eval_net.forward(state)
            action = torch.max(action_value, 1)[1].numpy()[0]      
        return action

    def learn(self):   
        if self.learn_step_counter % TARGET_REPLACE_ITER == 0:
            self.target_net.load_state_dict(self.eval_net.state_dict())
        self.learn_step_counter += 1
        sample_index = np.random.choice(memory_capability, batch_size)
        b_memory = self.memory[sample_index, :]
        b_s = torch.FloatTensor(b_memory[:, :n_state])
        b_a = torch.LongTensor(b_memory[:, n_state:n_state + 1].astype(int))
        b_r = torch.FloatTensor(b_memory[:, n_state + 1:n_state + 2])
        b_s_ = torch.FloatTensor(b_memory[:, -n_state:])

        q_eval = self.eval_net(b_s).gather(1, b_a)  # shape (batch, 1)
        q_next = self.target_net(b_s_).detach()  
        q_target = b_r + gamma * q_next.max(1)[0]  # other people's code said the shape is (batch, 1)=(32,1),but when i ran ,it was (batch,batch)=(32,32),i don't know why
        loss = self.loss_func(q_eval, q_target)        
        self.optimizer.zero_grad()
        loss.backward()
        self.optimizer.step()

    def store_transition(self,state,action,reward,state_):
        transition = np.hstack((state,action,reward,state_))
        index = self.memory_cntr % memory_capability
        self.memory[index,:] = transition
        self.memory_cntr += 1
</code></pre>
<p>the problem is probably in learn(),but i don't know how to modify.I will appreciate it if someone can help me,thanks a lot</p>
",19150880,,,,,2022-06-16 12:20:47,how can i make target size equals input size in my DQN code?,<python><reinforcement-learning><dqn>,1,0,,,,CC BY-SA 4.0,
204,72564316,1,72872886,,2022-06-09 17:24:23,,0,66,"<p>Is there any way to get around this error? I have a model with a 15x15 input grid, which leads to two outputs. Each output has 15 possible values, which are x or y coordinates. I did this because it is significantly simpler than having 225 separate outputs for every location on the grid.
The problem is that when i try to train the model using this code:</p>
<pre><code>def build_agent(model,actions)
  policy = BoltzmannQPolicy()
  memory = SequentialMemory(limit=100000, window_length=1)
  dqn = DQNAgent(model=model, memory=memory,policy=policy,nb_actions=actions,nb_steps_warmup=100, target_model_update=1e-2)
  return(dqn)
dqn = build_agent(model, np.array([15,15]))
dqn.compile(Adam(learning_rate = 0.01), metrics=['mae'])
dqn.fit(env, nb_steps=10000, action_repetition=1, visualize=False, verbose=1,nb_max_episode_steps=10000)
plt.show()
</code></pre>
<p>I get the error: &quot;Model has more than one output. DQN expects a model that has a single output&quot;.
The model summary is below so you can see there are 2 output layers.</p>
<pre><code>Model: &quot;model_1&quot;
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_2 (InputLayer)           [(None, 1, 15, 15)]  0           []                               
                                                                                                  
 conv2d_2 (Conv2D)              (None, 12, 13, 13)   120         ['input_2[0][0]']                
                                                                                                  
 conv2d_3 (Conv2D)              (None, 10, 11, 3)    354         ['conv2d_2[0][0]']               
                                                                                                  
 flatten_1 (Flatten)            (None, 330)          0           ['conv2d_3[0][0]']               
                                                                                                  
 dropout_1 (Dropout)            (None, 330)          0           ['flatten_1[0][0]']              
                                                                                                  
 dense_2 (Dense)                (None, 15)           4965        ['dropout_1[0][0]']              
                                                                                                  
 dense_3 (Dense)                (None, 15)           4965        ['dropout_1[0][0]']              
                                                                                                  
==================================================================================================
Total params: 10,404
Trainable params: 10,404
Non-trainable params: 0
__________________________________________________________________________________________________
</code></pre>
<p>Standard Keras allows a model with multiple outputs using the functional api but from the errpr message i assume that feature is just not supported for Keras-rl? If thats true, is there any way to get around this issue?</p>
",15673855,,,,,2022-07-05 16:40:54,"Keras-rl ValueError""Model has more than one output. DQN expects a model that has a single output""",<keras><deep-learning><neural-network><dqn><keras-rl>,1,4,,,,CC BY-SA 4.0,
216,62535763,1,62536051,,2020-06-23 13:20:22,,0,273,"<p>I am trying to train a DQN Agent to solve AI Gym's Cartpole-v0 environment. I have started with this person's <a href=""https://github.com/the-deep-learners/TensorFlow-LiveLessons/blob/master/notebooks/cartpole_dqn.ipynb"" rel=""nofollow noreferrer"">implementation</a> just to get some hands-on experience. What I noticed is that during training, after many episodes the agent finds the solution and is able to keep the pole upright for the maximum amount of timesteps. However, after further training, the policy looks like it becomes more stochastic and it can't keep the pole upright anymore and goes in and out of a good policy. I'm pretty confused by this why wouldn't further training and experience help the agent? At episodes my epsilon for random action becomes very low, so it should be operating on just making the next prediction. So why does it on some training episodes fail to keep the pole upright and on others it succeeds?</p>
<p>Here is a picture of my reward-episode curve during the training process of the above linked implementation.</p>
<p><a href=""https://i.stack.imgur.com/YP6Px.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/YP6Px.png"" alt=""enter image description here"" /></a></p>
",6393275,,6393275,,2020-06-23 13:53:45,2020-06-23 13:53:45,Deep Q Learning agent finds solution then diverges again,<deep-learning><reinforcement-learning><dqn>,1,0,,,,CC BY-SA 4.0,
230,70382999,1,70405951,,2021-12-16 17:10:39,,1,559,"<p>I'm trying to implement a DQN. As a warm up I want to solve CartPole-v0 with a MLP consisting of two hidden layers along with input and output layers. The input is a 4 element array [cart position, cart velocity, pole angle, pole angular velocity] and output is an action value for each action (left or right).  I am not exactly implementing a DQN from the &quot;Playing Atari with DRL&quot; paper (no frame stacking for inputs etc).  I also made a few non standard choices like putting <code>done</code> and the target network prediction of action value in the experience replay, but those choices shouldn't affect learning.</p>
<p>In any case I'm having a lot of trouble getting the thing to work. No matter how long I train the agent it keeps predicting a higher value for one action over another, for example Q(s, Right)&gt; Q(s, Left) for all states s. Below is my learning code, my network definition, and some results I get from training</p>
<pre><code>class DQN:
    def __init__(self, env, steps_per_episode=200):
        self.env = env
        self.agent_network = MlpPolicy(self.env)
        self.target_network = MlpPolicy(self.env)
        self.target_network.load_state_dict(self.agent_network.state_dict())
        self.target_network.eval()
        self.optimizer = torch.optim.RMSprop(
            self.agent_network.parameters(), lr=0.005, momentum=0.95
        )
        self.replay_memory = ReplayMemory()
        self.gamma = 0.99
        self.steps_per_episode = steps_per_episode
        self.random_policy_stop = 1000
        self.start_learning_time = 1000
        self.batch_size = 32

    def learn(self, episodes):
        time = 0
        for episode in tqdm(range(episodes)):
            state = self.env.reset()
            for step in range(self.steps_per_episode):
                if time &lt; self.random_policy_stop:
                    action = self.env.action_space.sample()
                else:
                    action = select_action(self.env, time, state, self.agent_network)
                new_state, reward, done, _ = self.env.step(action)
                target_value_pred = predict_target_value(
                    new_state, reward, done, self.target_network, self.gamma
                )
                experience = Experience(
                    state, action, reward, new_state, done, target_value_pred
                )
                self.replay_memory.append(experience)
                if time &gt; self.start_learning_time:  # learning step
                    experience_batch = self.replay_memory.sample(self.batch_size)
                    target_preds = extract_value_predictions(experience_batch)
                    agent_preds = agent_batch_preds(
                        experience_batch, self.agent_network
                    )
                    loss = torch.square(agent_preds - target_preds).sum()
                    self.optimizer.zero_grad()
                    loss.backward()
                    self.optimizer.step()
                if time % 1_000 == 0:  # how frequently to update target net
                    self.target_network.load_state_dict(self.agent_network.state_dict())
                    self.target_network.eval()

                state = new_state
                time += 1

                if done:
                    break
</code></pre>
<pre><code>
def agent_batch_preds(experience_batch: list, agent_network: MlpPolicy):
    &quot;&quot;&quot;
    Calculate the agent action value estimates using the old states and the
    actual actions that the agent took at that step.
    &quot;&quot;&quot;
    old_states = extract_old_states(experience_batch)
    actions = extract_actions(experience_batch)
    agent_preds = agent_network(old_states)
    experienced_action_values = agent_preds.index_select(1, actions).diag()
    return experienced_action_values
</code></pre>
<pre><code>def extract_actions(experience_batch: list) -&gt; list:
    &quot;&quot;&quot;
    Extract the list of actions from experience replay batch and torchify
    &quot;&quot;&quot;
    actions = [exp.action for exp in experience_batch]
    actions = torch.tensor(actions)
    return actions
</code></pre>
<pre><code>class MlpPolicy(nn.Module):
    &quot;&quot;&quot;
    This class implements the MLP which will be used as the Q network. I only
    intend to solve classic control problems with this.
    &quot;&quot;&quot;

    def __init__(self, env):
        super(MlpPolicy, self).__init__()
        self.env = env
        self.input_dim = self.env.observation_space.shape[0]
        self.output_dim = self.env.action_space.n
        self.fc1 = nn.Linear(self.input_dim, 32)
        self.fc2 = nn.Linear(32, 128)
        self.fc3 = nn.Linear(128, 32)
        self.fc4 = nn.Linear(32, self.output_dim)

    def forward(self, x):
        if type(x) != torch.Tensor:
            x = torch.tensor(x).float()
        x = F.relu(self.fc1(x))
        x = F.relu(self.fc2(x))
        x = F.relu(self.fc3(x))
        x = self.fc4(x)
        return x
</code></pre>
<p>Learning results:</p>
<blockquote>
<p><img src=""https://i.stack.imgur.com/xL5qI.png"" alt="""" /></p>
</blockquote>
<p>Here I'm seeing one action always valued over the others (Q(right, s) &gt; Q(left, s)). It's also clear that the network is predicting the same action values for every state.</p>
<p>Does anyone have an idea about what's going on? I've done a lot of debugging and careful reading of the original papers (also thought about &quot;normalizing&quot; the observation space even though the velocities can be infinite) and could be missing something obvious at this point. I can include more code for the helper functions if that would be useful.</p>
",12435095,,6296561,,2021-12-22 15:55:39,2021-12-22 15:55:39,DQN predicts same action value for every state (cart pole),<python><deep-learning><pytorch><reinforcement-learning><dqn>,1,0,,,,CC BY-SA 4.0,
234,73162598,1,73162737,,2022-07-29 06:45:25,,1,42,"<p>I am try to train a DQN model with the following code. The GPU (cuda) usage is always lower than 25 percent. I know the tensorflow backend is consulting the GPU resources, but the usage is low. Is there any way I can improve the utilization of the GPU (When I train a CNN network, the GPU (cude) utilization is around 70 percent)?</p>
<p>How can I modify the code and make it run faster or use more GPU resources. Or it is very hard for the Dqn to work on a GPU?</p>
<pre><code>import numpy as np
from tensorflow.keras import Sequential
from tensorflow.keras.layers import Dense, Dropout, Conv2D, MaxPooling2D, Activation, Flatten
from tensorflow.keras.optimizers import Adam
import tensorflow as tf
from collections import deque
import time
import random
from tqdm import tqdm
import os
from PIL import Image
import cv2


DISCOUNT = 0.99
REPLAY_MEMORY_SIZE = 50_000  # How many last steps to keep for model training
MIN_REPLAY_MEMORY_SIZE = 1_000  # Minimum number of steps in a memory to start training
MINIBATCH_SIZE = 64  # How many steps (samples) to use for training
UPDATE_TARGET_EVERY = 5  # Terminal states (end of episodes)
MODEL_NAME = '2x256'
MIN_REWARD = -200  # For model save
MEMORY_FRACTION = 0.20

# Environment settings
EPISODES = 20_000

# Exploration settings
epsilon = 1  # not a constant, going to be decayed
EPSILON_DECAY = 0.99975
MIN_EPSILON = 0.001

#  Stats settings
AGGREGATE_STATS_EVERY = 50  # episodes
SHOW_PREVIEW = False


class Blob:
    def __init__(self, size):
        self.size = size
        self.x = np.random.randint(0, size)
        self.y = np.random.randint(0, size)

    def __str__(self):
        return f&quot;Blob ({self.x}, {self.y})&quot;

    def __sub__(self, other):
        return (self.x-other.x, self.y-other.y)

    def __eq__(self, other):
        return self.x == other.x and self.y == other.y

    def action(self, choice):
        '''
        Gives us 9 total movement options. (0,1,2,3,4,5,6,7,8)
        '''
        if choice == 0:
            self.move(x=1, y=1)
        elif choice == 1:
            self.move(x=-1, y=-1)
        elif choice == 2:
            self.move(x=-1, y=1)
        elif choice == 3:
            self.move(x=1, y=-1)

        elif choice == 4:
            self.move(x=1, y=0)
        elif choice == 5:
            self.move(x=-1, y=0)

        elif choice == 6:
            self.move(x=0, y=1)
        elif choice == 7:
            self.move(x=0, y=-1)

        elif choice == 8:
            self.move(x=0, y=0)

    def move(self, x=False, y=False):

        # If no value for x, move randomly
        if not x:
            self.x += np.random.randint(-1, 2)
        else:
            self.x += x

        # If no value for y, move randomly
        if not y:
            self.y += np.random.randint(-1, 2)
        else:
            self.y += y

        # If we are out of bounds, fix!
        if self.x &lt; 0:
            self.x = 0
        elif self.x &gt; self.size-1:
            self.x = self.size-1
        if self.y &lt; 0:
            self.y = 0
        elif self.y &gt; self.size-1:
            self.y = self.size-1


class BlobEnv:
    SIZE = 10
    RETURN_IMAGES = True
    MOVE_PENALTY = 1
    ENEMY_PENALTY = 300
    FOOD_REWARD = 25
    OBSERVATION_SPACE_VALUES = (SIZE, SIZE, 3)  # 4
    ACTION_SPACE_SIZE = 9
    PLAYER_N = 1  # player key in dict
    FOOD_N = 2  # food key in dict
    ENEMY_N = 3  # enemy key in dict
    # the dict! (colors)
    d = {1: (255, 175, 0),
         2: (0, 255, 0),
         3: (0, 0, 255)}

    def reset(self):
        self.player = Blob(self.SIZE)
        self.food = Blob(self.SIZE)
        while self.food == self.player:
            self.food = Blob(self.SIZE)
        self.enemy = Blob(self.SIZE)
        while self.enemy == self.player or self.enemy == self.food:
            self.enemy = Blob(self.SIZE)

        self.episode_step = 0

        if self.RETURN_IMAGES:
            observation = np.array(self.get_image())
        else:
            observation = (self.player-self.food) + (self.player-self.enemy)
        return observation

    def step(self, action):
        self.episode_step += 1
        self.player.action(action)

        #### MAYBE ###
        #enemy.move()
        #food.move()
        ##############

        if self.RETURN_IMAGES:
            new_observation = np.array(self.get_image())
        else:
            new_observation = (self.player-self.food) + (self.player-self.enemy)

        if self.player == self.enemy:
            reward = -self.ENEMY_PENALTY
        elif self.player == self.food:
            reward = self.FOOD_REWARD
        else:
            reward = -self.MOVE_PENALTY

        done = False
        if reward == self.FOOD_REWARD or reward == -self.ENEMY_PENALTY or self.episode_step &gt;= 200:
            done = True

        return new_observation, reward, done

    def render(self):
        img = self.get_image()
        img = img.resize((300, 300))  # resizing so we can see our agent in all its glory.
        cv2.imshow(&quot;image&quot;, np.array(img))  # show it!
        cv2.waitKey(1)

    # FOR CNN #
    def get_image(self):
        env = np.zeros((self.SIZE, self.SIZE, 3), dtype=np.uint8)  # starts an rbg of our size
        env[self.food.x][self.food.y] = self.d[self.FOOD_N]  # sets the food location tile to green color
        env[self.enemy.x][self.enemy.y] = self.d[self.ENEMY_N]  # sets the enemy location to red
        env[self.player.x][self.player.y] = self.d[self.PLAYER_N]  # sets the player tile to blue
        img = Image.fromarray(env, 'RGB')  # reading to rgb. Apparently. Even tho color definitions are bgr. ???
        return img


env = BlobEnv()

# For stats
ep_rewards = [-200]

# For more repetitive results
random.seed(1)
np.random.seed(1)
tf.compat.v1.set_random_seed(1)

# Memory fraction, used mostly when trai8ning multiple agents
#gpu_options = tf.GPUOptions(per_process_gpu_memory_fraction=MEMORY_FRACTION)
#backend.set_session(tf.Session(config=tf.ConfigProto(gpu_options=gpu_options)))

# Create models folder
if not os.path.isdir('models'):
    os.makedirs('models')




# Agent class
class DQNAgent:
    def __init__(self):

        # Main model
        self.model = self.create_model()

        # Target network
        self.target_model = self.create_model()
        self.target_model.set_weights(self.model.get_weights())

        # An array with last n steps for training
        self.replay_memory = deque(maxlen=REPLAY_MEMORY_SIZE)


        # Used to count when to update target network with main network's weights
        self.target_update_counter = 0

    def create_model(self):
        model = Sequential()

        model.add(Conv2D(256, (3, 3), input_shape=env.OBSERVATION_SPACE_VALUES))  # OBSERVATION_SPACE_VALUES = (10, 10, 3) a 10x10 RGB image.
        model.add(Activation('relu'))
        model.add(MaxPooling2D(pool_size=(2, 2)))
        model.add(Dropout(0.2))

        model.add(Conv2D(256, (3, 3)))
        model.add(Activation('relu'))
        model.add(MaxPooling2D(pool_size=(2, 2)))
        model.add(Dropout(0.2))

        model.add(Flatten())  # this converts our 3D feature maps to 1D feature vectors
        model.add(Dense(64))

        model.add(Dense(env.ACTION_SPACE_SIZE, activation='linear'))  # ACTION_SPACE_SIZE = how many choices (9)
        model.compile(loss=&quot;mse&quot;, optimizer=Adam(lr=0.001), metrics=['accuracy'])
        return model

    # Adds step's data to a memory replay array
    # (observation space, action, reward, new observation space, done)
    def update_replay_memory(self, transition):
        self.replay_memory.append(transition)

    # Trains main network every step during episode
    def train(self, terminal_state, step):

        # Start training only if certain number of samples is already saved
        if len(self.replay_memory) &lt; MIN_REPLAY_MEMORY_SIZE:
            return

        # Get a minibatch of random samples from memory replay table
        minibatch = random.sample(self.replay_memory, MINIBATCH_SIZE)

        # Get current states from minibatch, then query NN model for Q values
        current_states = np.array([transition[0] for transition in minibatch])/255
        current_qs_list = self.model.predict(current_states, verbose=0)

        # Get future states from minibatch, then query NN model for Q values
        # When using target network, query it, otherwise main network should be queried
        new_current_states = np.array([transition[3] for transition in minibatch])/255
        future_qs_list = self.target_model.predict(new_current_states, verbose=0)

        X = []
        y = []

        # Now we need to enumerate our batches
        for index, (current_state, action, reward, new_current_state, done) in enumerate(minibatch):

            # If not a terminal state, get new q from future states, otherwise set it to 0
            # almost like with Q Learning, but we use just part of equation here
            if not done:
                max_future_q = np.max(future_qs_list[index])
                new_q = reward + DISCOUNT * max_future_q
            else:
                new_q = reward

            # Update Q value for given state
            current_qs = current_qs_list[index]
            current_qs[action] = new_q

            # And append to our training data
            X.append(current_state)
            y.append(current_qs)

        # Fit on all samples as one batch, log only on terminal state
        self.model.fit(np.array(X)/255, np.array(y), batch_size=MINIBATCH_SIZE, verbose=0, shuffle=False)

        # Update target network counter every episode
        if terminal_state:
            self.target_update_counter += 1

        # If counter reaches set value, update target network with weights of main network
        if self.target_update_counter &gt; UPDATE_TARGET_EVERY:
            self.target_model.set_weights(self.model.get_weights())
            self.target_update_counter = 0

    # Queries main network for Q values given current observation space (environment state)
    def get_qs(self, state):
        return self.model.predict(np.array(state).reshape(-1, *state.shape)/255)[0]


agent = DQNAgent()

# Iterate over episodes
for episode in tqdm(range(1, EPISODES + 1), ascii=True, unit='episodes'):


    # Restarting episode - reset episode reward and step number
    episode_reward = 0
    step = 1

    # Reset environment and get initial state
    current_state = env.reset()

    # Reset flag and start iterating until episode ends
    done = False
    while not done:

        # This part stays mostly the same, the change is to query a model for Q values
        if np.random.random() &gt; epsilon:
            # Get action from Q table
            action = np.argmax(agent.get_qs(current_state))
        else:
            # Get random action
            action = np.random.randint(0, env.ACTION_SPACE_SIZE)

        new_state, reward, done = env.step(action)

        # Transform new continous state to new discrete state and count reward
        episode_reward += reward

        if SHOW_PREVIEW and not episode % AGGREGATE_STATS_EVERY:
            env.render()

        # Every step we update replay memory and train main network
        agent.update_replay_memory((current_state, action, reward, new_state, done))
        agent.train(done, step)

        current_state = new_state
        step += 1

    # Append episode reward to a list and log stats (every given number of episodes)
    ep_rewards.append(episode_reward)
    if not episode % AGGREGATE_STATS_EVERY or episode == 1:
        average_reward = sum(ep_rewards[-AGGREGATE_STATS_EVERY:])/len(ep_rewards[-AGGREGATE_STATS_EVERY:])
        min_reward = min(ep_rewards[-AGGREGATE_STATS_EVERY:])
        max_reward = max(ep_rewards[-AGGREGATE_STATS_EVERY:])

        # Save model, but only when min reward is greater or equal a set value
        if min_reward &gt;= MIN_REWARD:
            agent.model.save(f'models/{MODEL_NAME}__{max_reward:_&gt;7.2f}max_{average_reward:_&gt;7.2f}avg_{min_reward:_&gt;7.2f}min__{int(time.time())}.model')

    # Decay epsilon
    if epsilon &gt; MIN_EPSILON:
        epsilon *= EPSILON_DECAY
        epsilon = max(MIN_EPSILON, epsilon)

</code></pre>
",19645691,,,,,2022-07-29 07:18:51,GPU utilization is low when training Deep Q Network (DQN),<python><reinforcement-learning><dqn>,1,0,,,,CC BY-SA 4.0,
235,71558407,1,71579947,,2022-03-21 13:27:58,,0,298,"<p>I am trying to create a DQN model for mario environment. But when I try to create the model it gives me this error:</p>
<blockquote>
<p>MemoryError: Unable to allocate 229. GiB for an array with shape (1000000, 1, 4, 240, 256) and data type uint8</p>
</blockquote>
<p>This is the code for creating the model:</p>
<pre><code>model = DQN('CnnPolicy', env, verbose=1, tensorboard_log=LOG_DIR, learning_rate=0.000001)
</code></pre>
<p>I am using Jupyter notebook for this project.</p>
",15527058,,,,,2022-03-22 23:02:04,Unable to allocate memory with array shape to create reinforcement learning model,<python><model><reinforcement-learning><dqn><stable-baselines>,1,0,,,,CC BY-SA 4.0,
242,71897010,1,71903002,,2022-04-16 19:47:19,,0,93,"<p>I am working on a DQN training model of the game &quot;CartPole-v1&quot;. In this model, the system did not remind any error information in the terminal. However, The result evaluation got worse.This is the output data:</p>
<pre><code>episode: 85 score: 18 avarage score: 20.21 epsilon: 0.66
episode: 86 score: 10 avarage score: 20.09 epsilon: 0.66
episode: 87 score: 9 avarage score: 19.97 epsilon: 0.66
episode: 88 score: 14 avarage score: 19.90 epsilon: 0.65
episode: 89 score: 9 avarage score: 19.78 epsilon: 0.65
episode: 90 score: 10 avarage score: 19.67 epsilon: 0.65
episode: 91 score: 14 avarage score: 19.60 epsilon: 0.64
episode: 92 score: 13 avarage score: 19.53 epsilon: 0.64
episode: 93 score: 17 avarage score: 19.51 epsilon: 0.64
episode: 94 score: 10 avarage score: 19.40 epsilon: 0.63
episode: 95 score: 16 avarage score: 19.37 epsilon: 0.63
episode: 96 score: 16 avarage score: 19.33 epsilon: 0.63
episode: 97 score: 10 avarage score: 19.24 epsilon: 0.62
episode: 98 score: 13 avarage score: 19.17 epsilon: 0.62
episode: 99 score: 12 avarage score: 19.10 epsilon: 0.62
episode: 100 score: 11 avarage score: 19.02 epsilon: 0.61
episode: 101 score: 17 avarage score: 19.00 epsilon: 0.61
episode: 102 score: 11 avarage score: 18.92 epsilon: 0.61
episode: 103 score: 9 avarage score: 18.83 epsilon: 0.61
</code></pre>
<p>I'll show my code here. Firstly I constructed a neuron network:</p>
<pre><code>import random
from torch.autograd import Variable
import torch as th
import torch.nn as nn
import torch.nn.functional as F
import numpy as np
import matplotlib.pyplot as plt
import gym
from collections import deque

# construct a neuron network (prepare for step1, step3.2 and 3.3)
class DQN(nn.Module):
    def __init__(self, s_space, a_space) -&gt; None:

        # inherit from DQN class in pytorch
        super(DQN, self).__init__()
        self.fc1 = nn.Linear(s_space, 360)
        self.fc2 = nn.Linear(360, 360)
        self.fc3 = nn.Linear(360, a_space)


    # DNN operation architecture
    def forward(self, input):
        out = self.fc1(input)
        out = F.relu(out)
        out = self.fc2(out)
        out = F.relu(out)
        out = self.fc3(out)
        return out
</code></pre>
<p>Instead of newing an agent class, I directly created the select function, which is used for select the corresponding action according to epsilon, and the back propagation function by gradient globally:</p>
<pre><code># define the action selection according to epsilon using neuron network (prepare for step3.2)
def select(net, epsilon, env, state):

    # randomly select an action if not greedy
    if(np.random.rand() &lt;= epsilon):
        action = env.action_space.sample()
        return action
    # select the maximum reward action by NN and the given state if greedy
    else:
        actions = net(Variable(th.Tensor(state))).detach().numpy()
        action = np.argmax(actions[0])
        return action
</code></pre>
<p>This is the back propagation function and the decreasing of epsilon:</p>
<pre><code># using loss function to improve neuron network (prepare for step3.3)
def backprbgt(net, store, batch_size, gamma, learning_rate):
   
    # step1: create loss function and Adam optimizer
    loss_F = nn.MSELoss()
    opt = th.optim.Adam(net.parameters(),lr=learning_rate)

    # step2: extract the sample in memory
    materials = random.sample(store, batch_size)

    # step3: Calculate arguments of loss function:

    for t in materials:

        Q_value = net(Variable(th.Tensor(t[0])))

        # step3.1 Calculate tgt_Q_value in terms of greedy:
        reward = t[3]
        if(t[4] == True):
            tgt = reward
        else:
            tgt = reward + gamma * np.amax(net(Variable(th.Tensor(t[2]))).detach().numpy()[0])
        # print(tgt)
        # tgt_Q_value = Variable(th.Tensor([[float(tgt)]]), requires_grad=True)

        # print(&quot;Q_value:&quot;,Q_value)
        Q_value[0][t[1]] = tgt
        tgt_Q_value = Variable(th.Tensor(Q_value))
        # print(&quot;tgt:&quot;,tgt_Q_value)

        # step3.2 Calculate evlt_Q_value
        
        # index = th.tensor([[t[1]]])
        # evlt_Q_value = Q_value.gather(1,index)  # gather tgt into the corresponding action
        evlt_Q_value = net(Variable(th.Tensor(t[0])))
        # print(&quot;evlt:&quot;,evlt_Q_value)


        # step4: backward and optimization
        loss = loss_F(evlt_Q_value, tgt_Q_value)
        # print(loss)
        opt.zero_grad()
        loss.backward()
        opt.step()

# step5: decrease epsilon for exploitation
def decrease(epsilon, min_epsilon, decrease_rate):
    if(epsilon &gt; min_epsilon):
        epsilon *= decrease_rate
</code></pre>
<p>After that, the parameters and training progress are like this:</p>
<pre><code># training process

# step 1: set parameters and NN
episode = 1500
epsilon = 1.0
min_epsilon = 0.01
dr = 0.995
gamma = 0.9
lr = 0.001
batch_size = 40
memory_store = deque(maxlen=1500)

# step 2: define game category and associated states and actions
env = gym.make(&quot;CartPole-v1&quot;)
s_space = env.observation_space.shape[0]
a_space = env.action_space.n

net = DQN(s_space, a_space)
score = 0

# step 3: trainning
for e in range(0, episode):

    # step3.1: at the start of each episode, the current result should be refreshed

    # set initial state matrix
    s = env.reset().reshape(-1, s_space)

    # step3.2: iterate the state and action
    for run in range(500):

        # select action and get the next state according to current state &quot;s&quot;
        a = select(net, epsilon, env, s)
        obs, reward, done, info = env.step(a)

        next_s = obs.reshape(-1,s_space)
        s = next_s

        score += 1

        if(done == True):
            reward = -10.0
            memory_store.append((s,a,next_s,reward,done))
            avs = score / (e+1)
            print(&quot;episode:&quot;, e+1, &quot;score:&quot;, run+1, &quot;avarage score: {:.2f}&quot;.format(avs), &quot;epsilon: {:.2}&quot;.format(epsilon))
            break

        # safe sample data
        memory_store.append((s, a, next_s, reward, done))

        if(run == 499):
            print(&quot;episode:&quot;, e+1, &quot;score:&quot;, run+1, &quot;avarage score:&quot;, avs)

    # step3.3 whenever the episode reach the integer time of batch size, 
    # we should backward to implore the NN
    if(len(memory_store) &gt; batch_size):
        backprbgt(net, memory_store, batch_size, gamma, lr) # here we need a backprbgt function to backward
        if(epsilon &gt; min_epsilon):
            epsilon = epsilon * dr
</code></pre>
<p>In the entire progress of training, there was no error or exception reminds. However, instead of the score increasing, the model performed lower score in the later steps. I think the theory of this model is correct but cannot find where the error appears although I tried lots of methods improving my code, including rechecking the input arguments of network, modifing the data structure of two arguments of loss function, etc. I paste my code here and hope to get some help on how to fix it. Thanks!</p>
",17548063,,17548063,,2022-04-17 07:26:52,2022-04-17 15:08:22,The DQN model cannot correctly come out the expected scores,<deep-learning><pytorch><reinforcement-learning><openai-gym><dqn>,1,4,,,,CC BY-SA 4.0,
245,71598690,1,71611353,,2022-03-24 07:46:33,,1,335,"<p>I tried to custom environment with a reinforcement learning(RL) project.</p>
<p>Some examples such as ping-pong, Aarti, Super-Mario, in this case, action, and observation space really small.</p>
<p>But, my project action, observation space is really huge size better than some examples.</p>
<p>And, I will use the space for at least 5000+ actions and observations.</p>
<p>Then, how can I effectively handle this massive amount of action and observation?</p>
<p>Currently, I am using Q-table learning, so I use a wrapper function to handle it.</p>
<p>But this seems to be very ineffective.</p>
",18469670,,18159334,,2022-03-30 14:01:18,2022-03-30 14:01:18,"Question about the reinforcement learning action, observation space size",<reinforcement-learning><dqn>,1,1,,,,CC BY-SA 4.0,
257,58440479,1,58442845,,2019-10-17 20:43:37,,0,37,"<p>On Microsoft's documentation page here: <a href=""https://learn.microsoft.com/en-us/windows/uwp/design/style/segoe-ui-symbol-font"" rel=""nofollow noreferrer"">https://learn.microsoft.com/en-us/windows/uwp/design/style/segoe-ui-symbol-font</a> it says the MergeCall symbol has unicode identifier EA3C. However, it appears to not work.</p>

<p>Putting in the xaml </p>

<pre><code>&lt;FontIcon FontFamily=""Segoe MDL2 Assets"" Glyph=""&amp;#xEA3C;""/&gt;
</code></pre>

<p>produces the unknown block.</p>

<p>Would their documentation be wrong? If so, what is the correct code? And how did you find the correct code?</p>

<p>Info, if it matters, I am on Windows 10 version 1809.</p>
",3047977,,,,,2019-10-18 06:38:55,"Is Microsoft's documentation wrong regarding the MergeCall symbol, unicode identifier EA3C?",<uwp-xaml>,1,0,,,,CC BY-SA 4.0,
258,60869267,1,60870380,,2020-03-26 14:13:42,,0,356,"<p>So right now I am taking the painful dive of exploring deep learning using Deep Learning 4j specifically RL4j and reinforcement learning. I have been relatively unsuccessful in teaching my computer how to play snake but I persevere. </p>

<p>Anyway so I have been running into a problem that I can't solve I'll set my program to run while I go to sleep or am at work (Yes I work in an essential industry) and when I check back it has thrown this error on all running threads and the program has completely stopped, mind you this usually happens about an hour into training.</p>

<pre><code>Exception in thread ""Thread-8"" java.lang.RuntimeException: Output from network is not a probability distribution: [[         ?,         ?,         ?]]
at org.deeplearning4j.rl4j.policy.ACPolicy.nextAction(ACPolicy.java:82)
at org.deeplearning4j.rl4j.policy.ACPolicy.nextAction(ACPolicy.java:37)
at org.deeplearning4j.rl4j.learning.async.AsyncThreadDiscrete.trainSubEpoch(AsyncThreadDiscrete.java:96)
at org.deeplearning4j.rl4j.learning.async.AsyncThread.handleTraining(AsyncThread.java:144)
at org.deeplearning4j.rl4j.learning.async.AsyncThread.run(AsyncThread.java:121)
</code></pre>

<p>Here is how I am setting up my network </p>

<pre><code>    private static A3CDiscrete.A3CConfiguration CARTPOLE_A3C =
        new A3CDiscrete.A3CConfiguration(
                (new java.util.Random()).nextInt(),            //Random seed
                220,            //Max step By epoch
                500000,         //Max step
                6,              //Number of threads
                50,              //t_max
                75,             //num step noop warmup
                0.1,           //reward scaling
                0.987,           //gamma
                1.0           //td-error clipping
        );


private static final ActorCriticFactorySeparateStdDense.Configuration CARTPOLE_NET_A3C =  ActorCriticFactorySeparateStdDense.Configuration
.builder().updater(new Adam(.005)).l2(.01).numHiddenNodes(32).numLayer(3).build();
</code></pre>

<p>Also the Input to my network is the entire grid for my snake game 16x16 put into a single double array.</p>

<p>Incase it has something to do with my reward function here is that </p>

<pre><code>if(!snake.inGame()) {
        return -5.3; //snake dies 
    }
    if(snake.gotApple()) {
        return 5.0+.37*(snake.getLength()); //snake gets apple
    }
    return 0; //survives
</code></pre>

<p><strong>My Question is</strong>
How do stop this error from occurring? 
I truly have no idea what is happening and its been making building my network rather difficult, yes I have already checked the web for answers all that comes up is like 2 GitHub tickets from 2018.</p>

<p>If it's of interest so you don't have to go digging here is the function from ACPolicy that is throwing the error </p>

<pre><code> public Integer nextAction(INDArray input) {
    INDArray output = actorCritic.outputAll(input)[1];
    if (rnd == null) {
        return Learning.getMaxAction(output);
    }
    float rVal = rnd.nextFloat();
    for (int i = 0; i &lt; output.length(); i++) {
        //System.out.println(i + "" "" + rVal + "" "" + output.getFloat(i));
        if (rVal &lt; output.getFloat(i)) {
            return i;
        } else
            rVal -= output.getFloat(i);
    }

    throw new RuntimeException(""Output from network is not a probability distribution: "" + output);
}
</code></pre>

<p>Any help that you can offer is greatly appreciated</p>
",7206778,,,,,2020-03-26 15:11:15,RL4J A3C DeepLearning Throwing a Output from network is not a probability distribution,<java><java-8><deep-learning><deeplearning4j>,1,0,,,,CC BY-SA 4.0,
262,59542414,1,59542686,,2019-12-31 09:52:13,,2,6342,"<p>I am using <code>Angular 8, tslint 5.15 &amp; typescript v3</code></p>

<p>I am reading file as <code>ArryaBuffer</code> using below code</p>

<pre><code>const reader = new FileReader();
    reader.readAsArrayBuffer(&lt;FileObject&gt;);
    reader.onload = () =&gt; {
      this.uploadedData= new Uint8Array(reader.result as ArrayBuffer);
    }
</code></pre>

<p>Now when i pass this <code>uploadedData</code> into API, I am converting into byteArray using below fucntion.</p>

<pre><code>convertLicenseToByteArray(uploadedData) {
    const bytesArray = [];
    for (const i of uploadedData) {
      bytesArray.push(i);
    }
    return bytesArray;
  }
</code></pre>

<p>The above code is giving error in ie11,</p>

<blockquote>
  <p>ERROR TypeError: Object doesn't support property or method
  'Symbol(Symbol.iterator)_a.srxqdvyfxlx'</p>
</blockquote>

<p>I tried to search on net and found that may be i need to add <code>babel-polyfill</code> but it's not working for me.</p>

<p>Any help?</p>
",3331076,,5535245,,2019-12-31 17:12:53,2020-05-13 12:41:01,IE11 Object doesn't support property or method 'Symbol(Symbol.iterator)_a.2p3bca3ct9h,<javascript><angular><internet-explorer><babel-polyfill>,4,1,,,,CC BY-SA 4.0,
268,46244514,1,46260427,,2017-09-15 17:05:47,,0,358,"<p>I tried using decimal and hexadecimal strings to create a <code>CFDataRef</code>, but what I created not what I need. I need a <code>CFDataRef</code> like ""<code>&lt;b1a3c3 d4b5&gt;</code>"", 
Here is my code:</p>

<pre><code>CFDataRef abc = CFDataCreate(kCFAllocatorDefault, ""b034958b0c6f"",strlen(""b034958b0c6f""));
CFDataRef abc = CFDataCreate(kCFAllocatorDefault, ""4294967295"", strlen(""4294967295""));
</code></pre>
",8520495,,1801544,,2017-09-15 17:13:53,2017-09-17 03:43:07,"What kind of CFDataRef is printed out like ""<b1a3c3 d4b5>""?",<ios><objective-c><macos>,1,3,,,,CC BY-SA 3.0,
270,42875684,1,42913086,,2017-03-18 14:36:58,,2,464,"<p>The A3C Algorithm (and N-Step Q Learning) updates the globaly shared network once every N timesteps. N is usually pretty small, 5 or 20 as far as I remember.</p>

<p>Wouldn't it be possible to set N to infinity, meaning that the networks are only trained at the end of an episode? I do not argue that it is necessarily better - tough, for me it sounds like it could be - but at least it should not be a lot worse, right?</p>

<p>The lacking asynchronous training based on the asynchronous exploration of the enviroment by multiple agents in different enviroments, and therefore the stabilization of the training procedure without replay memory, might be a problem if the training is done sequentially (as in: for each worker thread, train the network on the whole observed SAR-sequence). Tough, the training could still be done asynchronously with sub-sequences, it would only make training with stateful LSTMs a little bit more complicated.</p>

<p>The reason why I am asking is the ""Evolution Strategies as a Scalable Alternative to Reinforcement Learning"" paper. To compare it to algorithms like A3C, it would make more sense - from a code engineering point of view - to train both algorithms in the same episodic way.</p>
",6380584,,,,,2017-03-20 20:12:10,Is it feasibly to train an A3C algorithm in an episodic context?,<tensorflow><deep-learning><reinforcement-learning><q-learning>,1,0,0,,,CC BY-SA 3.0,
271,67458325,1,67458685,,2021-05-09 13:34:57,,0,2122,"<p>I found the code below at <a href=""https://www.bogotobogo.com/VideoStreaming/YouTube/youtube-dl-embedding.php"" rel=""nofollow noreferrer"">Youtube download using Youtube-dl embedded with Python - 2020</a></p>
<p>After I ran <code>pip3 list</code> I saw <code>youtube-dl 2020.3.24</code> in the list.</p>
<p>However, when I run...</p>
<pre><code># ydl1.py
from __future__ import unicode_literals
import youtube_dl

ydl_opts = {}
with youtube_dl.YoutubeDL(ydl_opts) as ydl:
    ydl.download(['https://www.youtube.com/watch?v=dP15zlyra3c'])
</code></pre>
<p>I see the following error...</p>
<pre><code>[youtube] dP15zlyra3c: Downloading webpage
ERROR: dP15zlyra3c: YouTube said: Unable to extract video data
Traceback (most recent call last):
  File &quot;/usr/lib/python3/dist-packages/youtube_dl/YoutubeDL.py&quot;, line 797, in extract_info
    ie_result = ie.extract(url)
  File &quot;/usr/lib/python3/dist-packages/youtube_dl/extractor/common.py&quot;, line 530, in extract
    ie_result = self._real_extract(url)
  File &quot;/usr/lib/python3/dist-packages/youtube_dl/extractor/youtube.py&quot;, line 1788, in _real_extract
    raise ExtractorError(
youtube_dl.utils.ExtractorError: dP15zlyra3c: YouTube said: Unable to extract video data

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File &quot;ydl1.py&quot;, line 7, in &lt;module&gt;
    ydl.download(['https://www.youtube.com/watch?v=dP15zlyra3c'])
  File &quot;/usr/lib/python3/dist-packages/youtube_dl/YoutubeDL.py&quot;, line 2018, in download
    res = self.extract_info(
  File &quot;/usr/lib/python3/dist-packages/youtube_dl/YoutubeDL.py&quot;, line 820, in extract_info
    self.report_error(compat_str(e), e.format_traceback())
  File &quot;/usr/lib/python3/dist-packages/youtube_dl/YoutubeDL.py&quot;, line 625, in report_error
    self.trouble(error_message, tb)
  File &quot;/usr/lib/python3/dist-packages/youtube_dl/YoutubeDL.py&quot;, line 595, in trouble
    raise DownloadError(message, exc_info)
youtube_dl.utils.DownloadError: ERROR: dP15zlyra3c: YouTube said: Unable to extract video data
y@y-peppy:~/.config/autokey$
</code></pre>
<p>What am I doing wrong?</p>
",15661404,,,,,2021-05-09 14:12:56,ERROR: dP15zlyra3c: YouTube said: Unable to extract video data,<python-3.x>,1,0,,,,CC BY-SA 4.0,
272,44045999,1,44320095,,2017-05-18 11:04:11,,0,656,"<p>When I contruct a LSTM with tf.nn.dynamic_rnn, the problem about name_scope lead to its fail and the platform is WINDOWS 10. I can not resolve it and any help would be appreciated.</p>

<p>Thanks very much!</p>

<p>The whole err is as following:</p>

<p>Traceback (most recent call last):</p>

<pre><code>  File ""a3c_prediction.py"", line 157, in &lt;module&gt;
    main()
  File ""a3c_prediction.py"", line 88, in main
    global_network = ACLSTMNetwork(ACTION_SIZE, device)
  File ""C:\Users\xjZhan\Desktop\a3c_predication\a3c_net_lstm.py"", line 139, in __init__
    scope = scope)
  File ""C:\Program Install\Anaconda3-4.1.0\lib\site-packages\tensorflow\python\ops\rnn.py"", line 553, in dynamic_rnn
    dtype=dtype)
  File ""C:\Program Install\Anaconda3-4.1.0\lib\site-packages\tensorflow\python\ops\rnn.py"", line 720, in _dynamic_rnn_loop
    swap_memory=swap_memory)
  File ""C:\Program Install\Anaconda3-4.1.0\lib\site-packages\tensorflow\python\ops\control_flow_ops.py"", line 2623, in while_loop
    result = context.BuildLoop(cond, body, loop_vars, shape_invariants)
  File ""C:\Program Install\Anaconda3-4.1.0\lib\site-packages\tensorflow\python\ops\control_flow_ops.py"", line 2456, in BuildLoop
    pred, body, original_loop_vars, loop_vars, shape_invariants)
  File ""C:\Program Install\Anaconda3-4.1.0\lib\site-packages\tensorflow\python\ops\control_flow_ops.py"", line 2406, in _BuildLoop
    body_result = body(*packed_vars_for_body)
  File ""C:\Program Install\Anaconda3-4.1.0\lib\site-packages\tensorflow\python\ops\rnn.py"", line 703, in _time_step
    skip_conditionals=True)
  File ""C:\Program Install\Anaconda3-4.1.0\lib\site-packages\tensorflow\python\ops\rnn.py"", line 177, in _rnn_step
    new_output, new_state = call_cell()
  File ""C:\Program Install\Anaconda3-4.1.0\lib\site-packages\tensorflow\python\ops\rnn.py"", line 691, in &lt;lambda&gt;
    call_cell = lambda: cell(input_t, state)
  File ""C:\Program Install\Anaconda3-4.1.0\lib\site-packages\tensorflow\contrib\rnn\python\ops\core_rnn_cell_impl.py"", line 241, in __call__
    concat = _linear([inputs, h], 4 * self._num_units, True)
  File ""C:\Program Install\Anaconda3-4.1.0\lib\site-packages\tensorflow\contrib\rnn\python\ops\core_rnn_cell_impl.py"", line 1044, in _linear
    _WEIGHTS_VARIABLE_NAME, [total_arg_size, output_size], dtype=dtype)
  File ""C:\Program Install\Anaconda3-4.1.0\lib\site-packages\tensorflow\python\ops\variable_scope.py"", line 1049, in get_variable
    use_resource=use_resource, custom_getter=custom_getter)
  File ""C:\Program Install\Anaconda3-4.1.0\lib\site-packages\tensorflow\python\ops\variable_scope.py"", line 948, in get_variable
    use_resource=use_resource, custom_getter=custom_getter)
  File ""C:\Program Install\Anaconda3-4.1.0\lib\site-packages\tensorflow\python\ops\variable_scope.py"", line 356, in get_variable
    validate_shape=validate_shape, use_resource=use_resource)
  File ""C:\Program Install\Anaconda3-4.1.0\lib\site-packages\tensorflow\python\ops\variable_scope.py"", line 341, in _true_getter
    use_resource=use_resource)
  File ""C:\Program Install\Anaconda3-4.1.0\lib\site-packages\tensorflow\python\ops\variable_scope.py"", line 671, in _get_single_variable
    ""VarScope?"" % name)
</code></pre>

<p><strong>ValueError: Variable A3C_net/basic_lstm_cell/weights does not exist, or was not created with tf.get_variable(). Did you mean to set reuse=None in VarScope?</strong></p>

<hr>

<pre><code>scope_name = ""A3C_net""
with tf.device(self._device),tf.variable_scope(scope_name) as scope:
    self.W_conv1, self.b_conv1 = self._conv_variable([8, 8, 4, 16])
    self.W_conv2, self.b_conv2 = self._conv_variable([4, 4, 16, 32])

    self.W_fc1, self.b_fc1 = self._fc_variable([2592, 256])
    # lstm
    self.lstm = tf.contrib.rnn.BasicLSTMCell(256, state_is_tuple=True, reuse=True)#
    # weight for policy output layer
    self.W_fc2, self.b_fc2 = self._fc_variable([256, action_size])
    # weight for value output layer
    self.W_fc3, self.b_fc3 = self._fc_variable([256, 1])

    self.s = tf.placeholder(""float"", [None, 84, 84, 4])

    h_conv1 = tf.nn.relu(self._conv2d(self.s,  self.W_conv1, 4) + self.b_conv1)
    h_conv2 = tf.nn.relu(self._conv2d(h_conv1, self.W_conv2, 2) + self.b_conv2)
    h_conv2_flat = tf.reshape(h_conv2, [-1, 2592])
    h_fc1 = tf.nn.relu(tf.matmul(h_conv2_flat, self.W_fc1) + self.b_fc1)
    h_fc1_reshaped = tf.reshape(h_fc1, [1,-1,256])  # batches steps inputs

    self.step_size = tf.placeholder(tf.float32, [1])
    self.initial_lstm_state0 = tf.placeholder(tf.float32, [1, 256])
    self.initial_lstm_state1 = tf.placeholder(tf.float32, [1, 256])
    self.initial_lstm_state = tf.contrib.rnn.LSTMStateTuple(
                                      self.initial_lstm_state0,
                                      self.initial_lstm_state1)
    lstm_outputs, self.lstm_state = tf.nn.dynamic_rnn(self.lstm,
    h_fc1_reshaped,initial_state = self.initial_lstm_state,dtype= tf.float32,
                                     sequence_length = self.step_size,
                                     time_major = False,
                                     scope = scope)

    lstm_outputs = tf.reshape(lstm_outputs, [-1,256])

    # policy (output)
    self.pi = tf.nn.softmax(tf.matmul(lstm_outputs, self.W_fc2) + self.b_fc2)
    # value (output)
    v_ = tf.matmul(lstm_outputs, self.W_fc3) + self.b_fc3
    self.v = tf.reshape( v_, [-1] )

    scope.reuse_variables()
    self.W_lstm = tf.get_variabl(""basic_lstm_cell/weights"")                  
    self.b_lstm = tf.get_variable(""basic_lstm_cell/biases"")
    self.reset_state()
</code></pre>
",8030496,,1344955,,2017-06-02 03:41:52,2017-06-02 03:41:52,"ValueError: Variable A3C_net/basic_lstm_cell/weights does not exist, or was not created with tf.get_variable()",<python><reinforcement-learning>,1,0,,,,CC BY-SA 3.0,
279,50964051,1,51046737,,2018-06-21 08:39:20,,0,680,"<p>I've built an A3C implementation in keras using this as referance: <a href=""https://jaromiru.com/2017/03/26/lets-make-an-a3c-implementation/"" rel=""nofollow noreferrer"">https://jaromiru.com/2017/03/26/lets-make-an-a3c-implementation/</a>
And I'm using custom environment, where an agent has a choise of purchasing some items, selling or exchanging them given their price as state. And it is given positive rewards for good deals and negative rewards for bad deals. I have tested it on DQN in the past and it sucessfully converged showing really good results. But when I use the same environment in A3C, it results in model just choosing the same action over and over. I tried changing some hyper-parametrs, but no result. I also tried using target model and updating it every n episodes, which resulted in better convergence with gym CartPole environment, but still no effect on performance of my model in my custom environment. I have found a few discussions on reddit about the same problem, but none of them were answered. </p>
",8075709,,6296561,,2019-07-10 13:07:26,2019-07-10 13:07:26,Convergence issues in a3c,<python-3.x><tensorflow><keras><reinforcement-learning>,1,2,,,,CC BY-SA 4.0,
280,68674852,1,68686830,,2021-08-06 00:36:30,,0,126,"<p>I'm trying to create a reinforcement learning agent that uses A3C (Asynchronous advantage actor critic) to make a <strong>yellow</strong> agent sphere go to the location of a <strong>red</strong> cube in the environment as shown below:</p>
<p><a href=""https://i.stack.imgur.com/oV1Ma.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/oV1Ma.png"" alt=""enter image description here"" /></a></p>
<p>The state space consists of the coordinates of the agent and the cube. The actions available to the agent are to move up, down, left, or right to the next square. This is a discrete action space. When I run my A3C algorithm, it seems to prematurely converge and choose a single action predominantly over the other actions, no matter what state is observed by the agent. For example, the first time I train it, it could choose to go left, even when the cube is to the right of the agent. Another time I train it, it could choose to predominantly go up, even when the target is below it.</p>
<p>The reward function is very simple. The agent receives a negative reward, and the size of this negative reward depends on its distance from the cube. The closer the agent is to the cube, the lower its negative reward. When the agent is very close to the cube, it gets a large positive reward and the episode is terminated. My agent is trained over 1000 episodes, with 200 steps per episode. There are multiple environments which simultaneously execute training, as described in A3C.</p>
<p>The neural network is as follows:</p>
<pre><code>dense1 = layers.Dense(64, activation='relu')
batchNorm1 = layers.BatchNormalization()
dense2 = layers.Dense(64, activation='relu')
batchNorm2 = layers.BatchNormalization()
dense3 = layers.Dense(64, activation='relu')
batchNorm3 = layers.BatchNormalization()
dense4 = layers.Dense(64, activation='relu')
batchNorm4 = layers.BatchNormalization()
policy_logits = layers.Dense(self.actionCount, activation=&quot;softmax&quot;)
values = layers.Dense(1, activation=&quot;linear&quot;)
</code></pre>
<p>I am using Adam optimiser with a learning rate of 0.0001, and gamme is set to 0.99.</p>
<p>How do I prevent my agent from choosing the same action every time, even if the state has changed? Is this an exploration issue, or is this something wrong with my reward function?</p>
",9132178,,9132178,,2021-08-06 00:57:46,2021-08-06 19:47:22,"A3C policy only selects a single action, no matter the input state",<tensorflow><deep-learning><reinforcement-learning>,1,0,,,,CC BY-SA 4.0,
282,63195873,1,64822022,,2020-07-31 16:40:42,,0,83,"<p>I am trying to use A3C with LSTM for an environment where states has 12 inputs ranging from -5000 to 5000.
I am using an LSTM layer of size 12 and then 2 fully connected hidden layers of size 256, then 1 fc for 3 action dim and 1 fc for 1 value function.
The reward is in range (-1,1).</p>
<p>However during initial training I am unable to get good results.</p>
<p>My question is- Is this Neural Network good enough for this kind of environment.</p>
<p>Below is the code for Actor Critic</p>
<pre><code>class ActorCritic(torch.nn.Module):

    def __init__(self, params):
        super(ActorCritic, self).__init__()

        self.state_dim = params.state_dim
        self.action_space = params.action_dim
        self.hidden_size = params.hidden_size
        state_dim = params.state_dim
        self.lstm = nn.LSTMCell(state_dim, state_dim)
        self.lstm.bias_ih.data.fill_(0)
        self.lstm.bias_hh.data.fill_(0)
        lst = [state_dim]
        for i in range(params.layers):
            lst.append(params.hidden_size)
        
        self.hidden = nn.ModuleList()
        for k in range(len(lst)-1):
            self.hidden.append(nn.Linear(lst[k], lst[k+1]))
        for layer in self.hidden:
            layer.apply(init_weights)

        self.critic_linear = nn.Linear(params.hidden_size, 1)
        self.critic_linear.apply(init_weights)
        self.actor_linear = nn.Linear(params.hidden_size, self.action_space)
        self.actor_linear.apply(init_weights)
        self.train()

    def forward(self, inputs):
        inputs, (hx, cx) = inputs
        inputs = inputs.reshape(1,-1)
        hx, cx = self.lstm(inputs, (hx, cx))
        x = hx
        for layer in self.hidden:
            x = torch.tanh(layer(x))
        return self.critic_linear(x), self.actor_linear(x), (hx, cx)

class Params():
    def __init__(self):
        self.lr = 0.0001
        self.gamma = 0.99
        self.tau = 1.
        self.num_processes = os.cpu_count()
        self.state_dim = 12
        self.action_dim = 3
        self.hidden_size = 256
        self.layers = 2
        self.epochs = 10
        self.lstm_layers = 1
        self.lstm_size = self.state_dim
        self.num_steps = 20
        self.window = 50
</code></pre>
",2783767,,2783767,,2020-11-13 12:58:38,2020-11-13 13:46:05,How much deep a Neural Network Required for 12 inputs of ranging from -5000 to 5000 in a3c Reinforcement Learning,<python><pytorch><reinforcement-learning>,1,0,,,,CC BY-SA 4.0,
283,34703423,1,34703481,,2016-01-10 08:14:28,,-1,4467,"<p>I'm implementing <code>Apple Push Notification Services</code> on my app. After getting the notification, I want to get the information and add it into a uitableview. I followed this <a href=""https://www.youtube.com/playlist?list=PL9zw34QbYRx5oq8jZ-wdfHA4ww5K2O_jq"" rel=""nofollow""><em>Pushbots</em> tutorial</a> like this:</p>

<p>In the <code>AppDelegate.m</code> file:</p>

<pre><code>- (BOOL)application:(UIApplication *)application didFinishLaunchingWithOptions:(NSDictionary *)launchOptions {
    [Pushbots sharedInstanceWithAppId:@""--my app id--""];
    [[Pushbots sharedInstance] receivedPush:launchOptions];
    return YES;
}

- (void)application:(UIApplication *)application didRegisterForRemoteNotificationsWithDeviceToken:(NSData *)deviceToken {
    // This method will be called everytime you open the app
    // Register the deviceToken on Pushbots
    [[Pushbots sharedInstance] registerOnPushbots:deviceToken];
}

-(void)application:(UIApplication *)application didFailToRegisterForRemoteNotificationsWithError:(NSError *)error{
    NSLog(@""Notification Registration Error %@"", [error userInfo]);
}

- (void)application:(UIApplication *)application didReceiveRemoteNotification:(NSDictionary *)userInfo {
    //Handle notification when the user click it while app is running in background or foreground.
    [[Pushbots sharedInstance] receivedPush:userInfo];
    NSString *msg = [userInfo valueForKey:@""aps""];
    NSLog(@""Push Notification:%@"",msg);

    [[NSUserDefaults standardUserDefaults]setObject:msg forKey:@""ReceivedNotifications""];
}
</code></pre>

<p>In my <code>ViewController.m</code>:</p>

<pre><code>@interface ViewController () &lt;UITableViewDataSource, UITableViewDelegate&gt;
@property (weak, nonatomic) IBOutlet UITableView *notifTableView;
@end

@implementation ViewController
{
    NSMutableArray *notif;
}

- (void)viewDidLoad {
    [super viewDidLoad];
    notif = [[NSUserDefaults standardUserDefaults] objectForKey:@""ReceivedNotifications""];
}

- (NSInteger)tableView:(UITableView *)tableView numberOfRowsInSection:(NSInteger)section
{
    return [notif count];
}

- (UITableViewCell *)tableView:(UITableView *)tableView cellForRowAtIndexPath:(NSIndexPath *)indexPath
{
    UITableViewCell *cell = [tableView dequeueReusableCellWithIdentifier:@""Cell""];
    cell.textLabel.text = [notif objectAtIndex:indexPath.row];
    return cell;
}
</code></pre>

<p>However, I keep getting an error at the console:</p>

<pre><code>[__NSCFDictionary objectAtIndex:]: unrecognized selector sent to instance 0x134d0a3c0
*** Terminating app due to uncaught exception 'NSInvalidArgumentException', reason: '-[__NSCFDictionary objectAtIndex:]: unrecognized selector sent to instance 0x134d0a3c0'
*** First throw call stack:
(0x180f45900 0x1805b3f80 0x180f4c61c 0x180f495b8 0x180e4d68c 0x100091b28 0x185f8931c 0x185f89484 0x185f787e8 0x185f8dfb0 0x185d2308c 0x185c33778 0x183642b2c 0x18363d738 0x18363d5f8 0x18363cc94 0x18363c9dc 0x1836360cc 0x180efc588 0x180efa32c 0x180e296a0 0x185ca6580 0x185ca0d90 0x100092a1c 0x1809ca8b8)
libc++abi.dylib: terminating with uncaught exception of type NSException
</code></pre>

<p>This is my <code>main.m</code> file:</p>

<pre><code>#import &lt;UIKit/UIKit.h&gt;
#import ""AppDelegate.h""

int main(int argc, char * argv[]) {
    @autoreleasepool {
        return UIApplicationMain(argc, argv, nil, NSStringFromClass([AppDelegate class]));
    }
}
</code></pre>

<p>What's wrong?</p>
",5754087,,3909115,,2016-01-10 18:31:16,2016-01-10 18:31:16,"Error ""'NSInvalidArgumentException', reason: '-[__NSCFDictionary objectAtIndex:]: unrecognized selector sent to instance 0x134d0a3c0'""",<ios><objective-c><uitableview><apple-push-notifications>,2,0,,,,CC BY-SA 3.0,
284,51510460,1,55908555,,2018-07-25 03:40:24,,3,1975,"<p>Is there any easy way to merge properties of PPO with an A3C method? A3C methods run a number of parrel actors and optimize the parameters. I am trying to merge PPO with A3C.</p>
",5915270,,,,,2019-04-29 18:04:17,What are the similarities between A3C and PPO in reinforcement learning policy gradient methods?,<reinforcement-learning>,1,0,,,,CC BY-SA 4.0,
285,48111120,1,48288434,,2018-01-05 09:57:20,,12,1503,"<p>I want to implement the Asynchronous Advantage Actor Critic (A3C) model for reinforcement learning in my local machine (1 CPU, 1 cuda compatible GPU). In this algorithm, several ""learner"" networks interact with copies of an environment and update a central model periodically.</p>

<p>I've seen implementations that create n ""worker"" networks and one ""global"" network inside the same graph and use threading to run these. In these approaches, the global net is updated by applying gradients to the trainable parameters with a ""global"" scope.</p>

<p>However, I recently read a bit about distributed tensorflow and now I'm a bit confused. Would it be easier/faster/better to implement this using the distributed tensorflow API? In the documentation and talks they always make expicit mention of using it in multi-device environments. I don't know if it's an overkill to use it in a local async algorithm.</p>

<p>I would also like to ask, is there a way to batch the gradients calculated by every worker to be applied together after n steps?</p>
",5331881,,681865,,2018-01-05 10:27:14,2018-01-17 15:02:34,A3C in Tensorflow - Should I use threading or the distributed Tensorflow API,<multithreading><tensorflow>,1,1,0,,,CC BY-SA 3.0,
286,48191567,1,48247428,,2018-01-10 16:08:36,,2,3380,"<p>I have a small project with <code>hibernate</code>+<code>jpa</code>,<code>Spring</code>, <code>Spring MVC</code>(without <code>SpringBoot</code>). Now I'm trying to add <code>Spring Security</code>. </p>

<p><code>UserDetailsServiceImpl.class</code>:</p>

<pre><code>public class UserDetailsServiceImpl implements UserDetailsService {
    @Autowired
    Service&lt;Account&gt; accountService;

    @Override
    public UserDetails loadUserByUsername(String mail) throws UsernameNotFoundException {
        Account account = new Account();
        account.setMail(mail);
        Account foundedAccount = accountService.readByObj(account);
        if (foundedAccount == null) {
            throw new UsernameNotFoundException(""Account not found"");
        }
        return new User(foundedAccount.getMail(), foundedAccount.getPassword()
                , true, true, true, true,
                Collections.singleton(new SimpleGrantedAuthority(""ROLE_"" + foundedAccount.getRole())));
    }
}
</code></pre>

<p><code>SecurityConfig.class</code>:</p>

<pre><code>@EnableWebSecurity
@Configuration
@ComponentScan({""com.myapp.webapp.config""})
public class SecurityConfig extends WebSecurityConfigurerAdapter {

    @Autowired
    private BCryptPasswordEncoder bCryptPasswordEncoder;

    @Autowired
    private UserDetailsService userDetailsService;

    @Override
    protected void configure(HttpSecurity http) throws Exception {
        http
                .authorizeRequests()
                .antMatchers(""/"").permitAll()
                .antMatchers(""/account/**"").hasRole(""USER"")
                .and()

                .formLogin()
                .loginPage(""/login"").permitAll()
                .loginProcessingUrl(""/login"")
                .usernameParameter(""mail"")
                .passwordParameter(""password"")
                .and()

                .logout()
                .logoutUrl(""/logout"")
                .logoutSuccessUrl(""/login"")
                .logoutSuccessHandler(new HttpStatusReturningLogoutSuccessHandler())
                .invalidateHttpSession(true)
                .deleteCookies(""mail"", ""password"", ""id"")
                .and()

                .rememberMe()
                .rememberMeParameter(""remember-me"");
                //.

    }

    @Override
    protected void configure(AuthenticationManagerBuilder auth) throws Exception {
        auth.userDetailsService(userDetailsService).passwordEncoder(bCryptPasswordEncoder);

    }
}
</code></pre>

<p><code>SecurityWebApplicationInitializer.class</code>:</p>

<pre><code>public class SecurityWebApplicationInitializer extends AbstractSecurityWebApplicationInitializer {


}
</code></pre>

<p><code>WebConfig.class</code>:</p>

<pre><code>@EnableWebMvc
@Configuration
@ComponentScan({""com.myapp.service.config"",
""com.myapp.webapp""
} )

public class WebConfig implements WebMvcConfigurer {

    private static final Logger logger = LoggerFactory.getLogger(AccountController.class);

    @Autowired
    EntityManagerFactory emf;


    @Override
    public void addInterceptors(InterceptorRegistry registry) {
        registry.addInterceptor(accountInterceptor()).addPathPatterns(""/account"", ""/editAccount"", ""/editSecurity"", ""/editMainInfo"");
        registry.addWebRequestInterceptor(entityManagerInViewInterceptor()).addPathPatterns(""/*"");
    }

    @Override
    public void configureDefaultServletHandling(DefaultServletHandlerConfigurer configurer) {
        configurer.enable();
    }

    @Override
    public void addResourceHandlers(final ResourceHandlerRegistry registry) {
        registry.addResourceHandler(""/resources "").addResourceLocations(""/resources/"");
        registry.addResourceHandler(""/WEB-INF/jsp "").addResourceLocations(""/jsp/"");
    }

    @Bean
    public ViewResolver viewResolver() {
        InternalResourceViewResolver viewResolver = new InternalResourceViewResolver();
        viewResolver.setViewClass(JstlView.class);
        viewResolver.setPrefix(""/WEB-INF/jsp/"");
        viewResolver.setSuffix("".jsp"");
        return viewResolver;
    }

    @Bean
    public CommonsMultipartResolver multipartResolver() {
        CommonsMultipartResolver resolver=new CommonsMultipartResolver();
        resolver.setDefaultEncoding(""utf-8"");
        return resolver;
    }

    @Bean
    public UserDetailsService userDetailsService() {
        return new UserDetailsServiceImpl();
    }

    @Bean
    public UserValidationInterceptor accountInterceptor() {
        return new UserValidationInterceptor();
    }

    @Bean
    public OpenEntityManagerInViewInterceptor entityManagerInViewInterceptor() {
        OpenEntityManagerInViewInterceptor openEntityManagerInViewInterceptor = new OpenEntityManagerInViewInterceptor();
        openEntityManagerInViewInterceptor.setEntityManagerFactory(emf);
        return openEntityManagerInViewInterceptor;
    }


    @Bean
    public BCryptPasswordEncoder passwordEncoder() {
        return new BCryptPasswordEncoder();
    }

}
</code></pre>

<p>and <code>AppInit.class</code>:</p>

<pre><code>public class AppInit extends AbstractAnnotationConfigDispatcherServletInitializer{
    @Override
    protected Class&lt;?&gt;[] getRootConfigClasses() {
        return new Class&lt;?&gt;[]{
                WebConfig.class,
                SecurityConfig.class
        };
    }

    @Override
    protected Class&lt;?&gt;[] getServletConfigClasses() {
       return new Class&lt;?&gt;[]{
                WebConfig.class
        };
    }

    @Override
    protected String[] getServletMappings() {
        return new String[]{""/""};
    }
}
</code></pre>

<p>After installing and deploying project in <code>Maven</code> trowing exception:</p>

<pre><code>10-Jan-2018 18:54:33.562 SEVERE [localhost-startStop-8] org.apache.catalina.core.ContainerBase.addChildInternal ContainerBase.addChild: start:  
 org.apache.catalina.LifecycleException: Failed to start component [StandardEngine[Catalina].StandardHost[localhost].StandardContext[/webapp-1.0-SNAPSHOT]] 
    at org.apache.catalina.util.LifecycleBase.start(LifecycleBase.java:167)
    at org.apache.catalina.core.ContainerBase.addChildInternal(ContainerBase.java:752)
    at org.apache.catalina.core.ContainerBase.addChild(ContainerBase.java:728)
    at org.apache.catalina.core.StandardHost.addChild(StandardHost.java:734)
    at org.apache.catalina.startup.HostConfig.deployWAR(HostConfig.java:952)
    at org.apache.catalina.startup.HostConfig$DeployWar.run(HostConfig.java:1823)
    at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
    at java.util.concurrent.FutureTask.run(FutureTask.java:266)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
    at java.lang.Thread.run(Thread.java:745)
Caused by: org.springframework.beans.factory.UnsatisfiedDependencyException: Error creating bean with name 'securityConfig': Unsatisfied dependency expressed through method 'setContentNegotationStrategy' parameter 0; nested exception is org.springframework.beans.factory.BeanCreationException: Error creating bean with name 'mvcContentNegotiationManager' defined in org.springframework.web.servlet.config.annotation.DelegatingWebMvcConfiguration: Bean instantiation via factory method failed; nested exception is org.springframework.beans.BeanInstantiationException: Failed to instantiate [org.springframework.web.accept.ContentNegotiationManager]: Factory method 'mvcContentNegotiationManager' threw exception; nested exception is java.lang.AbstractMethodError: com.myapp.webapp.config.WebConfig$$EnhancerBySpringCGLIB$$b5f5a3c0.configureContentNegotiation(Lorg/springframework/web/servlet/config/annotation/ContentNegotiationConfigurer;)V    
    at org.springframework.beans.factory.annotation.AutowiredAnnotationBeanPostProcessor$AutowiredMethodElement.inject(AutowiredAnnotationBeanPostProcessor.java:667)
    at org.springframework.beans.factory.annotation.InjectionMetadata.inject(InjectionMetadata.java:88)
    at org.springframework.beans.factory.annotation.AutowiredAnnotationBeanPostProcessor.postProcessPropertyValues(AutowiredAnnotationBeanPostProcessor.java:366)
    at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.populateBean(AbstractAutowireCapableBeanFactory.java:1264)
    at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.doCreateBean(AbstractAutowireCapableBeanFactory.java:553)
    at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBean(AbstractAutowireCapableBeanFactory.java:483)
    at org.springframework.beans.factory.support.AbstractBeanFactory$1.getObject(AbstractBeanFactory.java:306)
    at org.springframework.beans.factory.support.DefaultSingletonBeanRegistry.getSingleton(DefaultSingletonBeanRegistry.java:230)
    at org.springframework.beans.factory.support.AbstractBeanFactory.doGetBean(AbstractBeanFactory.java:302)
    at org.springframework.beans.factory.support.AbstractBeanFactory.getBean(AbstractBeanFactory.java:197)
    at org.springframework.beans.factory.support.DefaultListableBeanFactory.preInstantiateSingletons(DefaultListableBeanFactory.java:761)
    at org.springframework.context.support.AbstractApplicationContext.finishBeanFactoryInitialization(AbstractApplicationContext.java:867)
    at org.springframework.context.support.AbstractApplicationContext.refresh(AbstractApplicationContext.java:543)
    at com.myapp.webapp.config.WebServletConfiguration.onStartup(WebServletConfiguration.java:28)
    at org.springframework.web.SpringServletContainerInitializer.onStartup(SpringServletContainerInitializer.java:169)
    at org.apache.catalina.core.StandardContext.startInternal(StandardContext.java:5196)
    at org.apache.catalina.util.LifecycleBase.start(LifecycleBase.java:150)
    ... 10 more
Caused by: org.springframework.beans.factory.BeanCreationException: Error creating bean with name 'mvcContentNegotiationManager' defined in org.springframework.web.servlet.config.annotation.DelegatingWebMvcConfiguration: Bean instantiation via factory method failed; nested exception is org.springframework.beans.BeanInstantiationException: Failed to instantiate [org.springframework.web.accept.ContentNegotiationManager]: Factory method 'mvcContentNegotiationManager' threw exception; nested exception is java.lang.AbstractMethodError: com.myapp.webapp.config.WebConfig$$EnhancerBySpringCGLIB$$b5f5a3c0.configureContentNegotiation(Lorg/springframework/web/servlet/config/annotation/ContentNegotiationConfigurer;)V  
    at org.springframework.beans.factory.support.ConstructorResolver.instantiateUsingFactoryMethod(ConstructorResolver.java:599)
    at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.instantiateUsingFactoryMethod(AbstractAutowireCapableBeanFactory.java:1173)
    at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBeanInstance(AbstractAutowireCapableBeanFactory.java:1067)
    at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.doCreateBean(AbstractAutowireCapableBeanFactory.java:513)
    at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBean(AbstractAutowireCapableBeanFactory.java:483)
    at org.springframework.beans.factory.support.AbstractBeanFactory$1.getObject(AbstractBeanFactory.java:306)
    at org.springframework.beans.factory.support.DefaultSingletonBeanRegistry.getSingleton(DefaultSingletonBeanRegistry.java:230)
    at org.springframework.beans.factory.support.AbstractBeanFactory.doGetBean(AbstractBeanFactory.java:302)
    at org.springframework.beans.factory.support.AbstractBeanFactory.getBean(AbstractBeanFactory.java:202)
    at org.springframework.beans.factory.config.DependencyDescriptor.resolveCandidate(DependencyDescriptor.java:208)
    at org.springframework.beans.factory.support.DefaultListableBeanFactory.doResolveDependency(DefaultListableBeanFactory.java:1138)
    at org.springframework.beans.factory.support.DefaultListableBeanFactory.resolveDependency(DefaultListableBeanFactory.java:1066)
    at org.springframework.beans.factory.annotation.AutowiredAnnotationBeanPostProcessor$AutowiredMethodElement.inject(AutowiredAnnotationBeanPostProcessor.java:659)
    ... 26 more
Caused by: org.springframework.beans.BeanInstantiationException: Failed to instantiate [org.springframework.web.accept.ContentNegotiationManager]: Factory method 'mvcContentNegotiationManager' threw exception; nested exception is java.lang.AbstractMethodError: com.myapp.webapp.config.WebConfig$$EnhancerBySpringCGLIB$$b5f5a3c0.configureContentNegotiation(Lorg/springframework/web/servlet/config/annotation/ContentNegotiationConfigurer;)V  
    at org.springframework.beans.factory.support.SimpleInstantiationStrategy.instantiate(SimpleInstantiationStrategy.java:189)
    at org.springframework.beans.factory.support.ConstructorResolver.instantiateUsingFactoryMethod(ConstructorResolver.java:588)
    ... 38 more
Caused by: java.lang.AbstractMethodError: com.myapp.webapp.config.WebConfig$$EnhancerBySpringCGLIB$$b5f5a3c0.configureContentNegotiation(Lorg/springframework/web/servlet/config/annotation/ContentNegotiationConfigurer;)V 
    at org.springframework.web.servlet.config.annotation.WebMvcConfigurerComposite.configureContentNegotiation(WebMvcConfigurerComposite.java:59)
    at org.springframework.web.servlet.config.annotation.DelegatingWebMvcConfiguration.configureContentNegotiation(DelegatingWebMvcConfiguration.java:62)
    at org.springframework.web.servlet.config.annotation.WebMvcConfigurationSupport.mvcContentNegotiationManager(WebMvcConfigurationSupport.java:368)
    at org.springframework.web.servlet.config.annotation.DelegatingWebMvcConfiguration$$EnhancerBySpringCGLIB$$3318d161.CGLIB$mvcContentNegotiationManager$25(&lt;generated&gt;)
    at org.springframework.web.servlet.config.annotation.DelegatingWebMvcConfiguration$$EnhancerBySpringCGLIB$$3318d161$$FastClassBySpringCGLIB$$3906aa77.invoke(&lt;generated&gt;)
    at org.springframework.cglib.proxy.MethodProxy.invokeSuper(MethodProxy.java:228)
    at org.springframework.context.annotation.ConfigurationClassEnhancer$BeanMethodInterceptor.intercept(ConfigurationClassEnhancer.java:358)
    at org.springframework.web.servlet.config.annotation.DelegatingWebMvcConfiguration$$EnhancerBySpringCGLIB$$3318d161.mvcContentNegotiationManager(&lt;generated&gt;)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
    at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
    at java.lang.reflect.Method.invoke(Method.java:498)
    at org.springframework.beans.factory.support.SimpleInstantiationStrategy.instantiate(SimpleInstantiationStrategy.java:162)
    ... 39 more
</code></pre>
",8338448,,8338448,,2018-01-10 16:14:25,2018-01-14 07:12:07,Caused by: java.lang.AbstractMethodError: com.myapp.webapp.config.WebConfig$$EnhancerBySpringCGLIB$$b5f5a3c0.configureContentNegotiation,<java><spring><maven><spring-mvc><spring-security>,1,1,,,,CC BY-SA 3.0,
287,66028677,1,66031189,,2021-02-03 13:37:09,,4,3703,"<p>I tried to use ViewBinding in my Android Projects, because Kotlin synthetics are deprecated. I followed the official documentation from Android Developer's site. In my build.gradle.kts I enabled ViewBinding:</p>
<pre class=""lang-kotlin prettyprint-override""><code>android {
    ...
    buildFeatures {
            viewBinding = true
        }
}
</code></pre>
<p>And I tried to use it in a Fragment like:</p>
<pre class=""lang-kotlin prettyprint-override""><code>    private var _binding: FragmentStartBinding? = null
    private val binding get() = _binding!!

    override fun onCreateView(
        inflater: LayoutInflater,
        container: ViewGroup?,
        savedInstanceState: Bundle?
    ): View? {
        _binding = FragmentStartBinding.inflate(inflater, container, false)
        return binding.root
    }
</code></pre>
<p>But <code>binding.root</code> shows the error: <code>Cannot access class 'android.widget.no_name_in_PSI_3d19d79d_1ba9_4cd0_b7f5_b46aa3cd5d40'. Check your module classpath for missing or conflicting dependencies</code>.</p>
<p>Android Studio Version: 4.1.2<br />
Kotlin Version: 1.4.30<br />
Android Gradle Plugin Version: 4.1.2<br />
Gradle Version: 6.8.1</p>
<p>I already tried Clean &amp; Rebuild, Invalidate Caches &amp; Restart, deleting .idea and .gradle folders. Also the ViewBinding files are generated, but it's impossible to use ViewBinding.</p>
<p>Does anybody have a solution for this problem?</p>
<p>Thanks in advance for any reply.</p>
",10927125,,,,,2022-12-06 00:25:40,Android ViewBinding Error: Cannot access class 'android.widget.no_name_in_PSI_3d19d79d_1ba9_4cd0_b7f5_b46aa3cd5d40',<android><kotlin><android-viewbinding>,3,1,0,,,CC BY-SA 4.0,
289,49585432,1,50606547,,2018-03-31 07:36:24,,0,311,"<p>I am currently running seperately my deep reinforcement learning algorithms on my simulink model. They are connected by a TCP/IP connection. Python sends action to simulink, simulink send environment values to python.</p>

<p>git: <a href=""https://github.com/qLience/eligibility_pump"" rel=""nofollow noreferrer"">https://github.com/qLience/eligibility_pump</a> </p>

<p>My problem is that my RL algorithm often is stuck in a local minimum where the A3C algorithm would help i thought because we are running multiple at the same time and by sharing memory then avoiding getting stuck in a local minimum.</p>

<p>But how do i run multiple simulink models with my A3C algorithm from python? I have tried to use two algorithms on my simulink running on different TCP/IP ports but i cannot do it.</p>

<p>I have also thought about a shared memory on the RAM between simulink and python but i really do not know how to execute this.</p>

<p>If you have a better suggestion and guide to do this then I will be in dent :D!</p>
",5950877,,,,,2019-04-26 03:09:10,A3C on simulink model,<python><parallel-processing><simulink><reinforcement-learning>,1,0,,,,CC BY-SA 3.0,
290,52334685,1,52336461,,2018-09-14 15:05:46,,1,580,"<p>I want to implement reinforcement learning for a game which uses the mouse to move. This game only cares about x-axis of the mouse.</p>

<p>My first try is to make it discrete.  The game will have 3 actions. Two actions are used to move the mouse 30 pixels to left and right and one action for standing still. It worked but now I want to make it continuous. </p>

<p>What I have done is to make the neural network output mean and std. Exactly like this code <a href=""https://github.com/stefanbo92/A3C-Continuous/blob/master/a3c.py"" rel=""nofollow noreferrer"">https://github.com/stefanbo92/A3C-Continuous/blob/master/a3c.py</a>. I even used this code on a second try. The width of the game is 480 so A_BOUND are [-240,240]. To make the problem always have a positive action, I added the predicted action to 240 then set the mouse position to the new one. </p>

<p>For example: If the action is 240 + -240, then the mouse x pos will be 0. The problem is that my neural network output only extremes from 240 to -240 consistently seconds after the start.</p>
",9954479,,9191460,,2018-09-14 17:39:43,2018-09-14 17:39:43,a3c continuous action probelm,<tensorflow><deep-learning><reinforcement-learning>,1,5,,,,CC BY-SA 4.0,
292,3800853,1,3806611,,2010-09-27 03:10:12,,2,664,"<p>I've been studying hierachial reinforcement learning problems, and while a lot of papers propose interesting ways for learning a policy, they all seem to assume they know in advance a graph structure describing the actions in the domain. For example, <a href=""http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.9.313&amp;rep=rep1&amp;type=ps"" rel=""nofollow"">The MAXQ Method for Hierarchial Reinforcement Learning</a> by Dietterich describes a complex graph of actions and sub-tasks for a simple Taxi domain, but not how this graph was discovered. How would you learn the hierarchy of this graph, and not just the policy?</p>
",247542,,247542,,2010-09-27 16:55:15,2010-10-06 23:25:32,Learning the Structure of a Hierarchical Reinforcement Task,<artificial-intelligence><machine-learning><reinforcement-learning>,3,8,0,,,CC BY-SA 2.5,
295,56777123,1,56777763,,2019-06-26 16:13:27,,2,1734,"<p>I read several materials about deep q-learning and I'm not sure if I understand it completely. From what I learned, it seems that Deep Q-learning calculates faster the Q-values rather than putting them on a table by using NN to perform a regression, calculating loss and backpropagating the error to update the weights. Then, in a testing scenario, it takes a state and the NN will return several Q-values for each action possible for that state. Then, the action with the highest Q-value will be chosen to be done at that state.</p>

<p>My only question is how the weights are updated. According to <a href=""https://sergioskar.github.io/Taking_Deep_Q_Networks_a_step_further/"" rel=""nofollow noreferrer"">this site</a> the weights are updated as follows:</p>

<p><a href=""https://i.stack.imgur.com/NTRDL.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/NTRDL.png"" alt=""enter image description here""></a></p>

<p>I understand that the weights are initialized randomly, R is returned by the environment, gamma and alpha are set manually, but I dont understand how Q(s',a,w) and Q(s,a,w) are initialized and calculated. Does it seem that we should build a table of Q-values and update them as with Q-learning or they are calculated automatically at each NN training epoch? what I am not understanding here? can somebody explain to me better such an equation?   </p>
",2163392,,,,,2019-06-26 16:57:35,Questions About Deep Q-Learning,<reinforcement-learning><q-learning><keras-rl>,1,0,,,,CC BY-SA 4.0,
299,39932611,1,39934348,,2016-10-08 12:54:57,,3,845,"<p>So I'm trying to implement Deep Q-learning algorithm created by Google DeepMind and I think I have got a pretty good hang of it now. Yet there is still one (pretty important) thing I don't really understand and I hope you could help. </p>

<p>Doesn't yj result to a double (Java) and the latter part to a matrix containing Q-values for each action in current state in the following line (4th last line in the algorithm):</p>

<hr>

<p><a href=""https://i.stack.imgur.com/qCqRa.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/qCqRa.png"" alt=""1""></a></p>

<hr>

<p>So how can I subtract them from each other. </p>

<p>Should I make yj a matrix containing all the data from here <a href=""https://i.stack.imgur.com/kBeHe.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/kBeHe.png"" alt=""2""></a> except replace the currently selected action with </p>

<p><a href=""https://i.stack.imgur.com/AauDf.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/AauDf.png"" alt=""enter image description here""></a></p>

<p>This doesn't seem like the right answer and I'm a bit lost here as you can see.</p>

<hr>

<p><a href=""https://i.stack.imgur.com/e1yMa.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/e1yMa.png"" alt=""enter image description here""></a></p>
",5609686,,5609686,,2016-10-08 13:02:37,2016-10-09 11:59:10,How to implement Deep Q-learning gradient descent,<java><algorithm><neural-network><deep-learning><q-learning>,1,12,,,,CC BY-SA 3.0,
301,45161016,1,45204475,,2017-07-18 08:10:42,,1,3298,"<p>i was implementing DQN in <a href=""https://gym.openai.com/envs/MountainCar-v0"" rel=""nofollow noreferrer"">mountain car problem</a> of openai gym. this problem is special as the positive reward is very sparse. so i thought of implementing prioritized experience replay as proposed in this <a href=""https://arxiv.org/pdf/1511.05952.pdf"" rel=""nofollow noreferrer"">paper by google deep mind</a>.</p>

<p>there are certain things that are confusing me:</p>

<ul>
<li>how do we store the replay memory. i get that p<sub>i</sub> is the priority of transition and there are two ways but what is this P(i)?</li>
<li>if we follow the rules given won't P(i) change every time a sample is added.</li>
<li>what does it mean when it says ""we sample according to this probability distribution"". what is the distribution.</li>
<li>finally how do we sample from it. i get that if we store it in a priority queue we can sample directly but we are actually storing it in a sum tree.</li>
</ul>

<p>thanks in advance </p>
",6588864,,,,,2020-10-30 16:21:10,prioritized experience replay in deep Q-learning,<deep-learning><priority-queue><reinforcement-learning><q-learning>,2,0,0,,,CC BY-SA 3.0,
302,49985848,1,49989745,,2018-04-23 16:30:21,,1,72,"<p>I am currently developing a deep reinforcement learning network however, I have a small doubt about the number of q-values I will have at the output of the NN. I will have a total of 150 q-values, which personally seems excessive to me. I have read on several papers and books that this could be a problem. I know that it will depend from the kind of NN I will build, but do you guys think that the number of q-values is too high? should I reduce it?</p>
",9518098,,,,,2018-04-23 20:50:59,Number of Q values for a deep reinforcement learning network,<neural-network><deep-learning><reinforcement-learning><q-learning>,1,2,,,,CC BY-SA 3.0,
303,37973108,1,37973461,,2016-06-22 16:00:59,,34,15423,"<p>What is the difference between <em>deep</em> reinforcement learning and reinforcement learning? I basically know what reinforcement learning is about, but what does the concrete term <strong>deep</strong> stand for in this context?</p>
",2916207,,3924118,,2018-10-31 08:43:02,2019-03-17 02:40:36,What is the difference between reinforcement learning and deep RL?,<machine-learning><reinforcement-learning><q-learning>,2,0,0,,,CC BY-SA 4.0,
306,46783760,1,46787296,,2017-10-17 06:25:25,,0,844,"<p>When I am training my model I have the following segment:</p>

<pre><code>s_t_batch, a_batch, y_batch = train_data(minibatch, model2)
# perform gradient step
loss.append(model.train_on_batch([s_t_batch, a_batch], y_batch))
</code></pre>

<p>where <code>s_t, a_</code> corresponds to current states and actions that were taken in those states respectively. <code>model2</code> is the same as <code>model</code> except that <code>model2</code> has an output of <code>num_actions</code> and <code>model</code> only outputs the value of the action that was taken in that state. </p>

<p>What I find strange (and is really the focus of this question) is in the function <code>train_data</code> I have the line:</p>

<pre><code>y_batch = r_batch + GAMMA * np.max(model.predict(s_t_batch), axis=1)
</code></pre>

<p>The strange part is the fact that I am using the model to generate my <code>y_batch</code> as well as training on them. Doesn't this become some sort of self fulfilling prophecy? If I understand correctly, the model tries to predict the expected maximum reward. Using the <strong>same</strong> model to try and generate <code>y_batch</code> is implying that it is the true model doesn't it?</p>

<p><strong>The question is</strong>, 1. what is the intuition behind using the same model to generate y_batch as it is to train them. 2. (optional) does loss value mean anything. When I plot it, it seems doesn't seem to be converging, however the sum of rewards seem to be increasing (see plots in link below).</p>

<p>The full code can be found <a href=""https://github.com/sachinruk/deepschool.io/blob/master/Lesson%2020%20-%20Deep%20Q%20Learning%20-%20Solutions.ipynb"" rel=""nofollow noreferrer"">here</a>, which is an implementation of Deep Q Learning on the CartPole-v0 problem: </p>

<h2>Comments from other forums:</h2>

<ol>
<li>y = r + gamma*np.max(model.predict(s_t_batch), axis=1) is totally natural and y will converge to the true state-action value. And if you don't break down the correlation between consecutive updates with something like experience replay (or better prioritized exp replay) your model WILL diverge. And there are better variants like DDQN, Duelling Network which performs better.</li>
<li>y_batch includes the reward. Both the target and online networks are estimates. It is indeed a somewhat self fulfilling prophecy as DQN's value function is overly optimistic. That is why Double DQN was added a few months later.</li>
<li>y will converge, but not necessarily to the true (I assume you mean optimal) state-action value. No one has proven that the converged value is the optimal value but it is the best approximation we have. However will converge to the the true value for simple enough problems (e.g. grid-world)</li>
</ol>
",2530674,,712995,,2019-10-19 08:00:48,2019-10-19 08:00:48,How does Deep Q learning work,<deep-learning><reinforcement-learning><openai-gym><q-learning>,1,0,0,,,CC BY-SA 3.0,
310,39848984,1,39916222,,2016-10-04 09:39:36,,3,1846,"<p>I'm trying to make a learning football game from scratch with Java and I'm  trying to implement the reinforcement learning with Google DeepMind's Deep Q-learning algorithm (without convolutional network though). I've already built neural network and Q-learning and now I'm trying to sum them up together but there are somethings I don't understand in this code.</p>

<ol>
<li>Aren't Q-values usually initialized with zeros instead of random values? Or does this mean the weights of the neural network (line 2)</li>
<li>What is meant by </li>
</ol>

<blockquote>
  <p>preprocessed sequenced <em>Î¦1 = Î¦(s1)</em> (line 4)</p>
</blockquote>

<p>I just couldn't figure out what does Î¦ stand for in this algorithm.</p>

<p><a href=""https://i.stack.imgur.com/Jnyff.jpg"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/Jnyff.jpg"" alt=""""></a></p>
",5609686,,157726,,2018-12-23 23:01:09,2018-12-23 23:01:09,What is phi in Deep Q-learning algorithm,<java><machine-learning><neural-network><deep-learning><q-learning>,1,0,,,,CC BY-SA 3.0,
314,50581232,1,50581688,,2018-05-29 09:45:54,,2,904,"<p>I'm new to reinforcement learning at all, so I may be wrong.</p>

<p>My questions are:</p>

<ul>
<li><p>Is the Q-Learning equation ( <code>Q(s, a) = r + y * max(Q(s', a'))</code> ) used in DQN only for computing  a loss function?</p></li>
<li><p>Is the equation recurrent? Assume I use DQN for, say, playing Atari Breakout, the number of possible states is very large (assuming the state is single game's frame), so it's not efficient to create a matrix of all the Q-Values. The equation should update the Q-Value of given [state, action] pair, so what will it do in case of DQN? Will it call itself recursively? If it will, the quation can't be calculated, because the recurrention won't ever stop.</p></li>
</ul>

<p>I've already tried to find what I want and I've seen many tutorials, but almost everyone doesn't show the background, just implements it using Python library like Keras.</p>

<p>Thanks in advance and I apologise if something sounds dumb, I just don't get that.</p>
",9822623,,,,,2018-05-29 12:39:43,Q-Learning equation in Deep Q Network,<neural-network><deep-learning><artificial-intelligence><reinforcement-learning><q-learning>,2,0,0,,,CC BY-SA 4.0,
316,34692318,1,34776449,,2016-01-09 10:26:27,,4,735,"<p>I was reading the deepmind nature paper on DQN network.
I almost got everything about it except one. I don't know why no one has asked this question before but it seems a little odd to me anyway.</p>

<p>My question:
Input to DQN is a 84*84*4 image. The first convolution layer consists of 32 filters of 8*8 with stide 4. I want to know what is the result of this convolution phase exactly? I mean, the input is 3D, but we have 32 filters which are all 2D. How does the third dimension (which corresponds to 4 last frames in the game) take part in the convolution?</p>

<p>Any ideas?
Thanks
Amin</p>
",5766399,,,,,2016-01-13 20:29:12,Deepmind Deep Q Network (DQN) 3D Convolution,<deep-learning><conv-neural-network><q-learning>,1,0,0,,,CC BY-SA 3.0,
318,62020946,1,64217061,,2020-05-26 11:17:36,,0,53,"<p>I'm having a hard time trying to make a Deep Q-Learning agent find the optimal policy. This is how my current model looks like in TensorFlow: </p>

<pre><code>model = Sequential()

model.add(Dense(units=32, activation=""relu"", input_dim=self.env.state.size)),
model.add(Dense(units=self.env.allActionsKeys.size, activation=""softmax""))

model.compile(loss=""mse"", optimizer=Adam(lr=0.00075), metrics=['accuracy'])
</code></pre>

<p>For the problem I'm working on at the moment 'self.env.state.size' is equal 6, and the number of possible actions ('self.env.allActionsKeys.size') is 30.</p>

<p>The input vector consists of bits, each of them with different ranges (not too different in this problem though). The range of 2 bits is [0,3], for other 2 [0,2] and for the remaining [0,1]. Please, note that this is supposed to be a simple problem, I'm also aiming for more complicated ones where the input size would be 15 for instance and the ranges can differ a bit more than that ([0,15], [0,3],...).</p>

<p>This is how my train method looks like:</p>

<pre><code>def train(self, terminal_state):
    if len(self.replay_memory) &lt; MIN_REPLAY_MEMORY_SIZE:
        return

    #Â Get MINIBATCH_SIZE random samples from replay_memory
    minibatch = random.sample(self.replay_memory, MINIBATCH_SIZE)

    # Transition: (current_state, action, reward, normalized_next_state, next_state, done)

    current_states = np.array([transition[0] for transition in minibatch])
    current_qs_minibatch = self.model.predict(current_states, batch_size=MINIBATCH_SIZE, use_multiprocessing=True)

    next_states = np.array([transition[3] for transition in minibatch])
    next_qs_minibatch = self.model.predict(next_states, batch_size=MINIBATCH_SIZE, use_multiprocessing=True)

    env_get_legal_actions = self.env.get_legal_actions
    np_max = np.max

    X = []
    y = []

    for index, (current_state, action, reward, normalized_next_state, next_state, done) in enumerate(minibatch):
        if not done:
            legalActionsIds = env_get_legal_actions(next_state)
            max_next_q = np_max(next_qs_minibatch[index][legalActionsIds])

            new_q = reward + DISCOUNT * max_next_q
        else:
            new_q = reward

        current_qs = current_qs_minibatch[index].copy()
        current_qs[action] = new_q

        X.append(current_state)
        y.append(current_qs)

    self.model.fit(np.array(X), np.array(y), batch_size=MINIBATCH_SIZE, verbose=0, shuffle=False)
</code></pre>

<p>where DISCOUNT = 0.99 and MINIBATCH_SIZE = 64</p>

<p>I read that it's recommended to normalized the input vector so I tested 2 different attribute normalization methods: min-max norm. and z-score norm. And, since the value ranges don't differ that much I also tested without normalization. None of these methods proved to be better than the others.</p>

<p>What happens is that at the beginning, during the exploration phase, the score gets better over time which means the model is learning something, but then, during the exploitation phase, when the epsilon value is low and the agent takes most of the actions greedly, the score decreases drastically meaning that it actually didn't learn anything good.</p>

<p>This is my Deep Q-Learning algorithm:</p>

<pre><code>epsilon = 1

for episode in range(1, EPISODES+1):
    episode_reward = 0
    step = 0
    done = False
    current_state = env_reset()

    while not done:
        normalized_current_state = env_normalize(current_state)

        if np_random_number() &gt; epsilon:  # Take legal action greedily
            actionsQValues = agent_get_qs(normalized_current_state)
            legalActionsIds = env_get_legal_actions(current_state)
            # Make the argmax selection among the legal actions
            action = legalActionsIds[np_argmax(actionsQValues[legalActionsIds])]
        else:  # Take random legal action
            action = env_sample()

        new_state, reward, done = env_step(action)

        episode_reward += reward

        agent_update_replay_memory((normalized_current_state, action, reward, env_normalize(new_state), new_state, done))
        agent_train(done)

        current_state = new_state
        step += 1

    # Decay epsilon
    if epsilon &gt; MIN_EPSILON:
        epsilon *= EPSILON_DECAY
</code></pre>

<p>where EPISODES = 4000 and EPSILON_DECAY = 0.9995.</p>

<p>I played around with all these hyper-parameters but the results are very similar. I don't know what else to try. Am I doing something wrong with normalization? Is there any other normalization method that is more recommended? Could the problem be in my neural network model that is not good enough?</p>

<p>I think it shouldn't be this difficult to make it work for a problem such simple as this one with an input size of 6, an output layer of 30 nodes and a hidden layer of 32.</p>

<p>Note that for the same problem I used a different representation of the state using a binary array of size 14 and it works fine with the same hyper-parameters. What might be the problem then when I use this other representation?</p>
",6820417,,,,,2020-10-05 22:30:58,Why does the score (accumulated reward) goes down during the exploitation phase in this Deep Q-Learning model?,<python><tensorflow><deep-learning><neural-network><q-learning>,1,0,,,,CC BY-SA 4.0,
320,50530546,1,50530559,,2018-05-25 13:39:34,,3,774,"<p>I've been studying reinforcement learning, and understand the concepts of value/policy iteration, TD(1)/TD(0)/TD(Lambda), and Q-learning. What I don't understand is why Q-learning can't be used for everything. Why do we need ""deep"" reinforcement learning as described in <a href=""https://arxiv.org/pdf/1312.5602.pdf"" rel=""nofollow noreferrer"">DeepMind's DQN paper</a>?</p>
",9840526,,9184597,,2018-05-25 13:42:32,2019-06-27 02:23:09,Why and when is deep reinforcement learning needed instead of q-learning?,<machine-learning><neural-network><deep-learning><reinforcement-learning><q-learning>,2,0,,,,CC BY-SA 4.0,
321,50542818,1,50542822,,2018-05-26 12:34:31,,12,2053,"<p>What's the difference between reinforcement learning, deep learning, and deep reinforcement learning? Where does Q-learning fit in?</p>
",,user9851027,,,,2022-11-09 23:36:31,"What's the difference between reinforcement learning, deep learning, and deep reinforcement learning?",<machine-learning><neural-network><deep-learning><reinforcement-learning><q-learning>,9,0,0,,,CC BY-SA 4.0,
325,56816743,1,56817130,,2019-06-29 10:19:47,,0,863,"<p>I'm trying to train a deep Q-learning Keras model to play CartPole-v1. However, it doesn't seem to get any better. I don't believe it's a bug but rather my lack of knowledge on how to use Keras and OpenAI Gym properly. I am following this tutorial (<a href=""https://adventuresinmachinelearning.com/reinforcement-learning-tutorial-python-keras/"" rel=""nofollow noreferrer"">https://adventuresinmachinelearning.com/reinforcement-learning-tutorial-python-keras/</a>), which shows how to train a bot to play NChain-v0 (which I was able to follow), but now I am trying to apply what I learned to a more complex environment: CartPole-v1. Here is the code below:</p>

<pre><code>###import libraries
import gym
import numpy as np
from keras.models import Sequential
from keras.layers import Dense
from keras.optimizers import Adam


###prepare environment
env = gym.make('CartPole-v1') #our environment is CartPole-v1


###make model
model = Sequential()
model.add(Dense(128, input_shape=(env.observation_space.shape[0],), activation='relu'))
model.add(Dense(64, activation='relu'))
model.add(Dense(env.action_space.n, activation='linear'))
model.compile(loss='mse', optimizer=Adam(), metrics=['mae'])


###train model
def train_model(n_episodes=500, epsilon=0.5, decay_factor=0.999, gamma=0.95):
    G_array = []
    for episode in range(n_episodes):
        observation = env.reset()
        observation = observation.reshape(-1, env.observation_space.shape[0])
        epsilon *= decay_factor
        G = 0
        done = False
        while done != True:
            if np.random.random() &lt; epsilon:
                action = env.action_space.sample()
            else:
                action = np.argmax(model.predict(observation))
            new_observation, reward, done, info = env.step(action) #It keeps going left! Why though?
            new_observation = new_observation.reshape(-1, env.observation_space.shape[0])
            target = reward + gamma*np.max(model.predict(new_observation))
            target_vector = model.predict(observation)[0]
            target_vector[action] = target
            model.fit(observation, target_vector.reshape(-1, env.action_space.n), epochs=1, verbose=0)
            observation = new_observation
            G += reward
        G_array.append(G)

    return G_array

G_array = train_model()
print(G_array)
</code></pre>

<p>The output for the 'G_array' (the total reward for each game) is the following:</p>

<pre><code>[14.0, 16.0, 18.0, 12.0, 16.0, 14.0, 17.0, 11.0, 11.0, 12.0, 11.0, 15.0, 13.0, 12.0, 12.0, 19.0, 13.0, 9.0, 10.0, 10.0, 11.0, 11.0, 14.0, 11.0, 10.0, 9.0, 10.0, 10.0, 12.0, 9.0, 15.0, 19.0, 11.0, 11.0, 10.0, 11.0, 13.0, 12.0, 13.0, 16.0, 12.0, 14.0, 9.0, 12.0, 20.0, 10.0, 12.0, 11.0, 9.0, 13.0, 13.0, 11.0, 13.0, 11.0, 24.0, 12.0, 11.0, 9.0, 9.0, 11.0, 10.0, 16.0, 10.0, 9.0, 9.0, 19.0, 10.0, 11.0, 13.0, 11.0, 11.0, 14.0, 23.0, 8.0, 13.0, 12.0, 15.0, 14.0, 11.0, 24.0, 9.0, 11.0, 11.0, 11.0, 10.0, 12.0, 11.0, 11.0, 10.0, 13.0, 18.0, 10.0, 17.0, 11.0, 13.0, 14.0, 12.0, 16.0, 13.0, 10.0, 10.0, 12.0, 22.0, 13.0, 11.0, 14.0, 10.0, 11.0, 11.0, 14.0, 14.0, 12.0, 18.0, 17.0, 9.0, 13.0, 12.0, 11.0, 11.0, 9.0, 16.0, 9.0, 18.0, 15.0, 12.0, 16.0, 13.0, 10.0, 13.0, 13.0, 17.0, 11.0, 11.0, 9.0, 9.0, 12.0, 9.0, 10.0, 9.0, 10.0, 18.0, 9.0, 11.0, 12.0, 10.0, 10.0, 10.0, 12.0, 12.0, 20.0, 13.0, 19.0, 9.0, 14.0, 14.0, 13.0, 19.0, 10.0, 18.0, 11.0, 11.0, 11.0, 8.0, 10.0, 14.0, 11.0, 16.0, 11.0, 13.0, 13.0, 9.0, 16.0, 11.0, 12.0, 13.0, 12.0, 11.0, 10.0, 11.0, 21.0, 12.0, 22.0, 12.0, 10.0, 13.0, 15.0, 19.0, 11.0, 10.0, 10.0, 11.0, 22.0, 11.0, 9.0, 26.0, 13.0, 11.0, 13.0, 13.0, 10.0, 10.0, 11.0, 12.0, 18.0, 9.0, 11.0, 13.0, 12.0, 13.0, 13.0, 12.0, 10.0, 11.0, 12.0, 12.0, 17.0, 11.0, 13.0, 13.0, 21.0, 12.0, 9.0, 14.0, 10.0, 15.0, 12.0, 12.0, 14.0, 11.0, 10.0, 14.0, 12.0, 12.0, 11.0, 8.0, 24.0, 9.0, 13.0, 10.0, 14.0, 10.0, 12.0, 13.0, 12.0, 13.0, 13.0, 14.0, 9.0, 17.0, 16.0, 9.0, 16.0, 14.0, 11.0, 9.0, 10.0, 15.0, 11.0, 9.0, 14.0, 12.0, 10.0, 13.0, 10.0, 10.0, 16.0, 15.0, 11.0, 8.0, 9.0, 9.0, 10.0, 9.0, 21.0, 13.0, 13.0, 10.0, 10.0, 11.0, 27.0, 13.0, 15.0, 11.0, 11.0, 12.0, 9.0, 10.0, 16.0, 10.0, 13.0, 13.0, 12.0, 12.0, 11.0, 17.0, 14.0, 9.0, 15.0, 26.0, 9.0, 9.0, 13.0, 9.0, 8.0, 12.0, 9.0, 10.0, 11.0, 9.0, 10.0, 9.0, 11.0, 9.0, 10.0, 12.0, 13.0, 13.0, 11.0, 11.0, 10.0, 15.0, 11.0, 11.0, 13.0, 10.0, 10.0, 12.0, 10.0, 10.0, 12.0, 9.0, 15.0, 29.0, 11.0, 9.0, 18.0, 11.0, 13.0, 13.0, 16.0, 13.0, 15.0, 10.0, 11.0, 18.0, 9.0, 9.0, 11.0, 15.0, 11.0, 11.0, 10.0, 25.0, 10.0, 9.0, 11.0, 15.0, 15.0, 11.0, 11.0, 11.0, 13.0, 9.0, 11.0, 9.0, 13.0, 12.0, 12.0, 14.0, 11.0, 14.0, 8.0, 10.0, 13.0, 10.0, 10.0, 10.0, 9.0, 13.0, 9.0, 12.0, 10.0, 11.0, 9.0, 11.0, 12.0, 20.0, 9.0, 10.0, 14.0, 9.0, 12.0, 13.0, 11.0, 11.0, 11.0, 10.0, 15.0, 14.0, 14.0, 12.0, 13.0, 12.0, 11.0, 10.0, 12.0, 12.0, 9.0, 11.0, 9.0, 11.0, 13.0, 10.0, 11.0, 11.0, 11.0, 12.0, 13.0, 13.0, 12.0, 8.0, 11.0, 13.0, 9.0, 12.0, 10.0, 10.0, 15.0, 12.0, 11.0, 10.0, 17.0, 10.0, 14.0, 9.0, 10.0, 10.0, 10.0, 12.0, 10.0, 10.0, 12.0, 10.0, 15.0, 10.0, 10.0, 9.0, 10.0, 10.0, 10.0, 19.0, 9.0, 10.0, 11.0, 10.0, 11.0, 11.0, 13.0, 10.0, 11.0, 12.0, 11.0, 12.0, 13.0, 11.0, 8.0, 12.0, 12.0, 14.0, 14.0, 11.0, 9.0, 11.0, 9.0, 12.0, 9.0, 8.0, 9.0, 12.0, 8.0, 10.0, 11.0, 13.0, 12.0, 12.0, 10.0, 11.0, 12.0, 10.0, 12.0, 13.0, 9.0, 9.0, 10.0, 15.0, 14.0, 16.0, 8.0, 19.0, 10.0]
</code></pre>

<p>This apparently means the model did not improve at all for all 500 episodes. Excuse me if I am a complete beginner at using Keras and OpenAI Gym (especially Keras). Any help is appreciated. Thank you.</p>

<p>UPDATE: Through some debugging, Iâ€™ve recently noticed that the model tends to go left, or choose action 0, most of the time. Does that mean I should make some if-statements to modify the reward system (e.g. increase the reward if the pole angle is less than 5 degrees)? In fact, I am doing that right now, but to no avail so far.</p>
",6942584,,6622587,,2019-07-03 03:20:49,2019-07-03 03:20:49,Keras Q-learning model performance doesn't improve when playing CartPole,<python><keras><reinforcement-learning><openai-gym><q-learning>,1,0,0,,,CC BY-SA 4.0,
328,56827875,1,60745714,,2019-06-30 20:33:16,,1,1089,"<p>I am trying to implement multiprocessing with OpenAI Gym. 
I know there are ready-made solutions out there but I am doing this to get some practice both on Gym and on multiprocessing.</p>

<p>The problem is that I am running in an error whose cause I can not identify. Here is the code:</p>

<pre class=""lang-py prettyprint-override""><code>import gym
import numpy as np
import multiprocessing as multip


class Environments:
    gym_environment = 'CarRacing-v0'

    def __init__(self, environments_n):
        self.environments_n = environments_n
        self.cores_count = multip.cpu_count()
        self.envs = []
        self.reset_all_environments()

    def __enter__(self):
        self.pool = multip.Pool(self.cores_count)
        return self

    def __exit__(self, exc_type, exc_value, tb):
        self.pool.close()
        self.pool.join()

    def reset_all_environments(self):
        for env in self.envs:
            env.close()
        self.envs = [gym.make(self.gym_environment) for _ in range(self.environments_n)]
        self.dones = [False for _ in range(self.environments_n)]
        self.last_obs = [env.reset() for env in self.envs]

    @staticmethod
    def step_single(env, action):
        print(action, env)
        return env.step(action)

    def step_all(self):
        actions = [env.action_space.sample() for env in self.envs]
        results = self.pool.starmap(self.step_single, zip(self.envs, actions))


if __name__ == '__main__':
    with Environments(2) as envs:
        print(envs.step_all())
</code></pre>

<p>In my expectation this should simply run one step in the 2 environments created, however I instead get this error:</p>

<pre><code>Track generation: 1134..1423 -&gt; 289-tiles track
retry to generate track (normal if there are not many of this messages)
Track generation: 1153..1445 -&gt; 292-tiles track
Track generation: 1188..1492 -&gt; 304-tiles track
retry to generate track (normal if there are not many of this messages)
Track generation: 1233..1554 -&gt; 321-tiles track
[0.01238655 0.20499504 0.07908794] &lt;TimeLimit&lt;CarRacing instance&gt;&gt;
[0.8294716 0.5289044 0.7956768] &lt;TimeLimit&lt;CarRacing instance&gt;&gt;
multiprocessing.pool.RemoteTraceback:
""""""
Traceback (most recent call last):
  File ""C:\Program Files\Python37\lib\multiprocessing\pool.py"", line 121, in worker
    result = (True, func(*args, **kwds))
  File ""C:\Program Files\Python37\lib\multiprocessing\pool.py"", line 47, in starmapstar
    return list(itertools.starmap(args[0], args[1]))
  File ""C:\Users\Nadni\PycharmProjects\ActorCritic\test.py"", line 33, in step_single
    return env.step(action)
  File ""C:\Program Files\Python37\lib\site-packages\gym\wrappers\time_limit.py"", line 15, in step
    observation, reward, done, info = self.env.step(action)
  File ""C:\Program Files\Python37\lib\site-packages\gym\envs\box2d\car_racing.py"", line 315, in step
    self.car.steer(-action[0])
AttributeError: 'NoneType' object has no attribute 'steer'
""""""

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File ""test.py"", line 42, in &lt;module&gt;
    print(envs.step_all())
  File ""test.py"", line 37, in step_all
    results = self.pool.starmap(self.step_single, zip(self.envs, actions))
  File ""C:\Program Files\Python37\lib\multiprocessing\pool.py"", line 276, in starmap
    return self._map_async(func, iterable, starmapstar, chunksize).get()
  File ""C:\Program Files\Python37\lib\multiprocessing\pool.py"", line 657, in get
    raise self._value
AttributeError: 'NoneType' object has no attribute 'steer'
Exception ignored in: &lt;function Viewer.__del__ at 0x000002DCC71E8A60&gt;
Traceback (most recent call last):
  File ""C:\Program Files\Python37\lib\site-packages\gym\envs\classic_control\rendering.py"", line 152, in __del__
  File ""C:\Program Files\Python37\lib\site-packages\gym\envs\classic_control\rendering.py"", line 71, in close
  File ""C:\Program Files\Python37\lib\site-packages\pyglet\window\win32\__init__.py"", line 305, in close
  File ""C:\Program Files\Python37\lib\site-packages\pyglet\window\__init__.py"", line 770, in close
ImportError: sys.meta_path is None, Python is likely shutting down
</code></pre>

<p>And I have no idea why it says <code>AttributeError: 'NoneType' object has no attribute 'steer'</code> even though it is clear from the print() that the <code>env</code> variable is correctly a member of <code>&lt;TimeLimit&lt;CarRacing instance&gt;&gt;</code></p>
",6625769,,,,,2020-03-18 18:45:24,"Error when implementing multiprocessing on OpenAI Gym, 'NoneType' object has no attribute 'steer'",<python><multiprocessing><python-multiprocessing><openai-gym>,1,0,,,,CC BY-SA 4.0,
333,52950547,1,52950774,,2018-10-23 13:38:50,,0,3757,"<pre><code>import random
import gym
import numpy as np
from collections import deque
from keras.models import Sequential
from keras.layers import Dense
from keras.optimizers import Adam
import os


env = gym.make('CartPole-v0')
state_size = env.observation_space.shape[0]

action_size = env.action_space.n


batch_size = 32

n_episodes = 1000

output_dir = 'model_output/cartpole'

if not os.path.exists(output_dir):
     os.makedirs(output_dir)


class DQNAgent:
     def __init__(self, state_size, action_size):
        self.state_size = state_size
        self.action_size = action_size

        self.memory = deque(maxlen=2000)

        self.gamma = 0.9
        self.epsilon = 1.0
        self.epsilon_decay = 0.995
        self.epsilon_min = 0.05

        self._learning_rate = 0.01

        self.model = self._build_model()

     def _build_model(self):

         model = Sequential()

         model.add(Dense(24, input_dim = self.state_size, activation='relu'))
         model.add(Dense(24,activation='relu'))
         model.add(Dense(24,activation='relu'))
         model.add(Dense(24,activation='relu'))
         model.add(Dense(24,activation='relu'))
         model.add(Dense(24,activation='relu'))
         model.add(Dense(24,activation='relu'))
         model.add(Dense(24,activation='relu'))
         model.add(Dense(24,activation='relu'))
         model.add(Dense(24,activation='relu'))
         model.add(Dense(24,activation='relu'))
         model.add(Dense(24,activation='relu'))
         model.add(Dense(24,activation='relu'))
         model.add(Dense(24,activation='relu'))
         model.add(Dense(24,activation='relu'))
         model.add(Dense(24,activation='relu'))
         model.add(Dense(24,activation='relu'))
         model.add(Dense(24,activation='relu'))
         model.add(Dense(24,activation='relu'))
         model.add(Dense(24,activation='relu'))
         model.add(Dense(24,activation='relu'))
         model.add(Dense(24,activation='relu'))
         model.add(Dense(24,activation='relu'))
         model.add(Dense(24,activation='relu'))
         model.add(Dense(24,activation='relu'))
         model.add(Dense(50,activation='relu'))

         model.add(Dense(self.action_size, activation='sigmoid'))
         model.compile(loss='mse', optimizer=Adam(lr=self._learning_rate))

         return model


     def remember(self, state, action, reward, next_state, done):
        self.memory.append((self, state, action, reward, next_state, done))

     def act(self, state):
        if np.random.rand() &lt;= self.epsilon:
           return random.randrange(self.action_size)
        act_values = self.model.predict(state)
        return np.argmax(act_values[0])

     def replay(self, batch_size):

         minibatch = random.sample(self.memory, batch_size)
         print(len(minibatch))
         for state, action, reward, next_state, done in minibatch:
            target = reward
            if not done:
                target = (reward + self.gamma*np.amax(self.model.predict(next_state)[0]))

            target_f = self.model.predict(state)
            target_f[0][action] = target

            self.model.fit(state, target_f, epochs=1, verboss=0)

            if self.epsilon &gt; self.epsilon_min:
               self.epsilon *= self.epsilon_decay

     def load(self,name):
         self.model.load_weights(name)

     def save(self, name):
          self.model.save_weights(name)



agent = DQNAgent(state_size, action_size)

done = False

for e in range(n_episodes):
     state = env.reset()
     state = np.reshape(state, [1, state_size])
     if agent.epsilon &gt; agent.epsilon_min:
        agent.epsilon *= agent.epsilon_decay

     for time in range(5000):

         # env.render()
          action = agent.act(state)

          next_state, reward, done,  _ = env.step(action)

          reward = reward if not done else -10

          next_state = np.reshape(next_state, [1, state_size])

          agent.remember(state, action, reward, next_state, done)

          state = next_state

          if done:
             print(""episode: {}/{}, score: {}, e: {:.2}"".format(e, n_episodes, time, agent.epsilon))
             break

     if len(agent.memory) &gt; batch_size:

        agent.replay(batch_size)

if e % 50 == 0:
    agent.save(output_dir + ""weights_"" + '{:04d}'.format(e) + "".hdf5"")          
</code></pre>

<p>I am creating an algorithm for the cartpole environment in openai gym, but I am getting this error:</p>

<p>Traceback (most recent call last):
  File ""C:/Users/ardao/Desktop/Ardaficial Intelligence/DQNs/CartPole.py"", line 145, in 
    agent.replay(batch_size)
  File ""C:/Users/ardao/Desktop/Ardaficial Intelligence/DQNs/CartPole.py"", line 93, in replay
    for state, action, reward, next_state, done in minibatch:
ValueError: too many values to unpack (expected 5)</p>

<p>I am following this tutorial: <a href=""https://www.youtube.com/watch?v=OYhFoMySoVs&amp;t=2444s"" rel=""nofollow noreferrer"">https://www.youtube.com/watch?v=OYhFoMySoVs&amp;t=2444s</a></p>

<p>Thanks</p>

<p>Arda</p>
",10546533,,,,,2022-09-18 18:09:27,Getting error: ValueError: too many values to unpack (expected 5),<python><python-3.x><openai-gym>,2,1,,,,CC BY-SA 4.0,
339,55986261,1,55986373,,2019-05-04 19:48:35,,0,503,"<p>I'm trying to do the full installation of OpenAI Gym, using the command: <code>pip install -e '.[all]'</code>. </p>

<p>However, I get the error:
<code>Fatal error in launcher: Unable to create process using '""c:\python37-32\python.exe""  ""C:\Python37-32\Scripts\pip.exe"" install -e '.[all]''</code>. Anyone knows what this means or came across this issue before? </p>

<p>I'm using Windows 7 (64-bit). Here's the info regarding the Python installed on my computer:
<code>Python 3.5.6 |Anaconda 4.2.0 (64-bit)| (default, Aug 26 2018, 16:05:27) [MSC v.1900 64 bit (AMD64)] on win32</code></p>

<p>Many thanks in advance for any help.</p>
",1766575,,,,,2019-05-04 20:05:23,"Unable to do full installation of OpenAI Gym - Unable to create process using '""c:\python37-32\python.exe""",<python><pip><openai-gym>,1,3,,,,CC BY-SA 4.0,
345,68818941,1,68986218,,2021-08-17 14:13:11,,0,225,"<p>I am trying to learn to work on <strong>ns-3 gym</strong> , the framework which helps in integrating <strong>openAigym</strong> and ns-3, by following the steps given <a href=""https://github.com/tkn-tub/ns3-gym"" rel=""nofollow noreferrer"">here</a>. But in the fourth step it says to install ns3gym from the src folder, the issue is that I'm not able to find the openaigym folder inside the src folder.</p>
<p>When I try to run the command it says not found.
Command used:</p>
<pre><code>pip3 install ./src/opengym/model/ns3gym
</code></pre>
<p>The exact error displayed is:</p>
<pre><code>ERROR: Invalid requirement: './src/opengym/model/ns3gym'
Hint: It looks like a path. File './src/opengym/model/ns3gym' does not exist.
</code></pre>
<p>It would be really helpful if someone could point the way or where i did wrong? Thanks.</p>
",13816376,,,,,2021-08-30 15:00:19,Trouble installing ns-3 gym,<python-3.x><openai-gym><ns-3>,1,0,,,,CC BY-SA 4.0,
351,57019490,1,57125317,,2019-07-13 13:11:25,,5,3293,"<p>I'm attempting to render OpenAI Gym environments in Colab via a Mac using the StarAI code referenced in previous questions on this topic. However, it fails. The key error (at least the first error) is shown in full below, but the import part seems to be ""Please install xdpyinfo!""</p>

<p>PyPI doesn't have xdpyinfo. What is it and how do I install it?</p>

<p>Full error message:</p>

<p>482780428160 abstractdisplay.py:151] xdpyinfo was not found, X start can not be checked! Please install xdpyinfo!
</p>
",11510783,,,,,2019-07-20 13:43:52,Missing package to enable rendering OpenAI Gym in Colab,<rendering><google-colaboratory><openai-gym>,1,0,0,,,CC BY-SA 4.0,
352,57174727,1,57218030,,2019-07-24 03:16:12,,0,298,"<p>Going through David Silverman's lectures and now trying to do some exercises to cement the knowledge, I have found that I don't understand what the probability returned actually refers to. In policy evaluation, we find </p>

<p>$v_{k+1}(s) = \sum_{a\in A} \pi(a|s)(R_s^a + \gamma\sum_{s'\in S}P^a_{ss'}v_k(s'))$</p>

<p>And I have successfully implemented this in Python for the gridworld environment;</p>

<pre><code>
def policy_eval(policy, env, discount_factor=1.0, theta=0.00001):
    V = np.zeros(env.nS)
    while True:
        delta = 0
        for state in range(env.nS):
            v = 0
            for action in range(env.nA):
                for prob, next_state, reward, done in env.P[state][action]:
                    v += policy[state][action] * prob * (reward + discount_factor * V[next_state])
            delta = max(delta, abs(v - V[state]))
            V[state] = v

        if delta &lt; theta:
            break
    return np.array(V)
</code></pre>

<p>I know policy[state][action] is the probability of doing that action in that state and reward is the reward of taking that action in that state, the other two and self-explanatory. I do not see how prob fits in and what it even does/refers to.</p>
",11424950,,,,,2019-07-26 10:21:44,"In OpenAI's gym, what does the 'prob' return from an environment relate to?",<probability><environment><policy><openai-gym>,1,0,,,,CC BY-SA 4.0,
355,55317749,1,55318251,,2019-03-23 19:52:32,,1,280,"<p>I have installed Gym Retro Mario Game. I am running in interactive mode and can see the award is getting printed on every move I make manually. Want to know how this award is getting calculated. If someone can point to any py file (location, line no) that would be great.</p>

<p>Gone through previous similar questions and gone through the code retro_env.py. Could not find the code of Mario Step and Reward</p>

<p>When I backtracked, I reached file retro_env.py. Below is the step function which should return the award:</p>

<pre><code>    def step(self, a):
        if self.img is None and self.ram is None:
            raise RuntimeError('Please call env.reset() before env.step()')

        for p, ap in enumerate(self.action_to_array(a)):
            if self.movie:
                for i in range(self.num_buttons):
                    self.movie.set_key(i, ap[i], p)
            self.em.set_button_mask(ap, p)

        if self.movie:
            self.movie.step()
        self.em.step()
        self.data.update_ram()
        ob = self._update_obs()
        rew, done, info = self.compute_step()
        return ob, rew, bool(done), dict(info)

</code></pre>

<p>However it is calling self.compute_step(), which is:</p>

<pre><code>    def compute_step(self):
        if self.players &gt; 1:
            reward = [self.data.current_reward(p) for p in range(self.players)]
        else:
            reward = self.data.current_reward()
        done = self.data.is_done()
        return reward, done, self.data.lookup_all()
</code></pre>

<p>This function calls <strong>current_reward()</strong> of <em>GameDataGlue</em> under retro._retro . However, there is no _retro folder in site-packages. Not sure how current_reward is getting calculated</p>

<p>I should be able to understand how mario reward is getting calculated. Then I would be able to apply to other games or even my own custom environment</p>
",10557045,,,,,2019-03-23 20:51:50,how openai gym retro game gets award,<openai-gym>,1,1,,,,CC BY-SA 4.0,
358,55313995,1,55326861,,2019-03-23 13:02:50,,0,407,"<p>following the guide at: <a href=""https://www.youtube.com/watch?v=8dY3nQRcsac&amp;list=PLTWFMbPFsvz3CeozHfeuJIXWAJMkPtAdS&amp;index=7"" rel=""nofollow noreferrer"">https://www.youtube.com/watch?v=8dY3nQRcsac&amp;list=PLTWFMbPFsvz3CeozHfeuJIXWAJMkPtAdS&amp;index=7</a> </p>

<p>when I run the python program the is an error in the python neat config file </p>

<p>it looks like it has something to do with the genome variable </p>

<p>my python neat config file now</p>

<pre><code>#--- parameters for the XOR-2 experiment ---#

[NEAT]
fitness_criterion     = max
fitness_threshold     = 10000
pop_size              = 20
reset_on_extinction   = True

[DefaultGenome]
# node activation options
activation_default      = sigmoid
activation_mutate_rate  = 0.05
activation_options      = sigmoid

# node aggregation options
aggregation_default     = sum
aggregation_mutate_rate = 0.05
aggregation_options     = sum

# node bias options
bias_init_mean          = 0.0
bias_init_stdev         = 1.0
bias_max_value          = 30.0
bias_min_value          = -30.0
bias_mutate_power       = 0.5
bias_mutate_rate        = 0.7
bias_replace_rate       = 0.1

# genome compatibility options
compatibility_disjoint_coefficient = 1.0
compatibility_weight_coefficient   = 0.5

# connection add/remove rates
conn_add_prob           = 0.5
conn_delete_prob        = 0.5

# connection enable options
enabled_default         = True
enabled_mutate_rate     = 0.01

feed_forward            = False
initial_connection      = unconn nected

# node add/remove rates
node_add_prob           = 0.5
node_delete_prob        = 0.2

# network parameters
num_hidden              = 0
num_inputs              = 1120
num_outputs             = 12

# node response options
response_init_mean      = 1.0
response_init_stdev     = 0.0
response_max_value      = 30.0
response_min_value      = -30.0
response_mutate_power   = 0.0
response_mutate_rate    = 0.0
response_replace_rate   = 0.0

# connection weight options
weight_init_mean        = 0.0
weight_init_stdev       = 1.0
weight_max_value        = 30
weight_min_value        = -30
weight_mutate_power     = 0.5
weight_mutate_rate      = 0.8
weight_replace_rate     = 0.1

[DefaultSpeciesSet]
compatibility_threshold = 205

[DefaultStagnation]
species_fitness_func = max
max_stagnation       = 50
species_elitism      = 0

[DefaultReproduction]
elitism            = 3
survival_threshold = 0.2
</code></pre>

<p>the Error code from the terminal </p>

<pre><code>    'config-feedforward')
  File ""/home/gym/OPAI/lib/python3.6/site-packages/neat/config.py"", line 189, in __init__
    self.genome_config = genome_type.parse_config(genome_dict)
  File ""/home/gym/OPAI/lib/python3.6/site-packages/neat/genome.py"", line 158, in parse_config
    return DefaultGenomeConfig(param_dict)
  File ""/home/gym/OPAI/lib/python3.6/site-packages/neat/genome.py"", line 72, in __init__
    assert self.initial_connection in self.allowed_connectivity
AssertionError
</code></pre>

<p>config code from the python neat code</p>

<pre><code>config = neat.Config(neat.DefaultGenome, neat.DefaultReproduction,
                     neat.DefaultSpeciesSet, neat.DefaultStagnation,
                     config-feedforward')
</code></pre>
",9867568,,,,,2019-03-24 18:04:41,How to fix config Error in python neat openai retro,<python><error-handling><config><openai-gym><neat>,1,0,0,,,CC BY-SA 4.0,
360,58041151,1,58041619,,2019-09-21 14:19:32,,0,2763,"<p>I'm new to Open Ai gym but I was trying to find a way to record each episodes of my agent's learning. I found the class that should do the job :  </p>

<pre><code>env = gym.make(""CartPole-v0"")
rec = VideoRecorder(env, ""recording.mp4"")
</code></pre>

<p>But each time, I'm using : </p>

<pre><code>rec.capture_frame()
</code></pre>

<p>I get the following error : </p>

<pre><code>PermissionError: [Errno 13] Permission denied: 'ffmpeg' Open AI GYM
</code></pre>

<p>Would you guys happen to know what could be the problem?</p>

<p>Thanks a lot !</p>
",11728128,,11702228,,2019-09-21 16:12:22,2019-09-21 16:12:22,PermissionError: [Errno 13] Permission denied: 'ffmpeg' Open AI GYM,<python><macos><reinforcement-learning><openai-gym>,1,3,,,,CC BY-SA 4.0,
363,58306485,1,58435584,,2019-10-09 14:29:23,,5,2511,"<p>I've created a custom openai gym environment with a discrete action space and a somewhat complicated state space.  The state space has been defined as a Tuple because it combines some dimensions which are continuous and others which are discrete:</p>

<pre><code>import gym
from gym import spaces

class CustomEnv(gym.Env):
    def __init__(self):
        self.action_space = spaces.Discrete(3)
        self.observation_space = spaces.Tuple((spaces.Discrete(16),
                                               spaces.Discrete(2),
                                               spaces.Box(0,20000,shape=(1,)),
                                               spaces.Box(0,1000,shape=(1,)))
    ...
</code></pre>

<p>I've had some luck training an agent using keras-rl, specifically the DQNAgent, however keras-rl is under-supported and very poorly documented.  Any recommendations for RL packages that can handle this type of observation space?  It doesn't appear that openai baselines, nor stable-baselines can handle it at present.  </p>

<p>Alternatively, is there a different way that I can define my state space in order to fit my environment into one of these better defined packages?</p>
",11614323,,,,,2021-10-26 07:12:49,Python Reinforcement Learning - Tuple Observation Space,<python><machine-learning><reinforcement-learning><openai-gym><keras-rl>,1,0,,,,CC BY-SA 4.0,
364,58551029,1,60792871,,2019-10-25 01:26:41,,6,5088,"<p>Rllib docs provide some information about how to <a href=""https://ray.readthedocs.io/en/latest/rllib-env.html"" rel=""noreferrer"">create and train a custom environment</a>. There is some information about registering that environment, but I guess it needs to work differently than <a href=""https://medium.com/@apoddar573/making-your-own-custom-environment-in-gym-c3b65ff8cdaa"" rel=""noreferrer"">gym registration</a>.</p>

<p>I'm testing this out working with the <a href=""https://github.com/ray-project/ray/blob/master/rllib/examples/custom_env.py"" rel=""noreferrer"">SimpleCorridor</a> environment. If I add the registration code to the file like so:</p>

<pre><code>from ray.tune.registry import register_env

class SimpleCorridor(gym.Env):
   ...


def env_creator(env_config):
    return SimpleCorridor(env_config)

register_env(""corridor"", env_creator)
</code></pre>

<p>Then I am able to train an algorithm using the string name no problem:</p>

<pre><code>if __name__ == ""__main__"":
    ray.init()
    tune.run(
        ""PPO"",
        stop={
            ""timesteps_total"": 10000,
        },
        config={
            ""env"": ""corridor"", # &lt;--- This works fine!
            ""env_config"": {
                ""corridor_length"": 5,
            },
        },
    )
</code></pre>

<p><strong>However</strong></p>

<p>It is kinda pointless to register the environment in the same file that you define the environment because you can just use the class. OpenAI gym registration is nice because if you install the environment, then you can use it anywhere just by writing </p>

<pre><code>include gym_corridor
</code></pre>

<p>It's not clear to me if there is a way to do the same thing for registering environments for rllib. Is there a way to do this?</p>
",7197899,,,,,2020-08-12 15:58:07,rllib use custom registered environments,<python><openai-gym><ray>,2,0,0,,,CC BY-SA 4.0,
367,60113091,1,60117462,,2020-02-07 11:59:51,,0,850,"<p>Please tell me what might be the problem?<br>
Only one environment starts correctly. (Trolley)</p>

<pre><code>import gym
env = gym.make('CarRacing-v0')
env.reset()
for _ in range(1000):
    env.render()
    env.step(env.action_space.sample())
</code></pre>

<blockquote>
  <p>AttributeError                            Traceback (most recent call last)  in ()
        1 import gym
  ----> 2 env = gym.make('CarRacing-v0')
        3 env.reset()
        4 for _ in range(1000):
        5     env.render()</p>
  
  <p>C:\Anaconda3\lib\site-packages\gym-0.15.6-py3.6.egg\gym\envs\registration.py
  in make(id, **kwargs)
      140 
      141 def make(id, **kwargs):
  --> 142     return registry.make(id, **kwargs)
      143 
      144 def spec(id):</p>
  
  <p>C:\Anaconda3\lib\site-packages\gym-0.15.6-py3.6.egg\gym\envs\registration.py
  in make(self, path, **kwargs)
       85             logger.info('Making new env: %s', path)
       86         spec = self.spec(path)
  ---> 87         env = spec.make(**kwargs)
       88         # We used to have people override _reset/_step rather than
       89         # reset/step. Set _gym_disable_underscore_compat = True on</p>
  
  <p>C:\Anaconda3\lib\site-packages\gym-0.15.6-py3.6.egg\gym\envs\registration.py
  in make(self, <strong>kwargs)
       56             env = self.entry_point(</strong>_kwargs)
       57         else:
  ---> 58             cls = load(self.entry_point)
       59             env = cls(**_kwargs)
       60 </p>
  
  <p>C:\Anaconda3\lib\site-packages\gym-0.15.6-py3.6.egg\gym\envs\registration.py
  in load(name)
       16     mod_name, attr_name = name.split("":"")
       17     mod = importlib.import_module(mod_name)
  ---> 18     fn = getattr(mod, attr_name)
       19     return fn
       20 </p>
  
  <p>AttributeError: module 'gym.envs.box2d' has no attribute 'CarRacing'</p>
</blockquote>
",12858238,,1723857,,2020-02-07 13:44:41,2020-02-07 16:29:02,Python-gym. Does not see the environment,<python><jupyter-notebook><openai-gym>,1,0,,,,CC BY-SA 4.0,
369,59944537,1,59945547,,2020-01-28 08:13:29,,2,1613,"<p>I wish to to fine-tune a GPT-2 implementation on some text data. I then want to use this model to complete a text prompt. I can do the first part easily enough using Max Woolf's <a href=""https://github.com/minimaxir/gpt-2-simple"" rel=""nofollow noreferrer"">gpt-2-simple</a> implementation. And <a href=""https://github.com/nshepperd/gpt-2"" rel=""nofollow noreferrer"">Neil Shepherd's fork</a> of OpenAI allows for GPT-2 to be trained on new data and completes text.</p>

<p>However, my corpus is too small to train on and not get gibberish back. Is there any way I can combine the two functions? Ideally, I'd like to be able to do this via a python interface (as opposed to CLI), as I'd like to use pandas for data cleaning and what-have-you. Thanks.</p>
",6289601,,1243762,,2020-11-29 12:02:45,2020-11-29 12:02:45,Is there a GPT-2 implementation that allows me to fine-tune and prompt for text completion?,<python-3.x><deep-learning><nlp><openai-gym><gpt-2>,1,0,0,,,CC BY-SA 4.0,
371,60129007,1,60129679,,2020-02-08 16:55:09,,2,2942,"<p>So i'm trying to perform some reinforcement learning in a custom environment using gym however I'm very confused as to how spaces.box works. What do each of the parameters mean? If I have a a game state that involves lots of information such as the hp of characters, their stats and abilities as an example, I'm not really sure something like this would be represented in Box as an observation state. Also in a game with a lot of abilities, would it be better to one-hot encode them or leave them as regular incremental Id's since i want to use a neural network to find expected Q values.</p>
",8963127,,,,,2022-10-03 19:32:00,gym.spaces.box Observation State Understanding,<deep-learning><reinforcement-learning><openai-gym>,2,0,,,,CC BY-SA 4.0,
373,56514941,1,56876122,,2019-06-09 13:23:20,,1,422,"<p>I am using vizdoom 1.1.7 on google colab.</p>

<p>For creating environment, i am using basic.cfg file.</p>

<p>But colab notebook is giving me an FileDoesNotExistException. </p>

<pre class=""lang-py prettyprint-override""><code>Code: 
from vizdoom import * 

game = DoomGame()
game.load_config(""basic.cfg"")
game.set_doom_scenario_path(""basic.wad"")
game.init()

Error: 
---------------------------------------------------------------------------
FileDoesNotExistException                 Traceback (most recent call last)
&lt;ipython-input-19-f8e9f2b2ba6f&gt; in &lt;module&gt;()
      1 
----&gt; 2 game, possible_actions = create_environment()

&lt;ipython-input-18-04cdc8c4b05b&gt; in create_environment()
      6 
      7     # Load the correct configuration
----&gt; 8     game.load_config(""basic.cfg"")
      9 
     10     # Load the correct scenario (in our case basic scenario)

FileDoesNotExistException: File ""basic.cfg"" does not exist.
</code></pre>
",9317030,,,,,2020-03-14 19:04:21,basic.cfg file not found in vizdoom while doom implementation,<python><openai-gym>,2,0,,,,CC BY-SA 4.0,
374,41975983,1,41979384,,2017-02-01 09:19:46,,1,878,"<p>Im trying to use Keras to solve the following OpenAi gym <a href=""https://gym.openai.com/envs/Humanoid-v1"" rel=""nofollow noreferrer"">environment</a>.
It uses <code>~360 neurons</code> for input and then it uses <code>17 real number outputs</code> with the <code>range [-0.4, 0.4]</code>. All the examples I have found online uses much simpler output layers with a single objective and no bounds.</p>

<p>My questions are:</p>

<ol>
<li>Do I need any special functions since the outputs are bounded?</li>
<li>Are there any example on how to constuct such a output layer with Keras?</li>
</ol>
",3139545,,,,,2017-02-01 12:05:18,Multiple objectives regression with bounded output in Keras,<machine-learning><tensorflow><deep-learning><keras><openai-gym>,1,0,,,,CC BY-SA 3.0,
375,42787924,1,42802225,,2017-03-14 13:55:06,,13,12424,"<p>When using the MountainCar-v0 environment from OpenAI-gym in Python the value done will be true after 200 time steps. Why is that? Because the goal state isn't reached, the episode shouldn't be done. </p>

<pre><code>import gym
env = gym.make('MountainCar-v0')
env.reset()
for _ in range(300):
    env.render()
    res = env.step(env.action_space.sample())
    print(_)
    print(res[2])
</code></pre>

<p>I want to run the step method until the car reached the flag and then break the for loop. Is this possible? Something similar to this:</p>

<pre><code>n_episodes = 10
done = False
for i in range(n_episodes):
    env.reset()
    while done == False:
        env.render()
        state, reward, done, _ = env.step(env.action_space.sample())
</code></pre>
",5580950,,,,,2017-03-15 23:35:10,Why is episode done after 200 time steps (Gym environment MountainCar)?,<python><openai-gym>,2,0,0,,,CC BY-SA 3.0,
380,58836093,1,58841279,,2019-11-13 11:26:55,,2,1769,"<p>I have created a custom gym environment where the actions can be any integer from -100 to +100. As far as I have seen it is no possible to create a discrete space that allows negative values, and the only solution I have come with is to create a Box space from -100 to +100 (notice that this is a continuous space).</p>

<p>Since most reinforcement learning agents assume a discrete space for the actions space I am having difficulties in running my code (I know there are some agents such as DDPG that runs in continuous action spaces).</p>

<p>It is possible to have in gym a discrete space that allows negative values?</p>
",4596240,,,,,2019-11-13 16:17:27,OpenAi-Gym Discrete Space with negative values,<python><python-3.x><reinforcement-learning><openai-gym>,1,0,,,,CC BY-SA 4.0,
384,43854157,1,43873915,,2017-05-08 17:49:27,,5,4591,"<p>Although I can manage to get the examples and my own code to run, I am more curious about the real semantics / expectations behind OpenAI gym API, in particular Env.reset()</p>

<p>When is reset expected/required? At the end of each episode? Or only after creating an environment?</p>

<p>I rather think it makes sense before each episode but I have not been able to read that explicitly!</p>
",157726,,157726,,2018-12-23 23:05:26,2022-10-08 15:41:30,OpenAI gym: when is reset required?,<python><machine-learning><reinforcement-learning><openai-gym>,2,0,,,,CC BY-SA 3.0,
386,44469266,1,44681040,,2017-06-10 03:38:43,,12,9635,"<p>I'm a complete newbie to Reinforcement Learning and have been searching for a framework/module to easily navigate this treacherous terrain. In my search I've come across two modules keras-rl &amp; OpenAI GYM.</p>

<p>I can get both of them two work on the examples they have shared on their WIKIs but they come with predefined environments and have little or no information on how to setup my own custom environment.</p>

<p>I would be really thankful if anyone could point me towards a tutorial or just explain it to me on how can i setup a non-game environment?</p>
",4570898,,562769,,2017-11-06 15:46:08,2017-11-06 15:46:08,How to implement custom environment in keras-rl / OpenAI GYM?,<keras><reinforcement-learning><openai-gym><keras-rl>,1,0,0,,,CC BY-SA 3.0,
390,55955630,1,55977591,,2019-05-02 15:22:49,,2,3187,"<p>I created a custom environment using OpenAI Gym. I want to have access to the max_episode_steps and reward_threshold that are specified in <strong><em>init.py</em></strong>
For eg:</p>

<pre><code>from gym.envs.registration import registry, register, make, spec
register(
    id='myenv-v0',
    entry_point='gym.envs.algorithmic:myenv',
    tags={'wrapper_config.TimeLimit.max_episode_steps': 200},
    reward_threshold=25.0,
)
</code></pre>

<p>But how do I access this from <strong><em>gym_myenv.py</em></strong>?
If I first create the environment and use env._max_episode_steps, I have access. However, I don't have access to <strong><em>_max_episode_steps</em></strong> from within <strong><em>gym_myenv.py</em></strong>.</p>
",7656080,,,,,2021-10-19 10:31:34,OpenAI Gym: How do I access environment registration data (for e.g. max_episode_steps) from within a custom OPenvironment?,<machine-learning><reinforcement-learning><openai-gym>,3,0,,,,CC BY-SA 4.0,
396,61874880,1,61884257,,2020-05-18 16:56:01,,1,988,"<p>How do I produce reproducible randomness with OpenAi-Gym and Scoop? </p>

<p>I want to have the exact same results every time I repeat the example. If possible I want this to work with existing libraries which use randomness-provider (e.g. random and np.random), which can be a problem because they usually use the global random-state and don't provide an interface for a local random state</p>

<p>My example script looks like this: </p>

<pre><code>import random
import numpy as np
from scoop import futures
import gym


def do(it):
    random.seed(it)
    np.random.seed(it)
    env.seed(it)
    env.action_space.seed(it)
    env.reset()
    observations = []
    for i in range(3):
        while True:
            action = env.action_space.sample()
            ob, reward, done, _ = env.step(action)
            observations.append(ob)
            if done:
                break
    return observations


env = gym.make(""BipedalWalker-v3"")
if __name__ == ""__main__"":
    maxit = 20
    results1 = futures.map(do, range(2, maxit))
    results2 = futures.map(do, range(2, maxit))
    for a,b in zip(results1, results2):
        if np.array_equiv(a, b):
            print(""equal, yay"")
        else:
            print(""not equal :("")

</code></pre>

<p>expected output: <code>equal, yay</code> on every line </p>

<p>actual output: <code>not equal :(</code> on multipe lines</p>

<p>full output: </p>

<pre><code>/home/chef/.venv/neuro/bin/python -m scoop /home/chef/dev/projekte/NeuroEvolution-CTRNN_new/random_test.py
[2020-05-18 18:05:03,578] launcher  INFO    SCOOP 0.7 1.1 on linux using Python 3.8.2 (default, Apr 27 2020, 15:53:34) [GCC 9.3.0], API: 1013
[2020-05-18 18:05:03,578] launcher  INFO    Deploying 4 worker(s) over 1 host(s).
[2020-05-18 18:05:03,578] launcher  INFO    Worker distribution: 
[2020-05-18 18:05:03,578] launcher  INFO       127.0.0.1:   3 + origin
/home/chef/.venv/neuro/lib/python3.8/site-packages/gym/logger.py:30: UserWarning: WARN: Box bound precision lowered by casting to float32
  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))
/home/chef/.venv/neuro/lib/python3.8/site-packages/gym/logger.py:30: UserWarning: WARN: Box bound precision lowered by casting to float32
  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))
/home/chef/.venv/neuro/lib/python3.8/site-packages/gym/logger.py:30: UserWarning: WARN: Box bound precision lowered by casting to float32
  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))
/home/chef/.venv/neuro/lib/python3.8/site-packages/gym/logger.py:30: UserWarning: WARN: Box bound precision lowered by casting to float32
  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))
equal, yay
not equal :(
not equal :(
not equal :(
not equal :(
not equal :(
equal, yay
not equal :(
equal, yay
equal, yay
equal, yay
equal, yay
equal, yay
not equal :(
equal, yay
equal, yay
equal, yay
not equal :(
[2020-05-18 18:05:08,554] launcher  (127.0.0.1:37729) INFO    Root process is done.
[2020-05-18 18:05:08,554] launcher  (127.0.0.1:37729) INFO    Finished cleaning spawned subprocesses.

Process finished with exit code 0

</code></pre>

<p>When I run this example without scoop, I get almost perfect results: </p>

<pre><code>/home/chef/.venv/neuro/bin/python /home/chef/dev/projekte/NeuroEvolution-CTRNN_new/random_test.py
/home/chef/.venv/neuro/lib/python3.8/site-packages/gym/logger.py:30: UserWarning: WARN: Box bound precision lowered by casting to float32
  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))
/home/chef/.venv/neuro/lib/python3.8/site-packages/scoop/fallbacks.py:38: RuntimeWarning: SCOOP was not started properly.
Be sure to start your program with the '-m scoop' parameter. You can find further information in the documentation.
Your map call has been replaced by the builtin serial Python map().
  warnings.warn(
not equal :(
equal, yay
equal, yay
equal, yay
equal, yay
equal, yay
equal, yay
equal, yay
equal, yay
equal, yay
equal, yay
equal, yay
equal, yay
equal, yay
equal, yay
equal, yay
equal, yay
equal, yay

Process finished with exit code 0
</code></pre>
",5132456,,,,,2020-05-19 06:03:20,How do I produce reproducible randomness with OpenAi-Gym and Scoop?,<random><concurrency><python-3.8><openai-gym><python-scoop>,1,0,0,,,CC BY-SA 4.0,
397,61893677,1,61894011,,2020-05-19 14:34:08,,0,452,"<p>I am following the <a href=""https://spinningup.openai.com/en/latest/user/installation.html"" rel=""nofollow noreferrer"">OpenAI's spinningUp tutorial</a> and I stucked in the installation part of the project. I am using Anaconda as said and when I do:</p>

<pre><code>pip install -e .
</code></pre>

<p>It gives following error:</p>

<pre><code>Obtaining file:///E:/Ege/UNIVERSITY-OF-SUSSEX/Dissertation/OpenAI/spinningup
Collecting cloudpickle==1.2.1
  Using cached cloudpickle-1.2.1-py2.py3-none-any.whl (25 kB)
Collecting gym[atari,box2d,classic_control]~=0.15.3
  Using cached gym-0.15.7.tar.gz (1.6 MB)
Collecting ipython
  Using cached ipython-7.14.0-py3-none-any.whl (782 kB)
Requirement already satisfied: joblib in c:\users\ege\anaconda3\envs\spinningup\lib\site-packages (from spinup==0.2.0) (0.14.1)
Collecting matplotlib==3.1.1
  Using cached matplotlib-3.1.1-cp36-cp36m-win_amd64.whl (9.1 MB)
Collecting mpi4py
  Using cached mpi4py-3.0.3-cp36-cp36m-win_amd64.whl (477 kB)
Requirement already satisfied: numpy in c:\users\ege\anaconda3\envs\spinningup\lib\site-packages (from spinup==0.2.0) (1.18.1)
Collecting pandas
  Using cached pandas-1.0.3-cp36-cp36m-win_amd64.whl (8.7 MB)
Collecting pytest
  Using cached pytest-5.4.2-py3-none-any.whl (247 kB)
Collecting psutil
  Using cached psutil-5.7.0-cp36-cp36m-win_amd64.whl (235 kB)
Requirement already satisfied: scipy in c:\users\ege\anaconda3\envs\spinningup\lib\site-packages (from spinup==0.2.0) (1.4.1)
Collecting seaborn==0.8.1
  Using cached seaborn-0.8.1.tar.gz (178 kB)
Requirement already satisfied: tensorflow&lt;2.0,&gt;=1.8.0 in c:\users\ege\anaconda3\envs\spinningup\lib\site-packages (from spinup==0.2.0) (1.15.0)
ERROR: Could not find a version that satisfies the requirement torch==1.3.1 (from spinup==0.2.0) (from versions: 0.1.2, 0.1.2.post1, 0.1.2.post2)
ERROR: No matching distribution found for torch==1.3.1 (from spinup==0.2.0)
</code></pre>

<p>My all packages are these in the environment:</p>

<pre><code># packages in environment at C:\Users\Ege\Anaconda3\envs\spinningup:
#
# Name                    Version                   Build  Channel
_pytorch_select           1.1.0                       cpu
absl-py                   0.9.0                    pypi_0    pypi
astor                     0.8.1                    pypi_0    pypi
blas                      1.0                         mkl
certifi                   2020.4.5.1               py36_0
cffi                      1.14.0           py36h7a1dbc1_0
cudatoolkit               10.2.89              h74a9793_1
freetype                  2.9.1                ha9979f8_1
gast                      0.2.2                    pypi_0    pypi
google-pasta              0.2.0                    pypi_0    pypi
grpcio                    1.29.0                   pypi_0    pypi
h5py                      2.10.0                   pypi_0    pypi
icc_rt                    2019.0.0             h0cc432a_1
importlib-metadata        1.6.0                    pypi_0    pypi
intel-openmp              2020.1                      216
joblib                    0.14.1                     py_0
jpeg                      9b                   hb83a4c4_2
keras-applications        1.0.8                    pypi_0    pypi
keras-preprocessing       1.1.2                    pypi_0    pypi
libpng                    1.6.37               h2a8f88b_0
libtiff                   4.1.0                h56a325e_0
markdown                  3.2.2                    pypi_0    pypi
mkl                       2020.1                      216
mkl-service               2.3.0            py36hb782905_0
mkl_fft                   1.0.15           py36h14836fe_0
mkl_random                1.1.0            py36h675688f_0
ninja                     1.9.0            py36h74a9793_0
numpy                     1.18.1           py36h93ca92e_0
numpy-base                1.18.1           py36hc3f5095_1
olefile                   0.46                     py36_0
opt-einsum                3.2.1                    pypi_0    pypi
pillow                    7.1.2            py36hcc1f983_0
pip                       20.0.2                   py36_3
protobuf                  3.12.0                   pypi_0    pypi
pycparser                 2.20                       py_0
python                    3.6.10               h9f7ef89_2
pytorch-metric-learning   0.9.86             pyh39e3cac_0    metric-learning
pyyaml                    5.3.1                    pypi_0    pypi
scikit-learn              0.22.1           py36h6288b17_0
scipy                     1.4.1            py36h9439919_0
setuptools                46.4.0                   py36_0
six                       1.14.0                   py36_0
sqlite                    3.31.1               h2a8f88b_1
tensorboard               1.15.0                   pypi_0    pypi
tensorflow                1.15.0                   pypi_0    pypi
tensorflow-estimator      1.15.1                   pypi_0    pypi
termcolor                 1.1.0                    pypi_0    pypi
tk                        8.6.8                hfa6e2cd_0
torchvision               0.2.2                      py_3    pytorch
tqdm                      4.46.0                     py_0
vc                        14.1                 h0510ff6_4
vs2015_runtime            14.16.27012          hf0eaf9b_1
werkzeug                  1.0.1                    pypi_0    pypi
wheel                     0.34.2                   py36_0
wincertstore              0.2              py36h7fe50ca_0
wrapt                     1.12.1                   pypi_0    pypi
xz                        5.2.5                h62dcd97_0
zipp                      3.1.0                    pypi_0    pypi
zlib                      1.2.11               h62dcd97_4
zstd                      1.3.7                h508b16e_0
</code></pre>

<p>And in the requirement.txt of the project doesn't even wants torch therefore I am assuming it is more about tensorflow:</p>

<pre><code>cloudpickle~=1.2.1
gym~=0.15.3
ipython
joblib
matplotlib
numpy
pandas
pytest
psutil
scipy
seaborn==0.8.1
sphinx==1.5.6
sphinx-autobuild==0.7.1       
sphinx-rtd-theme==0.4.1 
tensorflow&gt;=1.8.0,&lt;2.0
tqdm
</code></pre>

<p>How can I solve this issue ?</p>
",12216259,,2650249,,2020-05-19 16:36:05,2020-05-19 16:36:05,Could not install pytorch to my anaconda virtual environment,<python><pip><torch><openai-gym>,1,2,,,,CC BY-SA 4.0,
400,56727832,1,59644786,,2019-06-23 21:23:48,,3,7733,"<p>code:</p>

<pre><code>import gym
env = gym.make('Breakout-v0')
</code></pre>

<p>I get an error:</p>

<pre><code>Traceback (most recent call last):
File ""C:/Users/danie/Downloads/Programming/Python/Programming/Pycharm/app.py"", line 40, in 
gym.make(""Breakout-v0"")
File ""C:\Users\danie\AppData\Local\Programs\Python\Python37\lib\site-packages\gym\envs\registration.py"", line 156, in make
return registry.make(id, **kwargs)
File ""C:\Users\danie\AppData\Local\Programs\Python\Python37\lib\site-packages\gym\envs\registration.py"", line 101, in make
env = spec.make(**kwargs)
File ""C:\Users\danie\AppData\Local\Programs\Python\Python37\lib\site-packages\gym\envs\registration.py"", line 72, in make
cls = load(self.entry_point)
File ""C:\Users\danie\AppData\Local\Programs\Python\Python37\lib\site-packages\gym\envs\registration.py"", line 17, in load
mod = importlib.import_module(mod_name)
File ""C:\Users\danie\AppData\Local\Programs\Python\Python37\lib\importlib_init.py"", line 127, in import_module
return _bootstrap._gcd_import(name[level:], package, level)
File """", line 1006, in _gcd_import
File """", line 983, in _find_and_load
File """", line 967, in find_and_load_unlocked
File """", line 677, in load_unlocked
File """", line 728, in exec_module
File """", line 219, in call_with_frames_removed
File ""C:\Users\danie\AppData\Local\Programs\Python\Python37\lib\site-packages\gym\envs\atari_init.py"", line 1, in 
from gym.envs.atari.atari_env import AtariEnv
File ""C:\Users\danie\AppData\Local\Programs\Python\Python37\lib\site-packages\gym\envs\atari\atari_env.py"", line 9, in 
import atari_py
File ""C:\Users\danie\AppData\Local\Programs\Python\Python37\lib\site-packages\atari_py_init.py"", line 1, in 
from .ale_python_interface import *
File ""C:\Users\danie\AppData\Local\Programs\Python\Python37\lib\site-packages\atari_py\ale_python_interface.py"", line 18, in 
'ale_interface/build/ale_c.dll'))
File ""C:\Users\danie\AppData\Local\Programs\Python\Python37\lib\ctypes_init.py"", line 434, in LoadLibrary
return self.dlltype(name)
File ""C:\Users\danie\AppData\Local\Programs\Python\Python37\lib\ctypes_init.py"", line 356, in init
self._handle = _dlopen(self._name, mode)
OSError: [WinError 126] The specified module could not be found
</code></pre>
",11689548,,9537244,,2019-06-24 00:01:01,2021-05-15 15:46:25,how to fix environment error in open-ai gym?,<python><openai-gym>,4,0,,,,CC BY-SA 4.0,
404,46762083,1,46790472,,2017-10-16 02:21:38,,9,4788,"<p>Does anyone know how to run one of the OpenAI gym environments as a player. As in letting a human player play a round of cart pole? I have seen that there is env.mode = 'human' but I have not been able to get it to run properly. I have tried to follow the example given here <a href=""https://www.pinchofintelligence.com/getting-started-openai-gym/"" rel=""noreferrer"">https://www.pinchofintelligence.com/getting-started-openai-gym/</a> but it doesn't seem to work for me.</p>

<p>Any help you can give would be greatly appreciated.</p>

<p>Thanks</p>
",3704648,,3704648,,2017-10-16 13:24:10,2018-05-01 18:18:18,OpenAI gym player mode,<python><machine-learning><reinforcement-learning><openai-gym><openair>,2,0,0,,,CC BY-SA 3.0,
405,44777068,1,45175415,,2017-06-27 09:42:58,,5,1621,"<p>While training in the OpenAI gym environment I have the idea that the environment sometimes ""stops"". For many frames in a row no ball is visible/stops spawning. </p>

<p>Is this an error in the gym environment? Is this something that is part of the game Breakout-v0? </p>

<p>I was also wondering what the possible actions are in Breakout-v0. What I kind of figured out: </p>

<p>0 - do nothing/stand still?</p>

<p>1 - do nothing/stand still?</p>

<p>2 - apply ""force"" to the right?</p>

<p>3 - apply ""force"" to the left? </p>

<p>Edit: 
For people wondering what I'm talking about: see this gif: <a href=""https://imgur.com/a/pBLGX"" rel=""noreferrer"">http://imgur.com/a/pBLGX</a>
The transition between 5 and 4 lives takes a lot of frames... Sometimes the break is even longer than this...</p>
",6487788,,6487788,,2017-06-27 11:03:53,2017-07-18 19:24:30,"OpenAI gym's breakout-v0 ""pauses""",<openai-gym>,1,0,,,,CC BY-SA 3.0,
408,44874198,1,45215404,,2017-07-02 18:57:03,,1,411,"<p>i am implementing my first reinforcement deep learning model using tensorflow for which i am implementing <a href=""https://gym.openai.com/envs/CartPole-v0"" rel=""nofollow noreferrer"">cartpole problem</a> .</p>

<p>i have resorted to a deep neural network using six layers which trains on dataset generated randomly which has score above a threshold. the problem is that the model is not converging and the final score remains around 10 pts on an average.</p>

<p>as suggested after reading certain posts i applied regularization and dropouts to reduce any over-fitting that may occur but still no luck. i also tried reducing the learning rate.</p>

<p>the accuracy also remains around .60 just after training one batch though loss is decreasing in every iteration which i think it memorizes even after these.
though this kind of model works on simple deep learning tasks.</p>

<p>here is my code:</p>

<pre><code>import numpy as np
import tensorflow as tf
import gym
import os
import random

os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'
model_path = ""C:/Users/sanka/codes/cart pole problem/tf_save3""
env = gym.make(""CartPole-v0"")
env.reset()


def train_set():           #training set generation function
    try:
        tx = np.load(""final_trainx.npy"")
        ty = np.load(""final_trainy.npy"")
        return tx,ty
    except:
        tx = []
        ty = []
        for _ in range(10000):
            env.reset()
            score = 0
            moves = []
            obs = []
            p = []
            for _ in range(500):
                action = np.random.randint(0, 2)
                observation, reward, done, info = env.step(action)
                if (len(p)==0):
                    p = observation
                else:
                    moves += [action]
                    obs += [observation]
                    p = observation
                score += reward
                if done:
                    break
            if (score &gt; 50):
                tx+=obs
                for i in range(len(moves)):
                    ac = moves[i]
                    if (ac == 1):
                        ty.append([0, 1])
                    else:
                        ty.append([1, 0])
        tx=np.array(tx)
        ty=np.array(ty)
        np.save(""final_trainx.npy"",tx)
        np.save(""final_trainy.npy"",ty)
        return tx, ty


weights = {
    1: tf.Variable(tf.truncated_normal([4, 128]), dtype=tf.float32),
    2: tf.Variable(tf.truncated_normal([128, 256]), dtype=tf.float32),
    3: tf.Variable(tf.truncated_normal([256, 512]), dtype=tf.float32),
    4: tf.Variable(tf.truncated_normal([512, 256]), dtype=tf.float32),
    5: tf.Variable(tf.truncated_normal([256, 128]), dtype=tf.float32),
    6: tf.Variable(tf.truncated_normal([128, 2]), dtype=tf.float32)
}

biases = {
    1: tf.Variable(tf.truncated_normal([128]), dtype=tf.float32),
    2: tf.Variable(tf.truncated_normal([256]), dtype=tf.float32),
    3: tf.Variable(tf.truncated_normal([512]), dtype=tf.float32),
    4: tf.Variable(tf.truncated_normal([256]), dtype=tf.float32),
    5: tf.Variable(tf.truncated_normal([128]), dtype=tf.float32),
    6: tf.Variable(tf.truncated_normal([2]), dtype=tf.float32)
}


def neural_network(x):
    x = tf.nn.relu(tf.add(tf.matmul(x, weights[1]), biases[1]))
    x = tf.nn.dropout(x, 0.8)
    x = tf.nn.relu(tf.add(tf.matmul(x, weights[2]), biases[2]))
    x = tf.nn.dropout(x, 0.8)
    x = tf.nn.relu(tf.add(tf.matmul(x, weights[3]), biases[3]))
    x = tf.nn.dropout(x, 0.8)
    x = tf.nn.relu(tf.add(tf.matmul(x, weights[4]), biases[4]))
    x = tf.nn.dropout(x, 0.8)
    x = tf.nn.relu(tf.add(tf.matmul(x, weights[5]), biases[5]))
    x = tf.nn.dropout(x, 0.8)
    x = tf.add(tf.matmul(x, weights[6]), biases[6])
    return x


def test_nn(x):
    x = tf.nn.relu(tf.add(tf.matmul(x, weights[1]), biases[1]))
    x = tf.nn.relu(tf.add(tf.matmul(x, weights[2]), biases[2]))
    x = tf.nn.relu(tf.add(tf.matmul(x, weights[3]), biases[3]))
    x = tf.nn.relu(tf.add(tf.matmul(x, weights[4]), biases[4]))
    x = tf.nn.relu(tf.add(tf.matmul(x, weights[5]), biases[5]))
    x = tf.nn.softmax(tf.add(tf.matmul(x, weights[6]), biases[6]))
    return x


def train_nn():
    prediction = neural_network(x)
    loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=prediction, labels=y))
    lo=tf.nn.l2_loss(weights[1])+tf.nn.l2_loss(weights[2])+tf.nn.l2_loss(weights[3])+tf.nn.l2_loss(weights[4])+tf.nn.l2_loss(weights[5])+tf.nn.l2_loss(weights[6])
    loss=tf.reduce_mean(loss+0.01*lo)
    optimizer = tf.train.AdamOptimizer(learning_rate=0.0001).minimize(loss)
    test_pred = test_nn(x)
    correct = tf.equal(tf.argmax(test_pred, 1), tf.argmax(y, 1))
    accuracy = tf.reduce_mean(tf.cast(correct, dtype=tf.float32))
    with tf.Session() as sess:
        sess.run(tf.global_variables_initializer())
        epoches = 5
        batch_size = 100
        for j in range(epoches):
            ep_loss=0
            for i in range(0,len(train_x),batch_size):
                epoch_x=train_x[i:min(i+batch_size,len(train_x))]
                epoch_y = train_y[i:min(i + batch_size, len(train_y))]
                _,c=sess.run([optimizer,loss],feed_dict={x:epoch_x,y:epoch_y})
                ep_loss+=c
                #print(""Accuracy is {0}"".format(sess.run(accuracy, feed_dict={x: epoch_x, y: epoch_y})))
            print(""epoch {0} completed out of {1} with loss {2}"".format(j,epoches,ep_loss))
            print(""Accuracy is {0}"".format(sess.run(accuracy,feed_dict={x:train_x,y:train_y})))

        scores = []
        choices = []
        for each_game in range(10):
            print(""game "", each_game)
            score = 0
            game_memory = []
            prev_obs = []
            env.reset()
            for _ in range(500):
                env.render()
                if (len(prev_obs) == 0):
                    action = random.randrange(0, 2)
                else:
                    x1 = np.array([prev_obs]).reshape(-1,4)
                    a = tf.argmax(test_pred, 1)
                    action = sess.run(a, feed_dict={x: x1})
                    action=action[0]

                choices.append(action)
                new_observation, reward, done, info = env.step(action)
                prev_obs = new_observation
                game_memory.append([new_observation, action])
                score += reward
                if done:
                    break

            scores.append(score)

        print('Average Score:', sum(scores) / len(scores))
        print('choice 1:{}  choice 0:{}'.format(choices.count(1) / len(choices), choices.count(0) / len(choices)))



train_x,train_y=train_set()
print(train_x.shape)
print(train_y.shape)
x=tf.placeholder(tf.float32,[None,4])
y=tf.placeholder(tf.int32,[None,2])
train_nn()
</code></pre>
",6588864,,,,,2017-07-20 12:55:44,training a tensorflow model on openai cartpole,<tensorflow><deep-learning><reinforcement-learning><openai-gym>,1,0,0,,,CC BY-SA 3.0,
413,62397819,1,62398655,,2020-06-15 22:10:05,,0,602,"<p>My current code is based off of Pytorch's example on their website where they use env.render() to make the next state. That makes the game run very slow and would like it to run much quicker without the renderings. Here is the the class that uses the render function and at the bottom is the full code. Any ideas how I can get this to work?</p>

<pre><code>class CartPoleEnvManager(): #manage cartpole env
    def __init__(self, device):
        self.device = device
        self.env = gym.make('CartPole-v0').unwrapped #unwrapped so we can see other dynamics that there is no access to otherwise
        self.env.reset() #reset to starting state
        self.current_screen = None #screen initialization
        self.done = False #if episode is finished

    def reset(self):
        self.env.reset()
        self.current_screen = None

    def close(self): #close env
        self.env.close()

    def render(self, mode='human'): #render the current state to screen
        return self.env.render(mode)

    def num_actions_available(self): #returns # actions available to agent (2)
        return self.env.action_space.n

    def take_action(self, action):# step returns tuple containing env observation, reward and diagnostic info -- all from taking a certain action
        _, reward, self.done, _ = self.env.step(action.item()) # only reward and done status are of importance
        return torch.tensor([reward], device=self.device)
    #####action is a tensor, action.item() gives a number, what step wants

    def just_starting(self):
        return self.current_screen is None

    def get_state(self): #return to the current state of env in the form of a processed image of the screen
        if self.just_starting() or self.done:
            self.current_screen = self.get_processed_screen() #state = processed image of diff of 2 separate screens
            black_screen = torch.zeros_like(self.current_screen)
            return black_screen
        else:
            s1 = self.current_screen
            s2 = self.get_processed_screen() ####what is get_processed_screen? 
            self.current_screen = s2
            return s2 - s1 # this represents a single state

    def get_screen_height(self):
        screen = self.get_processed_screen()
        return screen.shape[2]

    def get_screen_width(self):
        screen = self.get_processed_screen()
        return screen.shape[3]

    def get_processed_screen(self):
        screen = self.env.render(mode='rgb_array').transpose((2, 0, 1)) # PyTorch expects CHW
        screen = self.crop_screen(screen)
        return self.transform_screen_data(screen)

    def crop_screen(self, screen):
        screen_height = screen.shape[1]

        # Strip off top and bottom
        top = int(screen_height * 0.4)
        bottom = int(screen_height * 0.8)
        screen = screen[:, top:bottom, :]
        return screen

    def transform_screen_data(self, screen):       
        # Convert to float, rescale, convert to tensor
        screen = np.ascontiguousarray(screen, dtype=np.float32) / 255
        screen = torch.from_numpy(screen)

        # Use torchvision package to compose image transforms
        resize = T.Compose([
            T.ToPILImage()
            ,T.Resize((40,90))
            ,T.ToTensor()
        ])


        return resize(screen).unsqueeze(0).to(self.device) # add a batch dimension (BCHW)
</code></pre>

<p>And the full code:</p>

<pre><code>import gym
import math
import random
import numpy as np
import matplotlib
import matplotlib.pyplot as plt
from collections import namedtuple
from itertools import count
from PIL import Image
import torch
import torch.nn as nnfr
import torch.optim as optim
import torch.nn.functional as F
import torchvision.transforms as T
import pennylane as qml
import numpy as np
import torch
import torch.nn as nn
from torch.nn.functional import relu, sigmoid



class DQN(nn.Module):
    def __init__(self, img_height, img_width):
        super().__init__()

        self.fc1 = nn.Linear(in_features=img_height*img_width*3, out_features=64)   
        self.fc2 = nn.Linear(in_features=64, out_features=48)
        self.fc3 = nn.Linear(in_features=48, out_features=32)
        self.out = nn.Linear(in_features=32, out_features=2)        



    def forward(self, b):
        b = b.flatten(start_dim=1)
        #t = F.relu(clayer_out)
        b = F.relu(self.fc1(b))
        b = F.relu(self.fc2(b))
        b = F.relu(self.fc3(b))
        b = self.out(b)
        return b



is_ipython = 'inline' in matplotlib.get_backend()
if is_ipython: from IPython import display

Experience = namedtuple(
    'Experience',
    ('state', 'action', 'next_state', 'reward')
) # use experiences to train network



class ReplayMemory():
    def __init__(self, capacity): # replay memory has set capacity, only need to
        self.capacity = capacity # initialize capacity for a ReplayMemory object
        self.memory = [] #memory will hold the experiences
        self.push_count = 0 #to show how many experiences we've added to memory

    def push(self, experience): # to add experiences in memory
        if len(self.memory) &lt; self.capacity: # ensuring that the length of memory doesn't exceed set capacity
            self.memory.append(experience) 
        else:
            self.memory[self.push_count % self.capacity] = experience # if memory greater than capacity, 
         # then the new experiences will be put to the front of memory, erasing the 
                            # oldest experiences in the memory array
        self.push_count += 1 
    def sample(self, batch_size): # returns a randome sample of experiences, to later use for 
        return random.sample(self.memory, batch_size) #random.sample(sequence, k), of the sequence, gives k randomly chosen experiences

    def can_provide_sample(self, batch_size): #to sample, batch size needs to be bigger than memory -- this is important at the beginning
        return len(self.memory) &gt;= batch_size


class EpsilonGreedyStrategy(): #explor vs. exploitation
    def __init__(self, start, end, decay):
        self.start = start
        self.end = end
        self.decay = decay

    def get_exploration_rate(self, current_step):  ####this was not explained in the video, woher kommt
        return self.end + self.start*(1/(1+self.decay*current_step))#self.end + (self.start - self.end) * \
            #math.exp(-1. * current_step * self.decay)


class LearningRate():
    def __init__(self, start, end, decay, current_step):
        self.start = start
        self.end = end
        self.decay = decay
        self.current_step = current_step
    def get_learning_rate(self, current_step):
        self.current_step += 1
        return self.end + self.start*(1/(1+self.decay*current_step))


class lr(): # learning rate class needed. Left for possible future use, need to update things beforehand
    def __init__(self, learning_rate):
        self.current_step = 0
        self.learning_rate = learning_rate
    def update_lr(self):    
        lrrate = learning_rate.get_learning_rate(self.current_step)
        self.current_step +=1
class Agent():
    def __init__(self, strategy, num_actions, device): # when we later create an agent object, need to get strategy from epsilon, num_actions = how many actions from a given state (2 for this game), device is the device in pytorch for tensor calculations CPU or GPU
        self.current_step = 0 # current step number in the environment
        self.strategy = strategy
        self.num_actions = num_actions
        self.device = device

    def select_action(self, state, policy_net): #policy_net is the policy trained by DQN
        rate = strategy.get_exploration_rate(self.current_step)
        self.current_step += 1

        if rate &gt; random.random():
            action = random.randrange(self.num_actions)
            return torch.tensor([action]).to(self.device) # explore      
        else:
            with torch.no_grad(): #turn off gradient tracking
                return policy_net(state).argmax(dim=1).to(self.device) # exploit



class CartPoleEnvManager(): #manage cartpole env
    def __init__(self, device):
        self.device = device
        self.env = gym.make('CartPole-v0').unwrapped #unwrapped so we can see other dynamics that there is no access to otherwise
        self.env.reset() #reset to starting state
        self.current_screen = None #screen initialization
        self.done = False #if episode is finished

    def reset(self):
        self.env.reset()
        self.current_screen = None

    def close(self): #close env
        self.env.close()

    def render(self, mode='human'): #render the current state to screen
        return self.env.render(mode)

    def num_actions_available(self): #returns # actions available to agent (2)
        return self.env.action_space.n

    def take_action(self, action):# step returns tuple containing env observation, reward and diagnostic info -- all from taking a certain action
        _, reward, self.done, _ = self.env.step(action.item()) # only reward and done status are of importance
        return torch.tensor([reward], device=self.device)
    #####action is a tensor, action.item() gives a number, what step wants

    def just_starting(self):
        return self.current_screen is None

    def get_state(self): #return to the current state of env in the form of a processed image of the screen
        if self.just_starting() or self.done:
            self.current_screen = self.get_processed_screen() #state = processed image of diff of 2 separate screens
            black_screen = torch.zeros_like(self.current_screen)
            return black_screen
        else:
            s1 = self.current_screen
            s2 = self.get_processed_screen() ####what is get_processed_screen? 
            self.current_screen = s2
            return s2 - s1 # this represents a single state

    def get_screen_height(self):
        screen = self.get_processed_screen()
        return screen.shape[2]

    def get_screen_width(self):
        screen = self.get_processed_screen()
        return screen.shape[3]

    def get_processed_screen(self):
        screen = self.env.render(mode='rgb_array').transpose((2, 0, 1)) # PyTorch expects CHW
        screen = self.crop_screen(screen)
        return self.transform_screen_data(screen)

    def crop_screen(self, screen):
        screen_height = screen.shape[1]

        # Strip off top and bottom
        top = int(screen_height * 0.4)
        bottom = int(screen_height * 0.8)
        screen = screen[:, top:bottom, :]
        return screen

    def transform_screen_data(self, screen):       
        # Convert to float, rescale, convert to tensor
        screen = np.ascontiguousarray(screen, dtype=np.float32) / 255
        screen = torch.from_numpy(screen)

        # Use torchvision package to compose image transforms
        resize = T.Compose([
            T.ToPILImage()
            ,T.Resize((40,90))
            ,T.ToTensor()
        ])


        return resize(screen).unsqueeze(0).to(self.device) # add a batch dimension (BCHW)


def plot(values, moving_avg_period):
    plt.figure(2)
    plt.clf()        
    plt.title('Training...')
    plt.xlabel('Episode')
    plt.ylabel('Duration')
    plt.plot(values)

    moving_avg = get_moving_average(moving_avg_period, values)
    plt.plot(moving_avg)    
    plt.pause(0.001)
    print(""Episode"", len(values), ""\n"", \
          moving_avg_period, ""episode moving avg:"", moving_avg[-1])
    if is_ipython: display.clear_output(wait=True)

def get_moving_average(period, values):
    values = torch.tensor(values, dtype=torch.float)
    if len(values) &gt;= period:
        moving_avg = values.unfold(dimension=0, size=period, step=1) \
            .mean(dim=1).flatten(start_dim=0)
        moving_avg = torch.cat((torch.zeros(period-1), moving_avg))
        return moving_avg.numpy()
    else:
        moving_avg = torch.zeros(len(values))
        return moving_avg.numpy()


def extract_tensors(experiences):
    # Convert batch of Experiences to Experience of batches
    batch = Experience(*zip(*experiences))

    t1 = torch.cat(batch.state)
    t2 = torch.cat(batch.action)
    t3 = torch.cat(batch.reward)
    t4 = torch.cat(batch.next_state)

    return (t1,t2,t3,t4)


class QValues():
    device = torch.device(""cuda"" if torch.cuda.is_available() else ""cpu"")

    @staticmethod
    def get_current(policy_net, states, actions):
        return policy_net(states).gather(dim=1, index=actions.unsqueeze(-1))

    @staticmethod        
    def get_next(target_net, next_states):                
        final_state_locations = next_states.flatten(start_dim=1) \
            .max(dim=1)[0].eq(0).type(torch.bool)
        non_final_state_locations = (final_state_locations == False)
        non_final_states = next_states[non_final_state_locations]
        batch_size = next_states.shape[0]
        values = torch.zeros(batch_size).to(QValues.device)
        values[non_final_state_locations] = target_net(non_final_states).max(dim=1)[0].detach()
        return values


batch_size = 128
gamma = 0.999
eps_start = 1
eps_end = 0.01
eps_decay = 0.0005
target_update = 10
memory_size = 500000
lr_start = 0.01
lr_end = 0.00001
lr_decay = 0.00009
num_episodes = 1000 # run for more episodes for better results

device = torch.device(""cuda"" if torch.cuda.is_available() else ""cpu"")
em = CartPoleEnvManager(device)
strategy = EpsilonGreedyStrategy(eps_start, eps_end, eps_decay)
agent = Agent(strategy, em.num_actions_available(), device)
memory = ReplayMemory(memory_size)


policy_net = DQN(em.get_screen_height(), em.get_screen_width()).to(device)
target_net = DQN(em.get_screen_height(), em.get_screen_width()).to(device)
target_net.load_state_dict(policy_net.state_dict())

target_net.eval() #tells pytorch that target_net is only used for inference, not training
optimizer = optim.Adam(params=policy_net.parameters(), lr=0.01)

i = 0
episode_durations = []
for episode in range(num_episodes): #iterate over each episode
    em.reset()
    state = em.get_state()

    for timestep in count():
        action = agent.select_action(state, policy_net)
        reward = em.take_action(action)
        next_state = em.get_state()
        memory.push(Experience(state, action, next_state, reward))
        state = next_state
        i = 0
        if memory.can_provide_sample(batch_size):
            scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=100, gamma=0.9)
            experiences = memory.sample(batch_size)
            states, actions, rewards, next_states = extract_tensors(experiences)

            current_q_values = QValues.get_current(policy_net, states, actions)
            next_q_values = QValues.get_next(target_net, next_states) #will get the max qvalues of the next state, q values of next state are used via next state
            target_q_values = (next_q_values * gamma) + rewards

            loss = F.mse_loss(current_q_values, target_q_values.unsqueeze(1))
            optimizer.zero_grad() # sets the gradiesnt of all weights n biases in policy_net to zero
            loss.backward() #computes gradient of loss with respect to all weights n biases in the policy net
            optimizer.step() # updates the weights n biases with the gradients that were computed form loss.backwards
            scheduler.step()

        if em.done:
            episode_durations.append(timestep)
            plot(episode_durations, 100)
            break
    if episode % target_update == 0:
        target_net.load_state_dict(policy_net.state_dict()) 


em.close()
</code></pre>
",8313547,,11839686,,2020-06-16 07:43:31,2020-06-16 07:43:31,How to remove showing renderings in the Cartpole game from OpenAI in Pytorch,<pytorch><openai-gym>,1,0,,,,CC BY-SA 4.0,
414,64773220,1,64857111,,2020-11-10 16:53:16,,1,613,"<p>What I would like to do is modify an environment, for example take the Super Mario Bros gym environment, and blur the image that the agent trains on, and see whether a reinforcement learning agent is still capable of learning on these &quot;blurred&quot; states.</p>
<p>Does OpenAI make it possible to do something like this? How would I add a gym environment pre-processing step?</p>
",7030203,,,,,2020-11-16 11:25:06,Is it possible to modify an OpenAI gym state before and during training?,<machine-learning><artificial-intelligence><openai-gym><openai>,1,0,,,,CC BY-SA 4.0,
417,45493987,1,47093917,,2017-08-03 20:28:52,,4,1713,"<p>I'm a complete newbie to Reinforcement Learning. And I have a question about the choice of the activation function of the output layer for the keras-rl agents. In all the examples provided by keras-rl (<a href=""https://github.com/matthiasplappert/keras-rl/tree/master/examples"" rel=""nofollow noreferrer"">https://github.com/matthiasplappert/keras-rl/tree/master/examples</a>) choose linear activation function in the output layer. Why is this? What effect would we expect if I go with different activation function? For example, if I work with an OpenAI environment with a discrete action space of 5, should I also consider using softmax in the output layer for an agent?
Thanks much in advance. </p>
",8413936,,,,,2017-11-03 10:47:11,why do keras-rl examples always choose linear activation in the output layer?,<keras><reinforcement-learning><openai-gym>,1,0,,,,CC BY-SA 3.0,
421,44252741,1,44279499,,2017-05-30 03:00:10,,2,254,"<p>In OpenAI baselines code on <a href=""https://github.com/openai/baselines/blob/master/baselines/deepq/simple.py"" rel=""nofollow noreferrer"">DQN</a>, <code>tf.stop_gradient</code> is used on the q values of the target network during building the operation graph to prevent the contributions of the target q values to the minimization of the loss. (line 213) </p>

<p>However, when calling <code>minimize</code>, the <code>var_list</code> is specified as only the <code>tf.Variable</code> with scope that falls under the q network being optimized, excluding the variables with scope under the target q network. (line 223)</p>

<p>I'm not sure why they do both. The two approaches seem to achieve the same result.</p>
",7076502,,,,,2017-05-31 08:27:36,OpenAI baselines: Why simultaneously use `tf.stop_gradient` and specify `var_list`?,<machine-learning><tensorflow><openai-gym>,1,1,,,,CC BY-SA 3.0,
423,44870717,1,45428893,,2017-07-02 12:15:57,,1,758,"<p>I would like to install the entire <a href=""https://gym.openai.com/"" rel=""nofollow noreferrer"">Open AI gym</a> package.
After (I am on a mac) <code>brew install cmake</code> , <code>sudo pip install gym[all]</code> gives me the following error</p>

<p><a href=""https://i.stack.imgur.com/2GRDe.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/2GRDe.png"" alt=""enter image description here""></a></p>

<p>I am using anaconda 4.3.22. I am <em>not</em> in a conda virtual environment. <code>which python</code> outputs <code>/anaconda/bin/python</code>. The interpreter that anaconda uses is python 3.6.1. Just running <code>sudo pip gym</code> (which must be the minimal version) ran fine.</p>

<p>Thanks for the help.</p>
",5379182,,,,,2017-08-01 04:17:22,Trouble installing OpenAI's gym,<python><anaconda><openai-gym>,1,1,,,,CC BY-SA 3.0,
429,65338895,1,65343913,,2020-12-17 10:28:06,,2,694,"<p>This is a general question on the advantages of using gym.Env as superclass (as opposed to nothing):</p>
<p>I am thinking of building my own reinforcement learning environment for a small experiment. I have read a couple of blog posts on how to build one with the Env class from the OpenAI Gym package (for example <a href=""https://medium.com/@apoddar573/making-your-own-custom-environment-in-gym-c3b65ff8cdaa"" rel=""nofollow noreferrer"">https://medium.com/@apoddar573/making-your-own-custom-environment-in-gym-c3b65ff8cdaa</a>). But it seems like I can create an environmnet without needing to use the class at all. E.g. if I wanted to create an env called Foo, the tutorials recommend I use something like</p>
<pre><code>class FooEnv(gym.Env)
</code></pre>
<p>But I can just as well use</p>
<pre><code>class FooEnv()
</code></pre>
<p>and my environmnent will still work in exactly the same way. I have seen one small benefit of using OpenAI Gym: I can initiate different versions of the environment in a cleaner way. But apart from that, can anyone describe or point out any resources on what big advantages the gym.Env superclass provides? I want to make sure I'm making full use of them :) thanks!</p>
",5013467,,,,,2020-12-17 15:46:45,Why use Env class from OpenAI Gym as opposed to nothing when creating a custom environment?,<python><reinforcement-learning><superclass><openai-gym>,1,0,,,,CC BY-SA 4.0,
430,65343997,1,65344326,,2020-12-17 15:51:18,,1,290,"<p>I'm trying to train an agent using Q learning to solve the maze.<br />
I created the environment using:</p>
<pre><code>import gym
import gym_maze 
import numpy as np

env = gym.make(&quot;maze-v0&quot;)
</code></pre>
<p>Since the states are in [x,y] coordinates and I wanted to have a 2D Q learning table, I created a dictionary that maps each state to a value:</p>
<pre><code>states_dic = {}
count = 0
for i in range(5):
    for j in range(5):
        states_dic[i, j] = count
        count+=1
</code></pre>
<p>Then I created the Q table:</p>
<pre><code>n_actions = env.action_space.n

#Initialize the Q-table to 0
Q_table = np.zeros((len(states_dic),n_actions))
print(Q_table)

[[0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]]
</code></pre>
<p>Some variables:</p>
<pre><code>#number of episode we will run
n_episodes = 10000
#maximum of iteration per episode
max_iter_episode = 100
#initialize the exploration probability to 1
exploration_proba = 1
#exploartion decreasing decay for exponential decreasing
exploration_decreasing_decay = 0.001
# minimum of exploration prob
min_exploration_proba = 0.01
#discounted factor
gamma = 0.99
#learning rate
lr = 0.1

rewards_per_episode = list()
</code></pre>
<p>But when I try to run the Q learning algorithm I get the error in the title.</p>
<pre><code>#we iterate over episodes
for e in range(n_episodes):
    #we initialize the first state of the episode
    current_state = env.reset()
    done = False
    
    #sum the rewards that the agent gets from the environment
    total_episode_reward = 0

    for i in range(max_iter_episode): 
        if np.random.uniform(0,1) &lt; exploration_proba:
            action = env.action_space.sample()
        else:
            action = np.argmax(Q_table[current_state,:])
            
        next_state, reward, done, _ = env.step(action)

        current_coordinate_x = int(current_state[0])
        current_coordinate_y = int(current_state[1])

        next_coordinate_x = int(next_state[0])
        next_coordinate_y = int(next_state[1])


        # update Q-table using the Q-learning iteration    
        current_Q_table_coordinates = states_dic[current_coordinate_x, current_coordinate_y]
        next_Q_table_coordinates = states_dic[next_coordinate_x, next_coordinate_y]
        
        Q_table[current_Q_table_coordinates, action] = (1-lr) *Q_table[current_Q_table_coordinates, action] +lr*(reward + gamma*max(Q_table[next_Q_table_coordinates,:]))
    
        total_episode_reward = total_episode_reward + reward
        # If the episode is finished, we leave the for loop
        if done:
            break
        current_state = next_state
    #We update the exploration proba using exponential decay formula 
    exploration_proba = max(min_exploration_proba,\
                            np.exp(-exploration_decreasing_decay*e))
    rewards_per_episode.append(total_episode_reward)
</code></pre>
<p>Update:<br />
Sharing the full error traceback:</p>
<pre><code>---------------------------------------------------------------------------
ValueError                                Traceback (most recent call last)
&lt;ipython-input-11-74e6fe3c1212&gt; in &lt;module&gt;()
     25         # The environment runs the chosen action and returns
     26         # the next state, a reward and true if the epiosed is ended.
---&gt; 27         next_state, reward, done, _ = env.step(action)
     28 
     29         ####    ####    ####    ####

/Users/x/anaconda3/envs/y/lib/python3.6/site-packages/gym/wrappers/time_limit.py in step(self, action)
     14     def step(self, action):
     15         assert self._elapsed_steps is not None, &quot;Cannot call env.step() before calling reset()&quot;
---&gt; 16         observation, reward, done, info = self.env.step(action)
     17         self._elapsed_steps += 1
     18         if self._elapsed_steps &gt;= self._max_episode_steps:

/Users/x/anaconda3/envs/y/lib/python3.6/site-packages/gym_maze-0.4-py3.6.egg/gym_maze/envs/maze_env.py in step(self, action)
     75             self.maze_view.move_robot(self.ACTION[action])
     76         else:
---&gt; 77             self.maze_view.move_robot(action)
     78 
     79         if np.array_equal(self.maze_view.robot, self.maze_view.goal):

/Users/x/anaconda3/envs/y/lib/python3.6/site-packages/gym_maze-0.4-py3.6.egg/gym_maze/envs/maze_view_2d.py in move_robot(self, dir)
     93         if dir not in self.__maze.COMPASS.keys():
     94             raise ValueError(&quot;dir cannot be %s. The only valid dirs are %s.&quot;
---&gt; 95                              % (str(dir), str(self.__maze.COMPASS.keys())))
     96 
     97         if self.__maze.is_open(self.__robot, dir):

ValueError: dir cannot be 1. The only valid dirs are dict_keys(['N', 'E', 'S', 'W']).
</code></pre>
<p>2nd update:
Fixed thanks to some of @Alexander L. Hayes debugging.</p>
<pre><code>#we iterate over episodes
for e in range(n_episodes):
    #we initialize the first state of the episode
    current_state = env.reset()
    done = False
    
    #sum the rewards that the agent gets from the environment
    total_episode_reward = 0

    for i in range(max_iter_episode): 
        current_coordinate_x = int(current_state[0])
        current_coordinate_y = int(current_state[1])
        current_Q_table_coordinates = states_dic[current_coordinate_x, current_coordinate_y]

        if np.random.uniform(0,1) &lt; exploration_proba:
            action = env.action_space.sample()
        else:
            action = int(np.argmax(Q_table[current_Q_table_coordinates]))


        next_state, reward, done, _ = env.step(action)

        next_coordinate_x = int(next_state[0])
        next_coordinate_y = int(next_state[1])


        # update our Q-table using the Q-learning iteration
        next_Q_table_coordinates = states_dic[next_coordinate_x, next_coordinate_y]
        
        Q_table[current_Q_table_coordinates, action] = (1-lr) *Q_table[current_Q_table_coordinates, action] +lr*(reward + gamma*max(Q_table[next_Q_table_coordinates,:]))
    
        total_episode_reward = total_episode_reward + reward
        # If the episode is finished, we leave the for loop
        if done:
            break
        current_state = next_state
    #We update the exploration proba using exponential decay formula 
    exploration_proba = max(min_exploration_proba,\
                            np.exp(-exploration_decreasing_decay*e))
    rewards_per_episode.append(total_episode_reward)


    
</code></pre>
",14735451,,14735451,,2020-12-17 17:09:37,2020-12-17 17:16:53,"OpenAI Gym - Maze - Using Q learning- ""ValueError: dir cannot be 0. The only valid dirs are dict_keys(['N', 'E', 'S', 'W']).""",<python><machine-learning><openai-gym><q-learning>,1,2,,,,CC BY-SA 4.0,
431,65389349,1,65943940,,2020-12-21 08:07:33,,1,4397,"<p>I wanted to install gym in my pc, so I tried
<code>pip install gym</code></p>
<pre><code>everything went fine till 
Collecting gym
  Using cached gym-0.18.0-py3-none-any.whl
Requirement already satisfied: numpy&gt;=1.10.4 in c:\users\csc\appdata\local\programs\python\python39\lib\site-packages (from gym) (1.19.4)
Requirement already satisfied: pyglet&lt;=1.5.0,&gt;=1.4.0 in c:\users\csc\appdata\local\programs\python\python39\lib\site-packages (from gym) (1.5.0)
Requirement already satisfied: scipy in c:\users\csc\appdata\local\programs\python\python39\lib\site-packages (from gym) (1.5.4)
Collecting cloudpickle&lt;1.7.0,&gt;=1.2.0
  Using cached cloudpickle-1.6.0-py3-none-any.whl (23 kB)
Collecting Pillow&lt;=7.2.0
  Using cached Pillow-7.2.0.tar.gz (39.1 MB)
Requirement already satisfied: future in c:\users\csc\appdata\local\programs\python\python39\lib\site-packages (from pyglet&lt;=1.5.0,&gt;=1.4.0-&gt;gym) (0.18.2)
Building wheels for collected packages: Pillow
  Building wheel for Pillow (setup.py) ... error
</code></pre>
<p>but got a big error</p>
<pre><code>ERROR: Command errored out with exit status 1:
   command: 'c:\users\csc\appdata\local\programs\python\python39\python.exe' -u -c 'import sys, setuptools, tokenize; sys.argv[0] = '&quot;'&quot;'C:\\Users\\CSC\\AppData\\Local\\Temp\\pip-install-xwue2xuw\\pillow_70b2da91b4b24fe8a3af6f2412c19502\\setup.py'&quot;'&quot;'; __file__='&quot;'&quot;'C:\\Users\\CSC\\AppData\\Local\\Temp\\pip-install-xwue2xuw\\pillow_70b2da91b4b24fe8a3af6f2412c19502\\setup.py'&quot;'&quot;';f=getattr(tokenize, '&quot;'&quot;'open'&quot;'&quot;', open)(__file__);code=f.read().replace('&quot;'&quot;'\r\n'&quot;'&quot;', '&quot;'&quot;'\n'&quot;'&quot;');f.close();exec(compile(code, __file__, '&quot;'&quot;'exec'&quot;'&quot;'))' bdist_wheel -d 'C:\Users\CSC\AppData\Local\Temp\pip-wheel-rj69kfwz'
       cwd: C:\Users\CSC\AppData\Local\Temp\pip-install-xwue2xuw\pillow_70b2da91b4b24fe8a3af6f2412c19502\
  Complete output (172 lines):
  C:\Users\CSC\AppData\Local\Temp\pip-install-xwue2xuw\pillow_70b2da91b4b24fe8a3af6f2412c19502\setup.py:42: RuntimeWarning: Pillow 7.2.0 does not support Python 3.9 and does not provide prebuilt Windows binaries. We do not recommend building from source on Windows.
    warnings.warn(
  running bdist_wheel
  running build
  running build_py
  creating build
  creating build\lib.win-amd64-3.9
  copying src\PIL\Image.py -&gt; build\lib.win-amd64-3.9\PIL
  copying src\PIL\ImageChops.py -&gt; build\lib.win-amd64-3.9\PIL
  copying src\PIL\ImageCms.py -&gt; build\lib.win-amd64-3.9\PIL
  writing dependency_links to src\Pillow.egg-info\dependency_links.txt
  writing top-level names to src\Pillow.egg-info\top_level.txt
  reading manifest file 'src\Pillow.egg-info\SOURCES.txt'
  reading manifest template 'MANIFEST.in'
  warning: no files found matching '*.c'
  warning: no files found matching '*.h'
  warning: no files found matching '*.sh'
  warning: no previously-included files found matching '.appveyor.yml'
  warning: no previously-included files found matching '.coveragerc'
  warning: no previously-included files found matching '.editorconfig'
  warning: no previously-included files found matching '.readthedocs.yml'
  warning: no previously-included files found matching 'codecov.yml'
  warning: no previously-included files matching '.git*' found anywhere in distribution
  warning: no previously-included files matching '*.pyc' found anywhere in distribution
  warning: no previously-included files matching '*.so' found anywhere in distribution
  no previously-included directories found matching '.ci'
  writing manifest file 'src\Pillow.egg-info\SOURCES.txt'
  running build_ext


  The headers or library files could not be found for zlib,
  a required dependency when compiling Pillow from source.

  Please see the install instructions at:
     https://pillow.readthedocs.io/en/latest/installation.html

  Traceback (most recent call last):
    File &quot;C:\Users\CSC\AppData\Local\Temp\pip-install-xwue2xuw\pillow_70b2da91b4b24fe8a3af6f2412c19502\setup.py&quot;, line 864, in &lt;module&gt;
      setup(
    File &quot;c:\users\csc\appdata\local\programs\python\python39\lib\site-packages\setuptools\__init__.py&quot;, line 153, in setup
      return distutils.core.setup(**attrs)
      self.run_command(cmd_name)
    File &quot;c:\users\csc\appdata\local\programs\python\python39\lib\distutils\cmd.py&quot;, line 313, in run_command
      self.distribution.run_command(command)
    File &quot;c:\users\csc\appdata\local\programs\python\python39\lib\distutils\dist.py&quot;, line 985, in run_command
      self.build_extensions()
    File &quot;C:\Users\CSC\AppData\Local\Temp\pip-install-xwue2xuw\pillow_70b2da91b4b24fe8a3af6f2412c19502\setup.py&quot;, line 694, in build_extensions
      raise RequiredDependencyException(f)
  __main__.RequiredDependencyException: zlib

  During handling of the above exception, another exception occurred:

  Traceback (most recent call last):
    File &quot;&lt;string&gt;&quot;, line 1, in &lt;module&gt;
    File &quot;C:\Users\CSC\AppData\Local\Temp\pip-install-xwue2xuw\pillow_70b2da91b4b24fe8a3af6f2412c19502\setup.py&quot;, line 918, in &lt;module&gt;
      raise RequiredDependencyException(msg)
  __main__.RequiredDependencyException:

  The headers or library files could not be found for zlib,
  a required dependency when compiling Pillow from source.

  Please see the install instructions at:
     https://pillow.readthedocs.io/en/latest/installation.html


  ----------------------------------------
  ERROR: Failed building wheel for Pillow
</code></pre>
<p>I don't know why is this happening I have pillow installed with out any problem but not this.
I even tried installing it by git but got same error.
I also edited the logs for making it short thinking most of it is present
I have :</p>
<ul>
<li>windows-10</li>
<li>python -3.9</li>
<li>pip-20.3.3</li>
</ul>
",,user14097926,11692264,,2020-12-21 08:56:11,2021-01-28 19:27:04,Problem with installing gym ( Building wheel for Pillow (setup.py) ... error),<python><pip><python-imaging-library><openai-gym>,2,0,,,,CC BY-SA 4.0,
435,74410196,1,74424489,,2022-11-12 03:32:28,,-1,25,"<p>I've built a simulator in python that emulates the process of accepting students into a university. It is fairly complex and I've based it off of real-world data so that I can run model-free reinforcement learning algorithms on it.</p>
<p>I've explored employing OpenAI gym, but it doesn't seem to make sense to use it with my already-built simulator, as I'd have to build an entirely new class reflecting the behavior of what I've already built, essentially re-writing it (and as I mentioned, it's fairly detailed).</p>
<p>I'm considering employing DQN in the simulator itself, but since it's not a graphic simulator, there are few resources available. Is there an easier way to do this with OpenAI gym, or is there another library/tool that might be of better use here?</p>
",6400631,,,,,2022-11-13 20:25:15,Best approach to apply RL to pre-built simulator?,<machine-learning><reinforcement-learning><simulator><openai-gym>,1,1,,,,CC BY-SA 4.0,
439,56282507,1,56302384,,2019-05-23 20:30:01,,1,153,"<p>I'm trying to make a simple python agent that can detect a turn in a game and turn left or right accordingly. However, I'm confused as to how to make the agent observe the screen and how to implement an agent into my code.</p>

<p>I'm still very new to machine learning and gym. I have the basic layout for using gym below,</p>

<pre><code>import gym
import universe

env = gym.make(â€˜flashgames.NeonRace-v0â€™)
env.configure(remotes=1)
observation_n = env.reset()

while True:
    action_n = [[('KeyEvent', 'ArrowUp', True)] for ob in observation_n]
    #Your agent here
    observation_n, reward_n, done_n, info = env.step(action_n)
    env.render()
</code></pre>

<p>Below is the layout for an agent,</p>

<pre><code>def getAgent():
   """""" The daemon searches for this callable function to create a new agent with """"""
   return MyAgent()

class MyAgent(object):

    def __init__(self):
       """""" standard object init """"""
       self.done = False

    def run(self, messaging, args):
       """""" Call by daemon when the agent is to start running """"""
       while not self.done:
           pass

    def stop(self):
       """""" Called by daemon when the thread is requested to stop """"""
       self.done = True
</code></pre>

<p>I would start implementing the code, but whenever it got to observing the screen I would get stuck.</p>
",9545957,,1866961,,2019-05-23 22:45:00,2019-05-25 06:35:54,How to make a python agent observe?,<python><machine-learning><openai-gym>,1,0,0,,,CC BY-SA 4.0,
445,65333060,1,65356962,,2020-12-17 00:27:38,,1,816,"<p>I'm very new to Ray RLlib and have an issue with using a custom simulator my team made.
We're trying to integrate a custom Python-based simulator into Ray RLlib to do a single-agent DQN training. However, I'm uncertain about how to integrate the simulator into RLlib as an environment.</p>
<p>According to the image below from Ray documentation, it seems like I have two different options:</p>
<ol>
<li><strong>Standard environment</strong>: according to the <a href=""https://github.com/layssi/Carla_Ray_Rlib/blob/master/carla_env.py"" rel=""nofollow noreferrer"">Carla simulator example</a>, it seems like I can just simply use the <code>gym.Env</code> class API to wrap my custom simulator and register as an environment using <code>ray.tune.registry.register_env</code> function.</li>
<li><strong>External environment</strong>: however, the image below and RLlib documentation gave me more confusion since it's suggesting that external simulators that can run independently outside the control of RLlib should be used via the <code>ExternalEnv</code> class.</li>
</ol>
<p>If anyone can suggest what I should do, it will be very much appreciated! Thanks!
<a href=""https://i.stack.imgur.com/yBmpi.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/yBmpi.png"" alt=""Ray RLlib Environments"" /></a></p>
",14622788,,,,,2021-02-04 13:08:45,Using Ray RLlib with custom simulator,<machine-learning><reinforcement-learning><openai-gym><ray><rllib>,2,0,,,,CC BY-SA 4.0,
452,69891729,1,69892224,,2021-11-09 01:18:14,,-2,2166,"<p>I am trying to  train a model to solve the FrozenLake-v0 problem.</p>
<p>In the process am trying to instantiate the environment in the following way. But encountering an error. Please help me with this</p>
<pre><code>!apt install xvfb -y
!pip install pyvirtualdisplay
!pip install piglet

from pyvirtualdisplay import Display
pip install gym
import torch
import time
import matplotlib.pyplot as plt
pip install pyglet
env = gym.make('FrozenLake-v0')

---------------------------------------------------------------------------
NameError                                 Traceback (most recent call last)
&lt;ipython-input-13-3d8d6ba4b349&gt; in &lt;module&gt;()
----&gt; 1 env = gym.make('FrozenLake-v0')

NameError: name 'gym' is not defined
</code></pre>
",17266399,,,,,2021-11-09 02:44:35,NameError: name 'env' is not defined,<python><reinforcement-learning><openai-gym>,1,2,,2021-11-09 02:49:28,,CC BY-SA 4.0,
459,54259943,1,54261003,,2019-01-18 18:57:26,,0,495,"<p>I have a simple pytorch neural net that I copied from <a href=""https://github.com/pytorch/examples/blob/master/reinforcement_learning/reinforce.py"" rel=""nofollow noreferrer"">openai</a>, and I modified it to some extent (mostly the input).</p>

<p>When I run my code, the output of the network remains the same on every episode, as if no training occurs.</p>

<p>I want to see if any training happens, or if some other reason causes the results to be the same.</p>

<p>How can I make sure any movement happens to the weights?</p>

<p>Thanks</p>
",913098,,913098,,2019-01-18 19:17:50,2019-02-27 00:10:43,How to look at the parameters of a pytorch model?,<machine-learning><neural-network><pytorch><openai-gym>,1,0,0,,,CC BY-SA 4.0,
461,54322590,1,54444618,,2019-01-23 08:09:22,,4,1364,"<p>I was wondering if anyone knows if there is a tutorial or any information about how to modify the environment CarRacing-v0 from openai gym, more exactly how to create different roads, I haven't found anything about it.</p>
<p>What I want to do is to create a track more difficult, with T-junction, narrow streets in some points maybe add some obstacles, etc. I have been looking at <code>_create_track</code>in <code>car_racing.py</code> but modifying it looks rather tedious and I don't want to start working on it if there is another easier solution.</p>
",4792548,,3924118,,2020-11-03 19:23:18,2020-11-03 19:23:18,How do I modify the gym's environment CarRacing-v0?,<openai-gym>,1,0,,,,CC BY-SA 4.0,
464,54259338,1,56561457,,2019-01-18 18:05:07,,13,7842,"<p>Following <a href=""https://github.com/openai/gym/issues/748"" rel=""noreferrer"">this</a> (unreadable) forum post, I thought it was fitting to post it up on stack overflow for future generations who search for it.</p>

<p><strong>How to pass arguments for gym environments on init?</strong></p>
",913098,,1609514,,2019-05-16 23:08:14,2020-09-15 09:31:29,How to pass arguments to openai-gym environments upon init,<openai-gym>,2,0,0,,,CC BY-SA 4.0,
466,54385568,1,54432783,,2019-01-27 06:17:31,,0,728,"<p>I've been learning tensorflow and rl for months, and for the past few days I've been trying to solve <em>OpenAI</em> <em>Cartpole</em> with my own code but my <em>Deep Q-Network</em> can't seem to solve it. I've checked and compared my code to other implementations and I don't see where I am going wrong? Can anyone look over my implementation and <em>teach me what I am messing up?</em> It would mean a lot, thanks. </p>

<p>My code: </p>

<p><div class=""snippet"" data-lang=""js"" data-hide=""false"" data-console=""true"" data-babel=""false"">
<div class=""snippet-code"">
<pre class=""snippet-code-html lang-html prettyprint-override""><code>import gym
import numpy as np
import tensorflow as tf
import math
import keras
import random

class cartpole:
    def __init__(self, sess, env):
        self.env = env
        self.state_size = env.observation_space.shape[0]
        self.num_actions = env.action_space.n
        self.sess = sess
        self.epsilon = 1.0
        self.return_loss = 0.0
        self.memory = []
        self.gamma = .95

        self.q_model()
        init = tf.global_variables_initializer()
        self.sess.run(init)
    
    def q_model(self):
        self.state_input = tf.placeholder(shape=[None, self.state_size], dtype=tf.float32)
        self.reward_labels = tf.placeholder(shape=[None, 1], dtype=tf.float32)

        self.hiddenlayer1_weights = tf.Variable(tf.random_normal([self.state_size, 32]))
        self.hiddenlayer1_bias = tf.Variable(tf.random_normal([32]))
        self.hiddenlayer1_output = tf.matmul(self.state_input, self.hiddenlayer1_weights) + self.hiddenlayer1_bias
        self.hiddenlayer1_output = tf.nn.relu(self.hiddenlayer1_output)

        self.hiddenlayer2_weights = tf.Variable(tf.random_normal([32, 16]))
        self.hiddenlayer2_bias = tf.Variable(tf.random_normal([16]))
        self.hiddenlayer2_output = tf.matmul(self.hiddenlayer1_output, self.hiddenlayer2_weights) + self.hiddenlayer2_bias
        self.hiddenlayer2_output = tf.nn.relu(self.hiddenlayer2_output)


        self.q_weights = tf.Variable(tf.random_normal([16, self.num_actions]))
        self.q_bias = tf.Variable(tf.random_normal([self.num_actions]))
        self.q_output = tf.matmul(self.hiddenlayer2_output, self.q_weights) + self.q_bias
        self.q_output = keras.activations.linear(self.q_output)
        
        
        self.max_q_value = tf.reshape(tf.reduce_max(self.q_output), (1,1))
        self.best_action = tf.squeeze(tf.argmax(self.q_output, axis=1))

        self.loss = tf.losses.mean_squared_error(self.max_q_value, self.reward_labels)
        self.train_model = tf.train.AdamOptimizer(learning_rate=0.001).minimize(self.loss)
    
    def predict_action(self, state):
        self.epsilon *= .995 + .01
        if (np.random.random() &lt; self.epsilon):
            action = env.action_space.sample()
        else:
            action = self.sess.run(self.best_action, feed_dict={self.state_input: state})
        return action

    def predict_value(self, state):
        state = np.array(state).reshape((1, 4))
        max_q_value = self.sess.run(self.max_q_value, feed_dict={self.state_input: state})[0][0]
        return max_q_value

    
    def train_q_model(self, state, reward):
        q_values, _, loss = self.sess.run([self.q_output, self.train_model, self.loss], feed_dict={self.state_input: state, self.reward_labels: reward})
        self.return_loss = loss

    def get_loss(self):
        return self.return_loss

    def experience_replay(self):
        if len(self.memory) &lt; 33:
            return
        del self.memory[0]
        batch = random.sample(self.memory, 32)
        for state, action, reward, new_state, done in self.memory:
            reward = reward if not done else - reward
            new_state = np.array(new_state).reshape((1, 4))
            if not done:
                reward = reward + (self.gamma * self.predict_value(new_state)) 
            reward = np.array(reward).reshape((1, 1))
            
            self.train_q_model(state, reward)



env = gym.make(""CartPole-v0"")
sess = tf.Session()
A2C = cartpole(sess, env)

episodes = 2000
reward_history = []
for i in range(episodes):
    state = env.reset()
    reward_total = 0
    while True:
        state = np.array(state).reshape((1, 4))
        average_best_reward = sum(reward_history[-100:]) / 100.0
        if (average_best_reward) &gt; 195:
            env.render()

        action = A2C.predict_action(state)
        new_state, reward, done, _ = env.step(action)
        reward_total += reward
        A2C.memory.append([state, action, reward, new_state, done])
        A2C.experience_replay()
        state = new_state


        if done:
            if (average_best_reward &gt;= 195):
                print(""Finished! Episodes taken: "", i, ""average reward: "", average_best_reward)
            print(""average reward  = "", average_best_reward, ""reward total = "", reward_total, ""loss = "", A2C.get_loss())
            reward_history.append(reward_total)
            break</code></pre>
</div>
</div>
</p>
",10906853,,,,,2020-10-22 09:10:51,Tensorflow DQN can't solve OpenAI Cartpole,<python-3.x><tensorflow><reinforcement-learning><openal><openai-gym>,2,0,,,,CC BY-SA 4.0,
469,70261352,1,70461422,,2021-12-07 13:52:47,,2,1285,"<p>I am trying to set a Deep-Q-Learning agent with a custom environment in OpenAI Gym. I have 4 continuous state variables with individual limits and 3 integer action variables with individual limits.</p>
<p>Here is the code:</p>
<pre><code>#%% import 
from gym import Env
from gym.spaces import Discrete, Box, Tuple
import numpy as np


#%%
class Custom_Env(Env):

    def __init__(self):
        
       # Define the state space
       
       #State variables
       self.state_1 = 0
       self.state_2 =  0
       self.state_3 = 0
       self.state_4_currentTimeSlots = 0
       
       #Define the gym components
       self.action_space = Box(low=np.array([0, 0, 0]), high=np.array([10, 20, 27]), dtype=np.int)    
                                                                             
       self.observation_space = Box(low=np.array([20, -20, 0, 0]), high=np.array([22, 250, 100, 287]),dtype=np.float16)

    def step(self, action ):

        # Update state variables
        self.state_1 = self.state_1 + action [0]
        self.state_2 = self.state_2 + action [1]
        self.state_3 = self.state_3 + action [2]

        #Calculate reward
        reward = self.state_1 + self.state_2 + self.state_3
       
        #Set placeholder for info
        info = {}    
        
        #Check if it's the end of the day
        if self.state_4_currentTimeSlots &gt;= 287:
            done = True
        if self.state_4_currentTimeSlots &lt; 287:
            done = False       
        
        #Move to the next timeslot 
        self.state_4_currentTimeSlots +=1

        state = np.array([self.state_1,self.state_2, self.state_3, self.state_4_currentTimeSlots ])

        #Return step information
        return state, reward, done, info
        
    def render (self):
        pass
    
    def reset (self):
       self.state_1 = 0
       self.state_2 =  0
       self.state_3 = 0
       self.state_4_currentTimeSlots = 0
       state = np.array([self.state_1,self.state_2, self.state_3, self.state_4_currentTimeSlots ])
       return state

#%% Set up the environment
env = Custom_Env()

#%% Create a deep learning model with keras


from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Flatten
from tensorflow.keras.optimizers import Adam

def build_model(states, actions):
    model = Sequential()
    model.add(Dense(24, activation='relu', input_shape=states))
    model.add(Dense(24, activation='relu'))
    model.add(Dense(actions[0] , activation='linear'))
    return model

states = env.observation_space.shape 
actions = env.action_space.shape 
print(&quot;env.observation_space: &quot;, env.observation_space)
print(&quot;env.observation_space.shape : &quot;, env.observation_space.shape )
print(&quot;action_space: &quot;, env.action_space)
print(&quot;action_space.shape : &quot;, env.action_space.shape )


model = build_model(states, actions)
print(model.summary())

#%% Build Agent wit Keras-RL
from rl.agents import DQNAgent
from rl.policy import BoltzmannQPolicy
from rl.memory import SequentialMemory

def build_agent (model, actions):
    policy = BoltzmannQPolicy()
    memory = SequentialMemory(limit = 50000, window_length=1)
    dqn = DQNAgent (model = model, memory = memory, policy=policy,
                    nb_actions=actions, nb_steps_warmup=10, target_model_update= 1e-2)
    return dqn

dqn = build_agent(model, actions)
dqn.compile(Adam(lr=1e-3), metrics = ['mae'])
dqn.fit (env, nb_steps = 4000, visualize=False, verbose = 1)
</code></pre>
<p>When I run this code I get the following error message</p>
<pre><code>ValueError: Model output &quot;Tensor(&quot;dense_23/BiasAdd:0&quot;, shape=(None, 3), dtype=float32)&quot; has invalid shape. DQN expects a model that has one dimension for each action, in this case (3,).
</code></pre>
<p>thrown by the line <code>    dqn = DQNAgent (model = model, memory = memory, policy=policy, nb_actions=actions, nb_steps_warmup=10, target_model_update= 1e-2)</code></p>
<p>Can anyone tell me, why this problem is occuring and how to solve this issue? I assume it has something to do with the built model and thus with the action and state spaces. But I could not figure out what exactly the problem is.</p>
<p><strong>Reminder on the bounty</strong>: My bounty is expiring quite soon and unfortunately, I still have not received any answer. If you at least have a guess how to tackle that problem, I'll highly appreciate if you share your thoughts with me and I would be quite thankful for it.</p>
",7133942,,7133942,,2021-12-23 08:17:53,2022-03-02 10:55:37,OpenAI-Gym and Keras-RL: DQN expects a model that has one dimension for each action,<python><keras><reinforcement-learning><openai-gym>,2,8,0,,,CC BY-SA 4.0,
471,54166346,1,54168517,,2019-01-13 05:33:32,,0,75,"<p>I've been trying to solve the OpenAI <code>MountainCarContinuous-v0</code> environment for a while but I have been stuck. </p>

<p>After spending weeks on my own trying to solve it, I am now just trying to understand someone else's code. <a href=""https://github.com/sezan92/Actor-Critic/blob/master/ac_mountain_car.py"" rel=""nofollow noreferrer"">Here is the link the person used to solve the enviroment</a>. Specifically, I need help with the loss function. </p>

<p>In the GitHub code is written as </p>

<pre><code>self.norm_dist = tf.contrib.distributions.Normal(self.mu, self.sigma)
self.loss = -tf.log(self.norm_dist.prob(self.action_train) + 1e-5) * self.advantage_train - self.lamb * self.norm_dist.entropy()
</code></pre>

<p>What is this loss function doing? If you could describe it in simple terms that would help me so much. </p>
",10906853,,4685471,,2019-01-13 16:15:14,2019-01-13 16:15:14,I need help understanding reinforcement learning code,<tensorflow><machine-learning><reinforcement-learning><openai-gym>,1,0,0,,,CC BY-SA 4.0,
473,54372092,1,62997234,,2019-01-25 20:05:47,,3,1235,"<p>When I import <code>universe</code> module from openAI. I get the following error. </p>

<pre class=""lang-none prettyprint-override""><code>Traceback (most recent call last):
  File ""&lt;stdin&gt;"", line 1, in &lt;module&gt;
  File ""/home/kunal/Desktop/OpenAi/openAI/lib/python3.7/site-packages/universe/__init__.py"", line 22, in &lt;module&gt;
    from universe import error, envs
  File ""/home/kunal/Desktop/OpenAi/openAI/lib/python3.7/site-packages/universe/envs/__init__.py"", line 1, in &lt;module&gt;
    import universe.envs.vnc_env
  File ""/home/kunal/Desktop/OpenAi/openAI/lib/python3.7/site-packages/universe/envs/vnc_env.py"", line 11, in &lt;module&gt;
    from universe.envs import diagnostics
  File ""/home/kunal/Desktop/OpenAi/openAI/lib/python3.7/site-packages/universe/envs/diagnostics.py"", line 94
    async = self.qr_pool.apply_async(self.method, (self._last_img, time.time(), available_at))
      ^
SyntaxError: invalid syntax
</code></pre>

<p>Why am I getting this? and how can I solve this?</p>
",10482316,,355230,,2019-01-25 21:34:05,2020-07-20 15:06:54,"Why am I getting ""invalid syntax"" error while importing universe module from openAI",<python><openai-gym>,1,1,,,,CC BY-SA 4.0,
474,70131381,1,70133119,,2021-11-27 01:12:32,,0,35,"<p>I am implementing REINFORCE applied to the CartPole-V0 openAI gym environment.  I am trying 2 different implementations of the same, and the issue I am not able to resolve is the following:</p>
<p>Upon passing a single state to the Policy Network, I get an output Tensor of size 2, containing the action probabilities of the 2 actions. However, when I pass a `batch of states' to the Policy Network to compute the output action probabilities of all of them, the values that I obtain are very different from when each state is individually passed to the network.</p>
<p>Can someone help me understand the issue?</p>
<p>My code for the same is below: (Note: this is NOT the complete REINFORCE algorithm -- I am aware that I need to compute the loss from the probabilities. But I am trying to understand the difference in the computation of the two probabilities, which I think should be the same, before proceeding.)</p>
<pre><code># architecture of the Policy Network
class PolicyNetwork(nn.Module):
    def __init__(self, state_dim, n_actions):
        super().__init__()
        self.n_actions = n_actions
        self.model = nn.Sequential(
            nn.Linear(state_dim, 64),
            nn.ReLU(),
            nn.Linear(64, n_actions),
            nn.Softmax(dim=0)
        ).float()

    def forward(self, X):
        return self.model(X)


def train_reinforce_agent(env, episode_length, max_episodes, gamma, visualize_step, learning_rate=0.003):

    # define the parametric model for the Policy: this is an instantiation of the PolicyNetwork class
    model = PolicyNetwork(env.observation_space.shape[0], env.action_space.n)
    # define the optimizer for updating the weights of the Policy Network
    optimizer = optim.Adam(model.parameters(), lr=learning_rate)


    # hyperparameters of the reinforce agent
    EPISODE_LENGTH = episode_length
    MAX_EPISODES = max_episodes
    GAMMA = gamma
    VISUALIZE_STEP = max(1, visualize_step)
    score = []


    for episode in range(MAX_EPISODES):
        # reset the environment
        curr_state = env.reset()
        done = False
        episode_t = []


        # rollout an entire episode from the Policy Network
        pred_vals = []
        for t in range(EPISODE_LENGTH):
            act_prob = model(torch.from_numpy(curr_state).float())
            pred_vals.append(act_prob)
            action = np.random.choice(np.array(list(range(env.action_space.n))), p=act_prob.data.numpy())
            prev_state = curr_state
            curr_state, _, done, info = env.step(action)
            episode_t.append((prev_state, action, t+1))
            if done:
                break
        score.append(len(episode_t))
        # reward_batch = torch.Tensor([r for (s,a,r) in episode_t]).flip(dims=(0,))
        reward_batch = torch.Tensor([r for (s, a, r) in episode_t])


        # compute the return for every state-action pair from the rewards at every time-step
        batch_Gvals = []
        for i in range(len(episode_t)):
            new_Gval = 0
            power = 0
            for j in range(i, len(episode_t)):
                new_Gval = new_Gval + ((GAMMA ** power) * reward_batch[j]).numpy()
            power += 1
            batch_Gvals.append(new_Gval)



        # normalize the returns for the batch
        expected_returns_batch = torch.FloatTensor(batch_Gvals)
        if torch.is_nonzero(expected_returns_batch.max()):
            expected_returns_batch /= expected_returns_batch.max()



        # batch the states, actions, prob after the episode
        state_batch = torch.Tensor([s for (s,a,r) in episode_t])
        print(&quot;State batch:&quot;, state_batch)
        all_states = [s for (s,a,r) in episode_t]
        print(&quot;All states:&quot;, all_states)
        action_batch = torch.Tensor([a for (s,a,r) in episode_t])
        pred_batch_v1 = model(state_batch)
        pred_batch_v2 = torch.stack(pred_vals)
        print(&quot;Batched state pred_vals:&quot;, pred_batch_v1)
        print(&quot;Individual state pred_vals:&quot;, pred_batch_v2) ### Why is this different from the above predicted values??
</code></pre>
<p>My main function where I pass the environment is:</p>
<pre><code>def main():
    env = gym.make('CartPole-v0')
    # train a REINFORCE-agent to learn the optimal policy
    episode_length = 500
    n_episodes = 500
    gamma = 0.99
    vis_steps = 50
    train_reinforce_agent(env, episode_length, n_episodes, gamma, vis_steps)

</code></pre>
",6526722,,4685471,,2021-11-27 22:19:35,2021-11-27 22:19:35,Policy Network returning different outputs for batched states and individual states,<deep-learning><neural-network><pytorch><reinforcement-learning><openai-gym>,1,0,,,,CC BY-SA 4.0,
477,54121047,1,54285312,,2019-01-10 02:10:52,,3,2099,"<p>The following code is excerpted from <a href=""https://bair.berkeley.edu/blog/2018/01/09/ray/"" rel=""nofollow noreferrer"">https://bair.berkeley.edu/blog/2018/01/09/ray/</a>.</p>

<pre><code>import gym

@ray.remote
class Simulator(object):
    def __init__(self):
        self.env = gym.make(""Pong-v0"")
        self.env.reset()

    def step(self, action):
        return self.env.step(action)

# Create a simulator, this will start a remote process that will run
# all methods for this actor.
simulator = Simulator.remote()

observations = []
for _ in range(4):
    # Take action 0 in the simulator. This call does not block and
    # it returns a future.
    observations.append(simulator.step.remote(0))
</code></pre>

<p>I feel very confused when I'm reading this code. Is this code really run on parallel? Based on my understanding, there is only one <code>env</code>, so the above code should take actions on a sequential order, i.e. actions are taken one by one. If that's the case, what's the point of doing something like the above? </p>
",7850499,,,,,2019-01-21 07:36:27,Run openai-gym environment on parallel,<python><parallel-processing><openai-gym><ray>,1,0,,,,CC BY-SA 4.0,
480,54661185,1,54661689,,2019-02-13 01:34:06,,1,1549,"<p>I am trying to install openai gym on a fresh Ubuntu 16VM. I'm following the documentation to install the Atari package, but I keep running into problems:</p>

<p>When I run 
    ipython > v2.7.12</p>

<pre><code>&gt;import gym
&gt;gym.make('SpaceInvaders-v0')
</code></pre>

<p>I get :</p>

<pre><code>....
TypeError: super() takes at least 1 argument (0 given)
</code></pre>

<p>It seems to run python3 code, however, during the installation I used the default settings. When I run </p>

<pre><code>python3 &gt; 3.5.2
&gt; import gym
ImportError: No module named 'gym'
</code></pre>

<p>What am I doing wrong? I installed gym into the user environment with <code>pip --user</code> parameters.</p>

<p>Solution:
Simple fix to use <code>pip3</code> which is unfortunately not mentioned in the Readme :(</p>
",2058333,,2058333,,2019-02-13 02:11:12,2019-02-13 02:43:25,OpenAi Gym Installation - python version mismatch,<python><python-3.x><artificial-intelligence><openai-gym>,1,4,,,,CC BY-SA 4.0,
486,54726239,1,54727594,,2019-02-16 18:21:07,,2,14248,"<p>I'm trying to create an gym environment that moves the mouse (in a VM, obviously)... I don't know much about classes, but is there supposed to be an argument for self or something...? Also, any improvements would be greatly appreciated...</p>

<p>This code is basically going to run on a VM, soo...I've tried to remove the line of code, but there's several lines that don't run... (I'm terrible at explaining things)</p>

<p>Here's the code:</p>

<pre><code>class MouseEnv(Env):
    def __init__(self):
        self.ACC = 0
        self.reward = 0
        self.done = False
        self.reset()

    def step(self, action):
        try:
            self.action = action
            done = False

            if self.action == 1:
                pyautogui.click()
                self.reward += 0.2
            else:
                if self.ACC == 1:
                    self.action = min((self.action/100), 1) * 1920
                    self.prev_action = min((self.prev_action/100), 1) * 1080
                    self.reward += 0.4

                else:
                    self.ACC = 1
                    self.prev_action = self.action()
                    self.reset()
            screen = ImageGrab.grab()
            self.observation = np.array(screen)
        except:
            done = True
        return self.observation, self.reward, done, {}           
    def reset(self):
        self.observation = np.array()
        self.reward = 0
        self.done = 0
        return self.observation
</code></pre>

<p>And the error:</p>

<pre><code>Traceback (most recent call last):
  File ""&lt;stdin&gt;"", line 1, in &lt;module&gt;
  File ""/home/rees/.local/lib/python3.6/site-packages/gym/envs/registration.py"", line 171, in make
    return registry.make(id, **kwargs)
  File ""/home/rees/.local/lib/python3.6/site-packages/gym/envs/registration.py"", line 123, in make
    env = spec.make(**kwargs)
  File ""/home/rees/.local/lib/python3.6/site-packages/gym/envs/registration.py"", line 87, in make
    env = cls(**_kwargs)
  File ""/home/rees/Desktop/gym-mouse/MouseGym/envs/mouse_env.py"", line 12, in __init__
    self.reset()
  File ""/home/rees/Desktop/gym-mouse/MouseGym/envs/mouse_env.py"", line 41, in reset
    self.observation = np.array()
TypeError: Required argument 'object' (pos 1) not found
</code></pre>

<p>Expected Result:</p>

<p>I expect the mouse to move based on the agent's input/actions, and the observation to be a live video feed of the screen...</p>
",10515022,,4518341,,2019-02-16 22:39:18,2019-02-16 22:39:18,Receiving a TypeError: Required argument 'object' (pos 1) not found,<python><pyautogui><openai-gym>,1,4,,,,CC BY-SA 4.0,
489,70468394,1,70477435,,2021-12-23 23:49:38,,3,609,"<p>I have this custom callback to log the reward in my custom vectorized environment, but the reward appears in console as always [0] and is not logged in tensorboard at all</p>
<pre><code>class TensorboardCallback(BaseCallback):
    &quot;&quot;&quot;
    Custom callback for plotting additional values in tensorboard.
    &quot;&quot;&quot;

    def __init__(self, verbose=0):
        super(TensorboardCallback, self).__init__(verbose)

    def _on_step(self) -&gt; bool:                
        self.logger.record('reward', self.training_env.get_attr('total_reward'))
        return True
</code></pre>
<p>And this is part of the main function</p>
<pre><code>model = PPO(
        &quot;MlpPolicy&quot;, env,
        learning_rate=3e-4,
        policy_kwargs=policy_kwargs,
        verbose=1,

# as the environment is not serializable, we need to set a new instance of the environment
loaded_model = model = PPO.load(&quot;model&quot;, env=env)
loaded_model.set_env(env)

# and continue training
loaded_model.learn(1e+6, callback=TensorboardCallback())
        tensorboard_log=&quot;./tensorboard/&quot;)
</code></pre>
",1106247,,1106247,,2021-12-24 23:37:45,2021-12-25 01:10:38,Stablebaselines3 logging reward with custom gym,<python><reinforcement-learning><openai-gym><stable-baselines>,1,0,,,,CC BY-SA 4.0,
495,55187826,1,55190541,,2019-03-15 17:22:32,,2,1574,"<p>I find out all of the reinforcement learning algorithms need to set the env.seed(#) in the first hand, I would like to know the reason behind it.</p>

<p>Thank you very much!</p>
",10668594,,,,,2019-03-15 20:53:57,Why do we always need to set env.seed(#) for open gym ai?,<reinforcement-learning><openai-gym>,1,0,,,,CC BY-SA 4.0,
499,74637712,1,74663320,,2022-12-01 06:25:26,,0,56,"<p>How do you use OpenAI Gym 'wrappers' with a custom Gym environment in <a href=""https://docs.ray.io/en/latest/tune/index.html"" rel=""nofollow noreferrer"">Ray Tune</a>?</p>
<p>Let's say I built a Python class called <code>CustomEnv</code> (similar to the '<a href=""https://github.com/openai/gym/blob/master/gym/envs/classic_control/cartpole.py"" rel=""nofollow noreferrer"">CartPoleEnv</a>' class used to create the OpenAI Gym <code>&quot;CartPole-v1&quot;</code> environment) to create my own (custom) reinforcement learning environment, and I am using <code>tune.run()</code> from <a href=""https://docs.ray.io/en/latest/tune/index.html"" rel=""nofollow noreferrer"">Ray Tune</a> (in Ray 2.1.0 with Python 3.9.15) to train an agent in my environment using the '<a href=""https://docs.ray.io/en/latest/rllib/rllib-algorithms.html"" rel=""nofollow noreferrer"">PPO</a>' algorithm:</p>
<pre><code>import ray
from ray import tune
tune.run(
        &quot;PPO&quot;,                         # 'PPO' algorithm
        config={&quot;env&quot;: CustomEnv,      # custom class used to create an environment
            &quot;framework&quot;: &quot;tf2&quot;,
            &quot;evaluation_interval&quot;: 100, 
            &quot;evaluation_duration&quot;: 100,
            },
        checkpoint_freq = 100,             # Save checkpoint at every evaluation
        local_dir=checkpoint_dir,          # Save results to a local directory
        stop{&quot;episode_reward_mean&quot;: 250},  # Stopping criterion
        )
</code></pre>
<p>This works fine, and I can use <a href=""https://www.tensorflow.org/tensorboard/get_started"" rel=""nofollow noreferrer"">TensorBoard</a> to monitor training progress, etc., but as it turns out, learning is slow, so I want to try using 'wrappers' from Gym to scale observations, rewards, and/or actions, limit variance, and speed-up learning. So I've got an ObservationWrapper, a RewardWrapper, and an ActionWrapper to do that--for example, something like this (the exact nature of the scaling is not central to my question):</p>
<pre><code>import gym

class ObservationWrapper(gym.ObservationWrapper):
    def __init__(self, env):
        super().__init__(env)
        self.o_min = 0.
        self.o_max = 5000.

    def observation(self, ob):
        # Normalize observations
        ob = (ob - self.o_min)/(self.o_max - self.o_min)
        return ob

class RewardWrapper(gym.RewardWrapper):
    def __init__(self, env):
        super().__init__(env)
        self.r_min = -500
        self.r_max = 100

    def reward(self, reward):
        # Scale rewards:
        reward = reward/(self.r_max - self.r_min)
        return reward

class ActionWrapper(gym.ActionWrapper):
    def __init__(self, env):
        super().__init__(env)

    def action(self, action):
        # Scale actions
        action = action/10
        return action
</code></pre>
<p>Wrappers like these work fine with my custom class when I create an instance of the class on my local machine and use it in traditional training loops, like this:</p>
<pre><code>from my_file import CustomEnv

env = CustomEnv()
wrapped_env = ObservationWrapper(RewardWrapper(ActionWrapper(env)))
episodes = 10

for episode in range(1,episodes+1):
    obs = wrapped_env.reset()
    done = False
    score = 0
    
    while not done:
        action = wrapped_env.action_space.sample()
        obs, reward, done, info = wrapped_env.step(action)
        score += reward

    print(f'Episode: {episode},  Score: {score:.3f}')
</code></pre>
<p>My question is:  How can I use wrappers like these with my custom class (<code>CustomEnv</code>) and <code>ray.tune()</code>?  This particular method expects the value for &quot;env&quot; to be passed either (1) as a class (such as <code>CustomEnv</code>) or (2) as a string associated with a registered Gym environment (such as <code>&quot;CartPole-v1&quot;</code>), as I found out while trying various incorrect ways to pass a wrapped version of my custom class:</p>
<pre><code>ValueError: &gt;&gt;&gt; is an invalid env specifier. You can specify a custom env as either a class (e.g., YourEnvCls) or a registered env id (e.g., &quot;your_env&quot;).
</code></pre>
<p>So I am not sure how to do it (assuming it is possible).  I would prefer to solve this problem without having to register my custom Gym environment, but I am open to any solution.</p>
<p>In learning about wrappers, I leveraged mostly '<a href=""https://blog.paperspace.com/getting-started-with-openai-gym/"" rel=""nofollow noreferrer"">Getting Started With OpenAI Gym: The Basic Building Blocks</a>' by Ayoosh Kathuria, and '<a href=""https://alexandervandekleut.github.io/gym-wrappers/"" rel=""nofollow noreferrer"">TF 2.0 for Reinforcement Learning</a>: Gym Wrappers'.</p>
",14820237,,14820237,,2022-12-02 05:38:25,2022-12-03 01:36:32,How do you use OpenAI Gym 'wrappers' with a custom Gym environment in Ray Tune?,<python><tensorflow><openai-gym><ray>,1,0,,,,CC BY-SA 4.0,
500,70550623,1,70550719,,2022-01-01 17:35:55,,2,1695,"<p>when i try to install gym[box2d] i get following error:
i tried: pip install gym[box2d].
on anaconda prompt i installed swig and gym[box2d] but i code in python3.9 env and it still not working.(my text editor is pycharm)
gym is already installed.and</p>
<pre><code>ERROR: Command errored out with exit status 1:
   command: 'C:\Users\hooman\AppData\Local\Programs\Python\Python39\python.exe' -u -c 'import io, os, sys, setuptools, tokenize; sys.argv[0] = '&quot;'&quot;'C:\\Users\\hoom
an\\AppData\\Local\\Temp\\pip-install-_vo0km5i\\box2d-py_aea38b0da25341cf93e6a6c9d4b9d296\\setup.py'&quot;'&quot;'; __file__='&quot;'&quot;'C:\\Users\\hooman\\AppData\\Local\\Temp\\pi
p-install-_vo0km5i\\box2d-py_aea38b0da25341cf93e6a6c9d4b9d296\\setup.py'&quot;'&quot;';f = getattr(tokenize, '&quot;'&quot;'open'&quot;'&quot;', open)(__file__) if os.path.exists(__file__) else
 io.StringIO('&quot;'&quot;'from setuptools import setup; setup()'&quot;'&quot;');code = f.read().replace('&quot;'&quot;'\r\n'&quot;'&quot;', '&quot;'&quot;'\n'&quot;'&quot;');f.close();exec(compile(code, __file__, '&quot;'&quot;'exe
c'&quot;'&quot;'))' bdist_wheel -d 'C:\Users\hooman\AppData\Local\Temp\pip-wheel-va_dgazk'
       cwd: C:\Users\hooman\AppData\Local\Temp\pip-install-_vo0km5i\box2d-py_aea38b0da25341cf93e6a6c9d4b9d296\
  Complete output (16 lines):
  Using setuptools (version 58.2.0).
  running bdist_wheel
  running build
  running build_py
  creating build
  creating build\lib.win-amd64-3.9
  creating build\lib.win-amd64-3.9\Box2D
  copying library\Box2D\Box2D.py -&gt; build\lib.win-amd64-3.9\Box2D
  copying library\Box2D\__init__.py -&gt; build\lib.win-amd64-3.9\Box2D
  creating build\lib.win-amd64-3.9\Box2D\b2
  copying library\Box2D\b2\__init__.py -&gt; build\lib.win-amd64-3.9\Box2D\b2
  running build_ext
  building 'Box2D._Box2D' extension
  swigging Box2D\Box2D.i to Box2D\Box2D_wrap.cpp
  swig.exe -python -c++ -IBox2D -small -O -includeall -ignoremissing -w201 -globals b2Globals -outdir library\Box2D -keyword -w511 -D_SWIG_KWARGS -o Box2D\Box2D_wr
ap.cpp Box2D\Box2D.i
  error: command 'swig.exe' failed: None
  ----------------------------------------
  ERROR: Failed building wheel for box2d-py
  Running setup.py clean for box2d-py
Failed to build box2d-py
Installing collected packages: box2d-py
    Running setup.py install for box2d-py ... error
    ERROR: Command errored out with exit status 1:
     command: 'C:\Users\hooman\AppData\Local\Programs\Python\Python39\python.exe' -u -c 'import io, os, sys, setuptools, tokenize; sys.argv[0] = '&quot;'&quot;'C:\\Users\\ho
oman\\AppData\\Local\\Temp\\pip-install-_vo0km5i\\box2d-py_aea38b0da25341cf93e6a6c9d4b9d296\\setup.py'&quot;'&quot;'; __file__='&quot;'&quot;'C:\\Users\\hooman\\AppData\\Local\\Temp\\
pip-install-_vo0km5i\\box2d-py_aea38b0da25341cf93e6a6c9d4b9d296\\setup.py'&quot;'&quot;';f = getattr(tokenize, '&quot;'&quot;'open'&quot;'&quot;', open)(__file__) if os.path.exists(__file__) el
se io.StringIO('&quot;'&quot;'from setuptools import setup; setup()'&quot;'&quot;');code = f.read().replace('&quot;'&quot;'\r\n'&quot;'&quot;', '&quot;'&quot;'\n'&quot;'&quot;');f.close();exec(compile(code, __file__, '&quot;'&quot;'e
xec'&quot;'&quot;'))' install --record 'C:\Users\hooman\AppData\Local\Temp\pip-record-yz4t1mq0\install-record.txt' --single-version-externally-managed --compile --install-he
aders 'C:\Users\hooman\AppData\Local\Programs\Python\Python39\Include\box2d-py'
         cwd: C:\Users\hooman\AppData\Local\Temp\pip-install-_vo0km5i\box2d-py_aea38b0da25341cf93e6a6c9d4b9d296\
    Complete output (16 lines):
    Using setuptools (version 58.2.0).
    running install
    running build
    running build_py
    creating build
    creating build\lib.win-amd64-3.9
    creating build\lib.win-amd64-3.9\Box2D
    copying library\Box2D\Box2D.py -&gt; build\lib.win-amd64-3.9\Box2D
    copying library\Box2D\__init__.py -&gt; build\lib.win-amd64-3.9\Box2D
    creating build\lib.win-amd64-3.9\Box2D\b2
    copying library\Box2D\b2\__init__.py -&gt; build\lib.win-amd64-3.9\Box2D\b2
    running build_ext
    building 'Box2D._Box2D' extension
    swigging Box2D\Box2D.i to Box2D\Box2D_wrap.cpp
    swig.exe -python -c++ -IBox2D -small -O -includeall -ignoremissing -w201 -globals b2Globals -outdir library\Box2D -keyword -w511 -D_SWIG_KWARGS -o Box2D\Box2D_
wrap.cpp Box2D\Box2D.i
    error: command 'swig.exe' failed: None
    ----------------------------------------
ERROR: Command errored out with exit status 1: 
</code></pre>
",8057397,,,,,2022-10-29 09:17:33,how can i install gym[box2d] despite errors below?,<python><pip><box2d><openai-gym>,4,0,,,,CC BY-SA 4.0,
511,54981988,1,56627995,,2019-03-04 11:07:47,,1,409,"<p>I'm trying to make a taxonomy of learning algorithms by reinforcement for an online shopping system (of which I have the data). </p>

<p>For this I have decided to use RecoGym, but I can't find a way to put my own data into it. Are they purely invented? Is there a way for the reinforcement algorithm to learn based only on the historical data I have?</p>

<p>I enclose the RecoGym usage code to see if you are able to see it.</p>

<pre><code>import gym, reco_gym

# env_0_args is a dictionary of default parameters (i.e. number of products)
from reco_gym import env_1_args

# you can overwrite environment arguments here:
env_1_args['random_seed'] = 42

# initialize the gym for the first time by calling .make() and .init_gym()
env = gym.make('reco-gym-v1')
env.init_gym(env_1_args)

# .reset() env before each episode (one episode per user)
env.reset()
done = False

# counting how many steps
i = 0 

while not done:
    action, observation, reward, done, info = env.step_offline()
    print(f""Step: {i} - Action: {action} - Observation: {observation} - Reward: {reward}"")
    i += 1

# instantiate instance of PopularityAgent class
num_products = 10
agent = PopularityAgent(num_products)

# resets random seed back to 42, or whatever we set it to in env_0_args
env.reset_random_seed()

# train on 1000 users offline
num_offline_users = 1000

for _ in range(num_offline_users):

    #reset env and set done to False
    env.reset()
    done = False

    while not done:
        old_observation = observation
        action, observation, reward, done, info = env.step_offline()
        agent.train(old_observation, action, reward, done)

# train on 100 users online and track click through rate
num_online_users = 100
num_clicks, num_events = 0, 0

for _ in range(num_online_users):

    #reset env and set done to False
    env.reset()
    observation, _, done, _ = env.step(None)
    reward = None
    done = None
    while not done:
        action = agent.act(observation, reward, done)
        observation, reward, done, info = env.step(action)

        # used for calculating click through rate
        num_clicks += 1 if reward == 1 and reward is not None else 0
        num_events += 1

ctr = num_clicks / num_events


print(f""Click Through Rate: {ctr:.4f}"")
</code></pre>

<p>The paper of the environment is here: <a href=""https://arxiv.org/pdf/1808.00720.pdf"" rel=""nofollow noreferrer"">https://arxiv.org/pdf/1808.00720.pdf</a></p>
",11067209,,,,,2019-06-17 10:40:52,RecoGym dataset is from?,<python><recommendation-engine><reinforcement-learning><openai-gym>,1,0,0,,,CC BY-SA 4.0,
519,70556522,1,70556642,,2022-01-02 13:50:13,,0,86,"<p>I am trying to create a crypto trading bot that can trade multiple crypto coins simulatenously. So for example if I have <code>n=4</code> coins (e.g. : 0-BTC, 1-ETH, 2-DOT, 3-SOL), then an example of action space would be something like:</p>
<pre><code>action_spaces: [ 1000, 0.0, -3000, 2300]
</code></pre>
<p>Where:</p>
<ul>
<li><code>BUY if action &gt; 0</code></li>
<li><code>HOLD if action == 0</code></li>
<li><code>Sell if action &lt; 0</code></li>
</ul>
<p>So, in the given example the actions would be:</p>
<ul>
<li>Index 0: Buy btc worth 1000 USDT</li>
<li>Index 1: Hold eth</li>
<li>Index 2: Sell DOT worth 3000 USDT</li>
<li>Index 3: Buy SOL worth of 2300 USDT</li>
</ul>
<p>So for an <code>n = x</code> with crypto list: <code>[crypto0, crypto1, crypto2, ..., cryptoX]</code> how can I define an action space that has the form: <code>action_space = [action0, action1, action2, ..., actionX]</code></p>
",15481917,,4685471,,2022-01-02 16:23:26,2022-01-02 16:23:26,How to define an array of action spaces?,<python><machine-learning><openai-gym>,1,1,,,,CC BY-SA 4.0,
524,55006689,1,56659969,,2019-03-05 15:51:55,,2,1176,"<p>They recently added the capability of generating a random frozen map. But there is no accompanying documentation of how to use it. I want to use it in an ongoing project.</p>
",10763761,,,,,2019-06-19 04:15:29,How to generate a random frozen lake map in OpenAI?,<openai-gym>,1,0,0,,,CC BY-SA 4.0,
533,61043973,1,61044062,,2020-04-05 14:13:47,,0,45,"<p>I try to run the example for gym package</p>

<p>Here are the line until the problem:</p>

<pre><code>library(gym)

remote_base &lt;- ""http://127.0.0.1:5000""
client &lt;- create_GymClient(remote_base)

# Create environment
env_id &lt;- ""CartPole-v0""
instance_id &lt;- env_create(client, env_id)
</code></pre>

<p>The error is this:</p>

<blockquote>
<pre><code>Error in curl::curl_fetch_memory(url, handle = handle) : 
  Failed to connect to 127.0.0.1 port 5000: Connection refused
</code></pre>
</blockquote>

<p>How is it possible to fix it?</p>

<p>I installed the gym package without any problem</p>
",11786778,,,,,2020-04-05 14:20:03,Setting gym enviroment locally,<r><openai-gym>,1,0,,,,CC BY-SA 4.0,
536,41009105,1,41011129,,2016-12-07 03:50:54,,1,405,"<p>I'm trying to run the getting started code at <a href=""https://github.com/openai/universe#run-your-first-agent"" rel=""nofollow noreferrer"">https://github.com/openai/universe#run-your-first-agent</a> and the program hangs on downloading the game content.</p>

<p><a href=""https://i.stack.imgur.com/G1HuZ.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/G1HuZ.png"" alt=""printout""></a></p>

<pre><code>import Universe
</code></pre>

<p>runs correctly and the internet connection is working fine. Any ideas what the issue could be?</p>
",2826818,,3469169,,2016-12-07 19:49:35,2016-12-07 19:49:35,OpenAI Universe getting started example hangs on download,<python><anaconda><openai-gym>,1,1,0,,,CC BY-SA 3.0,
538,43718347,1,43741074,,2017-05-01 11:26:00,,3,1932,"<p>I am working on a variation of A3C/ACER and I have several workers, each running on its own thread. I am using OpenAI gym environments.</p>

<p>Python threading works fine but it cannot fully utilize all cores. As there are no blocking I/O, it does not context switch. </p>

<p>I would like workers to somehow to release the GIL while executing actions in their respective environments. </p>

<p>I would appreciate your feedback: Does it make sense and it is possible? </p>
",157726,,157726,,2017-08-03 14:11:21,2017-08-03 14:11:21,OpenAI gym and Python threading,<python><machine-learning><reinforcement-learning><openai-gym>,1,2,,,,CC BY-SA 3.0,
539,44404281,1,44404347,,2017-06-07 05:33:50,,38,32133,"<p>I want to setup an RL agent on the OpenAI <code>CarRacing-v0</code> environment, but before that I want to understand the action space. In <a href=""https://github.com/openai/gym/blob/master/gym/envs/box2d/car_racing.py"" rel=""noreferrer"">the code on github</a> line 119 says:</p>

<pre><code>self.action_space = spaces.Box( np.array([-1,0,0]), np.array([+1,+1,+1]))  # steer, gas, brake
</code></pre>

<p>How do I read this line? Although my problem is concrete wrt <code>CarRacing-v0</code> I would like to understand the <code>spaces.Box()</code> notation in general</p>
",3747801,,3747801,,2018-05-17 07:24:14,2019-03-28 11:49:31,OpenAI Gym: Understanding `action_space` notation (spaces.Box),<reinforcement-learning><openai-gym>,1,0,0,,,CC BY-SA 4.0,
540,44409176,1,49088439,,2017-06-07 09:49:27,,1,1129,"<p><a href=""https://i.stack.imgur.com/pyc0W.png"" rel=""nofollow noreferrer"">Screenshot</a></p>

<p>This is what <code>env.render()</code> outputs on the screen, the environment is SpaceInvader-v0.</p>

<p>Am I doing something wrong here? My code is literally right off the OpenAI Gym website, only changed the environment. </p>
",2240521,,2240521,,2019-01-03 12:11:42,2019-01-03 12:11:42,Why does OpenAI Gym Atari give a wrong observation while rendering?,<openai-gym>,2,0,,,,CC BY-SA 4.0,
546,59589776,1,59729706,,2020-01-04 10:17:42,,5,531,"<p>I want to simulate <a href=""https://space.stackexchange.com/questions/10307/what-is-a-suicide-burn"">suicide burn</a> to learn and understand rocket landing. OpenAI gym already has an LunarLander enviroment which is used for training reinforcement learning agents. I am using this enviroment to simulate suicide burn in python. I have extracted the coordinates <code>(x,y)</code> from the first two values of state vector of this enviroment. From these values, considering <code>y</code> coordinates as the altitude; I have calculated velocity and accelartion of the falling lander using these equations</p>

<pre><code>velocity(v) = delta_y/ delta_t
acceleartion(a) = delta_v/delta_t
</code></pre>

<p>As the simulation is incrementing stepwise the difference in time <code>delta_t</code> was taken as 1. Unable to find the gravity parameter of LunarLander I gave it a default value <code>g=1</code>. Then using the below equation from this <a href=""https://www.reddit.com/r/KerbalAcademy/comments/4c42rz/maths_help_calculating_when_to_suicide_burn/d1fgqkf/"" rel=""noreferrer"">reddit comment</a></p>

<blockquote>
  <p>altitude to start suicide burn = [ (current altitude)(acceleration of gravity) + (1/2)(current velocity)2 ] / (acceleration of engines)</p>
</blockquote>

<p>I tried to calculate altitude to start suicide burn. This is my full python code. I am only planning to use two actions 0(do nothing) and 2(start main engine) of the four possible actions.</p>

<pre><code>import gym
env = gym.make('LunarLander-v2')
env.seed(0)

g = 1
delta_t = 1
action = 0

state = env.reset()

# x0 = state[0]
y0 = state[1]
v0 = 0

for t in range(3000):
    state, reward, done, _  = env.step(action)
    y = state[1]
    if done or y &lt;0:
        break
    v = (y-y0)/delta_t  # velocity
    a = (v - v0)/delta_t # acceleration

    # (altitude to start suicide burn) = [ (current altitude)(acceleration of gravity) + (1/2)(current velocity)2 ] / (acceleration of engines)
    alt_burn = [y*g+0.5*v*v]/a

    v0 = v
    y0 = y

    print("" y"",round(y,5),"" v"",round(v,5),"" a"",round(a,5),"" Alt_burn"",round(alt_burn[0],5))
</code></pre>

<p>The output results looks something like this</p>

<pre><code> y 1.41542  v 0.00196  a 0.00196  Alt_burn 722.35767
 y 1.41678  v 0.00136  a -0.0006  Alt_burn -2362.78166
 y 1.41754  v 0.00076  a -0.0006  Alt_burn -2362.63867
 y 1.4177  v 0.00016  a -0.0006  Alt_burn -2362.43506
 y 1.41726  v -0.00044  a -0.0006  Alt_burn -2362.64046
 y 1.41622  v -0.00104  a -0.0006  Alt_burn -2359.03148
 y 1.41458  v -0.00164  a -0.0006  Alt_burn -2358.17355
 y 1.41233  v -0.00224  a -0.0006  Alt_burn -2353.50518
 y 1.40949  v -0.00284  a -0.0006  Alt_burn -2349.24118
 y 1.40605  v -0.00344  a -0.0006  Alt_burn -2343.51016
 y 1.40201  v -0.00404  a -0.0006  Alt_burn -2336.31535
 y 1.39737  v -0.00464  a -0.0006  Alt_burn -2329.04954
</code></pre>

<p>If we look at altitude(y) its a very small value less than 1.5 whereas the calculated altitude to start suicide burn are very high. How can I solve this problem?</p>

<p>In the reddit comments they have only mentioned to start the engine but not to end it. Anyone knows the math for killing the engine dynamically?</p>
",996366,,996366,,2020-01-04 11:49:48,2020-01-14 08:08:58,Simulation of suicide burn in openai-gym's LunarLander,<python><simulation><game-physics><physics><openai-gym>,1,4,0,,,CC BY-SA 4.0,
552,41193650,1,43859937,,2016-12-16 23:04:14,,10,7613,"<p>When I try to install <a href=""https://pypi.python.org/pypi/universe/0.20.1#installation"" rel=""nofollow noreferrer"">OpenAi Universe</a> on my Windows machine via python pip I get following stacktrace:</p>

<pre><code>Traceback (most recent call last):
      File ""&lt;string&gt;"", line 1, in &lt;module&gt;
      File ""C:\Users\Me\AppData\Local\Temp\pip-build-yjf_mrwx\fastzbarlight\setup.py"", line 49, in &lt;module&gt;
        proc = subprocess.Popen(['ld', '-liconv'], stderr=subprocess.PIPE)
      File ""E:\Python3.5.2\lib\subprocess.py"", line 947, in __init__
        restore_signals, start_new_session)
      File ""E:\Python3.5.2\lib\subprocess.py"", line 1224, in _execute_child
        startupinfo)
    FileNotFoundError: [WinError 2] The system cannot find the file specified
</code></pre>

<p>And this error code:</p>

<pre><code>Command ""python setup.py egg_info"" failed with error code 1 in C:\Users\Me\AppData\Local\Temp\pip-build-yjf_mrwx\fastzbarlight\
</code></pre>

<p>I tried everything mentioned <a href=""https://stackoverflow.com/questions/35991403/python-pip-install-gives-command-python-setup-py-egg-info-failed-with-error-c"">here</a>. I also read through the documentation and found this:</p>

<pre><code>""While we donâ€™t officially support Windows, we expect our code to be very close to working there. Weâ€™d be happy to take pull requests that take our Windows compatibility to 100%.""
</code></pre>

<p>So I guess it might just not work, but I think that it should work.</p>
",6509751,,-1,,2017-05-23 12:26:15,2017-08-23 05:44:17,How to install OpenAI Universe without getting error code 1 on Windows?,<python><windows><pip><python-3.5><openai-gym>,2,0,0,,,CC BY-SA 3.0,
560,59590152,1,59590153,,2020-01-04 11:16:03,,4,3575,"<p>I am trying to run a basic OpenAI-gym program available on their OpenAI-gym's official documentation:</p>

<pre><code>import gym
env = gym.make(""CartPole-v1"")
observation = env.reset()
for _ in range(1000):
  env.render()
  action = env.action_space.sample() # your agent here (this takes random actions)
  observation, reward, done, info = env.step(action)

  if done:
    observation = env.reset()
env.close()
</code></pre>

<p>But the program outputs the following error:</p>

<p><code>AttributeError: module 'gym' has no attribute 'make'</code>.</p>
",9159044,,9159044,,2020-01-04 14:02:38,2021-04-22 15:39:23,Getting error: module 'gym' has no attribute 'make',<python><openai-gym>,1,0,0,,,CC BY-SA 4.0,
566,48778899,1,48795754,,2018-02-14 02:47:57,,1,1446,"<p>I am a newbie in Python and I wanted to run some reinforcement learning algorithms with Python using AI Gym: 
<a href=""https://github.com/openai/gym"" rel=""nofollow noreferrer"">https://github.com/openai/gym</a></p>

<p>I want to use tensorflow so I made a python 3.5 environment called ""tensorflow"" with Anaconda and installed it there. In the same environment, I installed AIgym using </p>

<blockquote>
  <p>pip install gym </p>
</blockquote>

<p>from the Anaconda prompt. 
With </p>

<blockquote>
  <p>conda list</p>
</blockquote>

<p>I can see ""gym"" as a package installed in that environment, but in Anaconda Navigator it is not listed (neither in the ""tensofrflow"" environment or anywhere else.</p>

<p>Anyhow if I try to do</p>

<blockquote>
  <p>import gym</p>
</blockquote>

<p>I get </p>

<blockquote>
  <p>No module named 'gym'</p>
</blockquote>

<p>I am sure I have the Anaconda interpreter selected in the project configurations. I also tried </p>

<blockquote>
  <p>pip install gym </p>
</blockquote>

<p>from the regular command-line prompt but it still would refuse to find 'gym'.</p>

<p>One suggestion was to include the Anaconda path to the system's Python PATH. I am not sure how to do this correctly. Most advice is for Linux and I use Windows 10. Is it the path of the python.exe inside the anaconda folder? I've seen people trying </p>

<blockquote>
  <p>export PATH=C:/path/anaconda:$PATH</p>
</blockquote>

<p>but export is not recognized for me in any command prompt.</p>

<blockquote>
  <p>which python</p>
</blockquote>

<p>is also not recognized in order to check which python is being used.
I also tried adding the anaconda path to the system path (This PC > Advanced > Environment Variables > System Variables > Path) but no dice.</p>

<p>Any suggestions would be appreciated.</p>

<p>--Update 1--</p>

<p>Thomas, thanks for your answer. I have tried what you suggested - tried importing gym from the python in-line interpreter from within the anaconda prompt/environment, and it worked!</p>

<p>cmd
<a href=""https://i.stack.imgur.com/YYp8X.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/YYp8X.png"" alt=""cmd""></a></p>

<p>In pyCharm, though, even while I am sure I have the Anaconda interpreter selected for the project, it doesn't work.</p>

<p>PyCharm
<a href=""https://i.stack.imgur.com/MEZpM.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/MEZpM.png"" alt=""pyCharm""></a></p>

<p>--Update 2--
I can see my envs and path but how exactly do you add an env in pyCharm?
The method I have been using was, configure the project to use the Anaconda interpreter and switch environments from the anaconda prompt.
A search gives the same answer. </p>

<p><a href=""https://i.stack.imgur.com/VZz2z.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/VZz2z.png"" alt=""env_advice""></a></p>

<p>It is strange because I have tensorflow installed only in my tensorflow environment, and tensorflow itself imports without problem, as you can see here:</p>

<p><a href=""https://i.stack.imgur.com/HqPVG.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/HqPVG.png"" alt=""tf_import""></a></p>

<p>Thus the tensorflow environment must be active.</p>

<p>Thanks in advance!</p>
",4184024,,1144382,,2018-02-15 22:05:30,2018-02-15 22:05:30,Cannot import file installed with Anaconda in pyCharm: Need to add anaconda to Python PATH?,<python><tensorflow><pip><anaconda><openai-gym>,1,3,,,,CC BY-SA 3.0,
569,56533094,1,56534016,,2019-06-10 20:26:43,,5,2481,"<p>I would like to create custom openai gym environment that has discrete state space, but with float values. To be more precise, it should be a range of values with 0.25 step:
10.0, 10.25, 10.5, 10.75, 11.0, ..., 19.75, 20.0</p>

<p>Is there a way to do this in openai gym custom environment, using spaces like Discrete, Box, MultiDiscrete or some others? Discrete requires an integer, and Box doesn't seem to have some kind of a step parameter.</p>
",9447767,,,,,2019-06-10 21:56:09,OpenAI Gym custom environment: Discrete observation space with real values,<python><reinforcement-learning><openai-gym><discretization>,1,0,0,,,CC BY-SA 4.0,
570,56584975,1,56629090,,2019-06-13 16:33:22,,2,2521,"<p>I am training a reinforcement learning agent using openAI's <a href=""https://github.com/hill-a/stable-baselines"" rel=""nofollow noreferrer"">stable-baselines</a>. I'm also optimising the agents hyperparameters using <a href=""https://optuna.org/"" rel=""nofollow noreferrer"">optuna</a>.</p>

<p>To speed up the process, I am using multiprocessing in different function calls. Specifically in <code>SubprocVecEnv</code> and <code>study.optimize</code> as suggested in the <a href=""https://buildmedia.readthedocs.org/media/pdf/stable-baselines/master/stable-baselines.pdf"" rel=""nofollow noreferrer"">docs here</a> (under 1.15.3 and 1.10.4 respectively).</p>

<pre><code>import numpy as np
from stable_baselines.common.vec_env import SubprocVecEnv
from stable_baselines import PPO2
from stable_baselines.common.policies import MlpLnLstmPolicy
import optuna

n_cpu = 4


def optimize_ppo2(trial):
    """""" Learning hyperparamters we want to optimise""""""
    return {
        'n_steps': int(trial.suggest_loguniform('n_steps', 16, 2048)),
        'gamma': trial.suggest_loguniform('gamma', 0.9, 0.9999),
        'learning_rate': trial.suggest_loguniform('learning_rate', 1e-5, 1.),
        'ent_coef': trial.suggest_loguniform('ent_coef', 1e-8, 1e-1),
        'cliprange': trial.suggest_uniform('cliprange', 0.1, 0.4),
        'noptepochs': int(trial.suggest_loguniform('noptepochs', 1, 48)),
        'lam': trial.suggest_uniform('lam', 0.8, 1.)
    }


def optimize_agent(trial):
    """""" Train the model and optimise
        Optuna maximises the negative log likelihood, so we
        need to negate the reward here
    """"""
    model_params = optimize_ppo2(trial)
    env = SubprocVecEnv([lambda: gym.make('CartPole-v1') for i in range(n_cpu)])
    model = PPO2(MlpLnLstmPolicy, env, verbose=0, nminibatches=1, **model_params)
    model.learn(10000)

    rewards = []
    n_episodes, reward_sum = 0, 0.0

    obs = env.reset()
    while n_episodes &lt; 4:
        action, _ = model.predict(obs)
        obs, reward, done, _ = env.step(action)
        reward_sum += reward

        if done:
            rewards.append(reward_sum)
            reward_sum = 0.0
            n_episodes += 1
            obs = env.reset()

    last_reward = np.mean(rewards)
    trial.report(-1 * last_reward)

    return -1 * last_reward


if __name__ == '__main__':
    study = optuna.create_study(study_name='cartpol_optuna', storage='sqlite:///params.db', load_if_exists=True)
    study.optimize(optimize_agent, n_trials=1000, n_jobs=4)
</code></pre>

<p>I am using a GPU in the google colab environment. My question is, using multiprocessing in both the <code>SubprocVecEnv</code> and <code>study.optimize</code> methods, how can I be sure that the hyperparameter tuning is being correctly executed in the backend? In other words, how do I know there aren't results being overwritten?</p>

<p>In addition, is there a better way to use GPU multiprocessing in this particular use case where both the <code>SubprocVecEnv</code> and <code>study.optimize</code> can run on multiple cores? (I'm unsure if creating too many threads in the same processor will actually slow things down by creating more overhead than running on less threads).</p>
",4139143,,,,,2019-06-17 10:08:22,Understanding openAI gym and Optuna hyperparameter tuning using GPU multiprocessing,<python><gpu><reinforcement-learning><openai-gym>,1,0,,,,CC BY-SA 4.0,
572,48861523,1,48868076,,2018-02-19 07:45:05,,4,8747,"<p>I am using gym version - '0.9.7', and mujoco_py version 1.50.1.41, Python 3.6.1 |Anaconda 4.4.0, installed on a mac.</p>

<p>When trying:</p>

<pre><code>import gym
env = gym.make('Humanoid-v1')
</code></pre>

<p>I am getting the following error:</p>

<pre><code>Traceback (most recent call last):
  File ""/Users/usr/gym/gym/envs/registration.py"", line 139, in spec
    return self.env_specs[id]
KeyError: 'Humanoid-v1'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""&lt;stdin&gt;"", line 1, in &lt;module&gt;
  File ""/Users/usr/gym/gym/envs/registration.py"", line 163, in make
    return registry.make(id)
  File ""/Users/usr/gym/gym/envs/registration.py"", line 118, in make
    spec = self.spec(id)
  File ""/Users/usr/gym/gym/envs/registration.py"", line 147, in spec
    raise error.DeprecatedEnv('Env {} not found (valid versions include {})'.format(id, matching_envs))
gym.error.DeprecatedEnv: Env Humanoid-v1 not found (valid versions include ['Humanoid-v2'])
</code></pre>

<p>I have tried looking for solutions online with no success. </p>
",9188226,,3977276,,2020-06-06 09:13:26,2022-10-03 23:15:50,"How to solve ""Env not found"" error in OpenAI Gym?",<python-3.x><openai-gym>,2,0,,,,CC BY-SA 4.0,
573,56633955,1,56747635,,2019-06-17 15:00:32,,5,13416,"<p>I'm making custom environment in OpenAI Gym and really don't understand, what is action_space for? And what should I put in it? Just to be accurate, I don't know what is action_space, I didn't used it in any code. And I didn't find anything on internet, what could answer my question normally.</p>
",11584263,,11584263,,2019-06-18 09:31:37,2019-06-25 05:53:59,What is the action_space for?,<openai-gym>,1,3,0,,,CC BY-SA 4.0,
583,49203023,1,49205552,,2018-03-09 22:18:13,,9,17936,"<p>I've just installed openAI gym on Google Colab, but when I try to run 'CartPole-v0' environment as <a href=""https://gym.openai.com/docs/"" rel=""noreferrer"">explained here</a>.</p>

<p>Code:</p>

<pre><code>import gym
env = gym.make('CartPole-v0')
for i_episode in range(20):
    observation = env.reset()
    for t in range(100):
        env.render()
        print(observation)
        action = env.action_space.sample()
        observation, reward, done, info = env.step(action)
        if done:
            print(""Episode finished after {} timesteps"".format(t+1))
            break
</code></pre>

<p>I get this:</p>

<pre><code>WARN: gym.spaces.Box autodetected dtype as &lt;class 'numpy.float32'&gt;. Please provide explicit dtype.
---------------------------------------------------------------------------
NameError                                 Traceback (most recent call last)
&lt;ipython-input-19-a81cbed23ce4&gt; in &lt;module&gt;()
      4     observation = env.reset()
      5     for t in range(100):
----&gt; 6         env.render()
      7         print(observation)
      8         action = env.action_space.sample()

/content/gym/gym/core.py in render(self, mode)
    282 
    283     def render(self, mode='human'):
--&gt; 284         return self.env.render(mode)
    285 
    286     def close(self):

/content/gym/gym/envs/classic_control/cartpole.py in render(self, mode)
    104 
    105         if self.viewer is None:
--&gt; 106             from gym.envs.classic_control import rendering
    107             self.viewer = rendering.Viewer(screen_width, screen_height)
    108             l,r,t,b = -cartwidth/2, cartwidth/2, cartheight/2, -cartheight/2

/content/gym/gym/envs/classic_control/rendering.py in &lt;module&gt;()
     21 
     22 try:
---&gt; 23     from pyglet.gl import *
     24 except ImportError as e:
     25     reraise(prefix=""Error occured while running `from pyglet.gl import *`"",suffix=""HINT: make sure you have OpenGL install. On Ubuntu, you can run 'apt-get install python-opengl'. If you're running on a server, you may need a virtual frame buffer; something like this should work: 'xvfb-run -s \""-screen 0 1400x900x24\"" python &lt;your_script.py&gt;'"")

/usr/local/lib/python3.6/dist-packages/pyglet/gl/__init__.py in &lt;module&gt;()
    225     else:
    226         from .carbon import CarbonConfig as Config
--&gt; 227 del base
    228 
    229 # XXX remove

NameError: name 'base' is not defined
</code></pre>

<p>The problem is the same in <a href=""https://stackoverflow.com/questions/44150310/open-ai-gym-nameerror"">this question about NameError in openAI gym</a></p>

<p>Nothing is being rendered. I don't know how I could use this in google colab: <code>'xvfb-run -s \""-screen 0 1400x900x24\"" python &lt;your_script.py&gt;'""</code></p>
",6387541,,,,,2021-06-11 22:28:21,openAI Gym NameError in Google Colaboratory,<python><google-colaboratory><openai-gym>,5,0,0,,,CC BY-SA 3.0,
589,46662829,1,46663968,,2017-10-10 09:10:18,,1,2624,"<p>I am a bit new to python and I am working with miniWOB from OpenAI. I want to print out with cv2 what my observations are for my agent. But really doesn matter. My problem is I do not know exact which data type I receive from the gym for my ""observation"". But I want it as a simple 3Dimensional numpy array. This format I can print out using cv2. So can anybody help me to convert my &lt; class 'list> observation to: &lt; type nunpy.ndarray>? I already tried observation = np.asarray(observation) but then I got this error: ""mat data type = 17"". </p>

<pre><code>import cv2
import random
import gym
import universe
import go_vncdriver
import numpy as np
def main():
   env = gym.make('wob.mini.ClickTest-v0')
   env.configure(remotes=1)  # create one flashgames Docker container
   observation = env.reset()
   while True:
       env.render()
       x = 110
       y = 270

        action_n = [universe.spaces.PointerEvent(x, y, 1), universe.spaces.PointerEvent(x, y, 0),
            universe.spaces.PointerEvent(x, y, 1)]

         action_n = [action_n for ob in observation]
         observation, reward_n, done_n, info = env.step(action_n)

         observation = np.asarray(observation) #this one converts to nd array but then I got the mat data type = 17 error
         if (observation != None):
            print(type(observation))
            cv2.imshow('pong voor net', observation)
            cv2.waitKey(0)
            cv2.destroyAllWindows()
main()
</code></pre>

<p>I receive this:</p>

<pre><code>    [{'vision': array([[[0, 0, 0],
    [0, 0, 0],
    [0, 0, 0],
    ..., 
    [0, 0, 0],
    [0, 0, 0],
    [0, 0, 0]],

   [[0, 0, 0],
    [0, 0, 0],
    [0, 0, 0],
    ..., 
    [0, 0, 0],
    [0, 0, 0],
    [0, 0, 0]],

   [[0, 0, 0],
    [0, 0, 0],
    [0, 0, 0],
    ..., 
    [0, 0, 0],
    [0, 0, 0],
    [0, 0, 0]],

   ..., 
   [[0, 0, 0],
    [0, 0, 0],
    [0, 0, 0],
    ..., 
    [0, 0, 0],
    [0, 0, 0],
    [0, 0, 0]],

   [[0, 0, 0],
    [0, 0, 0],
    [0, 0, 0],
    ..., 
    [0, 0, 0],
    [0, 0, 0],
    [0, 0, 0]],

   [[0, 0, 0],
    [0, 0, 0],
    [0, 0, 0],
    ..., 
    [0, 0, 0],
    [0, 0, 0],
    [0, 0, 0]]], dtype=uint8), 'text': []}]
</code></pre>

<p>I want to convert it to this type:</p>

<pre><code>[[[144  72  17]
[144  72  17]
[144  72  17]
..., 
[144  72  17]
[144  72  17]
[144  72  17]]

[[144  72  17]
[144  72  17]
[144  72  17]
..., 
[144  72  17]
[144  72  17]
[144  72  17]]

[[144  72  17]
[144  72  17]
[144  72  17]
..., 
[144  72  17]
[144  72  17]
[144  72  17]]
</code></pre>
",6339438,,6339438,,2017-10-10 09:33:30,2017-10-10 10:03:02,convert <class 'list'> to numpy.ndarray,<python><opencv><numpy><openai-gym>,1,0,,,,CC BY-SA 3.0,
592,46597809,1,50303347,,2017-10-06 03:25:57,,0,1135,"<p>I am a beginner in Reinforcement Learning and am trying to implement policy gradient methods to solve the Open AI Gym CartPole task using Tensorflow. However, my code seems to run extremely slowly; the first episode runs at an acceptable pace, whereas it is very slow starting from episode 2. Why is this the case, and how can I solve this problem?</p>

<p>My code:</p>

<pre><code>import tensorflow as tf
import numpy as np
import gym

env = gym.make('CartPole-v0')

class Policy:
    def __init__(self):
        self.input_layer_fake = tf.placeholder(tf.float32, [4,1])
        self.input_layer = tf.reshape(self.input_layer_fake, [1,4])
        self.dense1 = tf.layers.dense(inputs = self.input_layer, units = 4,
                                  activation = tf.nn.relu)
        self.logits = tf.layers.dense(inputs = self.dense1, units = 2,
                                  activation = tf.nn.relu)
    def predict(self, inputObservation):
        sess = tf.InteractiveSession()
        tf.global_variables_initializer().run()
        x = tf.reshape(inputObservation, [4,1]).eval()
        return (sess.run(self.logits, feed_dict = {self.input_layer_fake: x}))

    def train(self, features_array, labels_array):
        for i in range(np.shape(features_array)[0]):
            print(""train"")
            print(i)
            sess1 = tf.InteractiveSession()
            tf.global_variables_initializer().run()
            self.cross_entropy = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels = labels_array[i], logits = self.logits))
            self.train_step = tf.train.GradientDescentOptimizer(0.5).minimize(self.cross_entropy)
            y = tf.reshape(features_array[i], [4,1]).eval()
            sess1.run(self.train_step, feed_dict={self.input_layer_fake:y})

agent = Policy()
train_array = []
features_array = []
labels_array = []
main_sess = tf.InteractiveSession()
tf.global_variables_initializer().run()

for i_episode in range(100):
    observation = env.reset()

    for t in range(200):
        prevObservation = observation
        env.render()

        if np.random.uniform(0,1) &lt; 0.2:
            action = env.action_space.sample()
        else:
            action = np.argmax(agent.predict((prevObservation)))

        observation, reward, done, info = env.step(action)
        add_in = np.random.uniform(0,1)
        if add_in &lt; 0.5:
            features_array.append(prevObservation)
            sarPreprocessed = agent.predict(prevObservation)
            sarPreprocessed[0][action] = reward
            labels_array.append(sarPreprocessed)
        if done:
            break

    agent.train(features_array, labels_array)
    features_array = []
    labels_array = []
</code></pre>

<p>Any help is greatly appreciated.</p>
",8083568,,,,,2018-05-12 06:02:52,Policy gradient methods for Open AI Gym Cartpole,<python><machine-learning><tensorflow><reinforcement-learning><openai-gym>,1,2,0,,,CC BY-SA 3.0,
593,64588828,1,66471598,,2020-10-29 10:02:15,,3,6321,"<p>I want to build a brute-force approach that tests all actions in a Gym action space before selecting the best one. Is there any simple, straight-forward way to get all possible actions?</p>
<p>Specifically, my action space is</p>
<pre class=""lang-py prettyprint-override""><code>import gym

action_space = gym.spaces.MultiDiscrete([5 for _ in range(4)])
</code></pre>
<p>I know I can sample a random action with <code>action_space.sample()</code> and also check if an action is contained in the action space, but I want to generate a list of all possible action within that space.</p>
<p>Is there anything more elegant (and performant) than just a bunch of for loops? The problem with for loops is that I want it to work with any size of action space, so I cannot hard-code 4 for loops to walk through the different actions.</p>
",2745116,,2745116,,2020-10-29 10:35:11,2021-06-10 23:20:56,OpenAI Gym: Walk through all possible actions in an action space,<python><for-loop><openai-gym>,2,0,0,,,CC BY-SA 4.0,
596,56904270,1,56926451,,2019-07-05 13:44:38,,21,9678,"<p>I can't find an exact description of the differences between the OpenAI Gym environments 'CartPole-v0' and 'CartPole-v1'.</p>

<p>Both environments have seperate official websites dedicated to them at (see <a href=""https://gym.openai.com/envs/CartPole-v0/"" rel=""noreferrer"">1</a> and <a href=""https://gym.openai.com/envs/CartPole-v1/"" rel=""noreferrer"">2</a>), though I can only find one code without version identification in the gym github repository (see <a href=""https://github.com/openai/gym/blob/master/gym/envs/classic_control/cartpole.py"" rel=""noreferrer"">3</a>). I also checked out the what files exactly are loaded via the debugger, though they both seem to load the same aforementioned file. The only difference seems to be in the their internally assigned <code>max_episode_steps</code> and <code>reward_threshold</code>, which can be accessed as seen below. CartPole-v0 has the values 200/195.0 and CartPole-v1 has the values 500/475.0. The rest seems identical at first glance.</p>

<pre><code>import gym

env = gym.make(""CartPole-v1"")
print(self.env.spec.max_episode_steps)
print(self.env.spec.reward_threshold)
</code></pre>

<p>I would therefore appreciate it if someone could describe the exact differences for me or forward me to a website that is doing so. Thank you very much!</p>
",3965581,,,,,2020-01-01 14:05:47,Difference between OpenAI Gym environments 'CartPole-v0' and 'CartPole-v1',<machine-learning><reinforcement-learning><openai-gym>,1,3,0,,,CC BY-SA 4.0,
600,57263759,1,57283046,,2019-07-30 03:01:37,,5,1680,"<p>I want to start the continuous Mountain Car environment of OpenAI Gym from a custom initial point. The OpenAI Gym does not provide any method to do that. I looked into the <a href=""https://github.com/openai/gym/blob/5e6f11d07781404459672e114cb86cb6a1a015c6/gym/envs/classic_control/continuous_mountain_car.py#L83"" rel=""noreferrer"">code</a> of the environment and found out that there is an attribute <code>state</code> which holds the state information. I tried to manually change that attribute. However, it does not work. </p>

<p>You can see the attached code, the observations being returned from the state function does not match the <code>env.state</code> variable. </p>

<p>I think it is some basic Python issue, which is not allowing me to access the attribute. Is there any way to access that attribute or some other way to start from a custom initial state?  I know I can create a custom environment (<a href=""https://medium.com/@apoddar573/making-your-own-custom-environment-in-gym-c3b65ff8cdaa"" rel=""noreferrer"">like this</a>) from the existing code and add the functionality too. I found one <a href=""https://github.com/openai/gym/issues/337"" rel=""noreferrer"">issue</a> at Github repo and I think they also suggested this. </p>

<pre><code>import gym
env = gym.make(""MountainCarContinuous-v0"")

env.reset()
print(env.state)
env.state = np.array([-0.4, 0])
print(env.state)

for i in range(50):
    obs, _, _, _ = env.step([1]) # Just taking right in every step   
    print(obs, env.state) #the observation and env.state is different
    env.render()
</code></pre>

<p>The output of the code:</p>

<pre><code>[-0.52196493  0.        ]
[-0.4  0. ]
[-0.52047719  0.00148775] [-0.4  0. ]
[-0.51751285  0.00296433] [-0.4  0. ]
[-0.51309416  0.00441869] [-0.4  0. ]
[-0.50725424  0.00583992] [-0.4  0. ]
...
</code></pre>
",8413477,,8413477,,2019-07-30 03:08:50,2019-07-31 22:00:13,How can I start the environment from a custom initial state for Mountain Car?,<python><openai-gym>,2,1,0,,,CC BY-SA 4.0,
601,57356257,1,57386909,,2019-08-05 09:59:17,,0,682,"<p>Stable-baselines allows you to define a <a href=""https://stable-baselines.readthedocs.io/en/master/guide/custom_policy.html#examples"" rel=""nofollow noreferrer"">custom network architetcure</a>; this varies the number of shared layers, value layers, policy layers and their respective sizes.</p>

<p>Stable-baselines also has default policies. <strong>What is the default network architecture for an <a href=""https://stable-baselines.readthedocs.io/en/master/modules/policies.html#stable_baselines.common.policies.MlpLnLstmPolicy"" rel=""nofollow noreferrer"">MlpLnLstmPolicy</a> network?</strong> In addition, it would be good to know the activations between layers and any dropout used, if applicable. I couldn't seem to find any of this information in the documentation.</p>
",4139143,,,,,2019-08-07 04:20:12,What is the defualt architecture for an MlpLnLstmPolicyin stable-baselines?,<python-3.x><reinforcement-learning><openai-gym>,1,0,,,,CC BY-SA 4.0,
604,44100977,1,44144498,,2017-05-21 19:31:56,,0,106,"<p>I am currently getting into tensorflow and have just now started to grasp the graph like concept of it. Now I tried to implement a NN using gradient descent(Adam optimizer) to solve the cartpole environment. I start by randomly intializing my weights and then take random actions(accounting for existing weights) during training. When testing I always take the action with maximum probability. However I always get a score that hovers around 10 and variance is around 0.8. Always. it doesn't change in a notable fashion at all making it look that it always takes purely random actions at every step, not learning anything at all. As I said it seems that the weights are never updated correctly. Where and how do I need to do that?</p>

<p>Here's my code:</p>

<pre><code>import tensorflow as tf
import numpy as np
from gym.envs.classic_control import CartPoleEnv



env = CartPoleEnv()

learning_rate = 10**(-3)
gamma = 0.9999

n_train_trials = 10**3
n_test_trials = 10**2

n_actions = env.action_space.n
n_obs = env.observation_space.high.__len__()

goal_steps = 200

should_render = False

print_per_episode = 100

state_holder = tf.placeholder(dtype=tf.float32, shape=(None, n_obs), name='symbolic_state')
actions_one_hot_holder = tf.placeholder(dtype=tf.float32, shape=(None, n_actions),
                                        name='symbolic_actions_one_hot_holder')
discounted_rewards_holder = tf.placeholder(dtype=tf.float32, shape=None, name='symbolic_reward')

# initialize neurons list dynamically
def get_neurons_list():
    i = n_obs
    n_neurons_list = [i]

    while i &lt; (n_obs * n_actions) // (n_actions // 2):
        i *= 2
        n_neurons_list.append(i)

    while i // 2 &gt; n_actions:
        i = i // 2
        n_neurons_list.append(i)

    n_neurons_list.append(n_actions)

    # print(n_neurons_list)

    return n_neurons_list


with tf.name_scope('nonlinear_policy'):
    # create list of layers with sizes
    n_neurons_list = get_neurons_list()

    network = None

    for i in range((len(n_neurons_list) - 1)):
        theta = tf.Variable(tf.random_normal([n_neurons_list[i], n_neurons_list[i+1]]))
        bias = tf.Variable(tf.random_normal([n_neurons_list[i+1]]))

        if network is None:
            network = tf.matmul(state_holder, theta) + bias
        else:
            network = tf.matmul(network, theta) + bias

        if i &lt; len(n_neurons_list) - 1:
            network = tf.nn.relu(network)

    action_probabilities = tf.nn.softmax(network)

    testing_action_choice = tf.argmax(action_probabilities, dimension=1, name='testing_action_choice')

with tf.name_scope('loss'):
    actually_chosen_probability = action_probabilities * actions_one_hot_holder

    L_theta = -1 * (tf.reduce_sum(tf.log(actually_chosen_probability)) * tf.reduce_sum(discounted_rewards_holder))


with tf.name_scope('train'):
    # We define the optimizer to use the ADAM optimizer, and ask it to minimize our loss
    gd_opt = tf.train.AdamOptimizer(learning_rate).minimize(L_theta)


sess = tf.Session()  # FOR NOW everything is symbolic, this object has to be called to compute each value of Q

# Start

sess.run(tf.global_variables_initializer())

observation = env.reset()
batch_rewards = []
states = []
action_one_hots = []

episode_rewards = []
episode_rewards_list = []
episode_steps_list = []

step = 0
episode_no = 0
while episode_no &lt;= n_train_trials:
    if should_render: env.render()
    step += 1

    action_probability_values = sess.run(action_probabilities,
                                         feed_dict={state_holder: [observation]})
    # Choose the action using the action probabilities output by the policy implemented in tensorflow.
    action = np.random.choice(np.arange(n_actions), p=action_probability_values.ravel())

    # Calculating the one-hot action array for use by tensorflow
    action_arr = np.zeros(n_actions)
    action_arr[action] = 1.
    action_one_hots.append(action_arr)

    # Record states
    states.append(observation)

    observation, reward, done, info = env.step(action)
    # We don't want to go above 200 steps
    if step &gt;= goal_steps:
        done = True

    batch_rewards.append(reward)
    episode_rewards.append(reward)

    # If the episode is done, and it contained at least one step, do the gradient updates
    if len(batch_rewards) &gt; 0 and done:

        # First calculate the discounted rewards for each step
        batch_reward_length = len(batch_rewards)
        discounted_batch_rewards = batch_rewards.copy()
        for i in range(batch_reward_length):
            discounted_batch_rewards[i] *= (gamma ** (batch_reward_length - i - 1))

        # Next run the gradient descent step
        # Note that each of action_one_hots, states, discounted_batch_rewards has the first dimension as the length
        # of the current trajectory
        gradients = sess.run(gd_opt, feed_dict={actions_one_hot_holder: action_one_hots, state_holder: states,
                                                discounted_rewards_holder: discounted_batch_rewards})


        action_one_hots = []
        states = []
        batch_rewards = []

    if done:
        # Done with episode. Reset stuff.
        episode_no += 1

        episode_rewards_list.append(np.sum(episode_rewards))
        episode_steps_list.append(step)

        episode_rewards = []

        step = 0

        observation = env.reset()

        if episode_no % print_per_episode == 0:
            print(""Episode {}: Average steps in last {} episodes"".format(episode_no, print_per_episode),
                  np.mean(episode_steps_list[(episode_no - print_per_episode):episode_no]), '+-',
                  np.std(episode_steps_list[(episode_no - print_per_episode):episode_no])
                  )


observation = env.reset()

episode_rewards_list = []
episode_rewards = []
episode_steps_list = []

step = 0
episode_no = 0

print(""Testing"")
while episode_no &lt;= n_test_trials:
    env.render()
    step += 1

    # For testing, we choose the action using an argmax.
    test_action, = sess.run([testing_action_choice],
                            feed_dict={state_holder: [observation]})

    observation, reward, done, info = env.step(test_action[0])
    if step &gt;= 200:
        done = True
    episode_rewards.append(reward)

    if done:
        episode_no += 1

        episode_rewards_list.append(np.sum(episode_rewards))
        episode_steps_list.append(step)

        episode_rewards = []
        step = 0
        observation = env.reset()

        if episode_no % print_per_episode == 0:
            print(""Episode {}: Average steps in last {} episodes"".format(episode_no, print_per_episode),
                  np.mean(episode_steps_list[(episode_no - print_per_episode):episode_no]), '+-',
                  np.std(episode_steps_list[(episode_no - print_per_episode):episode_no])
                  )
</code></pre>
",6301103,,6301103,,2017-05-22 14:06:16,2017-05-23 20:34:20,Deep Neural Network does not update weights upon training,<python-3.x><tensorflow><neural-network><openai-gym>,1,5,,,,CC BY-SA 3.0,
606,57839665,1,57849327,,2019-09-08 06:30:02,,7,11877,"<p>Today, when I was trying to implement an rl-agent under the environment openai-gym, I found a problem that it seemed that all agents are trained from the most initial state: <code>env.reset()</code>, i.e.  </p>

<pre class=""lang-py prettyprint-override""><code>import gym

env = gym.make(""CartPole-v0"")
initial_observation = env.reset()  # &lt;-- Note
done = False

while not done:
    action = env.action_space.sample()  
    next_observation, reward, done, info = env.step(action)

env.close()  # close the environment
</code></pre>

<p>So it is natural that the agent can behave down the route <code>env.reset() -(action)-&gt; next_state -(action)-&gt; next_state -(action)-&gt; ... -(action)-&gt; done</code>, this is an episode. But how can an agent start from a sepecific state like a middle state, then take an action from that state? For example, I sample an experience from the replay buffer, i.e. <code>(s, a, r, ns, done)</code>, what if I want train the agent start directly from the state <code>ns</code>, and get an action with a <code>Q-Network</code>, then for an <code>n-step</code> steps forward. Something like that:  </p>

<pre class=""lang-py prettyprint-override""><code>import gym

env = gym.make(""CartPole-v0"")
initial_observation = ns  # not env.reset() 
done = False

while not done:
    action = DQN(ns) 
    next_observation, reward, done, info = env.step(action)
    # n-step later or done is true, break

env.close()  # close the environment
</code></pre>

<p>But even though I set a variable <code>initial_observation</code> as <code>ns</code>, I think the agent or the <code>env</code> will not aware it at all. How can I tell the <code>gym.env</code> that I want set the initial observation as <code>ns</code> and let the agent know the specific start state, get continue train directly from that specific observation(get start with that specific environment)?</p>
",7121726,,7121726,,2019-09-08 06:37:14,2022-11-26 18:03:22,How to set a openai-gym environment start with a specific state not the `env.reset()`?,<python-3.x><reinforcement-learning><openai-gym>,5,0,0,,,CC BY-SA 4.0,
607,57967451,1,57968475,,2019-09-17 04:40:07,,2,718,"<p>I'm new to Open Ai Gym and currently running Reinforcement Learning (RL) in the Taxi Environment and my research requires me to be able to call the State tuple (or called ""State Space"" in the <code>Taxi.py</code> file ) for some data mining / state-action pair operation. </p>

<p>Is there a function to call this?</p>

<p>Eg: <code>State(123) = (taxi_row, taxi_col, passenger_location, destination)</code></p>

<p>In RL the state and actions are represented in matrices form, <code>column = state, row = action</code>. </p>

<p>In the source code (<code>taxi.py</code>) its called ""state space is represented by (<code>taxi_row</code>, <code>taxi_col</code>, <code>passenger_location</code>, <code>destination</code>)""</p>
",12077363,,8566549,,2019-09-17 04:51:46,2019-09-17 07:40:51,Calling Env State Tuple,<python><reinforcement-learning><openai-gym>,1,0,,,,CC BY-SA 4.0,
609,58210563,1,58220254,,2019-10-02 23:31:04,,1,206,"<p>First of all, this is for practice and comparison, I know there are more efficient ways to tile state space than with an linear grid.</p>

<p>To run some reinforcement learning algorithm, I would like to tile my state and action space lineary. As result I want to have every space-action-pair in array form. The problem is, there are different (gym) environments with different state- and action-space dimensions. Therefore I dontlike to have hard coded variables or dimensions.
So I need to calculate every state-action pair given only the min and max for each.</p>

<p>I've mostly solved the easy problems, but none of the solutions are ""pretty"".</p>

<p>First lets compute state and action space. Tile the area with linspace from min to max. I've given the variables for one random test environment.</p>

<pre class=""lang-py prettyprint-override""><code>import numpy as np
NOF_ACTION_SPACE_TILES = 20
NOF_STATE_SPACE_TILES = 10
action_low = np.array([-2])
state_low = np.array([-1, -1, -8])

action_space = np.vstack([*[x.flatten() for x in (np.meshgrid(*(np.linspace(action_low, action_high, NOF_ACTION_SPACE_TILES).T)))]]).T

state_space = np.vstack([*[x.flatten() for x in (np.meshgrid(*(np.linspace(state_low, state_high, NOF_STATE_SPACE_TILES).T)))]]).T
</code></pre>

<p>That works as intended and gives all the possible combinations for the states and actions on their own. Any way to do this more straight forward? I needed to use the *[] two times, due to np.meshgrid returning multiple matrices and trying to flatten the vectors.</p>

<p>Now to the funny part...</p>

<p>In the end I want to have every possible state-action pair. Every state with every action. This is coded pretty fast with for loops, but well... numpy and for loops are no speedy friends.
So heres my workaround, that works for 1D action space:</p>

<pre class=""lang-py prettyprint-override""><code>s_s, a_s = np.meshgrid(state_space, action_space)

state_action_space = np.concatenate((
   s_s.reshape(-1, state_space.shape[1]),
   a_s.reshape(state_space.shape[1], action_space.shape[1], -1)[0].T), axis=1)
</code></pre>

<p>With <code>state_space.shape[1]</code> beeing the dim of a single state / action.</p>

<p>One problem beeing, that <code>np.meshgrid</code> returns a_s for each of the 3 state-space dimensions, and reshaping it like above does not work, because we need to reshape the states to 3xn and the action to 1xn. </p>

<p>This is even worse than the code above, but works for now. Does anyone have suggestions how to use meshgrid or sth else properly and fast?</p>

<p>In the end, for the second step, its just a combination of every row of the two matrices. There has to be a better way...</p>
",5889825,,,,,2019-10-03 13:31:37,More efficient way for multidimensional state-action space tiling than with np.meshgrid?,<python><arrays><numpy><mesh><openai-gym>,1,3,,,,CC BY-SA 4.0,
610,56964657,1,58730298,,2019-07-10 06:35:23,,1,5602,"<p>Hi I'm trying to train a DQN to solve gym's Cartpole problem. 
For some reason the <a href=""https://i.stack.imgur.com/uHxpR.png"" rel=""nofollow noreferrer"">Loss</a> looks like this (orange line). Can y'all take a look at my code and help with this? I've played around with the hyperparameters a decent bit so I don't think they're the issue here.</p>

<pre class=""lang-py prettyprint-override""><code>class DQN(nn.Module):
    def __init__(self, input_dim, output_dim):
        super(DQN, self).__init__()
        self.linear1 = nn.Linear(input_dim, 16)
        self.linear2 = nn.Linear(16, 32)
        self.linear3 = nn.Linear(32, 32)
        self.linear4 = nn.Linear(32, output_dim)


    def forward(self, x):
        x = F.relu(self.linear1(x))
        x = F.relu(self.linear2(x))
        x = F.relu(self.linear3(x))
        return self.linear4(x)


final_epsilon = 0.05
initial_epsilon = 1
epsilon_decay = 5000
global steps_done
steps_done = 0


def select_action(state):
    global steps_done
    sample = random.random()
    eps_threshold = final_epsilon + (initial_epsilon - final_epsilon) * \
                    math.exp(-1. * steps_done / epsilon_decay)
    if sample &gt; eps_threshold:
        with torch.no_grad():
            state = torch.Tensor(state)
            steps_done += 1
            q_calc = model(state)
            node_activated = int(torch.argmax(q_calc))
            return node_activated
    else:
        node_activated = random.randint(0,1)
        steps_done += 1
        return node_activated


class ReplayMemory(object): # Stores [state, reward, action, next_state, done]

    def __init__(self, capacity):
        self.capacity = capacity
        self.memory = [[],[],[],[],[]]

    def push(self, data):
        """"""Saves a transition.""""""
        for idx, point in enumerate(data):
            #print(""Col {} appended {}"".format(idx, point))
            self.memory[idx].append(point)

    def sample(self, batch_size):
        rows = random.sample(range(0, len(self.memory[0])), batch_size)
        experiences = [[],[],[],[],[]]
        for row in rows:
            for col in range(5):
                experiences[col].append(self.memory[col][row])
        return experiences

    def __len__(self):
        return len(self.memory[0])


input_dim, output_dim = 4, 2
model = DQN(input_dim, output_dim)
target_net = DQN(input_dim, output_dim)
target_net.load_state_dict(model.state_dict())
target_net.eval()
tau = 2
discount = 0.99

learning_rate = 1e-4
optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)

memory = ReplayMemory(65536)
BATCH_SIZE = 128


def optimize_model():
    if len(memory) &lt; BATCH_SIZE:
        return 0
    experiences = memory.sample(BATCH_SIZE)
    state_batch = torch.Tensor(experiences[0])
    action_batch = torch.LongTensor(experiences[1]).unsqueeze(1)
    reward_batch = torch.Tensor(experiences[2])
    next_state_batch = torch.Tensor(experiences[3])
    done_batch = experiences[4]

    pred_q = model(state_batch).gather(1, action_batch)

    next_state_q_vals = torch.zeros(BATCH_SIZE)

    for idx, next_state in enumerate(next_state_batch):
        if done_batch[idx] == True:
            next_state_q_vals[idx] = -1
        else:
            # .max in pytorch returns (values, idx), we only want vals
            next_state_q_vals[idx] = (target_net(next_state_batch[idx]).max(0)[0]).detach()


    better_pred = (reward_batch + next_state_q_vals).unsqueeze(1)

    loss = F.smooth_l1_loss(pred_q, better_pred)
    optimizer.zero_grad()
    loss.backward()
    for param in model.parameters():
        param.grad.data.clamp_(-1, 1)
    optimizer.step()
    return loss


points = []
losspoints = []

#save_state = torch.load(""models/DQN_target_11.pth"")
#model.load_state_dict(save_state['state_dict'])
#optimizer.load_state_dict(save_state['optimizer'])



env = gym.make('CartPole-v0')
for i_episode in range(5000):
    observation = env.reset()
    episode_loss = 0
    if episode % tau == 0:
        target_net.load_state_dict(model.state_dict())
    for t in range(1000):
        #env.render()
        state = observation
        action = select_action(observation)
        observation, reward, done, _ = env.step(action)

        if done:
            next_state = [0,0,0,0]
        else:
            next_state = observation

        memory.push([state, action, reward, next_state, done])
        episode_loss = episode_loss + float(optimize_model(i_episode))
        if done:
            points.append((i_episode, t+1))
            print(""Episode {} finished after {} timesteps"".format(i_episode, t+1))
            print(""Avg Loss: "", episode_loss / (t+1))
            losspoints.append((i_episode, episode_loss / (t+1)))
            if (i_episode % 100 == 0):
                eps = final_epsilon + (initial_epsilon - final_epsilon) * \
                    math.exp(-1. * steps_done / epsilon_decay)
                print(eps)
            if ((i_episode+1) % 5001 == 0):
                save = {'state_dict': model.state_dict(), 'optimizer': optimizer.state_dict()}
                torch.save(save, ""models/DQN_target_"" + str(i_episode // 5000) + "".pth"")
            break
env.close()




x = [coord[0] * 100 for coord in points]
y = [coord[1] for coord in points]

x2 = [coord[0] * 100 for coord in losspoints]
y2 = [coord[1] for coord in losspoints]

plt.plot(x, y)
plt.plot(x2, y2)
plt.show()
</code></pre>

<p>I basically followed the tutorial pytorch has, except using the state returned by the env rather than the pixels. I also changed the replay memory because I was having issues there. Other than that, I left everything else pretty much the same.</p>

<p>Edit:</p>

<p>I tried overfitting on a small batch and the Loss looks like <a href=""https://i.stack.imgur.com/ZwjrU.png"" rel=""nofollow noreferrer"">this</a> without updating the target net and <a href=""https://i.stack.imgur.com/VOOPM.png"" rel=""nofollow noreferrer"">this</a> when updating it</p>

<p>Edit 2:</p>

<p>This is definitely an issue with the target net, I tried removing it and loss seemed to not increase exponentially</p>
",8327565,,8327565,,2019-07-10 07:16:24,2019-11-07 15:43:48,Cartpole-v0 loss increasing using DQN,<python><pytorch><reinforcement-learning><openai-gym>,1,0,0,,,CC BY-SA 4.0,
611,57105073,1,57111651,,2019-07-19 03:19:27,,0,939,"<p>I'm trying to run <code>examples/agents/cem.py</code> from the OpenAI gym github repository that looks like this:</p>

<pre><code>from __future__ import print_function

import gym
from gym import wrappers, logger
import numpy as np
from six.moves import cPickle as pickle
import json, sys, os
from os import path
from _policies import BinaryActionLinearPolicy # Different file so it can be unpickled
import argparse

def cem(f, th_mean, batch_size, n_iter, elite_frac, initial_std=1.0):
    """"""
    Generic implementation of the cross-entropy method for maximizing a black-box function
    f: a function mapping from vector -&gt; scalar
    th_mean: initial mean over input distribution
    batch_size: number of samples of theta to evaluate per batch
    n_iter: number of batches
    elite_frac: each batch, select this fraction of the top-performing samples
    initial_std: initial standard deviation over parameter vectors
    """"""
    n_elite = int(np.round(batch_size*elite_frac))
    th_std = np.ones_like(th_mean) * initial_std

    for _ in range(n_iter):
        ths = np.array([th_mean + dth for dth in  th_std[None,:]*np.random.randn(batch_size, th_mean.size)])
        ys = np.array([f(th) for th in ths])
        elite_inds = ys.argsort()[::-1][:n_elite]
        elite_ths = ths[elite_inds]
        th_mean = elite_ths.mean(axis=0)
        th_std = elite_ths.std(axis=0)
        yield {'ys' : ys, 'theta_mean' : th_mean, 'y_mean' : ys.mean()}

def do_rollout(agent, env, num_steps, render=False):
    total_rew = 0
    ob = env.reset()
    for t in range(num_steps):
        a = agent.act(ob)
        (ob, reward, done, _info) = env.step(a)
        total_rew += reward
        if render and t%3==0: env.render()
        if done: break
    return total_rew, t+1

if __name__ == '__main__':
    logger.set_level(logger.INFO)

    parser = argparse.ArgumentParser()
    parser.add_argument('--display', action='store_true')
    parser.add_argument('target', nargs=""?"", default=""CartPole-v0"")
    args = parser.parse_args()

    env = gym.make(args.target)
    env.seed(0)
    np.random.seed(0)
    params = dict(n_iter=10, batch_size=25, elite_frac=0.2)
    num_steps = 200

    # You provide the directory to write to (can be an existing
    # directory, but can't contain previous monitor results. You can
    # also dump to a tempdir if you'd like: tempfile.mkdtemp().
    outdir = '/tmp/cem-agent-results'
    env = wrappers.Monitor(env, outdir, force=True)

    # Prepare snapshotting
    # ----------------------------------------
    def writefile(fname, s):
        with open(path.join(outdir, fname), 'w') as fh: fh.write(s)
    info = {}
    info['params'] = params
    info['argv'] = sys.argv
    info['env_id'] = env.spec.id
    # ------------------------------------------

    def noisy_evaluation(theta):
        agent = BinaryActionLinearPolicy(theta)
        rew, T = do_rollout(agent, env, num_steps)
        return rew

    # Train the agent, and snapshot each stage
    for (i, iterdata) in enumerate(
        cem(noisy_evaluation, np.zeros(env.observation_space.shape[0]+1), **params)):
        print('Iteration %2i. Episode mean reward: %7.3f'%(i, iterdata['y_mean']))
        agent = BinaryActionLinearPolicy(iterdata['theta_mean'])
        if args.display: do_rollout(agent, env, 200, render=True)
        writefile('agent-%.4i.pkl'%i, str(pickle.dumps(agent, -1)))

    # Write out the env at the end so we store the parameters of this
    # environment.
    writefile('info.json', json.dumps(info))

    env.close()
</code></pre>

<p>When it runs, I get this error:</p>

<pre><code>Traceback (most recent call last):
  File ""cart.py"", line 9, in &lt;module&gt;
    from _policies import BinaryActionLinearPolicy # Different file so it can be unpickled
ModuleNotFoundError: No module named '_policies'
</code></pre>

<p>I already ran <code>pip3 install policies</code> but that didn't fix the problem. I also tried removing the underscore from the import, but that threw another error, saying it couldn't import the name 'BinaryActionLinearPolicy'. How can I fix this?</p>
",11506519,,8421999,,2019-07-19 12:50:15,2019-07-19 12:50:15,OpenAI gym - no module named '_policies',<python><reinforcement-learning><openai-gym>,1,0,,,,CC BY-SA 4.0,
615,58224472,1,58237398,,2019-10-03 17:53:07,,2,7650,"<p>I am trying to install the gym package in conda for Linux. I have created a virtual environment and am using the following command to try and install:</p>

<pre><code>(gym_env) [quantrill@baesvlfil003 ~]$ conda install --name gym_env -c hcc gym
</code></pre>

<p>But am getting the following issue:</p>

<pre><code>Collecting package metadata (current_repodata.json): done
Solving environment: failed with initial frozen solve. Retrying with flexible solve.
Solving environment: failed with repodata from current_repodata.json, will retry with next repodata source.
Collecting package metadata (repodata.json): done
Solving environment: failed with initial frozen solve. Retrying with flexible solve.
Solving environment: /
Found conflicts! Looking for incompatible packages.                                                                                                        failed

UnsatisfiableError: The following specifications were found to be incompatible with each other:



Package libgcc-ng conflicts for:
python=3.6 -&gt; libgcc-ng[version='&gt;=7.2.0|&gt;=7.3.0']
Package libstdcxx-ng conflicts for:
python=3.6 -&gt; libstdcxx-ng[version='&gt;=7.2.0|&gt;=7.3.0']
Package xz conflicts for:
python=3.6 -&gt; xz[version='&gt;=5.2.3,&lt;6.0a0|&gt;=5.2.4,&lt;6.0a0']
Package libffi conflicts for:
python=3.6 -&gt; libffi[version='3.2.*|&gt;=3.2.1,&lt;4.0a0']
Package sqlite conflicts for:
python=3.6 -&gt; sqlite[version='&gt;=3.20.1,&lt;4.0a0|&gt;=3.22.0,&lt;4.0a0|&gt;=3.23.1,&lt;4.0a0|&gt;=3.24.0,&lt;4.0a0|&gt;=3.25.2,&lt;4.0a0|&gt;=3.26.0,&lt;4.0a0|&gt;=3.29.0,&lt;4.0a0']
Package requests conflicts for:
gym -&gt; requests[version='&gt;=2.0']
Package pyglet conflicts for:
gym -&gt; pyglet[version='&gt;=1.2.0']
Package tk conflicts for:
python=3.6 -&gt; tk[version='8.6.*|&gt;=8.6.7,&lt;8.7.0a0|&gt;=8.6.8,&lt;8.7.0a0']
Package openssl conflicts for:
python=3.6 -&gt; openssl[version='1.0.*|1.0.*,&gt;=1.0.2l,&lt;1.0.3a|&gt;=1.0.2m,&lt;1.0.3a|&gt;=1.0.2n,&lt;1.0.3a|&gt;=1.0.2o,&lt;1.0.3a|&gt;=1.0.2p,&lt;1.0.3a|&gt;=1.1.1a,&lt;1.1.2a|&gt;=1.1.1c,&lt;1.1.2a']
Package zlib conflicts for:
python=3.6 -&gt; zlib[version='&gt;=1.2.11,&lt;1.3.0a0']
Package ncurses conflicts for:
python=3.6 -&gt; ncurses[version='6.0.*|&gt;=6.0,&lt;7.0a0|&gt;=6.1,&lt;7.0a0']
Package numpy conflicts for:
gym -&gt; numpy[version='&gt;=1.10.4']
Package six conflicts for:
gym -&gt; six
Package pip conflicts for:
python=3.6 -&gt; pip
Package readline conflicts for:
python=3.6 -&gt; readline[version='7.*|&gt;=7.0,&lt;8.0a0']
Package scipy conflicts for:
gym -&gt; scipy
</code></pre>

<p>I am finding it difficult to interpret this error so am not sure where to go from here to try and fix it.</p>

<p>I also tried installing the package at the same time as creating the environemt but there was the same issue.</p>

<p><strong>Python version - 3.6.9
conda version - 4.7.12</strong></p>

<p>Hopefully someone can help me!</p>
",7174264,,,,,2022-07-01 03:00:58,How to conda install openAI gym on Linux - Package Conflicts,<python><python-3.x><linux><conda><openai-gym>,3,1,0,,,CC BY-SA 4.0,
616,58094934,1,58524607,,2019-09-25 09:07:29,,0,724,"<p>I am using a pixel based <a href=""https://gym.openai.com/"" rel=""nofollow noreferrer"">gym</a> environment in my code. For some reasons, I need to modify the <code>render</code> function. Using <code>pyglet</code>, I have created my own class <code>ShowRender</code> with a <code>render</code> and a <code>close</code> function:</p>

<pre class=""lang-py prettyprint-override""><code>import cv2
import pyglet
from gym.envs.classic_control import rendering

class ShowRender(object):

    def __init__(self):
        self.viewer = rendering.SimpleImageViewer()
        self.viewer.width = 512
        self.viewer.height = 512
        self.viewer.window = pyglet.window.Window(width=self.viewer.width, height=self.viewer.height,
                                                  display=self.viewer.display, vsync=False, resizable=True)

    def render(self, observation):
        self.viewer.imshow(observation)

    def close(self):
        self.viewer.close()

</code></pre>

<p>Function <code>render</code> works perfectly: a new window is created and displays the environment's pixels.
But when I use <code>close</code> function, the window stays open and does not disappear. Any advice to close it properly ? Thanks</p>
",10484775,,10484775,,2019-09-26 06:35:07,2019-10-23 13:59:35,Python: close a pyglet window,<python-3.x><window><pyglet><openai-gym>,1,3,0,,,CC BY-SA 4.0,
621,63079830,1,63236529,,2020-07-24 18:53:18,,0,15,"<p>I'm working in the <a href=""https://github.com/maximecb/gym-minigrid"" rel=""nofollow noreferrer"">minigrid environment</a>. I currently have an object moving across the screen at one grid space per step count (from left to right). This object periodically appears currently half time on screen, and half off. I'd like to slow down the object so that it moves slower across the screen. I'm not sure how to do it without losing the periodic appearance attribute. Current code is below:</p>
<pre><code>        idx = (self.step_count+2)%(2*self.width) # 2 is the ratio of appear not appear
        if idx &lt; self.width:
            try:
                self.put_obj(self.obstacles[i_obst], idx , old_pos[1]) 
                self.grid.set(*old_pos, None) # deletes old  obstacle
            except:
                pass
        else:
            self.grid.set(*old_pos, None) # deletes old  obstacle  
</code></pre>
",3672854,,,,,2020-08-03 20:29:27,Changing the speed of a periodically appearing object in grid,<openai-gym>,1,0,,,,CC BY-SA 4.0,
626,63111050,1,63352539,,2020-07-27 08:00:58,,1,857,"<p>I was just getting started to learn OpenAI. I write my first OpenAI code, i want to make CartPole AI. But something happened. Here is my code</p>
<pre><code>import gym

# Environment
env = gym.make('CartPole-v0')

# Obsevation
obs = env.reset()

for _ in range(100):
    env.render()

    cart_pos,cart_vel,ang,ang_vel = obs

    if ang &gt; 0:
        action = 1
    else:
        action = 0

    obs,reward,done,info = env.step(action)
</code></pre>
<p>When i run the code, a warning is printed</p>
<pre><code>(base) F:\Python Scripts\CartPole-OpenAI&gt;python CartPole.py
C:\Users\PHILIP\Anaconda3\lib\site-packages\gym\logger.py:30: UserWarning: [33mWARN: You are calling 'step()' even though this environment has already returned done = True. You should always call 'reset()' once you receive 'done = True' -- any further steps are undefined behavior.[0m
  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))
</code></pre>
<p>When the loop is done</p>
<pre><code>Exception ignored in: &lt;function Viewer.__del__ at 0x00000209A1788048&gt;
Traceback (most recent call last):
  File &quot;C:\Users\PHILIP\Anaconda3\lib\site-packages\gym\envs\classic_control\rendering.py&quot;, line 162, in __del__
  File &quot;C:\Users\PHILIP\Anaconda3\lib\site-packages\gym\envs\classic_control\rendering.py&quot;, line 81, in close
  File &quot;C:\Users\PHILIP\Anaconda3\lib\site-packages\pyglet\window\win32\__init__.py&quot;, line 299, in close
  File &quot;C:\Users\PHILIP\Anaconda3\lib\site-packages\pyglet\window\__init__.py&quot;, line 820, in close
ImportError: sys.meta_path is None, Python is likely shutting down
</code></pre>
<p>I appreciate your help. Thanks</p>
",11811336,,,,,2020-08-11 06:20:02,"ImportError: sys.meta_path is None, Python is likely shutting down in Python OpenAI CartPole",<python><python-3.x><openai-gym>,1,0,0,,,CC BY-SA 4.0,
630,49637378,1,49678668,,2018-04-03 19:02:27,,1,2239,"<p>[<strong>Introduction</strong>] I'm a beginner with OpenAI, I have made a custom game into which I would like to implement a self-learning agent. I followed <a href=""https://github.com/openai/gym/tree/master/gym/envs"" rel=""nofollow noreferrer"">this guide</a> to set up a repository on GitHub, however I do not understand how I could format my code to work with the contents of gym-foo/gym_foo/envs/foo_env.py</p>

<p>[<strong>Question</strong>] Is there any chance someone could guide me on how to structure my code to so itâ€™s compatible with:  </p>

<pre><code>class FooEnv(gym.Env):
metadata = {'render.modes': ['human']}

def __init__(self):
  ...
def step(self, action):
  ...
def reset(self):
...
def render(self, mode='human', close=False):
  ...
</code></pre>

<p>[<strong>Code</strong>]</p>

<pre><code>import pygame
import time
import random
from pygame.locals import *

pygame.init()

display_width = 1002
display_height = 720

black = (0,0,0)
white = (255,255,255)
red = (220,0,0)
blue = (53,155,255)
green = (0,190,0)
bright_red = (255,0,0)
bright_green = (0,255,0)
dark_blue = (0,102,204)
yellow = (255, 255, 0)

gameDisplay = pygame.display.set_mode((display_width,display_height)) #creates surface/ display
pygame.display.set_caption('Blob Arena') #name of project
clock = pygame.time.Clock() #sets a clock

#________________________________________________________________________________________

blobImage = pygame.image.load('blob2.png')
blobIcon = pygame.image.load('blob_img.png')
bulletpicture = pygame.image.load(""bullet.png"")
pygame.display.set_icon(blobIcon)
pause = True
blob_width = 51
blob_height = 51
bullet_width = 12
bullet_height = 5

bullets=[]
bullets2=[]

def blob(x,y):
    gameDisplay.blit(blobImage,(x,y)) #drawing to background

def bullets_hit(count):
    font = pygame.font.SysFont(None, 25)
    text = font.render(""Score: ""+str(count), True, black)
    gameDisplay.blit(text,(0,20))

def player_lives(count):
    font = pygame.font.SysFont(None, 25)
    text = font.render(""Lives Left: ""+str(count), True, bright_red)
    gameDisplay.blit(text,(0,0))

def game_intro():
    intro = True
    while intro:
        for event in pygame.event.get():
            if event.type == pygame.QUIT:
                pygame.quit()
                quit()

        gameDisplay.fill(dark_blue)
        largeText = pygame.font.Font('freesansbold.ttf', 110)
        TextSurf, TextRect = text_objects(""Blob Arena"", largeText)  # Returns text surface and rectangle
        TextRect.center = ((display_width / 2), (display_height / 2.5))
        gameDisplay.blit(TextSurf, TextRect)
        button(""Training 1"", 200, 430, 140, 53, green, bright_green, game_loop)
        button(""Training 2"", 431, 430, 140, 53, green, bright_green, game_loop)
        button(""Training 3"", 662, 430, 140, 53, green, bright_green, game_loop)
        button(""Human vs AI"", 315.5, 550, 140, 53, green, bright_green, game_loop)
        button(""Quit"", 546.5, 550, 140, 53, red, bright_red, quit_game)

        pygame.display.update()
        clock.tick(15)

def button(msg,x,y,w,h,ic,ac,action=None):
    mouse = pygame.mouse.get_pos()
    click = pygame.mouse.get_pressed() #collects mouse left, right and middle button
    if x + w &gt; mouse[0] &gt; x and y + h &gt; mouse[1] &gt; y:
        pygame.draw.rect(gameDisplay, ac, (x, y, w, h))
        if click[0] == 1 and action!= None:
            action()
    else:
        pygame.draw.rect(gameDisplay, ic, (x, y, w, h))

    smallText = pygame.font.Font(""freesansbold.ttf"", 20)
    textSurf, textRect = text_objects(msg, smallText)
    textRect.center = ((x + (w / 2)), (y + (h / 2)))
    gameDisplay.blit(textSurf, textRect)

def unpause():
    global pause
    pause = False

def paused():
    largeText = pygame.font.Font('freesansbold.ttf', 110)
    TextSurf, TextRect = text_objects(""Paused"", largeText)  # Returns text surface and rectangle
    TextRect.center = ((display_width / 2), (display_height / 2.5))
    gameDisplay.blit(TextSurf, TextRect)

    while pause:
        for event in pygame.event.get():
            if event.type == pygame.QUIT:
                pygame.quit()
                quit()

        button(""Continue"", 315.5, 450, 140, 53, green, bright_green, unpause)
        button(""Quit"", 546.5, 450, 140, 53, red, bright_red, quit_game)

        pygame.display.update()
        clock.tick(15)

def text_objects(text, font):
    textSurface = font.render(text, True, black)
    return textSurface, textSurface.get_rect()

def quit_game():
    pygame.quit()
    quit()

def game_over():
    largeText = pygame.font.Font('freesansbold.ttf', 110)
    TextSurf, TextRect = text_objects(""Game Over"", largeText)  # Returns text surface and rectangle
    TextRect.center = ((display_width / 2), (display_height / 2.5))
    gameDisplay.blit(TextSurf, TextRect)

    while True:
        for event in pygame.event.get():
            if event.type == pygame.QUIT:
                pygame.quit()
                quit()

        button(""Play Again"", 280.5, 450, 140, 53, green, bright_green,game_loop)
        button(""Quit"", 581.5, 450, 140, 53, red, bright_red,quit_game)

        pygame.display.update()
        clock.tick(15)

def game_loop():
    global pause
    x = (display_width * 0.08)
    y = (display_height * 0.2)

    x_change = 0
    y_change = 0

    blob_speed = 2

    velocity = [2, 2]

    score = 0
    lives = 3

    pos_x = display_width/1.2
    pos_y = display_height/1.2

    previous_time = pygame.time.get_ticks()
    previous_time2 = pygame.time.get_ticks()

    gameExit = False
    while not gameExit:
        for event in pygame.event.get():#monitors hardware movement/ clicks
            if event.type == pygame.QUIT:
                pygame.quit()
                quit()

        pos_x += velocity[0]
        pos_y += velocity[1]

        if pos_x + blob_width &gt; display_width or pos_x &lt; 601:
            velocity[0] = -velocity[0]

        if pos_y + blob_height &gt; display_height or pos_y &lt; 0:
            velocity[1] = -velocity[1]

        for b in range(len(bullets2)):
            bullets2[b][0] -= 6

        for bullet in bullets2:
            if bullet[0] &lt; 0:
                bullets2.remove(bullet)


        current_time2 = pygame.time.get_ticks()
        #ready to fire when 500 ms have passed.
        if current_time2 - previous_time2 &gt; 500:
            previous_time2 = current_time2
            bullets2.append([pos_x+25, pos_y+24])

# Checks to see if any keys are held down and remembers them with the variable keys.
        keys = pygame.key.get_pressed()

        for b in range(len(bullets)):
            bullets[b][0] += 6

        for bullet in bullets:
            if bullet[0] &gt; 1005:
                bullets.remove(bullet)

        if keys[pygame.K_SPACE]:
            current_time = pygame.time.get_ticks()
            #ready to fire when 500 ms have passed.
            if current_time - previous_time &gt; 500:
                previous_time = current_time
                bullets.append([x+25, y+24])

# If the player is holding down one key or the other the blob moves in that direction
        if x &lt; 0:
            x = 0
        if keys[pygame.K_a]:
            x_change = -blob_speed
        if x &gt; 401 - blob_width:
            x = 401 - blob_width
        if keys[pygame.K_d]:
            x_change = blob_speed
        if keys[pygame.K_p]:
            pause = True
            paused()


# If the player is holding down both or neither of the keys the blob stops
        if keys[pygame.K_a] and keys[pygame.K_d]:
            x_change = 0
        if not keys[pygame.K_a] and not keys[pygame.K_d]:
            x_change = 0

        if y &lt; 0:
            y = 0
        if keys[pygame.K_w]:
            y_change = -blob_speed
        if y &gt; display_height - blob_height:
            y = display_height - blob_height
        if keys[pygame.K_s]:
            y_change = blob_speed


        if keys[pygame.K_w] and keys[pygame.K_s]:
            y_change = 0
        if not keys[pygame.K_w] and not keys[pygame.K_s]:
            y_change = 0


        #print(event)
        # Reset x and y to new position
        x += x_change
        y += y_change

        gameDisplay.fill(blue)  #changes background surface
        bullets_hit(score)
        player_lives(lives)
        pygame.draw.line(gameDisplay, black, (601, display_height), (601, 0), 3)
        pygame.draw.line(gameDisplay, black, (401, display_height), (401, 0), 3)
        blob(pos_x, pos_y)
        blob(x, y)

        for bullet in bullets:
            gameDisplay.blit(bulletpicture, pygame.Rect(bullet[0], bullet[1], 0, 0))
            if bullet[0] &gt; pos_x and bullet[0] &lt; pos_x + blob_width:
                if bullet[1] &gt; pos_y and bullet[1] &lt; pos_y + blob_height or bullet[1] + bullet_height &gt; pos_y and bullet[1] + bullet_height &lt; pos_y + blob_height:
                    bullets.remove(bullet)
                    score+=1

        for bullet in bullets2:
            gameDisplay.blit(bulletpicture, pygame.Rect(bullet[0], bullet[1], 0, 0))
            if bullet[0] + bullet_width &lt; x + blob_width and bullet[0] &gt; x:
                if bullet[1] &gt; y and bullet[1] &lt; y + blob_height or bullet[1] + bullet_height &gt; y and bullet[1] + bullet_height &lt; y + blob_height:
                    bullets2.remove(bullet)
                    lives-=1

        if lives == 0:
            game_over()


        pygame.display.update() #update screen
        clock.tick(120)#moves frame on (fps in parameters)

game_intro()
game_loop()
pygame.quit()
quit()
</code></pre>

<p>[<strong>Additional Information</strong>] I will be using a reinforcement learning algorithm from <a href=""https://github.com/Hvass-Labs/TensorFlow-Tutorials/blob/master/reinforcement_learning.py"" rel=""nofollow noreferrer"">https://github.com/Hvass-Labs/TensorFlow-Tutorials/blob/master/reinforcement_learning.py</a> which from my understanding requires a gym environment in order for gym.make() to work. Any help would greatly be appreciated, if anything more is needed please tell me.</p>
",9244222,,9244222,,2018-04-05 11:11:27,2018-04-05 17:58:54,OpenAI Integrating custom game into a gym environment,<python><reinforcement-learning><openai-gym>,1,0,,2018-04-08 02:46:17,,CC BY-SA 3.0,
637,58109587,1,62156396,,2019-09-26 04:31:28,,0,172,"<p>I am just starting out with reinforcement learning and trying to create a custom environment with OpenAI gym. However, I am stumped with trying to create an environment (with roads and intersections) from map data, say Google Maps. </p>

<p>Would appreciate any help I can get. </p>
",12122783,,,,,2020-06-02 16:11:08,Creating OpenAI Gym Environment from Map Data,<reinforcement-learning><openai-gym>,1,0,,,,CC BY-SA 4.0,
638,58256345,1,58257398,,2019-10-06 10:24:12,,2,2809,"<p>I m trying to perform reinforcement learning algorithms on the gridworld environment but i can't find a way to load it. </p>

<p>I have successfully installed gym and gridworld 0.14.0 then I executed this command</p>

<pre class=""lang-py prettyprint-override""><code>env = gym.make(""gridworld-v0"")
</code></pre>

<p>I then obtained the following error stack</p>

<pre class=""lang-py prettyprint-override""><code>---------------------------------------------------------------------------
KeyError                                  Traceback (most recent call last)
~/anaconda3/lib/python3.7/site-packages/gym/envs/registration.py in spec(self, path)
    131         try:
--&gt; 132             return self.env_specs[id]
    133         except KeyError:

KeyError: 'gridworld-v0'

During handling of the above exception, another exception occurred:

UnregisteredEnv                           Traceback (most recent call last)
&lt;ipython-input-36-b3991c5b334f&gt; in &lt;module&gt;
----&gt; 1 env = gym.make(""gridworld-v0"")
      2 env.setPlan(""gridworldPlans/plan1.txt"", {0: -0.001, 3: 1, 4: 1, 5: -1, 6: -1})
      3 statedic, mdp = env.getMDP()

~/anaconda3/lib/python3.7/site-packages/gym/envs/registration.py in make(id, **kwargs)
    154 
    155 def make(id, **kwargs):
--&gt; 156     return registry.make(id, **kwargs)
    157 
    158 def spec(id):

~/anaconda3/lib/python3.7/site-packages/gym/envs/registration.py in make(self, path, **kwargs)
     98         else:
     99             logger.info('Making new env: %s', path)
--&gt; 100         spec = self.spec(path)
    101         env = spec.make(**kwargs)
    102         # We used to have people override _reset/_step rather than

~/anaconda3/lib/python3.7/site-packages/gym/envs/registration.py in spec(self, path)
    140                 raise error.DeprecatedEnv('Env {} not found (valid versions include {})'.format(id, matching_envs))
    141             else:
--&gt; 142                 raise error.UnregisteredEnv('No registered env with id: {}'.format(id))
    143 
    144     def register(self, id, **kwargs):

UnregisteredEnv: No registered env with id: gridworld-v0

</code></pre>

<p>I d expect it to be enough to load the environment but it is apparently enough. </p>

<p>Could you please advice me on how to proceed?</p>

<p>Cheers,
Paul</p>
",8551629,,,,,2021-02-02 10:57:08,Cannot load gridworld-v0 environment,<python><openai-gym>,1,1,,,,CC BY-SA 4.0,
639,45068568,1,47132897,,2017-07-12 22:32:21,,97,80389,"<p>I have an assignment to make an AI Agent that will learn to play a video game using ML. I want to create a new environment using OpenAI Gym because I don't want to use an existing environment. How can I create a new, custom Environment?</p>
<p>Also, is there any other way I can start to develop making AI Agent to play a specific video game without the help of OpenAI Gym?</p>
",6480803,,6480803,,2020-09-24 02:45:36,2021-11-09 08:16:17,How to create a new gym environment in OpenAI?,<machine-learning><artificial-intelligence><openai-gym>,2,0,0,,,CC BY-SA 4.0,
640,58225719,1,58243229,,2019-10-03 19:29:31,,0,124,"<p>We start building an Enviroment, based on cartpole-v0. We are trying to achieve a similar behaviour of our pole, so it doesn't rotate in the centerpoint, but at the Bottom. We are using set_rotation function from gym.classic_control.rendering. But there is no opportunity to set an anchorpoint. </p>

<p>We tried to translate the filledPolygon before rotation in different directions, but the anchorpoint remains in centerpoint.  </p>

<pre class=""lang-py prettyprint-override""><code>import math
import gym
from gym import spaces, logger
from gym.utils import seeding
import numpy as np
from os import path  

class THT_Env(gym.Env):
  ''' 
  I shorten the code to the render. The other parts of the code is working fine.
  '''
    def render(self, mode='human'):
        screen_width = 600
        screen_height = 600

        lead_width = 6  #Lead diameter = 0.6mm
        lead_length = 103 #Lead lenght ca. 10.3 mm
        lead_spacing = 50 #Lead spacing 5 mm

        body_height = 80 #Body height 8 mm
        body_width = 65 #Body width 6.5 mm

        pcb_thickness = 16 #1.6 mm
        #pcb_hole_diameter = 9 #0.9mm
        pcb_side = 270.5 
        pcb_middle = 41


        if self.viewer is None:
            from gym.envs.classic_control import rendering
            self.viewer = rendering.Viewer(screen_width, screen_height)
            # Initialize Body
            fname = path.join(path.dirname(__file__), ""assets/WYO_1nM.png"")
            body = rendering.Image(fname,body_width, body_height)
            self.bodytrans = rendering.Transform()
            body.add_attr(self.bodytrans)
            self.viewer.add_geom(body)

            # Initialize Lead 1
            l, r, t, b = -lead_width/2, lead_width/2, lead_length/2, -lead_length/2
            lead_1 = rendering.FilledPolygon([(l,b),(l,t),(r,t),(r,b)])
            lead_1.set_color(.4, .4, .4)
            self.lead_1_trans = rendering.Transform(translation=(lead_spacing/2, (-lead_length-body_height)/2))
            lead_1.add_attr(self.lead_1_trans)
            lead_1.add_attr(self.bodytrans)
            self.viewer.add_geom(lead_1)

            lead_2 = rendering.FilledPolygon([(l,b),(l,t),(r,t),(r,b)])
            lead_2.set_color(.4, .4, .4)
            #self.lead_2_trans = rendering.Transform(translation=(-lead_spacing/2, (-lead_length-body_height)/2))
            self.lead_2_trans = rendering.Transform(translation=(0, (-lead_length-body_height)/2),rotation=np.pi/2)
            lead_2.add_attr(self.lead_2_trans)
            lead_2.add_attr(self.bodytrans)
            self.viewer.add_geom(lead_2)

            l, r, t, b = -pcb_side/2, pcb_side/2, pcb_thickness/2, -pcb_thickness/2
            pcb_1 = rendering.FilledPolygon([(l,b),(l,t),(r,t),(r,b)])
            pcb_1.set_color(.0, .42, .0)
            self.pcb_1_trans = rendering.Transform(translation=(0+pcb_side/2, 110))
            pcb_1.add_attr(self.pcb_1_trans)
            self.viewer.add_geom(pcb_1)

            pcb_2 = rendering.FilledPolygon([(l,b),(l,t),(r,t),(r,b)])
            pcb_2.set_color(.0, .42, .0)
            self.pcb_2_trans = rendering.Transform(translation=(screen_width-pcb_side/2, 110))
            pcb_2.add_attr(self.pcb_2_trans)
            self.viewer.add_geom(pcb_2)

            l, r, t, b = -pcb_middle/2, pcb_middle/2, pcb_thickness/2, -pcb_thickness/2
            self.pcb_mid = rendering.FilledPolygon([(l,b),(l,t),(r,t),(r,b)])
            self.pcb_mid.set_color(.0, .42, .0)
            self.pcb_mid_trans = rendering.Transform(translation=(screen_width/2, 110))
            self.pcb_mid.add_attr(self.pcb_mid_trans)
            self.viewer.add_geom(self.pcb_mid)

        if self.state is None: return None

        x = self.state
        body_x = x[0]+screen_width/2
        body_y = x[1]+screen_height/2+200
        self.bodytrans.set_translation(body_x, body_y)
        #self.lead_1_trans.set_translation(0,-lead_length/2)
        #self.lead_1_trans.set_rotation(np.pi/2)

        return self.viewer.render(return_rgb_array= mode == 'rgb_array')

    def close(self):
        if self.viewer:
            self.viewer.close()
            self.viewer = None
</code></pre>

<p>The rotation on the right image results in the circumstance the the pole(lead) is no longer connected to the body. What I expect is that the Anchor lies in the lower part of the body.</p>

<p><img src=""https://lh3.googleusercontent.com/7JDYkdeYPLSMWg3l5hqEEq7MmZhxWDiE1XwtrRaX91xfjs_JW7poFYCK1PnMaKETaVS1dG8Z39t7BnXqBWmvLh3DdqML_PtaesACAVjIUD13SZ2cjXHbQ8zYXKor3IyxmIkR_fD8yKyXV9gKVFBKhSAq8hZraOnrIZA6_UX4SLb1KR8pNbMMtAKaucfwUnM15mDqOWKpCCGtJW3EQeIEXNYyAb39en0nbed-CVUEvIKXYXhHqjnzFUkvf2mD6ykTrmPYFJ605RD-eCWbfrMZHq1KK6mPi6a6RM3n0UhoBl5W9RO3jVbQO1W3tT37pKh45TjPIB4jNl2Y-k1C3PP9HXHO5jxuin8NWTxXSkdf_q6l7vbQX_zfUI-UmPd-u1QrrkoLwYJORepUIII7jCaSlsTOxn9Alora0Y49y358EEeGw2lz_ZOJpM_nAVKK400vNA6rvIF7NB59s9Km3AY6gNOHVAZ3qoSYcaO5il0iw5JrgZqeC32xmoxgkaUZ15IPlUBNjNNYehP4yAD1lhnFMhZaZ9WEcJttljxz1gAMosjyDLpOls-92aDTdv0oClT0lToXt8nERxkZ-23NClFv-Xz2n-zaNTvLaTUMpRLJDTRfXdUvTQl3yaJboOADGEDB84ZPkoH7XKHr0B2muyAVvr2WfhDC93EiWFgoUI4EGtCXJBpHz5VVV9k=s200-no"" alt=""Img1"">
<img src=""https://lh3.googleusercontent.com/nr-FwBP-TwGzauN9eSpRrX-uYPRbFcdZftf-MYpZY01MrXl-awT8kO3viSKQM8bG2szQStC7Q3Xv82L1eDVZ7GR7lXPbcMkA9DE2V1EvONtfA_a-8cHLd2JQuFLNmUm85BEtiY3LKElQEsHyfHJ83ae6XXWg7BitQ8erDkJZBxd7AalYNSw8DvJmuAXBd2pts6ZcuhXR8sD0hjJ_k-fIZvuRH1W6-yBru6Ld-UI6G5bSx74_uEsJ8f8yElx6tkE1xGqJe4Dh49iEAAmNiSH52n7kufkNEI1rR6xQVs0bamWyIc7_QPULnTVxkdWEvzjYMM14ee-8nq5nGsIqKxHZRfbPaXdWTatrk8c96AwchiTtfAsk9X9H4illM5ne9_ULUg03uwgJmnpmgVEwM08ADvaSXmxY9CMjKxklo_dDLpGz8L5QMn0Lb_THEK0_t7LoQ-j3XiOcrbwZ122evbpord8t3y6WBPh4g_y2cfZEWNufZe8sH2VUjhBI318Ywq3peCUPVx-DUgrW-1uFjU8SDTegYTscwfIcHUY4uOeEkPMMLWI_WGrjs0oV6cNJgIQS0oIE7Tvnl5pB9QfChoPHOh2VzrRZJDg1Ty17Ajh-z4IvBOL5nzUEbl2u9_2hxdr1qCg-XOz3oy_HltU64MJUKcCEdXDE_pNKG12GRIm9Qv3AXeYvR1DLzjs=s200-no"" alt=""Img2""></p>
",10985257,,,,,2019-10-04 21:09:51,Achieve Rotation in openAI Gym about another anchor,<python><openai-gym>,1,0,0,,,CC BY-SA 4.0,
646,49737552,1,49760991,,2018-04-09 16:30:07,,1,755,"<p>One of the actions I want the agent to do needs to have a delay between every action. For context, in pygame I have the following code for shooting a bullet:</p>

<pre><code>if keys[pygame.K_SPACE]:
    current_time = pygame.time.get_ticks()
    # ready to fire when 600 ms have passed.
    if current_time - previous_time &gt; 600:
        previous_time = current_time
        bullets.append([x + 25, y + 24])
</code></pre>

<p>I've set a timer to prevent bullet spamming, how would I construct this to work with the step() method? My other actions are moving up, down, left, right. </p>

<p>This is my first time creating a project with OpenAI-gym so I'm not sure what the capabilities of the toolkit are, any help would be greatly appreciated. </p>
",9244222,,6735980,,2018-04-10 18:51:40,2018-04-10 18:51:40,OpenAI-gym how to implement a timer for a certain action in step(),<python><pygame><openai-gym>,1,0,,,,CC BY-SA 3.0,
650,50041271,1,50047986,,2018-04-26 10:59:30,,1,243,"<p>The lunar lander on DQlearning doesn't perform well, so I try to improve the performance of lunar lander (dq learning) by optimising the parameters to make it better. Which part can I adjust? could anyone provide some ideas for me? add more layers or change the activation type or others methods??</p>

<p>here is the code:</p>

<pre class=""lang-py prettyprint-override""><code>import numpy as np
import gym
import csv

from keras.models import Sequential
from keras.layers import Dense, Activation, Flatten
from keras.optimizers import Adam

from rl.agents.dqn import DQNAgent
from rl.policy import BoltzmannQPolicy, EpsGreedyQPolicy
from rl.memory import SequentialMemory

import io
import sys
import csv

# Path environment changed to make things work properly
# export DYLD_FALLBACK_LIBRARY_PATH=$DYLD_FALLBACK_LIBRARY_PATH:/usr/lib


# Get the environment and extract the number of actions.
ENV_NAME = 'LunarLander-v2'
env = gym.make(ENV_NAME)
np.random.seed(123)
env.seed(123)
nb_actions = env.action_space.n

# Next, we build a very simple model.
model = Sequential()
model.add(Flatten(input_shape=(1,) + env.observation_space.shape))
model.add(Dense(16))
model.add(Activation('relu'))
model.add(Dense(16))
model.add(Activation('relu'))
model.add(Dense(16))
model.add(Activation('tanh'))
model.add(Dense(nb_actions))
model.add(Activation('linear'))
print(model.summary())

# Finally, we configure and compile our agent. You can use every built-in Keras optimizer and
# even the metrics!
memory = SequentialMemory(limit=300000, window_length=1)
policy = EpsGreedyQPolicy()
dqn = DQNAgent(model=model, nb_actions=nb_actions, memory=memory, nb_steps_warmup=10,
               target_model_update=1e-2, policy=policy)
dqn.compile(Adam(lr=1e-3), metrics=['mae'])

dqn.fit(env, nb_steps=30000, visualize=True, verbose=2)

# After training is done, we save the final weights.
dqn.save_weights('dqn_{}_weights.h5f'.format(ENV_NAME))

# Redirect stdout to capture test results
old_stdout = sys.stdout
sys.stdout = mystdout = io.StringIO()

# Evaluate our algorithm for a few episodes.
dqn.test(env, nb_episodes=200, visualize=False)

# Reset stdout
sys.stdout = old_stdout

results_text = mystdout.getvalue()

# Print results text
print(""results"")
print(results_text)

# Extact a rewards list from the results
total_rewards = list()
for idx, line in enumerate(results_text.split('\n')):
    if idx &gt; 0 and len(line) &gt; 1:
        reward = float(line.split(':')[2].split(',')[0].strip())
        total_rewards.append(reward)

# Print rewards and average
print(""total rewards"", total_rewards)
print(""average total reward"", np.mean(total_rewards))

# Write total rewards to file
f = open(""lunarlander_rl_rewards.csv"",'w')
wr = csv.writer(f)
for r in total_rewards:
     wr.writerow([r,])
f.close()
</code></pre>

<p>Thanks~</p>
",9591860,,9591860,,2018-04-28 20:37:21,2018-04-28 20:37:21,how to improve the performance Machine Learning - DQ learning model,<tensorflow><machine-learning><artificial-intelligence><reinforcement-learning><openai-gym>,1,0,,,,CC BY-SA 3.0,
657,50481591,1,50493740,,2018-05-23 06:49:16,,0,546,"<p>In order to make my own environment and use the some codes of github, I need to see what does happen inside <code>gym.make('env')</code> for instance <code>gym.make('carpole0')</code></p>

<p>Where inside the gym github, I am able to find it? I found <code>https://github.com/openai/gym/blob/master/gym/envs/classic_control/cartpole.py</code> but it does not have make?</p>

<p>How to write the update section of defining an environment (env) for DQN which is not in gym library?
I am looking for an environment definition <strong>""env""</strong> example on github or another resource that is not designed for Atari games. I saw several models but most of them use OpenAI's <code>gym</code> library and are written for playing the Atari games which have relatively simple environments. I am looking for a game environment with more complicated states. </p>

<p>I want to write an update function (step function of environment) for the  state <code>t+1</code> based on state <code>t</code>. What is my problem is that <strong>if the state depends on more than one state before</strong> how do I implement that? I am looking for an example to demonstrate this. It seems to have an obligation to send the time t in environment.</p>

<p>It would be more helpful for me if an example is defined for an <strong>adaptive control</strong> problem.</p>
",9708375,,9708375,,2018-06-07 21:12:45,2018-06-07 21:12:45,how to see what happens inside gym.make('env'),<keras><controls><environment><reinforcement-learning><openai-gym>,1,0,,,,CC BY-SA 4.0,
658,50440522,1,50452826,,2018-05-21 00:16:00,,0,720,"<p>I am creating a deep neural network using Keras using images from the Gym library from Open AI. </p>

<p>I tried to reshape the images using the following code:</p>

<pre><code>def reshape_dimensions(observation):
    processed = np.mean(observation,2,keepdims = False)
    cropped = processed[35:195]
    result = cropped[::2,::2]

    return result
</code></pre>

<p>This gives me an image of shape (80,80) but every time I try to input that shape in the first layer of the Keras network it doesn't work.</p>

<p>What should be the shape I should use so I can further develop the network?</p>

<p>Attached the whole code:</p>

<p><strong>PART I</strong> retrieves the training data</p>

<pre><code>import gym
import random
import numpy as np
from statistics import mean, median
from collections import Counter


### GAME VARIABLE SETTINGS ###
env = gym.make('MsPacman-v0')
env.reset()

goal_steps = 2000
score_requirement = 250
initial_games = 200

print('Options to play: ',env.unwrapped.get_action_meanings())


### DEFINE FUNCTIONS ####

def reshape_dimensions(observation):
    processed = np.mean(observation,2,keepdims = False)
    cropped = processed[35:195]
    result = cropped[::2,::2]

    return result

def initial_population():
    training_data = []
    scores = []
    accepted_scores = []

    for _ in range(initial_games):
        score = 0
        game_memory = []
        prev_obvservation = []
        for _ in range(goal_steps):
            #env.render()
            action = env.action_space.sample() #Take random action in the env
            observation, reward, done, info = env.step(action)

            reshape_observation = reshape_dimensions(observation)

            if len(prev_obvservation) &gt; 0:
                game_memory.append([prev_obvservation, action])

            prev_obvservation = reshape_observation

            score = score + reward
            if done: 
                break

        if score &gt;= score_requirement:
            accepted_scores.append(score)

            for data in game_memory: 
                if data[1] == 0:
                    output = [1,0,0,0,0,0,0,0,0]
                elif data[1] == 1:
                    output = [0,1,0,0,0,0,0,0,0]
                elif data[1] == 2:
                    output = [0,0,1,0,0,0,0,0,0]
                elif data[1] == 3:
                    output = [0,0,0,1,0,0,0,0,0]
                elif data[1] == 4:
                    output = [0,0,0,0,1,0,0,0,0]
                elif data[1] == 5:
                    output = [0,0,0,0,0,1,0,0,0]
                elif data[1] == 6:
                    output = [0,0,0,0,0,0,1,0,0]
                elif data[1] == 7:
                    output = [0,0,0,0,0,0,0,1,0]
                elif data[1] == 8:
                    output = [0,0,0,0,0,0,0,0,1]

                training_data.append([data[0],output])



        env.reset()
        scores.append(score)


    print('Average accepted scores:', mean(accepted_scores))
    print('Median accepted scores:', median(accepted_scores))
    print(Counter(accepted_scores))

    return training_data 



### RUN CODE ###

training_data = initial_population()
np.save('data_for_training_200.npy', training_data)
</code></pre>

<p><strong>PART II</strong> trains the model </p>

<pre><code>import gym
import random
import numpy as np
import keras
from statistics import mean, median
from collections import Counter
from keras.models import Sequential
from keras.layers import Dense
from keras.callbacks import EarlyStopping
from keras.optimizers import Adam


### LOAD DATA ###

raw_training_data = np.load(""data_for_training_200.npy"")
training_data = [i[0:2] for i in raw_training_data]

print(np.shape(training_data))

### DEFINE FUNCTIONS ###


def neural_network_model():

    network = Sequential()
    network.add(Dense(100, activation = 'relu', input_shape = (80,80)))
    network.add(Dense(9,activation = 'softmax'))

    optimizer = Adam(lr = 0.001)

    network.compile(optimizer = optimizer, loss = 'categorical_crossentropy', metrics=['accuracy'])

    return network

def train_model(training_data):

    X = [i[0] for i in training_data]
    y = [i[1] for i in training_data]

    #X = np.array([i[0] for i in training_data])
    #y = np.array([i[1] for i in training_data])

    print('shape of X: ', np.shape(X))
    print('shape of y: ', np.shape(y))

    early_stopping_monitor = EarlyStopping(patience = 3)

    model = neural_network_model()

    model.fit(X, y, epochs = 20, callbacks = [early_stopping_monitor])

    return model

train_model(training_data = training_data)
</code></pre>
",7733971,,,,,2018-05-21 16:17:08,Input shape in Keras,<python-3.x><neural-network><keras><deep-learning><openai-gym>,1,2,,,,CC BY-SA 4.0,
662,50125950,1,50180236,,2018-05-02 01:24:49,,4,9021,"<p>I am currently trying to use the Atari module for gym/openai. I have successfully managed to install the dependency.</p>

<pre><code>Patricks-MacBook-Pro:~ patrickmaynard$ python3.6 -m pip install gym[atari]
Requirement already satisfied: gym[atari] in /usr/local/lib/python3.6/site-packages (0.10.5)
Requirement already satisfied: six in /usr/local/lib/python3.6/site-packages (from gym[atari]) (1.11.0)
Requirement already satisfied: pyglet&gt;=1.2.0 in /usr/local/lib/python3.6/site-packages (from gym[atari]) (1.3.2)
Requirement already satisfied: requests&gt;=2.0 in /usr/local/lib/python3.6/site-packages (from gym[atari]) (2.18.4)
Requirement already satisfied: numpy&gt;=1.10.4 in /usr/local/lib/python3.6/site-packages (from gym[atari]) (1.14.3)
Requirement already satisfied: Pillow; extra == ""atari"" in /usr/local/lib/python3.6/site-packages (from gym[atari]) (5.1.0)
Requirement already satisfied: PyOpenGL; extra == ""atari"" in /usr/local/lib/python3.6/site-packages (from gym[atari]) (3.1.0)
Requirement already satisfied: atari-py&gt;=0.1.1; extra == ""atari"" in /usr/local/lib/python3.6/site-packages (from gym[atari]) (0.1.1)
Requirement already satisfied: future in /usr/local/lib/python3.6/site-packages (from pyglet&gt;=1.2.0-&gt;gym[atari]) (0.16.0)
Requirement already satisfied: certifi&gt;=2017.4.17 in /usr/local/lib/python3.6/site-packages (from requests&gt;=2.0-&gt;gym[atari]) (2018.4.16)
Requirement already satisfied: urllib3&lt;1.23,&gt;=1.21.1 in /usr/local/lib/python3.6/site-packages (from requests&gt;=2.0-&gt;gym[atari]) (1.22)
Requirement already satisfied: chardet&lt;3.1.0,&gt;=3.0.2 in /usr/local/lib/python3.6/site-packages (from requests&gt;=2.0-&gt;gym[atari]) (3.0.4)
Requirement already satisfied: idna&lt;2.7,&gt;=2.5 in /usr/local/lib/python3.6/site-packages (from requests&gt;=2.0-&gt;gym[atari]) (2.6)
</code></pre>

<p>However, when I try to run the file the compiler cannot find the module and I get the following error:</p>

<pre><code>Traceback (most recent call last):
  File ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/gym/envs/atari/atari_env.py"", line 9, in &lt;module&gt;
    import atari_py
ModuleNotFoundError: No module named 'atari_py'
</code></pre>

<p>During handling of the above exception, another exception occurred:</p>

<pre><code>Traceback (most recent call last):
  File ""/Users/patrickmaynard/TicTacToe/recipe-578816-1.py"", line 170, in &lt;module&gt;
    env = gym.make('Pong-v0')
  File ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/gym/envs/registration.py"", line 167, in make
    return registry.make(id)
  File ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/gym/envs/registration.py"", line 119, in make
    env = spec.make()
  File ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/gym/envs/registration.py"", line 85, in make
    cls = load(self._entry_point)
  File ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/gym/envs/registration.py"", line 14, in load
    result = entry_point.load(False)
  File ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/pkg_resources/__init__.py"", line 2291, in load
    return self.resolve()
  File ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/pkg_resources/__init__.py"", line 2297, in resolve
    module = __import__(self.module_name, fromlist=['__name__'], level=0)
  File ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/gym/envs/atari/__init__.py"", line 1, in &lt;module&gt;
    from gym.envs.atari.atari_env import AtariEnv
  File ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/gym/envs/atari/atari_env.py"", line 11, in &lt;module&gt;
    raise error.DependencyNotInstalled(""{}. (HINT: you can install Atari dependencies by running 'pip install gym[atari]'.)"".format(e))
gym.error.DependencyNotInstalled: No module named 'atari_py'. (HINT: you can install Atari dependencies by running 'pip install gym[atari]'.)
</code></pre>

<p>gym and cmake have already been installed.</p>
",8782268,,9598813,,2018-05-02 01:48:53,2021-10-19 17:14:17,No module named 'atari_py' after installation,<python-3.x><pip><openai-gym>,3,0,0,,,CC BY-SA 4.0,
667,41502795,1,41503352,,2017-01-06 09:38:27,,2,2690,"<p>I'm having trouble installing gym-atari in a virtualenv.  From what I can tell, it <em>looks</em> like the issue is with zlib, but zlib is installed on the system (so is cmake).  Anyone have any ideas?  This is on a Gentoo system, for what it's worth:</p>

<pre><code>(tensorflow)alaya ~ # pip install gym[atari] 
Requirement already satisfied: gym[atari] in     /opt/tensorflow/lib/python2.7/site-packages 
Requirement already satisfied: six in /usr/lib64/python2.7/site-packages (from gym[atari]) 
Requirement already satisfied: requests&gt;=2.0 in /usr/lib64/python2.7/site-packages (from gym[atari]) 
Requirement already satisfied: pyglet&gt;=1.2.0 in /opt/tensorflow/lib/python2.7/site-packages (from gym[atari]) 
Requirement already satisfied: numpy&gt;=1.10.4 in /opt/tensorflow/lib/python2.7/site-packages (from gym[atari]) 
Requirement already satisfied: PyOpenGL; extra == ""atari"" in /usr/lib64/python2.7/site-packages (from gym[atari]) 
Collecting atari-py&gt;=0.0.17; extra == ""atari"" (from gym[atari])   Using cached atari-py-0.0.18.tar.gz 
Requirement already satisfied: Pillow; extra == ""atari"" in /usr/lib64/python2.7/site-packages (from gym[atari]) 
Building wheels for collected packages: atari-py   
Running setup.py bdist_wheel for atari-py ... error   
Complete output from command /opt/tensorflow/bin/python2.7 -u -c ""import setuptools, tokenize;__file__='/tmp/pip-build-Xd0Ga4/atari-py/setup.py';f=getattr(tokenize, 'open', open)(__file__);code=f.read().replace('\r\n', '\n');f.close();exec(compile(code, __file__, 'exec'))"" bdist_wheel -d /tmp/tmpCxn1j3pip-wheel- --python-tag cp27: 
. 
. 
.
[  9%] Building CXX object CMakeFiles/ale-bin.dir/src/common/display_screen.cpp.o   
In file included from /tmp/pip-build-Xd0Ga4/atari-py/atari_py/ale_interface/src/emucore/unzip.c:13:0: /tmp/pip-build-Xd0Ga4/atari-py/atari_py/ale_interface/src/emucore/unzip.h:114:45: 
error: expected '=', ',', ';', 'asm' or '__attribute__' before 'OF'    extern int ZEXPORT unzStringFileNameCompare OF ((const char* fileName1,
                                               ^   
/tmp/pip-build-Xd0Ga4/atari-py/atari_py/ale_interface/src/emucore/unzip.h:127:32: 
error: expected '=', ',', ';', 'asm' or '__attribute__' before 'OF'    extern unzFile ZEXPORT unzOpen OF((const char *path));
                                  ^   
/tmp/pip-build-Xd0Ga4/atari-py/atari_py/ale_interface/src/emucore/unzip.h:138:29: 
error: expected '=', ',', ';', 'asm' or '__attribute__' before 'OF'    extern int ZEXPORT unzClose OF((unzFile file)); 
. . .
</code></pre>

<p>Any ideas?  My only guess is that somehow zlib isn't getting pulled into the virtualenv???  Maybe it's not even zlib though?  </p>
",3054726,,,,,2018-09-27 13:36:11,Installing gym[atari] in a virtualenv,<python><linux><virtualenv><gentoo><openai-gym>,2,1,0,,,CC BY-SA 3.0,
670,59879297,1,59894667,,2020-01-23 13:06:34,,0,817,"<p>I have built a custom environment by implementing the <code>step, reset and render</code> methods from <code>StableBaselines</code> but I don't know how to print some plots. </p>

<p>For instance, how many times my agent (in a Discrete action space) took action = 0, 1, 2, etc...</p>

<p>What signal did the environment give.</p>

<p>How the rewards moved?</p>

<p>I found about results_plotter but couldn't find many info of it.</p>

<pre><code>results_plotter.plot_results(["".""], 10e6, results_plotter.X_TIMESTEPS, ""Market rewards"")
</code></pre>
",6705485,,,,,2020-01-24 10:32:59,"Plot entropy, avg rewards etc from Stable Baselines",<python><openai-gym>,1,0,,,,CC BY-SA 4.0,
672,41137192,1,42570586,,2016-12-14 07:40:50,,1,5691,"<p>I installed gym by <code>pip install -e '.[all]'</code>. But I want to uninstall it now, how can I achieve that? I have tried like <code>pip uninstall gym</code>, but did not succeed with errors like <code>Can't uninstall 'gym'. No files were found to uninstall.</code></p>
",6693951,,,,,2017-03-03 04:19:14,How to uninstall gym if I installed it by pip install -e '.[all]'?,<openai-gym>,1,0,,,,CC BY-SA 3.0,
673,59889597,1,59889907,,2020-01-24 02:15:24,,0,487,"<p>I am modifying some Python 2.7 code to be compatible with Python 3.7. I made as many edits as I could, until this happens:</p>

<pre class=""lang-py prettyprint-override""><code>C:\Users\AyazA\Desktop\schema-games&gt;python schema_games/breakout/play.py StandardBreakout
pygame 1.9.6
Hello from the pygame community. https://www.pygame.org/contribute.html
[34m--------------------------------------------------------------------------------[0m
[34mStarting interactive game. Press &lt;ESC&gt; at any moment to terminate.[0m
[34m--------------------------------------------------------------------------------[0m
Traceback (most recent call last):
  File ""schema_games/breakout/play.py"", line 93, in &lt;module&gt;
    play_game(getattr(games, variant), debug=debug, cheat_mode=cheat_mode)
  File ""schema_games/breakout/play.py"", line 54, in play_game
    play(env, fps=fps, keys_to_action=keys_to_action, zoom=ZOOM_FACTOR)
  File ""C:\Python37\lib\site-packages\gym\utils\play.py"", line 79, in play
    env.reset()
  File ""C:\Users\AyazA\Desktop\schema-games\schema_games\breakout\core.py"", line 301, in reset
    self.layout_sanity_check()
  File ""C:\Users\AyazA\Desktop\schema-games\schema_games\breakout\core.py"", line 473, in layout_sanity_check
    all_occupied_nzis = [nzi for obj in considered_objects
  File ""C:\Users\AyazA\Desktop\schema-games\schema_games\breakout\core.py"", line 474, in &lt;listcomp&gt;
    for nzi in obj.offset_nzis if obj.visible]
  File ""C:\Users\AyazA\Desktop\schema-games\schema_games\breakout\objects.py"", line 190, in offset_nzis
    offset_nzis_from_position(self._nzis, self._position)
  File ""C:\Users\AyazA\Desktop\schema-games\schema_games\breakout\utils.py"", line 57, in offset_nzis_from_position
    return zip(*(np.add(nzis, np.array(pos))).T)
TypeError: unsupported operand type(s) for +: 'zip' and 'int'
</code></pre>

<p>I tried unzipping the contents of the zipped tuple, but it was of no use. I tried even further to look into forcing the two to add (both <code>pos</code> and <code>nzis</code> are of type <code>ndarray</code>), only to not work. Is this another Python 2.7 incompatibility that I am missing? <a href=""https://github.com/ayaz-amin/schema-games"" rel=""nofollow noreferrer"">Here</a> is the source code and the <a href=""https://github.com/vicariousinc/schema-games"" rel=""nofollow noreferrer"">corresponding repository</a> that I forked from</p>
",11365858,,,,,2020-01-24 03:06:32,Python - TypeError: unsupported operand type(s) for +: 'zip' and 'int',<numpy><pygame><openai-gym>,1,4,,,,CC BY-SA 4.0,
674,40509002,1,40533076,,2016-11-09 14:12:15,,7,12267,"<p>Running almost every code from OpenAi gym in spyder by Anaconda (for instance this code: <a href=""https://gym.openai.com/evaluations/eval_y5dnhk0ZSMqlqJKBz5vJQw"" rel=""noreferrer"">https://gym.openai.com/evaluations/eval_y5dnhk0ZSMqlqJKBz5vJQw</a> )
I run into the following error message:</p>

<pre><code>DependencyNotInstalled: Found neither the ffmpeg nor avconv executables. On OS X, you can install ffmpeg via 'brew install ffmpeg'. On most Ubuntu variants, 'sudo apt-get install ffmpeg' should do it. On Ubuntu 14.04, however, you'll need to install avconv with 'sudo apt-get install libav-tools'.
</code></pre>

<p>However when I type <code>brew install ffmpeg</code> in terminal, I get <code>Warning: ffmpeg-3.2 already installed</code> and nothing happens.</p>

<p>Any ideas on how to fix this error?</p>
",4588128,,,,,2019-09-29 16:46:44,ffmpeg is not being detected by Spyder,<ffmpeg><spyder><openai-gym>,2,0,0,,,CC BY-SA 3.0,
677,62334284,1,62363354,,2020-06-11 22:00:43,,1,835,"<p>I'm trying to implement MCTS on Openai's atari gym environments, which requires the ability to plan: acting in the environment and restoring it to a previous state. I read that this can be done with the ram version of the games:</p>

<p>recording the current state in a snapshot:
<code>snapshot = env.ale.cloneState()</code></p>

<p>restoring the environment to a specific state recorded in snapshot:
<code>env.ale.restoreState(snapshot)</code></p>

<p>so I tried using the ram version of breakout:</p>

<pre><code>env = gym.make(""Breakout-ram-v0"")
env.reset()

print(""initial_state:"")
plt.imshow(env.render('rgb_array'))
env.close()

# create first snapshot
snap0 = env.ale.cloneState()
</code></pre>

<p>executing the code above shows the image of the start of the game. We recorded the first state with snap0. Now let's play until the end:</p>

<pre><code>while True:
    #is_done = env.ale.act(env.action_space.sample())[2]
    r = env.ale.act(env.action_space.sample())
    is_done = env.ale.game_over()
    if is_done:
        print(""Whoops! We died!"")
        break

print(""final state:"")
plt.imshow(env.render('rgb_array'))
</code></pre>

<p>executing the code above shows the image of the end of the game.
now let's load the first state again to the environment:</p>

<pre><code>env.ale.restoreState(snap0)
print(""\n\nAfter loading snapshot"")
plt.imshow(env.render('rgb_array'))
</code></pre>

<p>Instead of showing me the image of the start of the game, it shows me the same image of the end of the game. The environment is not reverting back even though I loaded the original first state.</p>

<p>If anyone got to work with ale and recording these kind of states, I'd really appreciate the help in figuring out what am I doing wrong. Thanks!</p>
",10146664,,,,,2020-06-13 17:45:52,How to restore previous state to gym environment,<deep-learning><reinforcement-learning><openai-gym><monte-carlo-tree-search>,1,0,0,,,CC BY-SA 4.0,
679,62108063,1,63091570,,2020-05-30 20:06:59,,-1,934,"<p>I have created a custom environment in open ai gym  and i am facing error while loading the weights Could some one help me to resolve the issue . I am training a TD3 network in a custom environment and i have trained successfully but while inferencing i am facing this issue </p>

<pre><code>class Actor(nn.Module):

  def __init__(self, state_dim, action_dim, max_action):
    super(Actor, self).__init__()
    self.layer_1 = nn.Linear(state_dim, 400)
    self.layer_2 = nn.Linear(400, 300)
    self.layer_3 = nn.Linear(300, action_dim)
    self.max_action = max_action

  def forward(self, x):
    x = F.relu(self.layer_1(x))
    x = F.relu(self.layer_2(x))
    x = self.max_action * torch.tanh(self.layer_3(x)) 
    return x

class Critic(nn.Module):

  def __init__(self, state_dim, action_dim):
    super(Critic, self).__init__()
    # Defining the first Critic neural network
    self.layer_1 = nn.Linear(state_dim + action_dim, 400)
    self.layer_2 = nn.Linear(400, 300)
    self.layer_3 = nn.Linear(300, 1)
    # Defining the second Critic neural network
    self.layer_4 = nn.Linear(state_dim + action_dim, 400)
    self.layer_5 = nn.Linear(400, 300)
    self.layer_6 = nn.Linear(300, 1)

  def forward(self, x, u):
    xu = torch.cat([x, u], 1)
    # Forward-Propagation on the first Critic Neural Network
    x1 = F.relu(self.layer_1(xu))
    x1 = F.relu(self.layer_2(x1))
    x1 = self.layer_3(x1)
    # Forward-Propagation on the second Critic Neural Network
    x2 = F.relu(self.layer_4(xu))
    x2 = F.relu(self.layer_5(x2))
    x2 = self.layer_6(x2)
    return x1, x2

  def Q1(self, x, u):
    xu = torch.cat([x, u], 1)
    x1 = F.relu(self.layer_1(xu))
    x1 = F.relu(self.layer_2(x1))
    x1 = self.layer_3(x1)
    return x1

# Selecting the device (CPU or GPU)
device = torch.device(""cuda"" if torch.cuda.is_available() else ""cpu"")

# Building the whole Training Process into a class

class TD3(object):

  def __init__(self, state_dim, action_dim, max_action):
    self.actor = Actor(state_dim, action_dim, max_action).to(device)
    self.actor_target = Actor(state_dim, action_dim, max_action).to(device)
    self.actor_target.load_state_dict(self.actor.state_dict())
    self.actor_optimizer = torch.optim.Adam(self.actor.parameters())
    self.critic = Critic(state_dim, action_dim).to(device)
    self.critic_target = Critic(state_dim, action_dim).to(device)
    self.critic_target.load_state_dict(self.critic.state_dict())
    self.critic_optimizer = torch.optim.Adam(self.critic.parameters())
    self.max_action = max_action

  def select_action(self, state):
    state = torch.Tensor(state.reshape(1, -1)).to(device)
    return self.actor(state).cpu().data.numpy().flatten()

  def train(self, replay_buffer, iterations, batch_size=100, discount=0.99, tau=0.005, policy_noise=0.2, noise_clip=0.5, policy_freq=2):

    for it in range(iterations):

      # Step 4: We sample a batch of transitions (s, sâ€™, a, r) from the memory
      batch_states, batch_next_states, batch_actions, batch_rewards, batch_dones = replay_buffer.sample(batch_size)
      state = torch.Tensor(batch_states).to(device)
      next_state = torch.Tensor(batch_next_states).to(device)
      action = torch.Tensor(batch_actions).to(device)
      reward = torch.Tensor(batch_rewards).to(device)
      done = torch.Tensor(batch_dones).to(device)

      # Step 5: From the next state sâ€™, the Actor target plays the next action aâ€™
      next_action = self.actor_target(next_state)

      # Step 6: We add Gaussian noise to this next action aâ€™ and we clamp it in a range of values supported by the environment
      noise = torch.Tensor(batch_actions).data.normal_(0, policy_noise).to(device)
      noise = noise.clamp(-noise_clip, noise_clip)
      next_action = (next_action + noise).clamp(-self.max_action, self.max_action)

      # Step 7: The two Critic targets take each the couple (sâ€™, aâ€™) as input and return two Q-values Qt1(sâ€™,aâ€™) and Qt2(sâ€™,aâ€™) as outputs
      target_Q1, target_Q2 = self.critic_target(next_state, next_action)

      # Step 8: We keep the minimum of these two Q-values: min(Qt1, Qt2)
      target_Q = torch.min(target_Q1, target_Q2)

      # Step 9: We get the final target of the two Critic models, which is: Qt = r + Î³ * min(Qt1, Qt2), where Î³ is the discount factor
      target_Q = reward + ((1 - done) * discount * target_Q).detach()

      # Step 10: The two Critic models take each the couple (s, a) as input and return two Q-values Q1(s,a) and Q2(s,a) as outputs
      current_Q1, current_Q2 = self.critic(state, action)

      # Step 11: We compute the loss coming from the two Critic models: Critic Loss = MSE_Loss(Q1(s,a), Qt) + MSE_Loss(Q2(s,a), Qt)
      critic_loss = F.mse_loss(current_Q1, target_Q) + F.mse_loss(current_Q2, target_Q)

      # Step 12: We backpropagate this Critic loss and update the parameters of the two Critic models with a SGD optimizer
      self.critic_optimizer.zero_grad()
      critic_loss.backward()
      self.critic_optimizer.step()

      # Step 13: Once every two iterations, we update our Actor model by performing gradient ascent on the output of the first Critic model
      if it % policy_freq == 0:
        actor_loss = -self.critic.Q1(state, self.actor(state)).mean()
        self.actor_optimizer.zero_grad()
        actor_loss.backward()
        self.actor_optimizer.step()

        # Step 14: Still once every two iterations, we update the weights of the Actor target by polyak averaging
        for param, target_param in zip(self.critic.parameters(), self.critic_target.parameters()):
          target_param.data.copy_(tau * param.data + (1 - tau) * target_param.data)

        # Step 15: Still once every two iterations, we update the weights of the Critic target by polyak averaging
        for param, target_param in zip(self.actor.parameters(), self.actor_target.parameters()):
          target_param.data.copy_(tau * param.data + (1 - tau) * target_param.data)

  # Making a save method to save a trained model
  def save(self, filename, directory):
    torch.save(self.actor.state_dict(), '%s/%s_actor.pth' % (directory, filename))
    torch.save(self.critic.state_dict(), '%s/%s_critic.pth' % (directory, filename))

  # Making a load method to load a pre-trained model
  def load(self, filename, directory):
    self.actor.load_state_dict(torch.load('%s/%s_actor.pth' % (directory, filename)))
    self.critic.load_state_dict(torch.load('%s/%s_critic.pth' % (directory, filename)))

def evaluate_policy(policy, eval_episodes=10):
  avg_reward = 0.
  for _ in range(eval_episodes):
    obs = env.reset()
    done = False
    while not done:
      action = policy.select_action(np.array(obs))
      obs, reward, done, _ = env.step(action)
      avg_reward += reward
  avg_reward /= eval_episodes
  print (""---------------------------------------"")
  print (""Average Reward over the Evaluation Step: %f"" % (avg_reward))
  print (""---------------------------------------"")
  return avg_reward

env_name = ""Pygame-v0""
seed = 0

file_name = ""%s_%s_%s"" % (""TD3"", env_name, str(seed))
print (""---------------------------------------"")
print (""Settings: %s"" % (file_name))
print (""---------------------------------------"")

eval_episodes = 10
save_env_vid = True
env = gym.make(env_name)
max_episode_steps = env._max_episode_steps
if save_env_vid:
  env = wrappers.Monitor(env, monitor_dir, force = True)
  env.reset()
env.seed(seed)
torch.manual_seed(seed)
np.random.seed(seed)
state_dim = env.observation_space.shape[0]
action_dim = env.action_space.shape[0]
max_action = float(env.action_space.high[0])
policy = TD3(state_dim, action_dim, max_action)
#policy.load(file_name, './pytorch_models/')
policy.load(file_name,""/content/gdrive/My Drive/reinforce/gym_game/pytorch_models"")
_ = evaluate_policy(policy, eval_episodes=eval_episodes)
</code></pre>

<p>Traceback:
I am facing a runtime error while loading the state_dict for actor model .I searched google but couldnt find similar issues .</p>

<pre><code>RuntimeError: Error(s) in loading state_dict for Actor:
    Missing key(s) in state_dict: ""layer_1.weight"", ""layer_1.bias"", ""layer_2.weight"", ""layer_2.bias"", ""layer_3.weight"", ""layer_3.bias"". 
    Unexpected key(s) in state_dict: ""encoder.0.weight"", ""encoder.0.bias"", ""encoder.2.weight"", ""encoder.2.bias"", ""encoder.2.running_mean"", ""encoder.2.running_var"", ""encoder.2.num_batches_tracked"", ""encoder.3.weight"", ""encoder.3.bias"", ""encoder.5.weight"", ""encoder.5.bias"", ""encoder.5.running_mean"", ""encoder.5.running_var"", ""encoder.5.num_batches_tracked"", ""encoder.6.weight"", ""encoder.6.bias"", ""encoder.8.weight"", ""encoder.8.bias"", ""encoder.8.running_mean"", ""encoder.8.running_var"", ""encoder.8.num_batches_tracked"", ""encoder.10.weight"", ""encoder.10.bias"", ""encoder.12.weight"", ""encoder.12.bias"", ""encoder.12.running_mean"", ""encoder.12.running_var"", ""encoder.12.num_batches_tracked"", ""encoder.13.weight"", ""encoder.13.bias"", ""encoder.15.weight"", ""encoder.15.bias"", ""encoder.15.running_mean"", ""encoder.15.running_var"", ""encoder.15.num_batches_tracked"", ""encoder.16.weight"", ""encoder.16.bias"", ""linear.0.weight"", ""linear.0.bias"", ""linear.2.weight"", ""linear.2.bias"". 
</code></pre>
",6515390,,,,,2020-07-25 17:45:02,RuntimeError: Error(s) in loading state_dict for Actor - torch.load(),<python-3.x><pytorch><reinforcement-learning><openai-gym>,1,2,,,,CC BY-SA 4.0,
680,62354041,1,62357583,,2020-06-13 00:02:27,,0,363,"<p>pls help me. I can not import lib</p>

<p>roboschool 1.0.49 
gym==0.15.4</p>

<p>input</p>

<pre><code>import roboschool
</code></pre>

<p>output</p>

<pre><code>D:\Users\gumin\anaconda3\python.exe D:/Users/gumin/anaconda3/Lib/site-packages/roboschool/test.py
Traceback (most recent call last):
  File ""D:/Users/gumin/anaconda3/Lib/site-packages/roboschool/test.py"", line 1, in &lt;module&gt;
    import roboschool
  File ""c:\users\gumin\roboschool\roboschool\__init__.py"", line 112, in &lt;module&gt;
    from roboschool.gym_pendulums import RoboschoolInvertedPendulum
  File ""c:\users\gumin\roboschool\roboschool\gym_pendulums.py"", line 1, in &lt;module&gt;
    from roboschool.scene_abstract import SingleRobotEmptyScene
  File ""c:\users\gumin\roboschool\roboschool\scene_abstract.py"", line 12, in &lt;module&gt;
    from roboschool  import cpp_household   as cpp_household
ImportError: cannot import name 'cpp_household' from 'roboschool' (c:\users\gumin\roboschool\roboschool\__init__.py)

Process finished with exit code 1
</code></pre>
",12858238,,,,,2020-06-13 09:03:40,ImportError: cannot import name 'cpp_household' from 'roboschool',<python-3.x><reinforcement-learning><openai-gym>,1,0,,2020-06-13 17:28:25,,CC BY-SA 4.0,
681,63169669,1,63213777,,2020-07-30 08:56:36,,0,294,"<p>I am trying to make a very simple DQN algorithm work with the FrozenLake-v0 game but I am getting errors. I understand that it could be an overkill using DQN instead of a Q-table, but I nonetheless would like it to work. Here is the code:</p>
<pre><code>import gym
import numpy as np
import tensorflow as tf

env = gym.make(&quot;FrozenLake-v0&quot;)

n_actions = env.action_space.n
input_dim = env.observation_space.n
model = tf.keras.Sequential() 
model.add(tf.keras.layers.Dense(64, input_dim = input_dim , activation = 'relu'))
model.add(tf.keras.layers.Dense(32, activation = 'relu'))
model.add(tf.keras.layers.Dense(n_actions, activation = 'linear'))
model.compile(optimizer=tf.keras.optimizers.Adam(), loss = 'mse')

def replay(replay_memory, minibatch_size=32):
    minibatch = np.random.choice(replay_memory, minibatch_size, replace=True)
    s_l =      np.array(list(map(lambda x: x['s'], minibatch)))
    a_l =      np.array(list(map(lambda x: x['a'], minibatch)))
    r_l =      np.array(list(map(lambda x: x['r'], minibatch)))
    sprime_l = np.array(list(map(lambda x: x['sprime'], minibatch)))
    done_l   = np.array(list(map(lambda x: x['done'], minibatch)))
    qvals_sprime_l = model.predict(sprime_l)
    target_f = model.predict(s_l) 
    for i,(s,a,r,qvals_sprime, done) in enumerate(zip(s_l,a_l,r_l,qvals_sprime_l, done_l)): 
        if not done:  target = r + gamma * np.max(qvals_sprime)
        else:         target = r
        target_f[i][a] = target
    model.fit(s_l,target_f, epochs=1, verbose=0)
    return model

n_episodes = 500
gamma = 0.99
epsilon = 0.9
minibatch_size = 32
r_sums = []  
replay_memory = []
mem_max_size = 100000

for n in range(n_episodes): 
    s = env.reset()
    done=False
    r_sum = 0
    print(s)
    while not done: 
        qvals_s = model.predict(s.reshape(16))
        if np.random.random() &lt; epsilon:  a = env.action_space.sample()
        else:                             a = np.argmax(qvals_s); 
        sprime, r, done, info = env.step(a)
        r_sum += r 
        if len(replay_memory) &gt; mem_max_size:
            replay_memory.pop(0)
        replay_memory.append({&quot;s&quot;:s,&quot;a&quot;:a,&quot;r&quot;:r,&quot;sprime&quot;:sprime,&quot;done&quot;:done})
        s=sprime
        model=replay(replay_memory, minibatch_size = minibatch_size)
    if epsilon &gt; 0.1:      epsilon -= 0.001
    r_sums.append(r_sum)
    if n % 100 == 0: print(n)
</code></pre>
<p>And the errors I am getting are:</p>
<pre><code>Traceback (most recent call last):
  File &quot;froz_versuch.py&quot;, line 48, in &lt;module&gt;
    qvals_s = model.predict(s.reshape(16))
ValueError: cannot reshape array of size 1 into shape (16,)
</code></pre>
<p>And when I try to then change <code>qvals_s = model.predict(s.reshape(16))</code> to <code>qvals_s = model.predict(s.reshape(1))</code> I get the error:</p>
<pre><code>ValueError: Input 0 of layer sequential is incompatible with the layer: expected axis -1 of input shape to have value 16 but received input with shape [None, 1]
</code></pre>
<p>I'd appreciate any help!</p>
",8313547,,8313547,,2020-08-02 08:14:10,2020-08-02 08:14:10,Errors when trying to use DQN algorithm for FrozenLake Openai game,<python><machine-learning><openai-gym><ray><rllib>,1,0,,,,CC BY-SA 4.0,
684,62341103,1,66892463,,2020-06-12 09:09:12,,1,674,"<p>In the OpenAI ProcGen gym, I am not getting a way <strong>to get the meanings of the action values</strong>, I can see that there are 15 actions for the coinrun environment using <code>env.action_space.n</code>. I have tried both the Gym and the Gym3 versions.</p>

<p>This is how I make the environment (gym version).</p>

<pre class=""lang-py prettyprint-override""><code>env = gym.make('procgen:procgen-%s-v0'%('coinrun'))
</code></pre>

<p>Neither of these are working to seem to work. </p>

<pre class=""lang-py prettyprint-override""><code>env.action_spec()
env.env.get_action_meanings()
</code></pre>

<p>I have tried to change <code>env</code> with <code>env.env</code> and <code>env.env.env</code>, nothing works. I get the message: <code>AttributeError: 'ToGymEnv' object has no attribute 'get_action_meanings'</code>.</p>

<p>Please tell me how I can get the labelled action list.</p>

<p>Object Types: <code>env</code> is a <code>ToGymEnv</code> object, <code>env.env</code> is , and <code>env.env.env</code> is <code>ProcgenGym3Env</code>.</p>
",4182274,,2181238,,2020-06-12 10:42:00,2021-03-31 17:25:05,OpenAI Gym ProcGen - Getting Action Meanings,<python><reinforcement-learning><openai-gym>,1,0,0,,,CC BY-SA 4.0,
686,63959994,1,63962844,,2020-09-18 17:05:18,,1,392,"<p>Been messing around with gym-retro and OpenCV. I have been looking at other code examples and tutorials. Several of them seem to be coded the same way but when I do it I get the following error. Has there been some type of update or something? Any suggestions on a fix welcome. I can comment out the reshaping and conversion to greyscale and it works. However then I am feeding too much info to my NN.</p>
<pre><code>import retro
import numpy as np
import cv2
import neat
import pickle

env = retro.make('SuperMarioBros3-Nes', '1Player.World1.Level1')

def eval_genomes(genomes, config):
    for genome_id,genome in genomes:
        ob = env.reset()
        ac = env.action_space.sample()
        inx, iny, inc = env.observation_space.shape
        inx = int(inx/8)
        iny = int(iny/8)
        net = neat.nn.recurrent.RecurrentNetwork.create(genome, config)
        current_max_fitness = 0
        fitness_current = 0
        frame = 0
        counter = 0
        xpos = 0
        xpos_max = 0
        done = False
        

        while not done:
            env.render()
            frame += 1
            #print(ob)
            
            ob = cv2.resize(ob, (inx,iny))
            ob = cv2.cvtColor(ob, cv2.COLOR_BGR2GRAY)
            ob = np.reshape(ob, (inx,iny))
            imgarray = np.ndarray.flatten(ob)
        
        nnOutput = net.activate(imgarray)
        
        ob, rew, done, info = env.step(nnOutput)
        #imgarray.clear()

config = neat.Config(neat.DefaultGenome, neat.DefaultReproduction,neat.DefaultSpeciesSet,neat.DefaultStagnation,'config-feedforward')

p = neat.Population(config)

winner = p.run(eval_genomes)
</code></pre>
<pre><code>(gameai) C:\Users\dgilk\anaconda3\envs\gameai&gt;python mario.py
Traceback (most recent call last):
  File &quot;mario.py&quot;, line 45, in &lt;module&gt;
    winner = p.run(eval_genomes)
  File &quot;C:\Users\dgilk\anaconda3\envs\gameai\lib\site-packages\neat\population.py&quot;, line 89, in run
    fitness_function(list(iteritems(self.population)), self.config)
  File &quot;mario.py&quot;, line 32, in eval_genomes
    ob = cv2.cvtColor(ob, cv2.COLOR_BGR2GRAY)
cv2.error: OpenCV(4.4.0) c:\users\appveyor\appdata\local\temp\1\pip-req-build-k8sx3e60\opencv\modules\imgproc\src\color.simd_helpers.hpp:92: error: (-2:Unspecified error) in function '__cdecl cv::impl::`anonymous-namespace'::CvtHelper&lt;struct cv::impl::`anonymous namespace'::Set&lt;3,4,-1&gt;,struct cv::impl::A0xbf2c9cd3::Set&lt;1,-1,-1&gt;,struct cv::impl::A0xbf2c9cd3::Set&lt;0,2,5&gt;,2&gt;::CvtHelper(const class cv::_InputArray &amp;,const class cv::_OutputArray &amp;,int)'
&gt; Invalid number of channels in input image:
&gt;     'VScn::contains(scn)'
&gt; where
&gt;     'scn' is 1
</code></pre>
<p>UPDATE:
Here is an output of the shrunken image. There appears to be color.
<a href=""https://i.stack.imgur.com/59R8Z.png"" rel=""nofollow noreferrer"">snippet of observation window</a></p>
",10181557,,10181557,,2020-09-18 23:15:20,2020-09-18 23:15:20,OpenCV + gym-retro: Invalid number of channels in input image,<python><python-3.x><opencv><machine-learning><openai-gym>,1,1,,,,CC BY-SA 4.0,
689,63963532,1,63963653,,2020-09-18 22:27:27,,4,1200,"<p><strong>Info:</strong> I am using OpenAI Gym to create RL environments but need multiple copies of an environment for something I am doing. I do not want to do anything like <code>[gym.make(...) for i in range(2)]</code> to make a new environment.</p>
<p><strong>Question:</strong> Given one gym env what is the best way to make a copy of it so that you have 2 duplicate but disconnected envs?</p>
<p>Here is an example:</p>
<pre><code>import gym

env = gym.make(&quot;CartPole-v0&quot;)
new_env = # NEED COPY OF ENV HERE

env.reset() # Should not alter new_env
</code></pre>
",,user12128336,,user12128336,2020-09-18 22:37:42,2022-05-19 03:46:29,How to copy gym environment?,<python><copy><openai-gym>,2,0,,,,CC BY-SA 4.0,
693,62303994,1,62306159,,2020-06-10 12:48:32,,4,6551,"<p>When using OpenAI gym, after importing the library with <code>import gym</code>, the action space can be checked with <code>env.action_space</code>. But this gives only the size of the action space. I would like to know what kind of actions each element of the action space corresponds to. Is there a simple way to do it? </p>
",12394113,,,,,2022-05-05 09:06:08,How to check the actions available in OpenAI gym environment?,<reinforcement-learning><openai-gym>,1,0,0,,,CC BY-SA 4.0,
700,58941164,1,58976547,,2019-11-19 19:03:50,,2,3012,"<p>I've been trying to use a custom openai gym environment for fixed wing uav from <a href=""https://github.com/eivindeb/fixed-wing-gym"" rel=""nofollow noreferrer"">https://github.com/eivindeb/fixed-wing-gym</a> by testing it with the openai stable-baselines algorithms but I have been running into issues for several days now. My baseline is the CartPole example <strong>Multiprocessing: Unleashing the Power of Vectorized Environments</strong> from <a href=""https://stable-baselines.readthedocs.io/en/master/guide/examples.html#multiprocessing-unleashing-the-power-of-vectorized-environments"" rel=""nofollow noreferrer"">https://stable-baselines.readthedocs.io/en/master/guide/examples.html#multiprocessing-unleashing-the-power-of-vectorized-environments</a> since I would need to supply arguments and I am trying to use multiprocessing which I believe this example is all I need.</p>

<p>I have modified the baseline example as follows:</p>

<pre><code>import gym
import numpy as np

from stable_baselines.common.policies import MlpPolicy
from stable_baselines.common.vec_env import SubprocVecEnv
from stable_baselines.common import set_global_seeds
from stable_baselines import ACKTR, PPO2
from gym_fixed_wing.fixed_wing import FixedWingAircraft


def make_env(env_id, rank, seed=0):
    """"""
    Utility function for multiprocessed env.

    :param env_id: (str) the environment ID
    :param num_env: (int) the number of environments you wish to have in subprocesses
    :param seed: (int) the inital seed for RNG
    :param rank: (int) index of the subprocess
    """"""

    def _init():
        env = FixedWingAircraft(""fixed_wing_config.json"")
        #env = gym.make(env_id)
        env.seed(seed + rank)
        return env

    set_global_seeds(seed)
    return _init

if __name__ == '__main__':
    env_id = ""fixed_wing""
    #env_id = ""CartPole-v1""
    num_cpu = 4  # Number of processes to use
    # Create the vectorized environment
    env = SubprocVecEnv([lambda: FixedWingAircraft for i in range(num_cpu)])
    #env = SubprocVecEnv([make_env(env_id, i) for i in range(num_cpu)])

    model = PPO2(MlpPolicy, env, verbose=1)
    model.learn(total_timesteps=25000)

    obs = env.reset()
    for _ in range(1000):
        action, _states = model.predict(obs)
        obs, rewards, dones, info = env.step(action)
        env.render()
</code></pre>

<p>and the error I keep getting is the following:</p>

<pre><code>Traceback (most recent call last):
  File ""/home/bonie/PycharmProjects/deepRL_fixedwing/fixed-wing-gym/gym_fixed_wing/ACKTR_fixedwing.py"", line 38, in &lt;module&gt;
    model = PPO2(MlpPolicy, env, verbose=1)
  File ""/home/bonie/PycharmProjects/deepRL_fixedwing/stable-baselines/stable_baselines/ppo2/ppo2.py"", line 104, in __init__
    self.setup_model()
  File ""/home/bonie/PycharmProjects/deepRL_fixedwing/stable-baselines/stable_baselines/ppo2/ppo2.py"", line 134, in setup_model
    n_batch_step, reuse=False, **self.policy_kwargs)
  File ""/home/bonie/PycharmProjects/deepRL_fixedwing/stable-baselines/stable_baselines/common/policies.py"", line 660, in __init__
    feature_extraction=""mlp"", **_kwargs)
  File ""/home/bonie/PycharmProjects/deepRL_fixedwing/stable-baselines/stable_baselines/common/policies.py"", line 540, in __init__
    scale=(feature_extraction == ""cnn""))
  File ""/home/bonie/PycharmProjects/deepRL_fixedwing/stable-baselines/stable_baselines/common/policies.py"", line 221, in __init__
    scale=scale)
  File ""/home/bonie/PycharmProjects/deepRL_fixedwing/stable-baselines/stable_baselines/common/policies.py"", line 117, in __init__
    self._obs_ph, self._processed_obs = observation_input(ob_space, n_batch, scale=scale)
  File ""/home/bonie/PycharmProjects/deepRL_fixedwing/stable-baselines/stable_baselines/common/input.py"", line 51, in observation_input
    type(ob_space).__name__))
NotImplementedError: Error: the model does not support input space of type NoneType
</code></pre>

<p>I am not sure what to really input as the <code>env_id</code> and for the <code>def make_env(env_id, rank, seed=0)</code> function. I am also thinking that the <code>VecEnv</code> function for parallel processes is not properly setup. </p>

<p>I am coding with Python v3.6 using PyCharm IDE in Ubuntu 18.04.</p>

<p>Any suggestions would really help at this point!</p>

<p>Thank you. </p>
",10565612,,10565612,,2019-11-19 21:05:01,2019-11-21 13:50:26,How to use a custom Openai gym environment with Openai stable-baselines RL algorithms?,<python><reinforcement-learning><agent><openai-gym><virtual-environment>,1,1,0,,,CC BY-SA 4.0,
702,58974034,1,58976751,,2019-11-21 11:34:43,,3,2299,"<p>Me and my classmate have decided to try and implement and AI agent into our own game. My friend have done most of the code, based on previous projects, and I was wondering how PyGame and OpenAI would work together. Have tried to do some research but can't really find any useful information about this specific topic. Some have said that it is hard to implement but some say it works. Either way, I'd like your opinion on this project and how you'd approach this if it was you.</p>

<p>The game is very basic (only has one input) and a linear difficulty. Basically you just try to dodge the green squares that spawn with the character (you) being centered to the screen. The space between the green squares are the same which means the spawn rate is exactly the same. Here's the code but it's not very well cleaned nor polished but our first example: </p>

<pre><code>    import pygame
    import time

    start_time = time.time()
    pygame.init()

    win_dim = 800
    win = pygame.display.set_mode((win_dim, win_dim))


    class Char:

       def __init__(self, x, y, dim, score):

           self.x = x
           self.y = y

           self.dim = dim
           self.score = score

           self.vsp = 0

       def draw_char(self, color):

           pygame.draw.rect(win, (color[0], color[1], color[2]), (self.x, self.y, self.dim, self.dim))

       def jump(self):

           keys = pygame.key.get_pressed()

           if keys[pygame.K_SPACE] and self.y == 400:
               self.vsp = 5

           self.y -= self.vsp

           if self.y &lt; 400:
               self.vsp -= 0.15
           else:
               self.y = 400


    class Cactus:

        def __init__(self, dim):

           self.dim = dim

           self.cac = []
           self.speed = 0

        def startcac(self, x):

           if x % 100 == 0:

               stop_time = time.time()
               self.cac.append(801)


           for i in range(0, len(self.cac)):

               if self.speed &lt; 4:
                   self.speed = 1 + x * 0.0005
               else:
                   self.speed == 10

               self.cac[i] -= self.speed
               pygame.draw.rect(win, (0, 200, 0), (self.cac[i], 400, 20, 20))

           try:
               if self.cac[0] &lt; 0:
                   self.cac.pop(0)
           except IndexError:
               pass


        def collision(self, blob_cords):

           if any(i in [int(i) for i in self.cac] for i in [i for i in range(380, 421)]) and blob_cords[1] &gt;= 380:
               self.cac = []

           dist = 0

           if len(self.cac) &gt;= 2:
               # print(self.cac[-1] - self.cac[-2], blob.x - 1)
               for i in self.cac:
                   if i &gt; 400 + 20:
                       dist = (i - 400 - 20) / (self.cac[-1] - self.cac[-2])

                       break

           if dist &lt;= 1:
               print(dist)
               print(self.speed / 4)




    blob = Char(400, 400, 20, 0)
    # ""player""
    cac = Cactus(20)
    # obstacle

    x = 1


    if __name__ == '__main__':

       game = True
       while game:
           pygame.time.delay(1)

           for event in pygame.event.get():
               if event.type == pygame.QUIT:
                   game = False

           # Draws things on screen

           win.fill((0, 0, 0))
           blob.jump()
           blob.draw_char([150, 0, 0])
           cac.startcac(x)
           x += 1
           cac.collision((blob.x, blob.y))

           # Updates movements and events

           # Update display

           pygame.display.update()

       pygame.QUIT()
</code></pre>

<p>Really sorry for the sloppy code, if even needed, but some guidance to just start or revamp the project would be much appreciated.</p>

<p>Thanks! </p>
",10687556,,,,,2019-11-21 14:02:26,Pygame and Open AI implementation,<python><python-3.x><reinforcement-learning><openai-gym>,1,0,,,,CC BY-SA 4.0,
709,41092355,1,41127535,,2016-12-11 23:36:30,,1,11894,"<p>I'm playing with OpenAI's universe, and when I click run, it comes up with an error.</p>

<p>Current script: </p>

<pre><code>import gym
import universe  # register the universe environments

env = gym.make('flashgames.CoasterRacer-v0')
observation_n = env.reset()
</code></pre>

<p>If I remove the observation_n variable, it won't come up with an error. With the observation_n it will cause an error. Right now I'm running <code>docker-py (1.10.3)</code>.</p>

<pre><code>[2016-12-11 18:32:12,128] Making new env: flashgames.CoasterRacer-v0
[2016-12-11 18:32:12,132] Writing logs to file: /tmp/universe-10547.log
Traceback (most recent call last):
  File ""/home/ubuntu/workspace/gim.py"", line 5, in &lt;module&gt;
    observation_n = env.reset()
  File ""/home/ubuntu/workspace/gym/gym/core.py"", line 137, in reset
    self.configure()
  File ""/home/ubuntu/workspace/gym/gym/core.py"", line 246, in configure
    self._configure(*args, **kwargs)
  File ""/home/ubuntu/workspace/universe/universe/vectorized/core.py"", line 57, in _configure
    super(Wrapper, self)._configure(**kwargs)
  File ""/home/ubuntu/workspace/gym/gym/core.py"", line 350, in _configure
    return self.env.configure(*args, **kwargs)
  File ""/home/ubuntu/workspace/gym/gym/core.py"", line 246, in configure
    self._configure(*args, **kwargs)
  File ""/home/ubuntu/workspace/universe/universe/wrappers/render.py"", line 18, in _configure
    super(Render, self)._configure(**kwargs)
  File ""/home/ubuntu/workspace/universe/universe/vectorized/core.py"", line 57, in _configure
    super(Wrapper, self)._configure(**kwargs)
  File ""/home/ubuntu/workspace/gym/gym/core.py"", line 350, in _configure
    return self.env.configure(*args, **kwargs)
  File ""/home/ubuntu/workspace/gym/gym/core.py"", line 246, in configure
    self._configure(*args, **kwargs)
  File ""/home/ubuntu/workspace/universe/universe/wrappers/throttle.py"", line 27, in _configure
    super(Throttle, self)._configure(**kwargs)
  File ""/home/ubuntu/workspace/universe/universe/vectorized/core.py"", line 57, in _configure
    super(Wrapper, self)._configure(**kwargs)
  File ""/home/ubuntu/workspace/gym/gym/core.py"", line 350, in _configure
  File ""/home/ubuntu/workspace/gym/gym/core.py"", line 246, in configure
    self._configure(*args, **kwargs)
  File ""/home/ubuntu/workspace/universe/universe/envs/vnc_env.py"", line 192, in _configure
    api_key=api_key,
  File ""/home/ubuntu/workspace/universe/universe/remotes/build.py"", line 19, in build
    n=n,
  File ""/home/ubuntu/workspace/universe/universe/remotes/docker_remote.py"", line 43, in __init__
    self._assigner = PortAssigner(reuse=reuse)
  File ""/home/ubuntu/workspace/universe/universe/remotes/docker_remote.py"", line 149, in __init__
    self.client, self.info = get_client()
  File ""/home/ubuntu/workspace/universe/universe/remotes/docker_remote.py"", line 143, in get_client
    return docker.Client(base_url=host, version=client_api_version), info
  File ""/usr/local/lib/python2.7/dist-packages/docker/client.py"", line 99, in __init__
    self._version = self._retrieve_server_version()
  File ""/usr/local/lib/python2.7/dist-packages/docker/client.py"", line 124, in _retrieve_server_version
    'Error while fetching server API version: {0}'.format(e)
docker.errors.DockerException: Error while fetching server API version: ('Connection aborted.', error(2, 'No such file or directory'))
</code></pre>
",6688561,,,,,2016-12-13 17:51:53,Docker Error while fetching server API version,<python-2.7><docker><openai-gym>,1,0,0,,,CC BY-SA 3.0,
720,65692797,1,65735624,,2021-01-12 22:11:46,,0,64,"<p>I am in the process of experimenting with deep reinforcement learning and have created the following in environment in which I am running a simulation of purchasing a raw material. Start qty is the amount of the material I have when looking to purchase for the next 12 weeks(sim_weeks). I have to purchase in multiples of 195000 pounds and am expected to use 45000 pounds of material a week.</p>
<pre><code>start_qty= 100000
sim_weeks = 12
purchase_mult = 195000
#days on hand cost =
forecast_qty = 45000


class ResinEnv(Env):
    def __init__(self):
        # Actions we can take: buy 0, buy 1x,
        self.action_space = Discrete(2)
        # purchase array space...
        self.observation_space = Box(low=np.array([-1000000]), high=np.array([1000000]))
        # Set start qty
        self.state = start_qty
        # Set purchase length
        self.purchase_length = sim_weeks
        #self.current_step = 1
        
    def step(self, action):
        # Apply action
        #this gives us qty_available at the end of the week
        self.state-=forecast_qty
        
        #see if we need to buy
        self.state += (action*purchase_mult)
       
        
        #now calculate the days on hand from this:
        days = self.state/forecast_qty/7
        
        
        # Reduce weeks left to purchase by 1 week
        self.purchase_length -= 1 
        #self.current_step+=1
        
        # Calculate reward: reward is the negative of days_on_hand
        if self.state&lt;0:
            reward = -10000
        else:
            reward = -days
        
        # Check if shower is done
        if self.purchase_length &lt;= 0: 
            done = True
        else:
            done = False
        
        # Set placeholder for info
        info = {}
        
        # Return step information
        return self.state, reward, done, info

    def render(self):
        # Implement viz
        pass
    
    def reset(self):
        # Reset qty
        self.state = start_qty
        self.purchase_length = sim_weeks
        
        return self.state
</code></pre>
<p>I am debating on if the reward function is sufficient. What I am attempting to do is minimize the sum of the days on hand from each step where the days on hand for a given step is defined by days in the code. I decided that since the goal is to maximize the reward function, then I could convert the days on hand value to a negative number and then use that new negative number as the reward (thus maximizing the reward would minimize the days on hand). Then I added the strong penalty for letting qty available at any given week be negative.</p>
<p>Is there a better way to do this? I am new to this subject and also new to Python in general. Any advice is greatly appreciated!
I</p>
",13188513,,,,,2021-01-15 11:52:55,Structuring reward function for Open AI RL environment for raw material purchasing,<python><openai-gym>,1,0,,,,CC BY-SA 4.0,
722,59517122,1,59519421,,2019-12-29 05:42:35,,2,937,"<p>Is the initial state randomly selected in reinforcement learning environments like OpenAI gym. In other words, does command env.reset() result in randomly selected initial state or specific initial state?</p>
",10437878,,,,,2019-12-29 12:29:01,In OpenAI gym environments the initial state is random or specific?,<reinforcement-learning><openai-gym>,1,0,,,,CC BY-SA 4.0,
723,50541672,1,50543086,,2018-05-26 10:19:03,,0,239,"<p>whenever I try to run my  program on WSL, I get the following error. I'm pretty new to pytorch and vizdoom, so I don't know how to solve this problem.</p>

<p><strong>Setup</strong><br>
- Windows 10 x64<br>
- Ubuntu 14 (on WSL)<br>
- Python 2.7.14  (Anaconda 2)<br>
- OpenAI Gym 0.9.5<br>
- Vizdoom 1.1.4<br>
- doom-py 0.0.14<br>
- ppaquette/gym-doom<br>
- pytorch 0.0.12  </p>

<pre><code>(doomenv) hybridsyntax@Blacklynx:/mnt/f/_TUTORIALS/ai/doom/code$ python ai.py  &gt; wsl.log
[2018-05-25 18:21:44,354] Making new env: ppaquette/DoomCorridor-v0
[2018-05-25 18:21:44,365] Clearing 2 monitor files from previous run (because force=True was provided)
Assertion 'pthread_mutex_unlock(&amp;m-&gt;mutex) == 0' failed at pulsecore/mutex-posix.c:108, function pa_mutex_unlock(). Aborting.
Traceback (most recent call last):
  File ""ai.py"", line 155, in &lt;module&gt;
    memory.run_steps(200)
  File ""/mnt/f/_TUTORIALS/ai/doom/code/experience_replay.py"", line 70, in run_steps
    entry = next(self.n_steps_iter) # 10 consecutive steps
  File ""/mnt/f/_TUTORIALS/ai/doom/code/experience_replay.py"", line 21, in __iter__
    state = self.env.reset()
  File ""/mnt/f/_TUTORIALS/ai/doom/gym/gym/core.py"", line 104, in reset
    return self._reset()
  File ""/mnt/f/_TUTORIALS/ai/doom/gym/gym/wrappers/monitoring.py"", line 39, in _reset
    observation = self.env.reset(**kwargs)
  File ""/mnt/f/_TUTORIALS/ai/doom/gym/gym/core.py"", line 104, in reset
    return self._reset()
  File ""/mnt/f/_TUTORIALS/ai/doom/gym/gym/core.py"", line 311, in _reset
    observation = self.env.reset(**kwargs)
  File ""/mnt/f/_TUTORIALS/ai/doom/gym/gym/core.py"", line 104, in reset
    return self._reset()
  File ""/mnt/f/_TUTORIALS/ai/doom/gym/gym/wrappers/frame_skipping.py"", line 33, in _reset
    return self.env.reset()
  File ""/mnt/f/_TUTORIALS/ai/doom/gym/gym/core.py"", line 104, in reset
    return self._reset()
  File ""/mnt/f/_TUTORIALS/ai/doom/gym/gym/core.py"", line 283, in _reset
    return self.env.reset(**kwargs)
  File ""/mnt/f/_TUTORIALS/ai/doom/gym/gym/core.py"", line 104, in reset
    return self._reset()
  File ""/mnt/f/_TUTORIALS/ai/doom/gym/gym/wrappers/time_limit.py"", line 49, in _reset
    return self.env.reset()
  File ""/mnt/f/_TUTORIALS/ai/doom/gym/gym/core.py"", line 104, in reset
    return self._reset()
  File ""/mnt/f/_TUTORIALS/ai/doom/gym-doom/ppaquette_gym_doom/doom_env.py"", line 244, in _reset
    return self._load_level()
  File ""/mnt/f/_TUTORIALS/ai/doom/gym-doom/ppaquette_gym_doom/doom_env.py"", line 157, in _load_level
    'singleton lock in memory.')
gym.error.Error: [ViZDoomUnexpectedExitException, ViZDoomErrorException] VizDoom exited unexpectedly. This is likely caused by a missing multiprocessing lock. To run VizDoom across multiple processes, you need to pass a lock when you configure the env [e.g. env.configure(lock=my_multiprocessing_lock)], or create and close an env before starting your processes [e.g. env = gym.make(""DoomBasic-v0""); env.close()] to cache a singleton lock in memory.
[2018-05-25 18:21:44,696] Finished writing results. You can upload them to the scoreboard via gym.upload('/mnt/f/_TUTORIALS/ai/doom/code/videos')
(doomenv) hybridsyntax@Blacklynx:/mnt/f/_TUTORIALS/ai/doom/code$
</code></pre>

<p>Thanks in advance</p>
",245314,,1033581,,2018-07-02 16:30:36,2018-07-02 16:30:36,Getting multiprocessing lock error when running vizdoom and pytorch program on Windows Subsystem for Linux,<python-2.7><multiprocessing><pytorch><windows-subsystem-for-linux><openai-gym>,1,0,,,,CC BY-SA 4.0,
725,59530490,1,59530657,,2019-12-30 12:00:43,,5,915,"<p>Anybody knows any OpenAI Gym environments where we can set the initial state of the game? For example, I found the MountainCarContinuous-v0 can do such thing so that we can select at which point the car starts. However, I am looking for another more complex environment. Thanks in advance for your help!</p>
",12625093,,,,,2019-12-30 12:13:14,Start OpenAI gym on arbitrary initial state,<reinforcement-learning><openai-gym>,1,1,0,,,CC BY-SA 4.0,
735,49346051,1,49346799,,2018-03-18 08:50:05,,3,2458,"<p><strong>[Introduction]</strong>
I have a custom made Python game which uses 'w' 's' keys for moving and 'space' key for shooting as inputs. I've found a reinforcement learning algorithm which I would like to try implement into the game.  </p>

<p>However, the RL algorithm uses openAI's atari games as the environment with command 'gym.make (env_name)'. I'm on Windows OS so can't experiment on the code as gym[atari] does not work for me. </p>

<pre><code>class Agent:
    def __init__(self, env_name, training, render=False, use_logging=True):

        self.env = gym.make(env_name)
</code></pre>

<p><strong>[Question]</strong>
Is there another command I could use instead of 'gym.make()' in this class to implement the RL algorithm to train on my custom made game, or is the only option to make my own gym environment? 
Would 'pygame.surfarray.array2d()' return something similar to 'gym.make()'?</p>

<p>Please tell me if additional information is required, I'm new to gym and tensorflow so my understanding may be flawed.</p>

<p><strong>[Edit]</strong>
I made my game using functions, if I was to convert the game into a gym environment, would the only option be to convert the functions to classes? As an example of how my code looks like, here is the game loop: (I can't post the whole code as it's a controlled assessment for end of year grade so would like to avoid any plagiarisation problems)</p>

<pre><code>def game_loop():
global pause
x = (display_width * 0.08)
y = (display_height * 0.2)

x_change = 0
y_change = 0

blob_speed = 2

velocity = [2, 2]

score = 0
lives = 3

pos_x = display_width/1.2
pos_y = display_height/1.2

previous_time = pygame.time.get_ticks()
previous_time2 = pygame.time.get_ticks()

gameExit = False
while not gameExit:
    for event in pygame.event.get():#monitors hardware movement/ clicks
        if event.type == pygame.QUIT:
            pygame.quit()
            quit()

    pos_x += velocity[0]
    pos_y += velocity[1]

    if pos_x + blob_width &gt; display_width or pos_x &lt; 601:
        velocity[0] = -velocity[0]

    if pos_y + blob_height &gt; display_height or pos_y &lt; 0:
        velocity[1] = -velocity[1]

    for b in range(len(bullets2)):
        bullets2[b][0] -= 6

    for bullet in bullets2:
        if bullet[0] &lt; 0:
            bullets2.remove(bullet)


    current_time2 = pygame.time.get_ticks()
    #ready to fire when 500 ms have passed.
    if current_time2 - previous_time2 &gt; 500:
        previous_time2 = current_time2
        bullets2.append([pos_x+25, pos_y+24])

    keys = pygame.key.get_pressed()

    for b in range(len(bullets)):
        bullets[b][0] += 6

    for bullet in bullets:
        if bullet[0] &gt; 1005:
            bullets.remove(bullet)

    if keys[pygame.K_SPACE]:
        current_time = pygame.time.get_ticks()
        #ready to fire when 500 ms have passed.
        if current_time - previous_time &gt; 600:
            previous_time = current_time
            bullets.append([x+25, y+24])


    if x &lt; 0:
        x = 0
    if keys[pygame.K_a]:
        x_change = -blob_speed
    if x &gt; 401 - blob_width:
        x = 401 - blob_width
    if keys[pygame.K_d]:
        x_change = blob_speed
    if keys[pygame.K_p]:
        pause = True
        paused()


    if keys[pygame.K_a] and keys[pygame.K_d]:
        x_change = 0
    if not keys[pygame.K_a] and not keys[pygame.K_d]:
        x_change = 0

    if y &lt; 0:
        y = 0
    if keys[pygame.K_w]:
        y_change = -blob_speed
    if y &gt; display_height - blob_height:
        y = display_height - blob_height
    if keys[pygame.K_s]:
        y_change = blob_speed


    if keys[pygame.K_w] and keys[pygame.K_s]:
        y_change = 0
    if not keys[pygame.K_w] and not keys[pygame.K_s]:
        y_change = 0


    #print(event)
    # Reset x and y to new position
    x += x_change
    y += y_change

    gameDisplay.fill(blue)  #changes background surface
    bullets_hit(score)
    player_lives(lives)
    pygame.draw.line(gameDisplay, black, (601, display_height), (601, 0), 3)
    pygame.draw.line(gameDisplay, black, (401, display_height), (401, 0), 3)
    blob(pos_x, pos_y)
    blob(x, y)

    for bullet in bullets:
        gameDisplay.blit(bulletpicture, pygame.Rect(bullet[0], bullet[1], 0, 0))
        if bullet[0] &gt; pos_x and bullet[0] &lt; pos_x + blob_width:
            if bullet[1] &gt; pos_y and bullet[1] &lt; pos_y + blob_height or bullet[1] + bullet_height &gt; pos_y and bullet[1] + bullet_height &lt; pos_y + blob_height:
                bullets.remove(bullet)
                score+=1

    for bullet in bullets2:
        gameDisplay.blit(bulletpicture, pygame.Rect(bullet[0], bullet[1], 0, 0))
        if bullet[0] + bullet_width &lt; x + blob_width and bullet[0] &gt; x:
            if bullet[1] &gt; y and bullet[1] &lt; y + blob_height or bullet[1] + bullet_height &gt; y and bullet[1] + bullet_height &lt; y + blob_height:
                bullets2.remove(bullet)
                lives-=1

    if lives == 0:
        game_over()


    pygame.display.update() #update screen
    clock.tick(120)#moves frame on (fps in parameters)
</code></pre>
",9244222,,9244222,,2018-03-18 11:11:34,2018-03-18 11:19:46,OpenAI/Tensorflow Custom Game Environment Instead of using 'gym.make()',<python><tensorflow><reinforcement-learning><openai-gym>,1,0,0,,,CC BY-SA 3.0,
741,50615201,1,50615402,,2018-05-31 00:53:21,,2,1339,"<p>I'm trying to install a module I'm developing. When I use 'pip install -e .', it outputs 'Successfully installed gym-mabifish' but when I try to import the module using 'import gym_mabifish' I'm getting 'ModuleNotFoundError: No module named ""gym_mabifish""'.</p>

<p>Here's the structure of the package:</p>

<pre><code>gym-mabifish/
     setup.py ( https://pastebin.com/1wNykyKw )
     gym_mabifish/
          __init__.py ( https://pastebin.com/GtQid3Nk )
          envs/
               __init__.py ( https://pastebin.com/Txfk0ezE )
               mabifish_env.py ( https://pastebin.com/g50zBbus )
</code></pre>

<p>I'm using the random_agent from OpenAI gym to test it. ( <a href=""https://pastebin.com/72LETtxd"" rel=""nofollow noreferrer"">https://pastebin.com/72LETtxd</a> )</p>

<p>The package is shown in pip list: </p>

<pre><code>gym-mabifish (0.0.1, x:\path\to\project\gym-mabifish)
</code></pre>
",6121572,,6121572,,2018-05-31 01:02:16,2018-05-31 01:34:01,"pip install -e successful, import fails: ModuleNotFoundError",<python><pip><openai-gym>,1,2,,,,CC BY-SA 4.0,
744,59813509,1,59827738,,2020-01-19 19:15:09,,2,1593,"<p>Is there any documentation where I could details regarding this kind of stuff?
For example: </p>

<pre><code>import gym

# environment for agent
env = gym.make('Pendulum-v0')
env.reset()

print(env.observation_space.high, env.observation_space.low)

# pendulum observation space ranges from [-1, -1, -8] to [1, 1, 8]
</code></pre>

<p>I cant figure out what each number in observation space means. I guess two of them are x and y coordinates (although I dont know which is which), and what does the third number stand for? </p>
",9642804,,,,,2022-03-31 09:35:42,openai-gym how to determine what the values in observation space mean,<python><openai-gym>,2,2,,,,CC BY-SA 4.0,
749,67316491,1,67331064,,2021-04-29 11:28:14,,0,1340,"<p>I have made a model in PyTorch for use in an openAI Gym environment. I have made it in the following way:</p>
<pre><code>class Policy(nn.Module):
    def __init__(self, s_size=8, h_size=16, a_size=4):
        super(Policy, self).__init__()
        self.fc1 = nn.Linear(s_size, h_size)
        self.fc2 = nn.Linear(h_size, 32)
        self.fc3 = nn.Linear(32, 64)
        self.fc4 = nn.Linear(64, a_size)

    def forward(self, x):
        x = F.relu(self.fc1(x))
        x = F.relu(self.fc2(x))
        x = F.relu(self.fc3(x))
        x = self.fc4(x)
        return F.softmax(x, dim=1 )
    
    def act(self, state):
        state = torch.from_numpy(state).float().unsqueeze(0).to(device)
        probs = self.forward(state).cpu()
        m = Categorical(probs)
        action = m.sample()
        return action.item(), m.log_prob(action)
</code></pre>
<p>I then save it's state in a dictionary and use it as following :</p>
<pre><code>env = gym.make('LunarLander-v2')

policy = Policy().to(torch.device('cpu'))
policy.load_state_dict(torch.load('best_params_cloud.ckpt', map_location='cpu'))
policy.eval()
ims = []
rewards = []
state = env.reset()
for step in range(STEPS):
    img = env.render(mode='rgb_array')
    action,log_prob = policy(state)
        # print(action)
    state,reward,done,i_ = env.step(action)
    rewards.append(reward)
    # print(reward,done)
    cv2_im_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
    pil_im = Image.fromarray(cv2_im_rgb)

    draw = ImageDraw.Draw(pil_im)

    # Choose a font
    font = ImageFont.truetype(&quot;Roboto-Regular.ttf&quot;, 20)

    # Draw the text
    draw.text((0, 0), f&quot;Step: {step} Action : {action} Reward: {int(reward)} Total Rewards: {int(np.sum(rewards))} done: {done}&quot;, font=font,fill=&quot;#FDFEFE&quot;)

    # Save the image
    img = cv2.cvtColor(np.array(pil_im), cv2.COLOR_RGB2BGR)
    im = plt.imshow(img, animated=True)
    ims.append([im])
    if done:
        env.close()


                
        
        break

Writer = animation.writers['pillow']
writer = Writer(fps=15, metadata=dict(artist='Me'), bitrate=1800)
im_ani = animation.ArtistAnimation(fig, ims, interval=50, repeat_delay=3000,
                                    blit=True)
im_ani.save('ll_train1.gif', writer=writer)
</code></pre>
<p>But this returns the error:</p>
<pre><code>TypeError                                 Traceback (most recent call last)
&lt;ipython-input-3-da32222edde2&gt; in &lt;module&gt;
      9 for step in range(STEPS):
     10     img = env.render(mode='rgb_array')
---&gt; 11     action,log_prob = policy(state)
     12         # print(action)
     13     state,reward,done,i_ = env.step(action)

~\anaconda3\lib\site-packages\torch\nn\modules\module.py in _call_impl(self, *input, **kwargs)
    887             result = self._slow_forward(*input, **kwargs)
    888         else:
--&gt; 889             result = self.forward(*input, **kwargs)
    890         for hook in itertools.chain(
    891                 _global_forward_hooks.values(),

&lt;ipython-input-2-66d42ebb791e&gt; in forward(self, x)
     33 
     34     def forward(self, x):
---&gt; 35         x = F.relu(self.fc1(x))
     36         x = F.relu(self.fc2(x))
     37         x = F.relu(self.fc3(x))

~\anaconda3\lib\site-packages\torch\nn\modules\module.py in _call_impl(self, *input, **kwargs)
    887             result = self._slow_forward(*input, **kwargs)
    888         else:
--&gt; 889             result = self.forward(*input, **kwargs)
    890         for hook in itertools.chain(
    891                 _global_forward_hooks.values(),

~\anaconda3\lib\site-packages\torch\nn\modules\linear.py in forward(self, input)
     92 
     93     def forward(self, input: Tensor) -&gt; Tensor:
---&gt; 94         return F.linear(input, self.weight, self.bias)
     95 
     96     def extra_repr(self) -&gt; str:

~\anaconda3\lib\site-packages\torch\nn\functional.py in linear(input, weight, bias)
   1751     if has_torch_function_variadic(input, weight):
   1752         return handle_torch_function(linear, (input, weight), input, weight, bias=bias)
-&gt; 1753     return torch._C._nn.linear(input, weight, bias)
   1754 
   1755 

TypeError: linear(): argument 'input' (position 1) must be Tensor, not numpy.ndarray
</code></pre>
<p>I tried to change the forward function by adding the following line of code:</p>
<pre><code>def forward(self, x):
        x = torch.tensor(x,dtype=torch.float32,device=DEVICE).unsqueeze(0) //Added this line
        x = F.relu(self.fc1(x))
        x = F.relu(self.fc2(x))
        x = F.relu(self.fc3(x))
        x = self.fc4(x)
        return F.softmax(x, dim=1 )
</code></pre>
<p>But this also returns an error : <code>ValueError: not enough values to unpack (expected 2, got 1)</code></p>
<p>The policy took a lot of time to train, and I am trying to avoid retraining it, is there a workaround for it to run without retraining?</p>
",13100489,,13100489,,2021-04-29 15:30:44,2021-04-30 09:02:27,Is it possible to resolve TypeError: argument 'input' (position 1) must be Tensor error without retraining the model?,<python><pytorch><typeerror><valueerror><openai-gym>,1,1,,,,CC BY-SA 4.0,
751,60268769,1,60277319,,2020-02-17 18:52:40,,2,6694,"<p>I'm trying to do the follow code with OpenAI:</p>

<pre class=""lang-py prettyprint-override""><code>import gym
env = gym.make('CarRacing-v0')
env.reset() 

for _ in range(1000):
 env.render()
 env.step(env.action_space.sample())
</code></pre>

<p>but it throws the error:</p>

<blockquote>
  <p>fn = getattr(mod, attr_name)
  AttributeError: module 'gym.envs.box2d' has no attribute 'CarRacing'</p>
</blockquote>

<p>And then I try to install box2d by pip install box2d-py throwing this error:</p>

<pre><code>ERROR: Command errored out with exit status 1:
     command: 'C:\Users\Junior\Anaconda\envs\gym\python.exe' -u -c 'import sys, setuptools, tokenize; sys.argv[0] = '""'""'C:\\Users\\Junior\\AppData\\Local\\Temp\\pip-install-w8awn22p\\box2d-py\\setup.py'""'""'; __file__='""'""'C:\\Users\\Junior\\AppData\\Local\\Temp\\pip-install-w8awn22p\\box2d-py\\setup.py'""'""';f=getattr(tokenize, '""'""'open'""'""', open)(__file__);code=f.read().replace('""'""'\r\n'""'""', '""'""'\n'""'""');f.close();exec(compile(code, __file__, '""'""'exec'""'""'))' install --record 'C:\Users\Junior\AppData\Local\Temp\pip-record-netg1nlq\install-record.txt' --single-version-externally-managed --compile --install-headers 'C:\Users\Junior\Anaconda\envs\gym\Include\box2d-py'
         cwd: C:\Users\Junior\AppData\Local\Temp\pip-install-w8awn22p\box2d-py\
    Complete output (16 lines):
    Using setuptools (version 45.2.0.post20200210).
    running install
    running build
    running build_py
    creating build
    creating build\lib.win-amd64-3.7
    creating build\lib.win-amd64-3.7\Box2D
    copying library\Box2D\Box2D.py -&gt; build\lib.win-amd64-3.7\Box2D
    copying library\Box2D\__init__.py -&gt; build\lib.win-amd64-3.7\Box2D
    creating build\lib.win-amd64-3.7\Box2D\b2
    copying library\Box2D\b2\__init__.py -&gt; build\lib.win-amd64-3.7\Box2D\b2
    running build_ext
    building 'Box2D._Box2D' extension
    swigging Box2D\Box2D.i to Box2D\Box2D_wrap.cpp
    swig.exe -python -c++ -IBox2D -small -O -includeall -ignoremissing -w201 -globals b2Globals -outdir library\Box2D -keyword -w511 -D_SWIG_KWARGS -o Box2D\Box2D_wrap.cpp Box2D\Box2D.i
    error: command 'swig.exe' failed: No such file or directory
    ----------------------------------------
ERROR: Command errored out with exit status 1: 'C:\Users\Junior\Anaconda\envs\gym\python.exe' -u -c 'import sys, setuptools, tokenize; sys.argv[0] = '""'""'C:\\Users\\Junior\\AppData\\Local\\Temp\\pip-install-w8awn22p\\box2d-py\\setup.py'""'""'; __file__='""'""'C:\\Users\\Junior\\AppData\\Local\\Temp\\pip-install-w8awn22p\\box2d-py\\setup.py'""'""';f=getattr(tokenize, '""'""'open'""'""', open)(__file__);code=f.read().replace('""'""'\r\n'""'""', '""'""'\n'""'""');f.close();exec(compile(code, __file__, '""'""'exec'""'""'))' install --record 'C:\Users\Junior\AppData\Local\Temp\pip-record-netg1nlq\install-record.txt' --single-version-externally-managed --compile --install-headers 'C:\Users\Junior\Anaconda\envs\gym\Include\box2d-py' Check the logs for full command output.
</code></pre>

<p>what I have to do to install it successfully?</p>
",,user12096782,,,,2022-10-29 09:18:38,Gym's box 2d (openAI) doesn't install successfully (pip error),<python><machine-learning><box2d><reinforcement-learning><openai-gym>,4,3,0,,,CC BY-SA 4.0,
754,59997727,1,60184677,,2020-01-31 04:09:11,,1,1677,"<p>Gym Taxi-v2 is deprecated. My implementation of Q-learning still works with Taxi-v3 but for some reason, env.render() shows the wrong taxi position at each step. </p>

<p>Anyway, apart from an added wall, what are the differences between Taxi-v2 v Taxi-v3?</p>
",3131604,,6393275,,2020-06-18 02:43:37,2020-06-18 02:43:37,RL Environment - OpenAI Gym Taxi-v2 vs Taxi-v3,<reinforcement-learning><openai-gym>,1,0,,,,CC BY-SA 4.0,
756,42605769,1,43283856,,2017-03-05 07:37:44,,44,41392,"<p>I'm having issues installing OpenAI Gym Atari environment on Windows 10. I have successfully installed and used OpenAI Gym already on the same system.  </p>

<p>It keeps tripping up when trying to run a makefile.</p>

<p>I am running the command <code>pip install gym[atari]</code></p>

<p>Here is the error:</p>

<p><a href=""https://i.stack.imgur.com/Sa2qb.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/Sa2qb.png"" alt=""enter image description here""></a></p>

<p>and here is what I currently have on my system...<code>cmake</code> and <code>make</code> are both clearly installed.</p>

<p><a href=""https://i.stack.imgur.com/FHqfP.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/FHqfP.png"" alt=""enter image description here""></a></p>
",6446734,,,,,2020-07-26 21:07:54,OpenAI Gym Atari on Windows,<python><windows><openai-gym>,5,0,0,,,CC BY-SA 3.0,
758,60911279,1,61020203,,2020-03-29 07:26:12,,1,4801,"<p>In environments like Atari space invaders state of the environment is its image, so in following line of code </p>

<p><code>observation, action, reward, _ = env.step()</code> <code>observation</code> variable holds the actual image of the environment, but for environment like Cartpole the observation would be some scalar numbers.</p>

<p>Is it possible to somehow access the picture of states in those environments?</p>
",9642804,,,,,2021-11-28 06:40:29,Is it possible to get an image of environment in OpenAI gym?,<reinforcement-learning><openai-gym>,3,0,0,,,CC BY-SA 4.0,
765,61217265,1,61243168,,2020-04-14 21:07:58,,0,270,"<p>Ok so I've spent an embarrassing amount of time on this problem and still am fairly confused.  I'm hoping someone can provide some guidance, resources, similar projects, etc.  I am unsure that what I am doing is the right way to go about this problem.</p>

<p>Background:</p>

<p>I am working on a project where I am developing an imitation of rocket league (a video game) so that I can run reinforcement learning algorithms on the game (I can't run the game itself because it would be too slow).  The imitation needs to comply with OpenAI Gym's API, which essentially demands a step method (which takes in the actions the AI determines necessary at that timestep and outputs the state of the game after the timestep), and a render method.  I have enough of the game implemented that I am currently working on the render method.  I want to package this and release it on pypi, so I chose pyglet to implement the graphics because I read it is fairly portable.</p>

<p>Problem:</p>

<p>I don't think I can use an event loop to run my graphics.  The way this api is set-up and used is that the user first instantiates the environment (the video game) and then sets up a loop in which they run the step function of the environment and optionally choose to also place the render method in that same loop depending on whether or not they want to see their AI's actions during that run. An example usage is available under environments on this <a href=""https://gym.openai.com/docs/"" rel=""nofollow noreferrer"">page</a>.  So I can't use an event loop because it would stop execution of the user's loop.  Instead I need to instantiate a window on the first call of render, and then update it with the state of the game on every subsequent call.</p>

<p>My current solution:</p>

<pre><code>    def render():
        if self.window is None:
            self.window = pyglet.window.Window(width, height)
        self.window.clear()
        self.window.dispatch_events()
          ... describe current 3d scence
        self.window.flip()
</code></pre>

<p>Problem cont:</p>

<p>My current solution feels a bit hacky which I don't love, but more of a problem is that I can't figure out how to implement user input for this solution.  I would like to be able to pan and move the camera all around the scene so that I view the 3-dimensional shape of objects, but I don't know how to implement that without the event loop and on_key_press decorator.</p>

<p>Also:</p>

<p>I am struggling to find good resources for 3d programming with OpenGL functions (the game is 3d).  I was wondering if anyone knew of a good place to learn that without all complexity I found on <a href=""https://learnopengl.com/"" rel=""nofollow noreferrer"">https://learnopengl.com/</a>.  I don't even know if pyglet/opengl is the right way to go about solving this problem. I know very little about 3d graphics and am open to any suggestions.</p>
",5381582,,,,,2020-04-16 05:18:43,Using opengl in pyglet to render 3d scene without event loop,<python><opengl><3d><pyglet><openai-gym>,1,1,0,,,CC BY-SA 4.0,
767,61163513,1,61170486,,2020-04-11 20:47:12,,0,323,"<p>I have been going through the implementation of neural network in openAI code for any Vanilla Policy Gradient (As a matter of fact, this part is used nearly everywhere). The code looks something like this :</p>

<pre><code>def mlp_categorical_policy(x, a, hidden_sizes, activation, output_activation, action_space):
    act_dim = action_space.n
    logits = mlp(x, list(hidden_sizes) + [act_dim], activation, None)
    logp_all = tf.nn.log_softmax(logits)
    pi = tf.squeeze(tf.random.categorical(logits, 1), axis=1)
    logp = tf.reduce_sum(tf.one_hot(a, depth=act_dim) * logp_all, axis=1)
    logp_pi = tf.reduce_sum(tf.one_hot(pi, depth=act_dim) * logp_all, axis=1)
    return pi, logp, logp_pi
</code></pre>

<p>and this multi-layered perceptron network is defined as follows :</p>

<pre><code>def mlp(x, hidden_sizes=(32,), activation=tf.tanh, output_activation=None):
    for h in hidden_sizes[:-1]:
        x = tf.layers.dense(inputs=x, units=h, activation=activation)
    return tf.layers.dense(inputs=x, units=hidden_sizes[-1], activation=output_activation)
</code></pre>

<p>My question is what is the return from this mlp function? I mean the structure or shape. Is it an N-dimentional tensor? If so, how is it given as an input to <code>tf.random_categorical</code>? If not, and its just has the shape <code>[hidden_layer2, output]</code>, then what happened to the other layers? As per their <a href=""https://www.tensorflow.org/api_docs/python/tf/random/categorical"" rel=""nofollow noreferrer"">website description about random_categorical</a> it only takes a 2-D input. The complete code of openAI's <a href=""https://github.com/openai/spinningup/blob/master/spinup/algos/tf1/vpg/vpg.py"" rel=""nofollow noreferrer"">VPG algorithm</a> can be found here. The mlp is implemented <a href=""https://github.com/openai/spinningup/blob/master/spinup/algos/tf1/vpg/core.py"" rel=""nofollow noreferrer"">here</a>. I would be highly grateful if someone would just tell me what this <code>mlp_categorical_policy()</code> is doing?</p>

<p>Note: The hidden size is [64, 64], the action dimension is 3</p>

<p>Thanks and cheers</p>
",4898906,,,,,2020-04-13 08:59:07,What would be the output from tensorflow dense layer if we assign itself as input and output while making a neural network?,<tensorflow><neural-network><reinforcement-learning><openai-gym>,1,0,,,,CC BY-SA 4.0,
781,44389007,1,45130447,,2017-06-06 11:37:32,,4,2774,"<p>I have installed OpenAI <code>gym</code> and the ATARI environments. I know that I can find all the ATARI games in the <a href=""https://gym.openai.com/envs#atari"" rel=""nofollow noreferrer"">documentation</a> but is there a way to do this in Python, <strong>without</strong> printing any other environments (e.g. NOT the classic control environments)</p>
",3747801,,,,,2022-06-05 07:37:07,OpenAI gym: How to get complete list of ATARI environments,<deep-learning><artificial-intelligence><reinforcement-learning><openai-gym>,3,0,,,,CC BY-SA 3.0,
785,50845926,1,50849216,,2018-06-13 20:39:13,,2,5889,"<p>I just installed it with pip, and wanted to import it in PyCharm, but got an error:</p>

<hr>

<p>Solving environment: ...working... failed</p>

<p>PackagesNotFoundError: The following packages are not available from current channels:</p>

<ul>
<li>gym</li>
</ul>

<p>Current channels:</p>

<ul>
<li><a href=""https://repo.anaconda.com/pkgs/main/win-64"" rel=""nofollow noreferrer"">https://repo.anaconda.com/pkgs/main/win-64</a></li>
<li><a href=""https://repo.anaconda.com/pkgs/main/noarch"" rel=""nofollow noreferrer"">https://repo.anaconda.com/pkgs/main/noarch</a></li>
<li><a href=""https://repo.anaconda.com/pkgs/free/win-64"" rel=""nofollow noreferrer"">https://repo.anaconda.com/pkgs/free/win-64</a></li>
<li><a href=""https://repo.anaconda.com/pkgs/free/noarch"" rel=""nofollow noreferrer"">https://repo.anaconda.com/pkgs/free/noarch</a></li>
<li><a href=""https://repo.anaconda.com/pkgs/r/win-64"" rel=""nofollow noreferrer"">https://repo.anaconda.com/pkgs/r/win-64</a></li>
<li><a href=""https://repo.anaconda.com/pkgs/r/noarch"" rel=""nofollow noreferrer"">https://repo.anaconda.com/pkgs/r/noarch</a></li>
<li><a href=""https://repo.anaconda.com/pkgs/pro/win-64"" rel=""nofollow noreferrer"">https://repo.anaconda.com/pkgs/pro/win-64</a></li>
<li><a href=""https://repo.anaconda.com/pkgs/pro/noarch"" rel=""nofollow noreferrer"">https://repo.anaconda.com/pkgs/pro/noarch</a></li>
<li><a href=""https://repo.anaconda.com/pkgs/msys2/win-64"" rel=""nofollow noreferrer"">https://repo.anaconda.com/pkgs/msys2/win-64</a></li>
<li><a href=""https://repo.anaconda.com/pkgs/msys2/noarch"" rel=""nofollow noreferrer"">https://repo.anaconda.com/pkgs/msys2/noarch</a></li>
</ul>

<p>To search for alternate channels that may provide the conda package you're
looking for, navigate to</p>

<pre><code>https://anaconda.org
</code></pre>

<p>and use the search bar at the top of the page.</p>

<hr>

<p>does anyone know how to fix this by any chance?</p>
",4414359,,4685471,,2018-06-14 08:55:04,2018-06-14 08:55:04,PyCharm won't install gym package,<python><module><pip><pycharm><openai-gym>,1,0,0,,,CC BY-SA 4.0,
786,67626206,1,67628238,,2021-05-20 18:35:15,,2,137,"<p>For the context, I'm trying to use  OpenAI gym together with a pyglet tetris game that I wrote. The problem I'm facing boils down to the MWE below.</p>
<p>After always the same amount of time, here  ~9 seconds, the window freezes, but the prints from the toto function AND the render function are still printing.
I'm going crazy on this. Pyglet seemed nice but I hardly find any documentation and the official one is hardly helpful.
If I do the same with a simpler code with a on_draw() function, no problem, but I need this for the gym part.</p>
<p>Thank you</p>
<pre class=""lang-py prettyprint-override""><code>import pyglet
import time

class Display(pyglet.window.Window):
    def __init__(self, ww, wh):
        super().__init__(width=ww, height=wh)

class Env:
    def __init__(self):
        self.window = Display(640, 480)

    def render(self, i):
        self.window.clear()
        label = pyglet.text.Label('iter {:f}'.format(i),
                                  font_size=16,
                                  x=300,
                                  y=200,
                                  anchor_x='left', anchor_y='center')
        label.draw()
        print('iter {:f}'.format(i))
        self.window.flip()

env = Env()

def toto(dt):
    for t in range(300):
        time.sleep(0.5)
        print(&quot;toto {:d}&quot;.format(t))
        env.render(t)
    print(&quot;done&quot;)
pyglet.clock.schedule_once(toto, 1)
pyglet.app.run()
</code></pre>
",1771099,,,,,2021-05-20 21:37:12,pyglet windows hang with schedule_once after some time,<python><pyglet><openai-gym>,1,0,0,,,CC BY-SA 4.0,
787,51040303,1,51040386,,2018-06-26 10:11:40,,0,173,"<p>As the title states. I want to install universe on my raspberry pi 3. I want to code a learning gamebot</p>

<p>I have tried:</p>

<pre><code>Pip install universe
</code></pre>

<p>But it returns errors. </p>

<p>Help pls</p>
",9116650,,,,,2018-06-26 10:15:29,How to install universe on rpi,<python><pip><raspberry-pi3><openai-gym>,1,1,,,,CC BY-SA 4.0,
788,47615458,1,47622125,,2017-12-03 05:07:25,,9,12948,"<p>I am trying to get the code below to work.  </p>

<pre><code>import gym

env = gym.make(""CartPole-v0"")

env.reset()

env.render()
</code></pre>

<p>I have no problems running the first 3 lines but when I run the 4th I get the error:</p>

<pre><code>Traceback (most recent call last):

  File ""&lt;ipython-input-3-a692a1a1ffe7&gt;"", line 1, in &lt;module&gt;
    env.render()

  File ""/home/mikedoho/gym/gym/core.py"", line 150, in render
    return self._render(mode=mode, close=close)

  File ""/home/mikedoho/gym/gym/core.py"", line 286, in _render
    return self.env.render(mode, close)

  File ""/home/mikedoho/gym/gym/core.py"", line 150, in render
    return self._render(mode=mode, close=close)

  File ""/home/mikedoho/gym/gym/envs/classic_control/cartpole.py"", line 116, in _render
    self.viewer = rendering.Viewer(screen_width, screen_height)

  File ""/home/mikedoho/gym/gym/envs/classic_control/rendering.py"", line 51, in __init__
    self.window = pyglet.window.Window(width=width, height=height, display=display)

  File ""/home/mikedoho/anaconda3/lib/python3.6/site-packages/pyglet/window/__init__.py"", line 504, in __init__
    screen = display.get_default_screen()

  File ""/home/mikedoho/anaconda3/lib/python3.6/site-packages/pyglet/canvas/base.py"", line 73, in get_default_screen
    return self.get_screens()[0]

  File ""/home/mikedoho/anaconda3/lib/python3.6/site-packages/pyglet/canvas/base.py"", line 65, in get_screens
    raise NotImplementedError('abstract')

NotImplementedError: abstract
</code></pre>

<p>I was hoping to get a cart on a pole to display in a new window.</p>

<p>This error is from code in Spyder.  I am using ubuntu 16.04; python 3.6.3; IPython 6.1.0.  I have not any luck using jupyter notebook.</p>

<p>Again I apologize.  I am very new at this an just downloaded ubuntu today.</p>
",9045339,,4826457,,2017-12-03 05:17:40,2018-03-28 05:05:44,Error following env.render() for OpenAI,<python-3.x><openai-gym>,1,0,0,,,CC BY-SA 3.0,
792,44369938,1,57329702,,2017-06-05 13:19:47,,29,13019,"<p>Is it possible to use <a href=""https://openai.com/"" rel=""noreferrer"">openai</a>'s <a href=""https://gym.openai.com/docs"" rel=""noreferrer"">gym environments</a> for multi-agent games? Specifically, I would like to model a card game with four players (agents). The player scoring a turn starts the next turn. How would I model the necessary coordination between the players (e.g. who's turn it is next)? Ultimately, I would like to use reinforcement learning on four agents that play against each other.</p>
",1228765,,,,,2021-10-09 07:52:11,Openai gym environment for multi-agent games,<reinforcement-learning><openai-gym>,4,2,0,,,CC BY-SA 3.0,
795,67087799,1,67087909,,2021-04-14 08:05:03,,1,646,"<p>I'm trying to register an environment that has been defined inside a cell of a jupyter notebook running on colab. My problem is concerned with the entry_point. Some module has to be specified before the colon. But how do I know the module's name of the underlying colab notebook? Is there some workaround to wrap my custom environment inside a module without moving the code to another file? I'd prefer to have everything inside the entire notebook.</p>
<pre><code>try:
  gym.envs.register(
      id='myenv-v0',
      entry_point=':MyEnv',
      max_episode_steps=320
  )
except:
    pass
</code></pre>
<p>If I call gym.make(&quot;myenv-v0&quot;)</p>
<pre><code>ModuleNotFoundError: No module named 'base'
</code></pre>
<p>The try block is due to the fact that an evnrinoment id cannot be registered twice. It may happen that I run again all notebook cells.</p>
",3515869,,,,,2022-09-12 09:53:44,Register gym environment that is defined inside a jupyter notebook cell,<python><jupyter-notebook><google-colaboratory><openai-gym>,2,0,,,,CC BY-SA 4.0,
797,67628686,1,67629572,,2021-05-20 22:21:04,,1,647,"<p>I get <code>ValueError: xxx not found in gym registry, you maybe meant</code> when trying to register a custom environment in stable baselines 3. I tried the following commands:</p>
<pre><code>apt-get install swig cmake ffmpeg freeglut3-dev xvfb
git clone --recursive https://github.com/DLR-RM/rl-baselines3-zoo
cd rl-baselines3-zoo
pip3 install -r requirements.txt
cd ..
git clone https://github.com/MatePocs/gym-basic.git
cd gym-basic
pip3 install -e .
cd ..
cd rl-baselines3-zoo
python3 train.py --algo td3 --env basic-v0 --eval-freq 1000 --save-freq 5000
</code></pre>
<p>The result is:</p>
<pre><code>Traceback (most recent call last):
  File &quot;train.py&quot;, line 107, in &lt;module&gt;
    raise ValueError(f&quot;{env_id} not found in gym registry, you maybe meant {closest_match}?&quot;)
ValueError: basic-v0 not found in gym registry, you maybe meant CubeCrash-v0?
</code></pre>
<p>Do you see which is the mistake?</p>
",693101,,,,,2021-05-21 00:53:51,What is missing in OpenAI Gym registration?,<python><openai-gym><stable-baselines>,2,2,,,,CC BY-SA 4.0,
804,48531314,1,49056767,,2018-01-30 22:06:37,,0,66,"<p>I'm trying to create a simple program using universe by openai but every time I close the VNC, the python launcher doesn't respond anymore and I have to force quit it. What can I do to solve this? Thanks</p>
",4807022,,4807022,,2018-03-01 18:53:41,2018-03-01 18:53:41,Python launcher not responding after closing VNC (mac),<python><vnc><openai-gym>,1,0,,,,CC BY-SA 3.0,
805,68954703,1,71053783,,2021-08-27 13:59:44,,0,2070,"<p>I am working on a deep reinforcement problem, I am new to this. I am writing a snippet of code and errors I am getting.</p>
<p>Broker_Node_Map is a list of values present in different positions in a machine. I don't know how to present these values as integers. This is the state I have as it's changing too accordingly. Please suggest what should I do. Please be kind, I am pretty new and trying to get hold of things.</p>
<pre class=""lang-py prettyprint-override""><code>def __init__(self):
    super(BrokerEnv2, self).__init__()
    reward = 0 
    self.action_space = DiscreteActions.get_action_space()
    self.observation_space = DiscreteObservations.get_observation_space() 

def reset(self):
    observed_State = self.Broker_Node_Map
    return observed_State 
</code></pre>
<p>While checking env on stable baselines  - <code>check_env(env) **Error** - AssertionError: The observation returned by reset() method must be an int</code></p>
<p><strong>EDIT 1 -</strong>
Very careless of me. Changed the space to box space but now another error emerged.
<strong>AssertionError: The observation returned by the reset() method does not match the given observation space</strong></p>
<p>This is what my reset() is returning -</p>
<pre><code>&lt;class 'numpy.ndarray'&gt; (46,) [26 33  0 50  0  0 73 26  0 29  0 34 27 67  0  0  0  0 35 60  0  0 24 22
  0  0  0  0 25  0 17  0  0  0 21  0  0 53 68 40 51  0 62  0 56  0] 
</code></pre>
<p>This is how I have defined my observation space -</p>
<pre><code>self.observation_space = spaces.Box(low=1, high=73, shape=(46,), dtype=np.int64)
</code></pre>
<p>Please help me out why this error is coming?</p>
",12987236,,12987236,,2021-08-28 06:48:14,2022-06-16 22:54:37,Defining Observation Space in Open AI Gym,<deep-learning><neural-network><reinforcement-learning><openai-gym><starcraftgym>,2,0,,,,CC BY-SA 4.0,
810,53228886,1,53229094,,2018-11-09 15:42:16,,2,3031,"<p>so I was trying to make a bot to train on one of the environment of <a href=""https://blog.openai.com/universe/"" rel=""nofollow noreferrer"">Universe</a> and on running it, I am receiving the error (<code>ImportError: No module named universe</code>). Help me understand what this is and how I can fix it.</p>
<p>My code -</p>
<pre><code>import gym
import universe
env = gym.make('HandManipulateEgg-v0')
observation_n = env.reset()

while True:
    action_n = [[('KeyEvent', 'ArrowUp', True)] for ob in observation_n]
    observation_n, reward_n, done_n, env.step(action_n)
    env.render ()
</code></pre>
<p>My system details -
<a href=""https://i.stack.imgur.com/FbU5m.jpg"" rel=""nofollow noreferrer"">Sayon's System Details</a></p>
<p>Any help will be deeply appreciated.</p>
",10629613,,-1,,2020-06-20 09:12:55,2018-11-09 16:29:22,Why am I getting a 'No module named OpenAI' Import error?,<python><machine-learning><google-colaboratory><openai-gym><universe>,1,5,0,,,CC BY-SA 4.0,
812,62586436,1,62638151,,2020-06-26 00:58:09,,1,496,"<p>I wrote a DQN to play the OpenAI gym cart pole game with TensorFlow and tf_agents. The code looks like the following:</p>
<pre class=""lang-py prettyprint-override""><code>def compute_avg_return(environment, policy, num_episodes=10):
    total_return = 0.0
    for _ in range(num_episodes):
        time_step = environment.reset()
        episode_return = 0.0
        while not time_step.is_last():
            action_step = policy.action(time_step)
            time_step = environment.step(action_step.action)
            episode_return += time_step.reward
        total_return += episode_return
    avg_return = total_return / num_episodes
    return avg_return.numpy()[0]


def collect_step(environment, policy, buffer):
    time_step = environment.current_time_step()
    action_step = policy.action(time_step)
    next_time_step = environment.step(action_step.action)
    traj = trajectory.from_transition(time_step, action_step, next_time_step)
    buffer.add_batch(traj)


def collect_data(env, policy, buffer, steps):
    for _ in range(steps):
        collect_step(env, policy, buffer)


def train_model(
    num_iterations=config.default_num_iterations,
    collect_steps_per_iteration=config.default_collect_steps_per_iteration,
    replay_buffer_max_length=config.default_replay_buffer_max_length,
    batch_size=config.default_batch_size,
    learning_rate=config.default_learning_rate,
    log_interval=config.default_log_interval,
    num_eval_episodes=config.default_num_eval_episodes,
    eval_interval=config.default_eval_interval,
    checkpoint_saver_directory=config.default_checkpoint_saver_directory,
    model_saver_directory=config.default_model_saver_directory,
    visualize=False,
    static_plot=False,
):
    env_name = 'CartPole-v0'
    train_py_env = suite_gym.load(env_name)
    eval_py_env = suite_gym.load(env_name)
    train_env = tf_py_environment.TFPyEnvironment(train_py_env)
    eval_env = tf_py_environment.TFPyEnvironment(eval_py_env)
    fc_layer_params = (100,)
    q_net = q_network.QNetwork(
        train_env.observation_spec(),
        train_env.action_spec(),
        fc_layer_params=fc_layer_params)
    optimizer = Adam(learning_rate=learning_rate)
    train_step_counter = tf.Variable(0)
    agent = dqn_agent.DqnAgent(
        train_env.time_step_spec(),
        train_env.action_spec(),
        q_network=q_net,
        optimizer=optimizer,
        td_errors_loss_fn=common.element_wise_squared_loss,
        train_step_counter=train_step_counter)
    agent.initialize()
    replay_buffer = tf_uniform_replay_buffer.TFUniformReplayBuffer(
        data_spec=agent.collect_data_spec,
        batch_size=train_env.batch_size,
        max_length=replay_buffer_max_length)
    dataset = replay_buffer.as_dataset(
        num_parallel_calls=3,
        sample_batch_size=batch_size,
        num_steps=2).prefetch(3)
    iterator = iter(dataset)
    agent.train_step_counter.assign(0)
    avg_return = compute_avg_return(eval_env, agent.policy, num_eval_episodes)
    returns = []
    loss = []
    for _ in range(num_iterations):
        for _ in range(collect_steps_per_iteration):
            collect_step(train_env, agent.collect_policy, replay_buffer)
        experience, unused_info = next(iterator)
        train_loss = agent.train(experience).loss
        step = agent.train_step_counter.numpy()
        avg_return = compute_avg_return(eval_env, agent.policy, num_eval_episodes)
        returns.append(avg_return)
</code></pre>
<p>Although the average reward is getting better and reached 200, the maximum score, in the end, the loss is not obviously decreasing.</p>
<p>Here is the loss plot:</p>
<p><a href=""https://i.stack.imgur.com/AcIYE.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/AcIYE.png"" alt=""loss plot"" /></a></p>
<p>Here is the reward plot:</p>
<p><a href=""https://i.stack.imgur.com/J01qu.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/J01qu.png"" alt=""reward plot"" /></a></p>
<p>The good point is that the model is successful, and it can play the game really well. However, I would really love to get some insight into why this is happening where an extremely high loss still yields a good reward.</p>
",6750238,,,,,2020-06-29 12:28:06,Why DQN for cartpole game has a ascending reward while loss is not descending?,<python><tensorflow><machine-learning><reinforcement-learning><openai-gym>,1,0,,,,CC BY-SA 4.0,
817,69035664,1,69251846,,2021-09-02 18:48:36,,0,253,"<p>I have a bunch of Java code that constitutes an environment and an agent. I want to use one of the Python reinforcement learning libraries (stable-baselines, tf-agents, rllib, etc.) to train a policy for the Java agent/environment. And then deploy the policy on the Java side for production. Is there standard practice for incorporating other languages into Python RL libraries? I was thinking of one of the following solutions:</p>
<ol>
<li>Wrap Java env/agent code into REST API, and implement custom environment in Python that calls that API to step through the environment.</li>
<li>Use Py4j to invoke Java from Python and implement custom environment.</li>
</ol>
<p>Which one would be better? Are there any other ways?</p>
<blockquote>
<p>Edit: I ended up going the former - deploying a web server that encapsulates the environments. works quite well for me. Leaving the question open in case there is a better practice to handle this kind of situations!</p>
</blockquote>
",151186,,151186,,2021-09-05 17:08:14,2021-09-20 09:13:29,Reinforcement Learning - Custom environment implementation in Java for Python RL framework,<java><python><reinforcement-learning><openai-gym><stable-baselines>,1,0,,,,CC BY-SA 4.0,
820,45263566,1,46301033,,2017-07-23 09:31:36,,9,9488,"<p>I try to run this code in openAi gym. but it can not.</p>
<pre><code>import mujoco_py
import gym
from os.path import dirname

env = gym.make('Hopper-v1')
env.reset()
for _ in range(1000):
    env.render()
    env.step(env.action_space.sample()) 
</code></pre>
<p>the error info:</p>
<pre><code>/Users/yunfanlu/anaconda/envs/py35/bin/python3.5 /Users/yunfanlu/WorkPlace/OpenAIGym/OpenGymL/c.py
[2017-07-23 17:17:15,633] Making new env: Hopper-v1
Traceback (most recent call last):
  File &quot;/Users/yunfanlu/anaconda/envs/py35/lib/python3.5/site-packages/gym/envs/mujoco/mujoco_env.py&quot;, line 12, in &lt;module&gt;
    from mujoco_py.mjlib import mjlib
ImportError: No module named 'mujoco_py.mjlib'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File &quot;/Users/yunfanlu/WorkPlace/OpenAIGym/OpenGymL/c.py&quot;, line 15, in &lt;module&gt;
    env = gym.make('Hopper-v1')
  File &quot;/Users/yunfanlu/anaconda/envs/py35/lib/python3.5/site-packages/gym/envs/registration.py&quot;, line 161, in make
    return registry.make(id)
  File &quot;/Users/yunfanlu/anaconda/envs/py35/lib/python3.5/site-packages/gym/envs/registration.py&quot;, line 119, in make
    env = spec.make()
  File &quot;/Users/yunfanlu/anaconda/envs/py35/lib/python3.5/site-packages/gym/envs/registration.py&quot;, line 85, in make
    cls = load(self._entry_point)
  File &quot;/Users/yunfanlu/anaconda/envs/py35/lib/python3.5/site-packages/gym/envs/registration.py&quot;, line 17, in load
    result = entry_point.load(False)
  File &quot;/Users/yunfanlu/anaconda/envs/py35/lib/python3.5/site-packages/setuptools-27.2.0-py3.5.egg/pkg_resources/__init__.py&quot;, line 2258, in load
  File &quot;/Users/yunfanlu/anaconda/envs/py35/lib/python3.5/site-packages/setuptools-27.2.0-py3.5.egg/pkg_resources/__init__.py&quot;, line 2264, in resolve
  File &quot;/Users/yunfanlu/anaconda/envs/py35/lib/python3.5/site-packages/gym/envs/mujoco/__init__.py&quot;, line 1, in &lt;module&gt;
    from gym.envs.mujoco.mujoco_env import MujocoEnv
  File &quot;/Users/yunfanlu/anaconda/envs/py35/lib/python3.5/site-packages/gym/envs/mujoco/mujoco_env.py&quot;, line 14, in &lt;module&gt;
    raise error.DependencyNotInstalled(&quot;{}. (HINT: you need to install mujoco_py, and also perform the setup instructions here: https://github.com/openai/mujoco-py/.)&quot;.format(e))
gym.error.DependencyNotInstalled: No module named 'mujoco_py.mjlib'. (HINT: you need to install mujoco_py, and also perform the setup instructions here: https://github.com/openai/mujoco-py/.)

Process finished with exit code 1
</code></pre>
<p>My Computer Envs:</p>
<p>OS: macOS
python: python3.5 (conda envs)</p>
<p>I have benn install &quot;pip install 'gym[all]'&quot; and install mujoco.</p>
<pre><code>âžœ  .mujoco tree
.
â”œâ”€â”€ LICENSE.txt
â”œâ”€â”€ mjkey.txt
â””â”€â”€ mjpro150
</code></pre>
<p>I can run the examples of mujoco. it is successful.</p>
<pre><code>(py35) âžœ  OpenGymL python body_interaction.py 
Creating window glfw
</code></pre>
",7522902,,4685471,,2021-10-18 19:30:32,2021-10-18 19:30:32,OpenAI gym mujoco ImportError: No module named 'mujoco_py.mjlib',<python><openai-gym>,1,0,0,,,CC BY-SA 4.0,
821,53194107,1,53241029,,2018-11-07 16:49:32,,3,3503,"<p>There are some things that I would like to modify in the OpenAI environments. If we use the <a href=""https://github.com/openai/gym/blob/master/gym/envs/classic_control/cartpole.py"" rel=""nofollow noreferrer"">Cartpole</a> example then we can edit things that are in the class init function but with environments that use <code>Box2D</code> it doesn't seem to be as straightforward.</p>
<p>For example, consider the <a href=""https://github.com/openai/gym/blob/master/gym/envs/box2d/bipedal_walker.py"" rel=""nofollow noreferrer"">BipedalWalker</a> environment.</p>
<p>In this case, how would I edit things like the <code>SPEED_HIP</code> or <code>SPEED_KNEE</code> variables?</p>
",3853339,,3924118,,2020-11-03 18:10:06,2020-11-03 18:10:06,Is it possible to modify OpenAI environments?,<reinforcement-learning><openai-gym>,2,0,0,,,CC BY-SA 4.0,
822,45886398,1,46256795,,2017-08-25 17:07:52,,0,287,"<p>I am trying to implement q-learning with an action-value approximation-function. I am using openai-gym and the ""MountainCar-v0"" enviroment to test my algorithm out. My problem is, it does not converge or find the goal at all. </p>

<p>Basically the approximator works like the following, you feed in the 2 features: position and velocity and one of the 3 actions in a one-hot encoding: 0 -> [1,0,0], 1 -> [0,1,0] and 2 -> [0,0,1]. The output is the action-value approximation Q_approx(s,a), for one specific action.</p>

<p>I know that usually, the input is the state (2 features) and the output layer contains 1 output for each action. The big difference that I see is that I have run the feed forward pass 3 times (one for each action) and take the max, while in the standard implementation you run it once and take the max over the output.</p>

<p>Maybe my implementation is just completely wrong and I am thinking wrong. Gonna paste the code here, it is a mess but I am just experimenting a bit:</p>

<pre><code>import gym
import numpy as np
from keras.models import Sequential
from keras.layers import Dense, Activation

env = gym.make('MountainCar-v0')

# The mean reward over 20 episodes
mean_rewards = np.zeros(20)
# Feature numpy holder
features = np.zeros(5)
# Q_a value holder
qa_vals = np.zeros(3)

one_hot = {
    0 : np.asarray([1,0,0]),
    1 : np.asarray([0,1,0]),
    2 : np.asarray([0,0,1])
}

model = Sequential()
model.add(Dense(20, activation=""relu"",input_dim=(5)))
model.add(Dense(10,activation=""relu""))
model.add(Dense(1))
model.compile(optimizer='rmsprop',
              loss='mse',
              metrics=['accuracy'])

epsilon_greedy = 0.1
discount = 0.9
batch_size = 16

# Experience replay containing features and target 
experience = np.ones((10*300,5+1))

# Ring buffer
def add_exp(features,target,index):
    if index % experience.shape[0] == 0:
        index = 0
        global filled_once
        filled_once = True
    experience[index,0:5] = features
    experience[index,5] = target
    index += 1
    return index

for e in range(0,100000):
    obs = env.reset()
    old_obs = None
    new_obs = obs
    rewards = 0
    loss = 0
    for i in range(0,300):

        if old_obs is not None:
            # Find q_a max for s_(t+1)
            features[0:2] = new_obs
            for i,pa in enumerate([0,1,2]):
                features[2:5] = one_hot[pa]
                qa_vals[i] = model.predict(features.reshape(-1,5))

            rewards += reward
            target = reward + discount*np.max(qa_vals) 

            features[0:2] = old_obs
            features[2:5] = one_hot[a]

            fill_index = add_exp(features,target,fill_index)

            # Find new action
            if np.random.random() &lt; epsilon_greedy:
                a = env.action_space.sample()
            else:
                a = np.argmax(qa_vals)
        else:
            a = env.action_space.sample()

        obs, reward, done, info = env.step(a)

        old_obs = new_obs
        new_obs = obs

        if done:
            break

        if filled_once:
            samples_ids = np.random.choice(experience.shape[0],batch_size)
            loss += model.train_on_batch(experience[samples_ids,0:5],experience[samples_ids,5].reshape(-1))[0]
    mean_rewards[e%20] = rewards
    print(""e = {} and loss = {}"".format(e,loss))
    if e % 50 == 0:
        print(""e = {} and mean = {}"".format(e,mean_rewards.mean()))
</code></pre>

<p>Thanks in advance!</p>
",2253345,,,,,2017-09-16 17:48:05,Function approximator and q-learning,<reinforcement-learning><openai-gym>,1,1,,,,CC BY-SA 3.0,
824,62982340,1,62982497,,2020-07-19 15:37:47,,0,76,"<p>In the following code written by Karpathy, why do we have this line(Why do we need to compare with the uniform distribution to select an action while the policy function did that)</p>
<pre><code>  # forward the policy network and sample an action from the returned probability
  aprob, h = policy_forward(x)
  action = 2 if np.random.uniform() &lt; aprob else 3 # roll the dice!
</code></pre>
<p>instead of just</p>
<pre><code> # forward the policy network and sample an action from the returned probability
  aprob, h = policy_forward(x)
  action = 2 if 0.5 &lt; aprob else 3 # roll the dice!
</code></pre>
<p>....Karpathy's full code from : <a href=""https://gist.github.com/karpathy/a4166c7fe253700972fcbc77e4ea32c5"" rel=""nofollow noreferrer"">https://gist.github.com/karpathy/a4166c7fe253700972fcbc77e4ea32c5</a></p>
<pre><code>&quot;&quot;&quot; Trains an agent with (stochastic) Policy Gradients on Pong. Uses OpenAI Gym. &quot;&quot;&quot;
import numpy as np
import cPickle as pickle
import gym

# hyperparameters
H = 200 # number of hidden layer neurons
batch_size = 10 # every how many episodes to do a param update?
learning_rate = 1e-4
gamma = 0.99 # discount factor for reward
decay_rate = 0.99 # decay factor for RMSProp leaky sum of grad^2
resume = False # resume from previous checkpoint?
render = False

# model initialization
D = 80 * 80 # input dimensionality: 80x80 grid
if resume:
  model = pickle.load(open('save.p', 'rb'))
else:
  model = {}
  model['W1'] = np.random.randn(H,D) / np.sqrt(D) # &quot;Xavier&quot; initialization
  model['W2'] = np.random.randn(H) / np.sqrt(H)
  
grad_buffer = { k : np.zeros_like(v) for k,v in model.iteritems() } # update buffers that add up gradients over a batch
rmsprop_cache = { k : np.zeros_like(v) for k,v in model.iteritems() } # rmsprop memory

def sigmoid(x): 
  return 1.0 / (1.0 + np.exp(-x)) # sigmoid &quot;squashing&quot; function to interval [0,1]

def prepro(I):
  &quot;&quot;&quot; prepro 210x160x3 uint8 frame into 6400 (80x80) 1D float vector &quot;&quot;&quot;
  I = I[35:195] # crop
  I = I[::2,::2,0] # downsample by factor of 2
  I[I == 144] = 0 # erase background (background type 1)
  I[I == 109] = 0 # erase background (background type 2)
  I[I != 0] = 1 # everything else (paddles, ball) just set to 1
  return I.astype(np.float).ravel()

def discount_rewards(r):
  &quot;&quot;&quot; take 1D float array of rewards and compute discounted reward &quot;&quot;&quot;
  discounted_r = np.zeros_like(r)
  running_add = 0
  for t in reversed(xrange(0, r.size)):
    if r[t] != 0: running_add = 0 # reset the sum, since this was a game boundary (pong specific!)
    running_add = running_add * gamma + r[t]
    discounted_r[t] = running_add
  return discounted_r

def policy_forward(x):
  h = np.dot(model['W1'], x)
  h[h&lt;0] = 0 # ReLU nonlinearity
  logp = np.dot(model['W2'], h)
  p = sigmoid(logp)
  return p, h # return probability of taking action 2, and hidden state

def policy_backward(eph, epdlogp):
  &quot;&quot;&quot; backward pass. (eph is array of intermediate hidden states) &quot;&quot;&quot;
  dW2 = np.dot(eph.T, epdlogp).ravel()
  dh = np.outer(epdlogp, model['W2'])
  dh[eph &lt;= 0] = 0 # backpro prelu
  dW1 = np.dot(dh.T, epx)
  return {'W1':dW1, 'W2':dW2}

env = gym.make(&quot;Pong-v0&quot;)
observation = env.reset()
prev_x = None # used in computing the difference frame
xs,hs,dlogps,drs = [],[],[],[]
running_reward = None
reward_sum = 0
episode_number = 0
while True:
  if render: env.render()

  # preprocess the observation, set input to network to be difference image
  cur_x = prepro(observation)
  x = cur_x - prev_x if prev_x is not None else np.zeros(D)
  prev_x = cur_x

  # forward the policy network and sample an action from the returned probability
  aprob, h = policy_forward(x)
  action = 2 if np.random.uniform() &lt; aprob else 3 # roll the dice!

  # record various intermediates (needed later for backprop)
  xs.append(x) # observation
  hs.append(h) # hidden state
  y = 1 if action == 2 else 0 # a &quot;fake label&quot;
  dlogps.append(y - aprob) # grad that encourages the action that was taken to be taken (see http://cs231n.github.io/neural-networks-2/#losses if confused)

  # step the environment and get new measurements
  observation, reward, done, info = env.step(action)
  reward_sum += reward

  drs.append(reward) # record reward (has to be done after we call step() to get reward for previous action)

  if done: # an episode finished
    episode_number += 1

    # stack together all inputs, hidden states, action gradients, and rewards for this episode
    epx = np.vstack(xs)
    eph = np.vstack(hs)
    epdlogp = np.vstack(dlogps)
    epr = np.vstack(drs)
    xs,hs,dlogps,drs = [],[],[],[] # reset array memory

    # compute the discounted reward backwards through time
    discounted_epr = discount_rewards(epr)
    # standardize the rewards to be unit normal (helps control the gradient estimator variance)
    discounted_epr -= np.mean(discounted_epr)
    discounted_epr /= np.std(discounted_epr)

    epdlogp *= discounted_epr # modulate the gradient with advantage (PG magic happens right here.)
    grad = policy_backward(eph, epdlogp)
    for k in model: grad_buffer[k] += grad[k] # accumulate grad over batch

    # perform rmsprop parameter update every batch_size episodes
    if episode_number % batch_size == 0:
      for k,v in model.iteritems():
        g = grad_buffer[k] # gradient
        rmsprop_cache[k] = decay_rate * rmsprop_cache[k] + (1 - decay_rate) * g**2
        model[k] += learning_rate * g / (np.sqrt(rmsprop_cache[k]) + 1e-5)
        grad_buffer[k] = np.zeros_like(v) # reset batch gradient buffer

    # boring book-keeping
    running_reward = reward_sum if running_reward is None else running_reward * 0.99 + reward_sum * 0.01
    print 'resetting env. episode reward total was %f. running mean: %f' % (reward_sum, running_reward)
    if episode_number % 100 == 0: pickle.dump(model, open('save.p', 'wb'))
    reward_sum = 0
    observation = env.reset() # reset env
    prev_x = None

  if reward != 0: # Pong has either +1 or -1 reward exactly when game ends.
    print ('ep %d: game finished, reward: %f' % (episode_number, reward)) + ('' if reward == -1 else ' !!!!!!!!')
</code></pre>
",6070053,,,,,2020-07-19 15:52:27,Why do we need to compare with the uniform distribution to select an action while the policy function did that in Deep RL,<python><artificial-intelligence><reinforcement-learning><openai-gym>,1,0,,,,CC BY-SA 4.0,
830,64639554,1,64641092,,2020-11-02 03:49:08,,1,650,"<p>My triangle drawing is appearing lopsided, how I make it proportional?</p>
<pre><code>    points = np.array([[x, 10], [x/2, 10], [x+1/2, np.sqrt(5**2 - 2**2)]])
    pivot = plt.Polygon(points, closed = True)
   
</code></pre>
",12412932,,12412932,,2020-11-03 18:35:55,2020-11-03 18:35:55,Matplotlib draw proportional triangle,<python><matplotlib><reinforcement-learning><openai-gym><openai>,1,0,,,,CC BY-SA 4.0,
836,46997041,1,46997332,,2017-10-29 05:13:12,,0,199,"<p>I'm trying to train CatPole-v0 using Q learning. When trying to update the replay buffer with experience I am getting the following error: </p>

<pre><code>ValueError: Cannot feed value of shape (128,) for Tensor 'Placeholder_1:0', which has shape '(?, 2)'
</code></pre>

<p>The related code snippet is:</p>

<pre><code>def update_replay_buffer(replay_buffer, state, action, reward, next_state, done, action_dim):
    # append to buffer
    experience = (state, action, reward, next_state, done)
    replay_buffer.append(experience)
    # Ensure replay_buffer doesn't grow larger than REPLAY_SIZE
    if len(replay_buffer) &gt; REPLAY_SIZE:
        replay_buffer.pop(0)
    return None
</code></pre>

<p>The placeholder to be fed is </p>

<pre><code>action_in = tf.placeholder(""float"", [None, action_dim])
</code></pre>

<p>Can someone clarify how action_dim should be used to resolve this error?</p>
",2978131,,,,,2017-10-29 06:08:24,Feeding a tensorflow placeholder from an array,<tensorflow><reinforcement-learning><q-learning><openai-gym>,1,0,,,,CC BY-SA 3.0,
838,65918970,1,65919082,,2021-01-27 12:14:56,,0,2719,"<p>I'm trying to create an environment for my reinforcement learning algorithm, however, there seems a bit of a problem in case of when calling the PPOPolicy. For this I developed the following environment <code>envFru</code>:</p>
<pre><code>import gym
import os, sys
import numpy as np
import pandas as pd
from gym import spaces
import random

class envFru(gym.Env):
    metadata ={'render.modes': ['human']}
    
    def __init__(self):
        self.df = df
        
        self.action_space = spaces.Discrete(2)
        self.observation_space = spaces.Box(low=np.array([0,0,0]), high=np.array([1,1,1]), dtype=np.float16)
    
    
    def reset(self):
        pass    
    
    def step(self, action):
        pass
    
    def _next_observation(self):
        pass
    
    def _take_action(self, action):
        pass
        
    def render(self, mode = 'human', close=False):
        pass

from stable_baselines.common.vec_env import DummyVecEnv
from stable_baselines.common.policies import MlpPolicy
from stable_baselines2.ppo.ppo import PPO


envF = DummyVecEnv([lambda : envFru()])

model = PPOPolicy(envF, MlpPolicy, learning_rate= 0.001)
model.learn(total_timesteps=20000)

obs = env.reset()
for i in range(MAX_EPISODES):
    action, _states = model.predict(obs)
    obs, reward,done,info = env.step(action)
    #env.render()
</code></pre>
<p>The traceback I'm getting is the following:</p>
<pre><code>AttributeError                            Traceback (most recent call last)
&lt;ipython-input-124-550b8c75c26b&gt; in &lt;module&gt;
     12 envF = DummyVecEnv([lambda : envFruit()])
     13 
---&gt; 14 model = PPOPolicy(envF, MlpPolicy, learning_rate= 0.001)
     15 model.learn(total_timesteps=20000)
     16 

~\Desktop\ImitationLearning\stable_baselines2\ppo\policies.py in __init__(self, observation_space, action_space, learning_rate, net_arch, activation_fn, adam_epsilon, ortho_init, log_std_init)
     29                  ortho_init=True, log_std_init=0.0):
     30         super(PPOPolicy, self).__init__(observation_space, action_space)
---&gt; 31         self.obs_dim = self.observation_space.shape[0]
     32 
     33         # Default network architecture, from stable-baselines

AttributeError: 'DummyVecEnv' object has no attribute 'shape'
</code></pre>
",7890380,,,,,2021-01-27 12:29:31,AttributeError: 'DummyVecEnv' object has no attribute 'shape',<python><reinforcement-learning><openai-gym>,1,1,,,,CC BY-SA 4.0,
842,66030450,1,66034235,,2021-02-03 15:20:40,,1,233,"<p>How can I reward an agent to move forward in a game like super Mario bros? The only data I have is the score and lives but is there a way to get the coordinates of an agent? I'm using NEAT to train my agent here is the code. I am currently rewarding it to get the highest score possible and rewarding it for pressing the Right button won't work because it will just push into a wall and farm rewards until the timer runs out.</p>
<pre><code>import retro
import numpy as np
import cv2
import neat
import pickle

env = retro.make('SuperMarioWorld-Snes', 'Start.state')

imgarray = []

xpos_end = 0


def eval_genomes(genomes, config):
    for genome_id, genome in genomes:
        ob = env.reset()
        ac = env.action_space.sample()

        inx, iny, inc = env.observation_space.shape

        inx = int(inx / 8)
        iny = int(iny / 8)

        net = neat.nn.recurrent.RecurrentNetwork.create(genome, config)

        current_max_fitness = 0
        fitness_current = 0
        frame = 0
        counter = 0
        xpos = 0
        xpos_max = 0

        done = False
        # cv2.namedWindow(&quot;main&quot;, cv2.WINDOW_NORMAL)

        while not done:

            env.render()
            frame += 1
            # scaledimg = cv2.cvtColor(ob, cv2.COLOR_BGR2RGB)
            # scaledimg = cv2.resize(scaledimg, (iny, inx))
            ob = cv2.resize(ob, (inx, iny))
            ob = cv2.cvtColor(ob, cv2.COLOR_BGR2GRAY)
            ob = np.reshape(ob, (inx, iny))
            # cv2.imshow('main', scaledimg)
            # cv2.waitKey(1)

            imgarray = np.ndarray.flatten(ob)

            nnOutput = net.activate(imgarray)
            for i in  range(len(nnOutput)):
                nnOutput[i] = int(nnOutput[i])
                if nnOutput[i] &lt; 0:
                    nnOutput[i] = 0


            ob, rew, done, info = env.step(nnOutput)

            # xpos = info['x']
            # xpos_end = info['screen_x_end']

            # if xpos &gt; xpos_max:
            # fitness_current += 1
            # xpos_max = xpos

            # if xpos == xpos_end and xpos &gt; 500:
            # fitness_current += 100000
            # done = True




            fitness_current += rew
            print(env.statename)
            if fitness_current &gt; current_max_fitness:
                current_max_fitness = fitness_current
                counter = 0
            else:
                counter += 1

            if done or counter == 250:
                done = True
                print(genome_id, fitness_current)

            genome.fitness = fitness_current


config = neat.Config(neat.DefaultGenome, neat.DefaultReproduction,
                     neat.DefaultSpeciesSet, neat.DefaultStagnation,
                     'config.txt')

p = neat.Population(config)

p.add_reporter(neat.StdOutReporter(True))
stats = neat.StatisticsReporter()
p.add_reporter(stats)
p.add_reporter(neat.Checkpointer(10))

winner = p.run(eval_genomes)

with open('winner.pkl', 'wb') as output:
    pickle.dump(winner, output, 1)



</code></pre>
",12610218,,4685471,,2021-02-12 13:07:15,2021-02-12 13:07:15,gym retro reward for moving forward,<python><artificial-intelligence><reinforcement-learning><openai-gym>,1,3,,,,CC BY-SA 4.0,
850,51180689,1,51181803,,2018-07-04 20:59:39,,1,409,"<p>I am working with the OpenAI gym environment (using policy gradient). My network is outputting an action which is higher than the possible action range. </p>

<pre><code>n_outputs = 9
learning_rate = 0.01

initializer = tf.variance_scaling_initializer()

X = tf.placeholder(tf.float32, shape=[None, 50, 70, 1])
network = tflearn.conv_2d(X, 32, 5, strides=2, activation='relu')
network = tflearn.max_pool_2d(network, 2)
network = tflearn.conv_2d(network, 32, 5, strides=2, activation='relu')
network = tflearn.max_pool_2d(network, 2)
network = tflearn.fully_connected(network, 256, activation='relu')
hidden = tf.layers.dense(network, 64, activation=tf.nn.relu, kernel_initializer=initializer)
logits = tf.layers.dense(hidden, n_outputs)
outputs = tf.nn.softmax(logits)
action = tf.multinomial(outputs, num_samples=1)
</code></pre>

<p>It outputs 9, which creates an error in the gym environment. </p>

<p>The <a href=""https://gist.github.com/AhmetHamzaEmra/e5c8603de73d3a35eb617b10d754aaa2"" rel=""nofollow noreferrer"">full code</a>.</p>
",7187509,,3924118,,2018-07-04 22:13:25,2018-07-04 23:39:39,tf.multinomial outputs number other numbers than range,<tensorflow><deep-learning><reinforcement-learning><tflearn><openai-gym>,1,1,,,,CC BY-SA 4.0,
856,67690650,1,67692410,,2021-05-25 15:01:43,,-2,701,"<p>This isn't specifically about troubleshooting code, but with helping me understand the gym Environment. I am inheriting <code>gym.Env</code> to create my own environment, but I am have a difficult time understanding the flow. I look through the documentation, but there are still questions and concepts that are unclear.</p>
<ol>
<li><p>I am still a little foggy how the actually agent knows what action to control? I know when you <code>__init__</code> the class, you have to distinguish if your actions are discrete or Box, but how does the agent know what parameters in their control?</p>
</li>
<li><p>When determining the lower and upper limit for the <code>spaces.Box</code> command, that tells the agent how big of a step-size that can take? For example, if my limits are [-1,1] they can implement any size within that domain?</p>
</li>
<li><p>I saw that the limits can be <code>[a,b], (-oo,a], [b,oo), (-oo,oo)</code> for the limits, if need to have my observation space, I just use the <code>np.inf</code> command?</p>
</li>
</ol>
<p>If there any documentation that you would recommend, that would be much appreciate.</p>
",10909779,,,,,2021-05-25 16:53:58,Understanding Gym Environment,<python><reinforcement-learning><openai-gym>,1,1,,,,CC BY-SA 4.0,
859,51704944,1,51722127,,2018-08-06 09:55:57,,0,4225,"<p>I am trying to install gym_gazebo on my Ubuntu 16.04 LTS system according to <a href=""https://github.com/erlerobot/gym-gazebo"" rel=""nofollow noreferrer"">https://github.com/erlerobot/gym-gazebo</a></p>

<p>Everything is getting installed correctly, however, while trying to run <strong>python circuit2_turtlebot_lidar_qlearn.py</strong> , I get error as </p>

<p><code>ModuleNotFoundError: No module named 'std_msgs'.</code></p>

<p>This should be part of <strong>ros_comm</strong> project which I have installed but still I cannot see <strong>std_msgs</strong> related code file anywhere.</p>

<p>Can someone please point me to how can I install std_msgs, so that I can get rid of this error message?</p>

<p>Thanks in advance!</p>
",8647273,,8647273,,2018-08-07 04:54:13,2018-08-07 08:23:14,ModuleNotFoundError: No module named 'std_msgs' - Gazebo installation,<ros><reinforcement-learning><q-learning><openai-gym><rospy>,1,1,,,,CC BY-SA 4.0,
864,51978282,1,52175371,,2018-08-23 04:23:02,,5,8619,"<p>I know <code>env=gym.make('CartPole-v0')</code> is of type <code>gym.wrappers.time_limit.TimeLimit</code></p>

<p>And I also know env is an ""instance"" of the class cartpole.py. My question is how, by just giving the name 'CartPole-v0', I got the access to the cartpole.py class. Where is that process implemented? I was trying to look for it on the gym folder from the site-package folder but I couldn't find/understand where that process takes place. I'm not sure if my statements above are accurate, I'm asking this question to understand the process behind the execution of gym.make('CartPole-v0') and of any topic related to it in order to learn more about coding in general. My guess is that I am misunderstanding something</p>
",6130605,,6243352,,2018-08-23 04:58:21,2018-09-05 04:05:44,What does gym.make('CartPole-v0') return and how it does it work?,<python><openai-gym>,1,0,0,,,CC BY-SA 4.0,
865,52061122,1,53308763,,2018-08-28 15:08:37,,0,1501,"<p>I'm interested in modelling a system that can use openai gym to make a model that not only performs well but hopefully even better yet continuously improves to converge on the best moves.
This is how I initialize the env</p>

<pre><code>import gym
env = gym.make(""CartPole-v0"")
env.reset()
</code></pre>

<p>it returns a set of info; observation, reward, done and info, info always nothing so ignore that.</p>

<p>reward I'd hope would signify whether the action taken is good or bad but it always returns a reward of 1 until the game ends, it's more of a counter of how long you've been playing.</p>

<p>The action can be sampled by </p>

<pre><code>action = env.action_space.sample()
</code></pre>

<p>which in this case is either 1 or 0.
To put into perspective for anyone who doesn't know what this game is, here's the <a href=""https://gym.openai.com/envs/CartPole-v0/"" rel=""nofollow noreferrer"">link</a> and it's objective is to balance a pole by moving left or right i.e. provide an input of 0 or 1.</p>

<p>The observation is the only key way to tell whether you're making a good or bad move.</p>

<pre><code>obs, reward, done, info = env.step(action)
</code></pre>

<p>and the observation looks something like this</p>

<pre><code>array([-0.02861881,  0.02662095, -0.01234258,  0.03900408])
</code></pre>

<p>as I said before reward is always 1 so not a good pointer of good or bad move based on the observation and done means the game has come to an end though I also can't tell if it means you lost or won also.</p>

<p>Since the objective as you'll see from the link to the page is to balance the pole for a total reward of +195 averaged over 100 games that's the determining guide of a successful game, not sure then if you've successfully then balanced it completely or just lasted long but still, I've followed a few examples and suggestion to generate a lot of random games and those that do rank well use them to train a model.</p>

<p>But this way feels sketchy and not inherently aware of what a failing move is i.e. when you're about to tip the pole more than 15 degrees or the cart moves 2.4 units from the center.</p>

<p>I've been able to gather data from running the simulation for over 200000 times and using this also found I've got a good number of games that lasted for more than 80 steps. (the goal is 195) so using this I graphed these <a href=""https://github.com/Six-wars/data-science/blob/master/openai%20gym/cartpole%20data%20harvester.ipynb"" rel=""nofollow noreferrer"">games</a> (&lt; ipython notebook) there's a number of graphs and since I'm graphing each observation individually per game it's too many graphs to put here just to hopefully then maybe see a link between a final observation and the game ending since these are randomly sampled actions so it's random moves. </p>

<p>What I thought I saw was maybe for the first observation that if it gets to 0 the game ends but I've also seen some others where the game runs with negative values. I can't make sense of the data even with graphing basically.</p>

<p>What I really would like to know is if possible what each value in the observation means and also if 0 means left or right but the later would be easier to deduce when I can understand the first.</p>
",7399312,,,,,2018-11-14 21:10:05,OpenAI gym cartpole-v0 understanding observation and action relationship,<python><openai-gym>,1,0,,,,CC BY-SA 4.0,
867,51725932,1,51728405,,2018-08-07 11:40:35,,1,1515,"<p>I'm doing reinforcement learning, and I'm having trouble with performance.</p>

<p>Situation, no custom code:</p>

<ul>
<li>I loaded a Google Deep Learning VM (<a href=""https://console.cloud.google.com/marketplace/details/click-to-deploy-images/deeplearning"" rel=""nofollow noreferrer"">https://console.cloud.google.com/marketplace/details/click-to-deploy-images/deeplearning</a>) on Google Cloud. This comes with all the prerequisites installed (CUDA, cuDNN, drivers) with a NVidia K80 videocard.</li>
<li>Installed <a href=""https://github.com/keras-rl/keras-rl"" rel=""nofollow noreferrer"">keras-rl</a>, <a href=""https://gym.openai.com/"" rel=""nofollow noreferrer"">OpenAI gym</a></li>
<li>Now when I run the (standard) example <a href=""https://github.com/keras-rl/keras-rl/blob/master/examples/dqn_cartpole.py"" rel=""nofollow noreferrer"">dqn_cartpole.py</a> with visualize=False on line 46, it uses about 20% of my GPU, resulting in around 100 steps per second, which is around 3x SLOWER than using the CPU on my Razer Blade 15 (i7-8750H).</li>
<li>I have checked all the bottlenecks I can think of, CPU usage, memory and HD I/O is all normal.</li>
</ul>

<p>Please help!</p>

<p>Thanks in advance</p>
",2385490,,10191554,,2018-08-07 16:00:31,2018-08-07 16:00:31,"Tensorflow, OpenAI Gym, Keras-rl performance issue on basic reinforcement learning example",<python><tensorflow><reinforcement-learning><openai-gym><keras-rl>,1,0,,,,CC BY-SA 4.0,
868,68072941,1,68086057,,2021-06-21 18:22:36,,0,733,"<p>I tried to use the MultiInputPolicy by :</p>
<pre><code>model = PPO(&quot;MultiInputPolicy&quot;, env, verbose = 1)

</code></pre>
<p>But, I get an error:</p>
<pre><code>KeyError: &quot;Error: unknown policy type MultiInputPolicy,the only registed policy type are: ['MlpPolicy', 'CnnPolicy']!&quot;

</code></pre>
<p>Please help. How can I fix this?</p>
",12905187,,,,,2021-06-22 15:09:11,Stablebaselines MultiInputpolicies,<openai-gym><stable-baselines>,1,0,,,,CC BY-SA 4.0,
870,48980368,1,48989130,,2018-02-26 01:47:48,,20,15668,"<p>How to list all <strong>currently registered</strong> environment IDs (as they are used for creating environments) in openai gym? </p>

<p>A bit context: there are many plugins installed which have customary ids such as atari, super mario, doom etc.</p>

<p>Not to be confused with game names for atari-py.</p>
",3769589,,,,,2022-10-27 15:11:04,List all environment id in openai gym,<python><reinforcement-learning><openai-gym>,3,0,0,,,CC BY-SA 3.0,
871,67674324,1,67679481,,2021-05-24 14:58:50,,1,1493,"<p>I'm developing an Autonomous Agent based on DQN. I am using the gym library to make the environments that I want to test, but I'm stuck in processing the frames of the state. The state that the gym environment returns, using the FrameStack wrapper, has the following observation space:</p>
<pre><code>env = gym.make('Bowling-v0')
env = gym.wrappers.FrameStack(env, 4)
print(env.observation_space)

Box(0, 255, (4, 210, 160, 3), uint8)
</code></pre>
<p>I want the observation space to be <code>Box(0, 255, (4, 88, 80, 1), uint8)</code>. How can I do? I've tried to use another wrapper like this:</p>
<pre><code>env = gym.wrappers.ResizeObservation(env, (4, 88, 80, 1))
print(env.observation_space)
</code></pre>
<p>But the observation space resulting is <code>Box(0, 255, (4, 88, 80, 1, 160, 3), uint8)</code>. What am I doing wrong?</p>
",11463826,,,,,2021-05-24 22:09:02,Resize a state of a gym environment composed by 4 frames (atari environment),<python><wrapper><environment><openai-gym><atari-2600>,1,1,,,,CC BY-SA 4.0,
875,49035549,1,49035950,,2018-02-28 17:40:12,,1,525,"<p>I'm trying to implement the Deep Q Learning algorithm introduced by DeepMind in this paper:</p>

<p><a href=""https://arxiv.org/pdf/1312.5602.pdf"" rel=""nofollow noreferrer"">https://arxiv.org/pdf/1312.5602.pdf</a></p>

<p>I'm using it to make an agent that learns to play Pong, however it doesn't seem to work (even after 2 hours of training I'm not seeing any improvement). This is the code,</p>

<pre><code>import gym
import universe
import numpy as np
import keras
from keras.models import Sequential
from keras.layers import Dense, Conv2D, MaxPooling2D, Flatten, Activation
from keras.models import load_model

import random

env = gym.make('gym-core.Pong-v0')
env.configure(remotes=1)


def num2str(number, obs):
    number = np.argmax(number)
    if number == 0:
        action = [[('KeyEvent', 'ArrowRight', False), ('KeyEvent', 'ArrowLeft', True)] for ob in obs]
    elif number == 1:
        action = [[('KeyEvent', 'ArrowLeft', False), ('KeyEvent', 'ArrowRight', True)] for ob in obs]

    return action


def preprocess(original_obs):
    obs = original_obs
    obs = np.array(obs)[0]['vision']
    obs = np.delete(obs, np.s_[195:769], axis=0)
    obs = np.delete(obs, np.s_[0:35], axis=0)
    obs = np.delete(obs, np.s_[160:1025], axis=1)
    obs = np.mean(obs, axis=2)
    obs = obs[::2,::2]
    obs = np.reshape(obs, (80, 80, 1))
    return obs



model = Sequential()
model.add(Conv2D(32, kernel_size = (8, 8), strides = (4, 4), border_mode='same', activation='relu', init='uniform', input_shape = (80, 80, 4)))
model.add(MaxPooling2D(pool_size = (2, 2)))
model.add(Conv2D(64, kernel_size = (2, 2), strides = (2, 2)))

model.add(Conv2D(64, kernel_size = (3, 3), strides = (1, 1)))

model.add(Flatten())
model.add(Dense(256, init='uniform', activation='relu'))
model.add(Dense(2, init='uniform', activation='linear'))

model.compile(loss='mse', optimizer='adam', metrics=['accuracy'])


init_observe_time = 500

D = []

e = 1.0
e_threshold = 0.05
e_decay = 0.01

gamma = 0.99

batch_size = 15
frequency = 10

Q_values = np.array([0, 0])

obs = env.reset()

while True:
    obs = env.step(num2str(np.array([random.randint(0, 1) for i in range(0, 2)]), obs))[0]
    if obs != [None]:
        break

x_t1 = preprocess(obs)
s_t1 = np.stack((x_t1, x_t1, x_t1, x_t1), axis = 2)
s_t1 = np.reshape(s_t1, (80, 80, 4))

t = 0
while True:

    print(""Time since last start: "", t)

    a_t = np.zeros(2)

    if random.random() &lt; e:
        a_index = random.randint(0, 1)
        a_t[a_index] = 1
    else:
        Q_values = model.predict(np.array([s_t1]))[0]
        a_index = np.argmax(Q_values)
        a_t[a_index] = 1

    print(""Q Values: "", Q_values)
    print(""action taken: "", np.argmax(a_t))
    print(""epsilon: "", e)

    if e &gt; e_threshold:
        e -= e_decay

    obs, r_t, done, info = env.step(num2str(a_t, obs))

    if obs == [None]:
        continue

    x_t2 = preprocess(obs)
    print(x_t2.shape, s_t1[:,:,0:3].shape)
    s_t2 = np.append(x_t2, s_t1[:,:,0:3], axis = 2)

    D.append((s_t1, a_t, r_t, s_t2, done))

    if t &gt; init_observe_time and t%frequency == 0:
        minibatch = random.sample(D, batch_size)

        s1_batch = [i[0] for i in minibatch]
        a_batch = [i[1] for i in minibatch]
        r_batch = [i[2] for i in minibatch]
        s2_batch = [i[3] for i in minibatch]

        q_batch = model.predict(np.array(s2_batch))
        y_batch = np.zeros((batch_size, 2))
        y_batch = model.predict(np.array(s1_batch))
        print(""Q batch: "",  q_batch)
        print(""y batch: "",  y_batch)
        for i in range(0, batch_size):
            if (minibatch[i][4]):
                y_batch[i][np.argmax(a_batch[i])] = r_batch[i][0]
            else:
                y_batch[i][np.argmax(a_batch[i])] = r_batch[i][0] + gamma * np.max(q_batch[i])


        model.train_on_batch(np.array(s1_batch), y_batch)
    s_t1 = s_t2

    t += 1
    env.render()
</code></pre>

<p>does anyone have any suggestion on how to make it work properly?</p>
",4807022,,,,,2018-02-28 18:07:51,How to implement DQN algorithm correctly,<machine-learning><deep-learning><artificial-intelligence><reinforcement-learning><openai-gym>,1,0,0,,,CC BY-SA 3.0,
876,66363245,1,66363576,,2021-02-25 06:08:49,,0,249,"<p><a href=""https://i.stack.imgur.com/oghiC.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/oghiC.png"" alt=""enter image description here"" /></a></p>
<p>I know that the Pong Game initializes to new game when one side scores 20 points.</p>
<p>By the way, the reward shows that it goes down below -20.<br>
Why is that so?</p>
<p>One thing to expect is that after one side gets 20 points, the game is reset by playing one more time. Does the game need to get 21 points to initialize?</p>
<p>(Use 8 workers, A2C, PongNoFrameskip-v4)</p>
",13514723,,,,,2021-02-25 06:41:12,Reward of Pong game - (OpenAI gym),<python><pytorch><reinforcement-learning><openai-gym><reward>,1,0,,,,CC BY-SA 4.0,
878,66418231,1,66433159,,2021-03-01 07:39:03,,2,1580,"<p>Below is a high level diagram of how my Agent should look like in order to be able to interact with a custom gym environment I made.</p>
<p><a href=""https://i.stack.imgur.com/L0OQI.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/L0OQI.png"" alt=""enter image description here"" /></a></p>
<h3>States and actions</h3>
<p>The environment has three states [s1, s2, s3] and six actions [a1, a2, a3, a4, a5, a6]
states and actions can be any value between 0 and 1</p>
<h3>Question:</h3>
<p>Which algorithms are suitable for my problem ? I am aware that there are algorithms that are good at handling continuous action space like (DDPG, PPO, etc.) but I can't see how they might operate when they should output multiple actions at each time-step.
Finally, are there any gym environments that have the described property (multiple actions) and are there any python implementations for solving those particular environments?</p>
",11512643,,8421999,,2021-03-02 07:15:17,2021-03-02 07:15:17,"Deep reinforcement learning with multiple ""continuous actions""",<python-3.x><reinforcement-learning><openai-gym>,1,0,0,,,CC BY-SA 4.0,
880,52040325,1,52229881,,2018-08-27 13:18:38,,2,7547,"<p>I'm currently reading <i>Hands-On Reinforcement Learning with Python</i> by Sudharsan Ravichandiran and on one of the first examples I run into this AttributeError:</p>

<p><code>AttributeError 'TimeLimit' object has no attribute 'P'</code> </p>

<p>raised by the following line:</p>

<pre><code>for next_sr in env.P[state][action]: 
</code></pre>

<p>I can't find any documentation regarding <i>env.P</i>, but I found a similar example written in python2 here: <a href=""https://gym.openai.com/evaluations/eval_48sirBRSRAapMjotYzjb6w/"" rel=""nofollow noreferrer"">https://gym.openai.com/evaluations/eval_48sirBRSRAapMjotYzjb6w/</a></p>

<p>I suppose <i>env.P</i> is part of an outdated library (even if the the book was published on June 2018 and the incriminated code is in python3), so how can i replace it?</p>
",9476159,,,,,2019-01-20 19:25:01,"openai gym env.P, AttributeError 'TimeLimit' object has no attribute 'P'",<python><python-3.x><reinforcement-learning><openai-gym>,3,0,,,,CC BY-SA 4.0,
881,52727233,1,52739974,,2018-10-09 18:28:18,,13,22845,"<p>I have created a custom environment, as per the OpenAI Gym framework; containing <code>step</code>, <code>reset</code>, <code>action</code>, and <code>reward</code> functions. I aim to run OpenAI baselines on this custom environment. But prior to this, the environment has to be registered on OpenAI gym. I would like to know how the custom environment could be registered on OpenAI gym? Also, Should I be modifying the OpenAI baseline codes to incorporate this?</p>
",6782650,,3924118,,2019-08-24 13:55:36,2022-02-21 21:01:37,How can I register a custom environment in OpenAI's gym?,<reinforcement-learning><openai-gym>,3,0,0,,,CC BY-SA 4.0,
883,67000544,1,67003580,,2021-04-08 08:54:43,,0,3804,"<p>I am trying to make a custom Gym Environment so that I can use it in a Keras Network. But there is a problem that is happening to me when I try to fit de neural network.</p>
<pre><code>ValueError: Error when checking input: expected dense_6_input to have 2 dimensions, but got array with shape (1, 1, 15)
</code></pre>
<p>What I understand about this problem is that the states (the inputs that the network receives) are structured as a 3 dimensional array, but I donÂ´t know why.</p>
<p>Here is my init method in the class that defines the environment:</p>
<pre><code>def __init__ (self):
    self.action_space = Discrete (4)
    self.observation_space = Box(low=0, high=100, shape=(15,))
    self.state = np.array([1,2,0,3,2,0,4,0,0,1,3,0,0,0,0], dtype=float)
    #self.state = state
    self.length = 15
    self.index = 0
</code></pre>
<p>After, i initialize two variables that save the shape of the states and the actions, so we can define the model.</p>
<pre><code>states = env.observation_space.shape
actions = env.action_space.n

def build_model(states, actions):
model = Sequential()    
model.add(Dense(24, activation='relu', input_shape=states))
model.add(Dense(24, activation='relu'))
model.add(Dense(actions, activation='linear'))
return model
</code></pre>
<p>The summary of the model:</p>
<p>Layer (type)                 Output Shape              Param #<br />
dense_6 (Dense)              (None, 24)                384<br />
dense_7 (Dense)              (None, 24)                600<br />
dense_8 (Dense)              (None, 4)                 100</p>
<p>The last step before the error is when i buid the agent. After that, we call the fit method and occurs the problem.</p>
<pre><code>def build_agent(model, actions):
   policy = BoltzmannQPolicy()
   memory = SequentialMemory(limit=50000, window_length=1)
   dqn = DQNAgent(model=model, memory=memory, policy=policy, 
              nb_actions=actions, nb_steps_warmup=10, target_model_update=1e-2)
   return dqn

dqn = build_agent(model, actions)
dqn.compile(Adam(lr=1e-3), metrics=['mae'])
dqn.fit(env, nb_steps=50000, visualize=False, verbose=1)
</code></pre>
<p>I tried to change the input_shape of the first layer to be (1, 1, 15) but doesnÂ´t seems to work. Maybe the problem is related with the definition of the environment (observation space) or how the environments provides the information to the network. I donÂ´t know...</p>
<p>I hope you can help me. let me know if you need some more information to handle the error.</p>
<p>Thank you so much!</p>
",15576485,,,,,2021-04-08 12:00:54,"ValueError: Error when checking input: expected dense_input to have 2 dimensions, but got array with shape (1, 1, 15)",<python><tensorflow><keras><neural-network><openai-gym>,1,4,0,,,CC BY-SA 4.0,
889,49621599,1,49623000,,2018-04-03 03:07:03,,3,1092,"<p>I am learning how to use Gym environments to train deep learning models built with TFLearn. </p>

<p>At the moment my array of observations has the following shape: <code>(210, 160, 3)</code></p>

<p>Any recommendations on what is the best way to reshape this array so it can be used in a TensorFlow classification model?</p>
",7733971,,,,,2020-12-18 11:26:46,Reshaping an Gym array for TensorFlow,<python><tensorflow><reinforcement-learning><tflearn><openai-gym>,2,0,,,,CC BY-SA 3.0,
890,52173787,1,52174002,,2018-09-04 20:47:14,,5,1703,"<p>I am trying to use the retro module and I jupyter notebooks I seemed to install it with a <code>!pip install retro</code> where it went thru the download/install ok.</p>

<p>But when I try to <code>import retro</code> I get an error</p>

<p>`</p>

<pre><code>Traceback (most recent call last):

  File ""/home/benbartling/anaconda3/lib/python3.6/site-packages/IPython/core/interactiveshell.py"", line 2963, in run_code
    exec(code_obj, self.user_global_ns, self.user_ns)

  File ""&lt;ipython-input-1-81066d3a41c5&gt;"", line 1, in &lt;module&gt;
    import retro

  File ""/home/benbartling/anaconda3/lib/python3.6/site-packages/retro/__init__.py"", line 206
    print ""%s: %s"" % (key, value)
                 ^
SyntaxError: invalid syntax
</code></pre>

<p>Would anyone have any ideas? Thank you</p>
",8372455,,4685471,,2018-09-04 20:57:54,2018-09-04 21:07:14,python openAI retro module,<python><anaconda><openai-gym>,1,0,0,,,CC BY-SA 4.0,
891,52240631,1,52262568,,2018-09-09 01:31:20,,2,1328,"<p>I am interested in implementing Q-learning (or some form of reinforcement learning) to find an optimal protocol. Currently, I have a function written in Python where I can take in the protocol or ""action"" and ""state"" and returns a new state and a ""reward"". However, I am having trouble finding a Python implementation of Q-learning that I can use in this situation (i.e. something that can learn the function as if it is a black box). I have looked at OpenAI gym but that would require writing a new environment. Would anyone know of a simpler package or script that I can adopt for this?</p>

<p>My code is of the form: </p>

<pre><code>def myModel (state, action, param1, param2):
    ...
    return (state, reward)
</code></pre>

<p>What I am looking for would be an algorithm of the form:</p>

<pre><code>def QLearning (state, reward):
    ...
    return (action)
</code></pre>

<p>And some way to keeping the actions that transition between states. If anyone has any idea where to look for this, I would be very excited!</p>
",3712008,,,,,2018-09-11 22:10:05,How to implement Q-learning to approximate an optimal control?,<python><reinforcement-learning><q-learning><openai-gym>,2,6,0,,,CC BY-SA 4.0,
898,67874282,1,67879834,,2021-06-07 15:15:58,,2,1234,"<p>I'm trying to solve cartpole from Gym. It turns out that the states are in double floating point precision whereas the pytorch by default creates model in single floating point precision.</p>
<pre><code>class QNetworkMLP(Module):
    def __init__(self,state_dim,num_actions):
        super(QNetworkMLP,self).__init__()
        self.l1 = Linear(state_dim,64)
        self.l2 = Linear(64,64)
        self.l3 = Linear(64,128)
        self.l4 = Linear(128,num_actions)
        self.relu = ReLU()
        self.lrelu = LeakyReLU()
    
    def forward(self,x) :
        x = self.lrelu(self.l1(x))
        x = self.lrelu(self.l2(x))
        x = self.lrelu(self.l3(x))
        x = self.l4(x)
        return x
</code></pre>
<p>I tried to convert it via</p>
<pre><code>model = QNetworkMLP(4,2).double()
</code></pre>
<p>But it still doesn't work I get the same error.</p>
<pre><code>File &quot;.\agent.py&quot;, line 117, in update_online_network
    predicted_Qval = self.online_network(states_batch).gather(1,actions_batch)
  File &quot;C:\Users\27abh\anaconda3\envs\gym\lib\site-packages\torch\nn\modules\module.py&quot;, line 722, in _call_impl
    result = self.forward(*input, **kwargs)
  File &quot;C:\Users\27abh\Desktop\OpenAI Gym\Cartpole\agent_model.py&quot;, line 16, in forward
    x = self.lrelu(self.l1(x))
  File &quot;C:\Users\27abh\anaconda3\envs\gym\lib\site-packages\torch\nn\modules\module.py&quot;, line 722, in _call_impl
    result = self.forward(*input, **kwargs)
  File &quot;C:\Users\27abh\anaconda3\envs\gym\lib\site-packages\torch\nn\modules\linear.py&quot;, line 91, in forward
    return F.linear(input, self.weight, self.bias)
  File &quot;C:\Users\27abh\anaconda3\envs\gym\lib\site-packages\torch\nn\functional.py&quot;, line 1674, in linear
    ret = torch.addmm(bias, input, weight.t())
RuntimeError: Expected object of scalar type Double but got scalar type Float for argument #2 'mat1' in call to _th_addmm
</code></pre>
",4726246,,,,,2021-07-09 23:29:26,Convert Pytorch Float Model into Double,<python><pytorch><openai-gym>,1,2,,,,CC BY-SA 4.0,
899,52452838,1,52452968,,2018-09-22 01:59:05,,0,933,"<p>I've strictly followed the <a href=""https://github.com/openai/gym#box2d"" rel=""nofollow noreferrer"">setup tutorial provided by gym</a> However, I receive an error when I run <code>pip install -e '.[all]'</code>
or <code>pip install -e '.[box2d]'</code></p>

<pre><code>  Running setup.py bdist_wheel for box2d-py ... error
  Complete output from command /home/aptx/anaconda3/envs/rl/bin/python -u -c ""import setuptools, tokenize;__file__='/tmp/pip-install-rx9l74ei/box2d-py/setup.py';f=getattr(tokenize, 'open', open)(__file__);code=f.read().replace('\r\n', '\n');f.close();exec(compile(code, __file__, 'exec'))"" bdist_wheel -d /tmp/pip-wheel-r9et1nxi --python-tag cp35:
  Using setuptools (version 39.1.0).
  running bdist_wheel
  running build
  running build_py
  creating build
  creating build/lib.linux-x86_64-3.5
  creating build/lib.linux-x86_64-3.5/Box2D
  copying library/Box2D/__init__.py -&gt; build/lib.linux-x86_64-3.5/Box2D
  copying library/Box2D/Box2D.py -&gt; build/lib.linux-x86_64-3.5/Box2D
  creating build/lib.linux-x86_64-3.5/Box2D/b2
  copying library/Box2D/b2/__init__.py -&gt; build/lib.linux-x86_64-3.5/Box2D/b2
  running build_ext
  building 'Box2D._Box2D' extension
  swigging Box2D/Box2D.i to Box2D/Box2D_wrap.cpp
  swig -python -c++ -IBox2D -small -O -includeall -ignoremissing -w201 -globals b2Globals -outdir library/Box2D -keyword -w511 -D_SWIG_KWARGS -o Box2D/Box2D_wrap.cpp Box2D/Box2D.i
  Box2D/Common/b2Math.h:67: Warning 302: Identifier 'b2Vec2' redefined by %extend (ignored),
  Box2D/Box2D_math.i:47: Warning 302: %extend definition of 'b2Vec2'.
  Box2D/Common/b2Math.h:158: Warning 302: Identifier 'b2Vec3' redefined by %extend (ignored),
  Box2D/Box2D_math.i:168: Warning 302: %extend definition of 'b2Vec3'.
  Box2D/Common/b2Math.h:197: Warning 302: Identifier 'b2Mat22' redefined by %extend (ignored),
  Box2D/Box2D_math.i:301: Warning 302: %extend definition of 'b2Mat22'.
  Box2D/Common/b2Math.h:271: Warning 302: Identifier 'b2Mat33' redefined by %extend (ignored),
  Box2D/Box2D_math.i:372: Warning 302: %extend definition of 'b2Mat33'.
  Box2D/Collision/b2DynamicTree.h:44: Warning 312: Nested union not currently supported (ignored).
  Box2D/Common/b2Settings.h:144: Warning 506: Can't wrap varargs with keyword arguments enabled
  Box2D/Common/b2Math.h:91: Warning 509: Overloaded method b2Vec2::operator ()(int32) effectively ignored,
  Box2D/Common/b2Math.h:85: Warning 509: as it is shadowed by b2Vec2::operator ()(int32) const.
  creating build/temp.linux-x86_64-3.5
  creating build/temp.linux-x86_64-3.5/Box2D
  creating build/temp.linux-x86_64-3.5/Box2D/Dynamics
  creating build/temp.linux-x86_64-3.5/Box2D/Dynamics/Contacts
  creating build/temp.linux-x86_64-3.5/Box2D/Dynamics/Joints
  creating build/temp.linux-x86_64-3.5/Box2D/Common
  creating build/temp.linux-x86_64-3.5/Box2D/Collision
  creating build/temp.linux-x86_64-3.5/Box2D/Collision/Shapes
  gcc -pthread -B /home/aptx/anaconda3/envs/rl/compiler_compat -Wsign-compare -DNDEBUG -g -fwrapv -O3 -Wall -Wstrict-prototypes -fPIC -I/home/aptx/anaconda3/envs/rl/include/python3.5m -c Box2D/Box2D_wrap.cpp -o build/temp.linux-x86_64-3.5/Box2D/Box2D_wrap.o -I. -Wno-unused
  cc1plus: warning: command line option â€˜-Wstrict-prototypesâ€™ is valid for C/ObjC but not for C++ [enabled by default]
  gcc -pthread -B /home/aptx/anaconda3/envs/rl/compiler_compat -Wsign-compare -DNDEBUG -g -fwrapv -O3 -Wall -Wstrict-prototypes -fPIC -I/home/aptx/anaconda3/envs/rl/include/python3.5m -c Box2D/Dynamics/b2WorldCallbacks.cpp -o build/temp.linux-x86_64-3.5/Box2D/Dynamics/b2WorldCallbacks.o -I. -Wno-unused
  cc1plus: warning: command line option â€˜-Wstrict-prototypesâ€™ is valid for C/ObjC but not for C++ [enabled by default]
  cc1plus: warning: command line option â€˜-Wstrict-prototypesâ€™ is valid for C/ObjC but not for C++ [enabled by default]
  gcc -pthread -B /home/aptx/anaconda3/envs/rl/compiler_compat -Wsign-compare -DNDEBUG -g -fwrapv -O3 -Wall -Wstrict-prototypes -fPIC -I/home/aptx/anaconda3/envs/rl/include/python3.5m -c Box2D/Dynamics/Contacts/b2ContactSolver.cpp -o build/temp.linux-x86_64-3.5/Box2D/Dynamics/Contacts/b2ContactSolver.o -I. -Wno-unused
  cc1plus: warning: command line option â€˜-Wstrict-prototypesâ€™ is valid for C/ObjC but not for C++ [enabled by default]
  Box2D/Dynamics/Contacts/b2ContactSolver.cpp: In member function â€˜bool b2ContactSolver::SolvePositionConstraints()â€™:
  Box2D/Dynamics/Contacts/b2ContactSolver.cpp:713:51: warning: â€˜psm.b2PositionSolverManifold::separationâ€™ may be used uninitialized in this function [-Wmaybe-uninitialized]
      float32 C = b2Clamp(b2_baumgarte * (separation + b2_linearSlop), -b2_maxLinearCorrection, 0.0f);

  cc1plus: warning: command line option â€˜-Wstrict-prototypesâ€™ is valid for C/ObjC but not for C++ [enabled by default]
  gcc -pthread -B /home/aptx/anaconda3/envs/rl/compiler_compat -Wsign-compare -DNDEBUG -g -fwrapv -O3 -Wall -Wstrict-prototypes -fPIC -I/home/aptx/anaconda3/envs/rl/include/python3.5m -c Box2D/Collision/Shapes/b2EdgeShape.cpp -o build/temp.linux-x86_64-3.5/Box2D/Collision/Shapes/b2EdgeShape.o -I. -Wno-unused
  cc1plus: warning: command line option â€˜-Wstrict-prototypesâ€™ is valid for C/ObjC but not for C++ [enabled by default]
  g++ -pthread -shared -L/home/aptx/anaconda3/envs/rl/lib -B /home/aptx/anaconda3/envs/rl/compiler_compat -Wl,-rpath=/home/aptx/anaconda3/envs/rl/lib,--no-as-needed build/temp.linux-x86_64-3.5/Box2D/Box2D_wrap.o build/temp.linux-x86_64-3.5/Box2D/Dynamics/b2WorldCallbacks.o build/temp.linux-x86_64-3.5/Box2D/Dynamics/b2Island.o build/temp.linux-x86_64-3.5/Box2D/Dynamics/b2Fixture.o build/temp.linux-x86_64-3.5/Box2D/Dynamics/b2ContactManager.o build/temp.linux-x86_64-3.5/Box2D/Dynamics/b2World.o build/temp.linux-x86_64-3.5/Box2D/Dynamics/b2Body.o build/temp.linux-x86_64-3.5/Box2D/Dynamics/Contacts/b2CircleContact.o build/temp.linux-x86_64-3.5/Box2D/Dynamics/Contacts/b2EdgeAndCircleContact.o build/temp.linux-x86_64-3.5/Box2D/Dynamics/Contacts/b2PolygonContact.o build/temp.linux-x86_64-3.5/Box2D/Dynamics/Contacts/b2PolygonAndCircleContact.o build/temp.linux-x86_64-3.5/Box2D/Dynamics/Contacts/b2ChainAndCircleContact.o build/temp.linux-x86_64-3.5/Box2D/Dynamics/Contacts/b2ChainAndPolygonContact.o build/temp.linux-x86_64-3.5/Box2D/Dynamics/Contacts/b2EdgeAndPolygonContact.o build/temp.linux-x86_64-3.5/Box2D/Dynamics/Contacts/b2Contact.o build/temp.linux-x86_64-3.5/Box2D/Dynamics/Contacts/b2ContactSolver.o build/temp.linux-x86_64-3.5/Box2D/Dynamics/Joints/b2GearJoint.o build/temp.linux-x86_64-3.5/Box2D/Dynamics/Joints/b2PulleyJoint.o build/temp.linux-x86_64-3.5/Box2D/Dynamics/Joints/b2FrictionJoint.o build/temp.linux-x86_64-3.5/Box2D/Dynamics/Joints/b2MouseJoint.o build/temp.linux-x86_64-3.5/Box2D/Dynamics/Joints/b2DistanceJoint.o build/temp.linux-x86_64-3.5/Box2D/Dynamics/Joints/b2RevoluteJoint.o build/temp.linux-x86_64-3.5/Box2D/Dynamics/Joints/b2MotorJoint.o build/temp.linux-x86_64-3.5/Box2D/Dynamics/Joints/b2PrismaticJoint.o build/temp.linux-x86_64-3.5/Box2D/Dynamics/Joints/b2WeldJoint.o build/temp.linux-x86_64-3.5/Box2D/Dynamics/Joints/b2Joint.o build/temp.linux-x86_64-3.5/Box2D/Dynamics/Joints/b2WheelJoint.o build/temp.linux-x86_64-3.5/Box2D/Dynamics/Joints/b2RopeJoint.o build/temp.linux-x86_64-3.5/Box2D/Common/b2StackAllocator.o build/temp.linux-x86_64-3.5/Box2D/Common/b2BlockAllocator.o build/temp.linux-x86_64-3.5/Box2D/Common/b2Timer.o build/temp.linux-x86_64-3.5/Box2D/Common/b2Math.o build/temp.linux-x86_64-3.5/Box2D/Common/b2Settings.o build/temp.linux-x86_64-3.5/Box2D/Common/b2Draw.o build/temp.linux-x86_64-3.5/Box2D/Collision/b2BroadPhase.o build/temp.linux-x86_64-3.5/Box2D/Collision/b2CollideCircle.o build/temp.linux-x86_64-3.5/Box2D/Collision/b2TimeOfImpact.o build/temp.linux-x86_64-3.5/Box2D/Collision/b2Collision.o build/temp.linux-x86_64-3.5/Box2D/Collision/b2CollidePolygon.o build/temp.linux-x86_64-3.5/Box2D/Collision/b2Distance.o build/temp.linux-x86_64-3.5/Box2D/Collision/b2DynamicTree.o build/temp.linux-x86_64-3.5/Box2D/Collision/b2CollideEdge.o build/temp.linux-x86_64-3.5/Box2D/Collision/Shapes/b2ChainShape.o build/temp.linux-x86_64-3.5/Box2D/Collision/Shapes/b2CircleShape.o build/temp.linux-x86_64-3.5/Box2D/Collision/Shapes/b2PolygonShape.o build/temp.linux-x86_64-3.5/Box2D/Collision/Shapes/b2EdgeShape.o -o build/lib.linux-x86_64-3.5/Box2D/_Box2D.cpython-35m-x86_64-linux-gnu.so
  /home/aptx/anaconda3/envs/rl/compiler_compat/ld: cannot find -lm
  /home/aptx/anaconda3/envs/rl/compiler_compat/ld: cannot find -lpthread
  /home/aptx/anaconda3/envs/rl/compiler_compat/ld: cannot find -lc
  collect2: error: ld returned 1 exit status
  error: command 'g++' failed with exit status 1

  ----------------------------------------
  Failed building wheel for box2d-py
</code></pre>

<p>I omit some warning messages for briefness. I've done a lot of search, but still nothing works. I hope someone could help me figure it out. Thanks in advance.</p>
",7850499,,,,,2018-09-22 02:24:00,Failed building wheel for box2d.py on Ubuntu 16.04,<python><box2d><openai-gym>,1,0,,,,CC BY-SA 4.0,
900,67905871,1,67970195,,2021-06-09 14:10:55,,0,80,"<p>I am working with OpenAI-Gym enviroment CarRacing-v0 which is based on Box2D Physics library.
Through env.car.hull I have access to position, angle, velocities but not in acceleration.</p>
<p>Searching inside Box2D I found (but not sure) the attributes showed below and I assume that there is not direct way to have the acceleration of the car</p>
<pre><code>'b2BodyDef': ['active', 'allowSleep', 'angle', 'angularDamping', 'angularVelocity', 
                             'awake', 'bullet', 'fixedRotation', 'fixtures', 
                             'inertiaScale', 'linearDamping', 'linearVelocity', 'position', 
                             'shapeFixture', 'shapes', 'type', 'userData', 
                             ],
</code></pre>
<p>Currently using integration of velocity to get acceleration but there must be better way through Box2D.
Can someone with experience in the specific library and environment help me out. Thanks</p>
",15393740,,,,,2021-06-14 12:26:05,Can I acquire acceleration measure directly from a Box2D body? (Python) (Box2D),<python><box2d><openai-gym>,1,0,,,,CC BY-SA 4.0,
902,52280897,1,55246234,,2018-09-11 17:04:24,,1,561,"<p>I'm currently using code from OpenAI baselines to train a model, using the following code in my <code>train.py</code>:</p>

<pre><code>from baselines.common import tf_util as U
import tensorflow as tf
import gym, logging

from visak_dartdeepmimic import VisakDartDeepMimicArgParse

def train(env, initial_params_path,
        save_interval, out_prefix, num_timesteps, num_cpus):
    from baselines.ppo1 import mlp_policy, pposgd_simple
    sess = U.make_session(num_cpu=num_cpus).__enter__()

    U.initialize()

    def policy_fn(name, ob_space, ac_space):
        print(""Policy with name: "", name)
        policy = mlp_policy.MlpPolicy(name=name, ob_space=ob_space, ac_space=ac_space,
            hid_size=64, num_hid_layers=2)
        saver = tf.train.Saver()
        if initial_params_path is not None:
            print(""Tried to restore from "", initial_params_path)
            saver.restore(tf.get_default_session(), initial_params_path)
        return policy

    def callback_fn(local_vars, global_vars):
        iters = local_vars[""iters_so_far""]
        saver = tf.train.Saver()
        if iters % save_interval == 0:
            saver.save(sess, out_prefix + str(iters))

    pposgd_simple.learn(env, policy_fn,
            max_timesteps=num_timesteps,
            callback=callback_fn,
            timesteps_per_actorbatch=2048,
            clip_param=0.2, entcoeff=0.0,
            optim_epochs=10, optim_stepsize=3e-4, optim_batchsize=64,
            gamma=1.0, lam=0.95, schedule='linear',
        )
    env.close()
</code></pre>

<p>Which is based off of the code that OpenAI itself provides <a href=""https://github.com/openai/baselines/blob/master/baselines/ppo1/run_humanoid.py"" rel=""nofollow noreferrer"">in the baselines repository</a></p>

<p>This works fine, except that I get some pretty weird looking learning curves which I suspect are due to some hyperparameters passed to the <code>learn</code> function which cause performance to decay / high variance as things go on (though I don't know for certain) </p>

<p><a href=""https://i.stack.imgur.com/MYtLo.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/MYtLo.png"" alt=""enter image description here""></a></p>

<p>Anyways, to confirm this hypothesis I'd like to retrain the model but not from scratch: I'd like to start it off from a high point: say, iteration 1600 for which I have a saved model lying around (having saved it with <code>saver.save</code> in <code>callback_fn</code></p>

<p>So now I call the <code>train</code> function, but this time I provide it with an <code>inital_params_path</code> pointing to the save prefix for iteration 1600. By my understanding, the call to <code>saver.restore</code> in <code>policy_fn</code> should restore ""reset"" the model to where it was at 1teration 1600 (and I've confirmed that the load routine runs using the print statement)</p>

<p>However, in practice I find that it's almost like nothing gets loaded. For instance, if I got statistics like </p>

<pre><code>----------------------------------
| EpLenMean       | 74.2         |
| EpRewMean       | 38.7         |
| EpThisIter      | 209          |
| EpisodesSoFar   | 662438       |
| TimeElapsed     | 2.15e+04     |
| TimestepsSoFar  | 26230266     |
| ev_tdlam_before | 0.95         |
| loss_ent        | 2.7640965    |
| loss_kl         | 0.09064759   |
| loss_pol_entpen | 0.0          |
| loss_pol_surr   | -0.048767302 |
| loss_vf_loss    | 3.8620138    |
----------------------------------
</code></pre>

<p>for iteration 1600, then for iteration 1 of the new trial (ostensibly using 1600's parameters as a starting point), I get something like</p>

<pre><code>----------------------------------
| EpLenMean       | 2.12         |
| EpRewMean       | 0.486        |
| EpThisIter      | 7676         |
| EpisodesSoFar   | 7676         |
| TimeElapsed     | 12.3         |
| TimestepsSoFar  | 16381        |
| ev_tdlam_before | -4.47        |
| loss_ent        | 45.355236    |
| loss_kl         | 0.016298374  |
| loss_pol_entpen | 0.0          |
| loss_pol_surr   | -0.039200217 |
| loss_vf_loss    | 0.043219414  |
----------------------------------
</code></pre>

<p>which is back to square one (this is around where my models trained from scratch start)</p>

<p>The funny thing is I know that the model is being saved properly at least, since I can actually replay it using <code>eval.py</code></p>

<pre><code>from baselines.common import tf_util as U
from baselines.ppo1 import mlp_policy, pposgd_simple
import numpy as np
import tensorflow as tf

class PolicyLoaderAgent(object):
    """"""The world's simplest agent!""""""
    def __init__(self, param_path, obs_space, action_space):
        self.action_space = action_space

        self.actor = mlp_policy.MlpPolicy(""pi"", obs_space, action_space,
                                        hid_size = 64, num_hid_layers=2)
        U.initialize()
        saver = tf.train.Saver()
        saver.restore(tf.get_default_session(), param_path)

    def act(self, observation, reward, done):
        action2, unknown = self.actor.act(False, observation)
        return action2


if __name__ == ""__main__"":

    parser = VisakDartDeepMimicArgParse()
    parser.add_argument(""--params-prefix"", required=True, type=str)
    args = parser.parse_args()
    env = parser.get_env()

    U.make_session(num_cpu=1).__enter__()

    U.initialize()

    agent = PolicyLoaderAgent(args.params_prefix, env.observation_space, env.action_space)

    while True:
        ob = env.reset(0, pos_stdv=0, vel_stdv=0)
        done = False
        while not done:
            action = agent.act(ob, reward, done)
            ob, reward, done, _ = env.step(action)
            env.render()
</code></pre>

<p>and I can clearly see that its learned something as compared to an untrained baseline. The loading action is the same across both files (or rather, if there's a mistake there then I can't find it), so it appears probable to me that <code>train.py</code> is correctly loading the model and then, due to something in the <a href=""https://github.com/openai/baselines/blob/master/baselines/ppo1/pposgd_simple.py"" rel=""nofollow noreferrer""><code>pposdg_simple.learn</code> function's</a>, promptly forgets about it. </p>

<p>Could anyone shed some light on this situation?</p>
",1543167,,,,,2019-03-20 11:44:50,Unable to use saved model as starting point for training Baselines' MlpPolicy?,<python><tensorflow><restore><reinforcement-learning><openai-gym>,1,0,,,,CC BY-SA 4.0,
903,52774793,1,52774794,,2018-10-12 07:50:57,,8,3028,"<p>Given:</p>

<pre><code>import gym
env = gym.make('CartPole-v0')
</code></pre>

<p>How do I get <code>CartPole-v0</code> in a way that works across any Gym env?</p>
",2854041,,,,,2018-10-12 16:21:00,Get name / id of a OpenAI Gym environment,<reinforcement-learning><openai-gym>,1,0,,,,CC BY-SA 4.0,
904,70858340,1,71030321,,2022-01-26 03:23:59,,2,672,"<p>Environment:</p>
<ul>
<li>Python: 3.9</li>
<li>OS: Windows 10</li>
</ul>
<p>When I try to create the ten armed bandits environment using the following code the error is thrown not sure of the reason.</p>
<pre><code>import gym
import gym_armed_bandits

env = gym.make('ten-armed-bandits-v0')
</code></pre>
<p>The error:</p>
<pre><code>---------------------------------------------------------------------------
KeyError                                  Traceback (most recent call last)
File D:\00_PythonEnvironments\01_RL\lib\site-packages\gym\envs\registration.py:158, in EnvRegistry.spec(self, path)
    157 try:
--&gt; 158     return self.env_specs[id]
    159 except KeyError:
    160     # Parse the env name and check to see if it matches the non-version
    161     # part of a valid env (could also check the exact number here)

KeyError: 'ten-armed-bandits-v0'

During handling of the above exception, another exception occurred:

UnregisteredEnv                           Traceback (most recent call last)
Input In [6], in &lt;module&gt;
----&gt; 1 env = gym.make('ten-armed-bandits-v0')

File D:\00_PythonEnvironments\01_RL\lib\site-packages\gym\envs\registration.py:235, in make(id, **kwargs)
    234 def make(id, **kwargs):
--&gt; 235     return registry.make(id, **kwargs)

File D:\00_PythonEnvironments\01_RL\lib\site-packages\gym\envs\registration.py:128, in EnvRegistry.make(self, path, **kwargs)
    126 else:
    127     logger.info(&quot;Making new env: %s&quot;, path)
--&gt; 128 spec = self.spec(path)
    129 env = spec.make(**kwargs)
    130 return env

File D:\00_PythonEnvironments\01_RL\lib\site-packages\gym\envs\registration.py:203, in EnvRegistry.spec(self, path)
    197     raise error.UnregisteredEnv(
    198         &quot;Toytext environment {} has been moved out of Gym. Install it via `pip install gym-legacy-toytext` and add `import gym_toytext` before using it.&quot;.format(
    199             id
    200         )
    201     )
    202 else:
--&gt; 203     raise error.UnregisteredEnv(&quot;No registered env with id: {}&quot;.format(id))

UnregisteredEnv: No registered env with id: ten-armed-bandits-v0
</code></pre>
<p>When I check the environments available, I am able to see it there.</p>
<pre><code>from gym import envs
print(envs.registry.all())

dict_values([EnvSpec(CartPole-v0), EnvSpec(CartPole-v1), EnvSpec(MountainCar-v0), EnvSpec(MountainCarContinuous-v0), EnvSpec(Pendulum-v1), EnvSpec(Acrobot-v1), EnvSpec(LunarLander-v2), EnvSpec(LunarLanderContinuous-v2), EnvSpec(BipedalWalker-v3), EnvSpec(BipedalWalkerHardcore-v3), EnvSpec(CarRacing-v0), EnvSpec(Blackjack-v1), EnvSpec(FrozenLake-v1), EnvSpec(FrozenLake8x8-v1), EnvSpec(CliffWalking-v0), EnvSpec(Taxi-v3), EnvSpec(Reacher-v2), EnvSpec(Pusher-v2), EnvSpec(Thrower-v2), EnvSpec(Striker-v2), EnvSpec(InvertedPendulum-v2), EnvSpec(InvertedDoublePendulum-v2), EnvSpec(HalfCheetah-v2), EnvSpec(HalfCheetah-v3), EnvSpec(Hopper-v2), EnvSpec(Hopper-v3), EnvSpec(Swimmer-v2), EnvSpec(Swimmer-v3), EnvSpec(Walker2d-v2), EnvSpec(Walker2d-v3), EnvSpec(Ant-v2), EnvSpec(Ant-v3), EnvSpec(Humanoid-v2), EnvSpec(Humanoid-v3), EnvSpec(HumanoidStandup-v2), EnvSpec(FetchSlide-v1), EnvSpec(FetchPickAndPlace-v1), EnvSpec(FetchReach-v1), EnvSpec(FetchPush-v1), EnvSpec(HandReach-v0), EnvSpec(HandManipulateBlockRotateZ-v0), EnvSpec(HandManipulateBlockRotateZTouchSensors-v0), EnvSpec(HandManipulateBlockRotateZTouchSensors-v1), EnvSpec(HandManipulateBlockRotateParallel-v0), EnvSpec(HandManipulateBlockRotateParallelTouchSensors-v0), EnvSpec(HandManipulateBlockRotateParallelTouchSensors-v1), EnvSpec(HandManipulateBlockRotateXYZ-v0), EnvSpec(HandManipulateBlockRotateXYZTouchSensors-v0), EnvSpec(HandManipulateBlockRotateXYZTouchSensors-v1), EnvSpec(HandManipulateBlockFull-v0), EnvSpec(HandManipulateBlock-v0), EnvSpec(HandManipulateBlockTouchSensors-v0), EnvSpec(HandManipulateBlockTouchSensors-v1), EnvSpec(HandManipulateEggRotate-v0), EnvSpec(HandManipulateEggRotateTouchSensors-v0), EnvSpec(HandManipulateEggRotateTouchSensors-v1), EnvSpec(HandManipulateEggFull-v0), EnvSpec(HandManipulateEgg-v0), EnvSpec(HandManipulateEggTouchSensors-v0), EnvSpec(HandManipulateEggTouchSensors-v1), EnvSpec(HandManipulatePenRotate-v0), EnvSpec(HandManipulatePenRotateTouchSensors-v0), EnvSpec(HandManipulatePenRotateTouchSensors-v1), EnvSpec(HandManipulatePenFull-v0), EnvSpec(HandManipulatePen-v0), EnvSpec(HandManipulatePenTouchSensors-v0), EnvSpec(HandManipulatePenTouchSensors-v1), EnvSpec(FetchSlideDense-v1), EnvSpec(FetchPickAndPlaceDense-v1), EnvSpec(FetchReachDense-v1), EnvSpec(FetchPushDense-v1), EnvSpec(HandReachDense-v0), EnvSpec(HandManipulateBlockRotateZDense-v0), EnvSpec(HandManipulateBlockRotateZTouchSensorsDense-v0), EnvSpec(HandManipulateBlockRotateZTouchSensorsDense-v1), EnvSpec(HandManipulateBlockRotateParallelDense-v0), EnvSpec(HandManipulateBlockRotateParallelTouchSensorsDense-v0), EnvSpec(HandManipulateBlockRotateParallelTouchSensorsDense-v1), EnvSpec(HandManipulateBlockRotateXYZDense-v0), EnvSpec(HandManipulateBlockRotateXYZTouchSensorsDense-v0), EnvSpec(HandManipulateBlockRotateXYZTouchSensorsDense-v1), EnvSpec(HandManipulateBlockFullDense-v0), EnvSpec(HandManipulateBlockDense-v0), EnvSpec(HandManipulateBlockTouchSensorsDense-v0), EnvSpec(HandManipulateBlockTouchSensorsDense-v1), EnvSpec(HandManipulateEggRotateDense-v0), EnvSpec(HandManipulateEggRotateTouchSensorsDense-v0), EnvSpec(HandManipulateEggRotateTouchSensorsDense-v1), EnvSpec(HandManipulateEggFullDense-v0), EnvSpec(HandManipulateEggDense-v0), EnvSpec(HandManipulateEggTouchSensorsDense-v0), EnvSpec(HandManipulateEggTouchSensorsDense-v1), EnvSpec(HandManipulatePenRotateDense-v0), EnvSpec(HandManipulatePenRotateTouchSensorsDense-v0), EnvSpec(HandManipulatePenRotateTouchSensorsDense-v1), EnvSpec(HandManipulatePenFullDense-v0), EnvSpec(HandManipulatePenDense-v0), EnvSpec(HandManipulatePenTouchSensorsDense-v0), EnvSpec(HandManipulatePenTouchSensorsDense-v1), EnvSpec(CubeCrash-v0), EnvSpec(CubeCrashSparse-v0), EnvSpec(CubeCrashScreenBecomesBlack-v0), EnvSpec(MemorizeDigits-v0), EnvSpec(three-armed-bandits-v0), EnvSpec(five-armed-bandits-v0), EnvSpec(ten-armed-bandits-v0), EnvSpec(MultiarmedBandits-v0)])
</code></pre>
",1181744,,,,,2022-02-08 08:01:15,gym package not identifying ten-armed-bandits-v0 env,<reinforcement-learning><openai-gym>,1,1,0,,,CC BY-SA 4.0,
907,53382095,1,53401541,,2018-11-19 20:23:19,,0,183,"<p>I've tried modifying the <code>FetchPickAndPlace-v1</code> OpenAI environment to replace the cube with a pair of scissors. Everything works perfectly except for the fact that my custom mesh seems to jitter a few millimeters in and out of the table every few time steps. I've included a picture mid-jitter below: </p>

<p><a href=""https://i.stack.imgur.com/Xp9Kj.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/Xp9Kj.png"" alt=""enter image description here""></a></p>

<p>As you can see, the scissors are caught mid-way through the surface of the table. How can I prevent this? All I've done is switch out the code for the cube in <code>pick_and_place.xml</code> with the asset related to the scissor mesh. Here's the code of interest:</p>

<pre><code>&lt;body name=""object0"" pos=""0.0 0.0 0.0""&gt;
            &lt;joint name=""object0:joint"" type=""free"" damping=""0.01""&gt;&lt;/joint&gt;
            &lt;geom size=""0.025 0.025 0.025"" mesh=""tool0:scissors"" condim=""3"" name=""object0"" material=""tool_mat"" class=""tool0:matte"" mass=""2""&gt;&lt;/geom&gt;
            &lt;site name=""object0"" pos=""0 0 0"" size=""0.02 0.02 0.02"" rgba=""1 0 0 1"" type=""sphere""&gt;&lt;/site&gt;
&lt;/body&gt;
</code></pre>

<p>I've tried playing around with the coordinates of the position and geometry but to no avail. Any tips? Replacing <code>mesh=""tool0:scissors""</code> with <code>type=""box""</code> gets rid of the problem entirely but I'm back to square one.</p>
",4283749,,,,,2018-11-20 21:06:44,Custom mesh jittering in Mujoco environment in OpenAI gym,<reinforcement-learning><openai-gym>,1,0,,,,CC BY-SA 4.0,
915,53304915,1,53319815,,2018-11-14 16:38:12,,2,630,"<p>I'm not able to select random actions for multi-agent gym environments.</p>

<pre><code>def make_env(scenario_name, benchmark=False):

    from multiagent.environment import MultiAgentEnv
    import multiagent.scenarios as scenarios

    # load scenario from script
    scenario = scenarios.load(scenario_name + "".py"").Scenario()
    # create world
    world = scenario.make_world()
    # create multiagent environment
    if benchmark:        
        env = MultiAgentEnv(world, scenario.reset_world, scenario.reward, scenario.observation, scenario.benchmark_data)
    else:
        env = MultiAgentEnv(world, scenario.reset_world, scenario.reward, scenario.observation)
    return env

env = make_env('simple_tag')
env.reset()
for i in range(100):
    env.render()
    actions = [action_space.sample() for action_space in env.action_space]
    env.step(actions)
</code></pre>

<p>The above code throws this error:</p>

<pre><code>Traceback (most recent call last):
  File ""hello.py"", line 22, in &lt;module&gt;
    env.step(actions)
  File ""c:\multiagent-particle-envs\multiagent\environment.py"", line 88, in step
    self._set_action(action_n[i], agent, self.action_space[i])
  File ""c:\multiagent-particle-envs\multiagent\environment.py"", line 174, in _set_action
    agent.action.u[0] += action[0][1] - action[0][2]
TypeError: 'int' object is not subscriptable
</code></pre>

<p>I can't find a fix since there's not enough talk about these multi agent environments.</p>
",6912045,,,,,2018-11-15 12:46:30,Random agent on multi-agent gym environments,<python><reinforcement-learning><openai-gym>,1,0,,,,CC BY-SA 4.0,
919,53570723,1,53641667,,2018-12-01 12:23:19,,0,427,"<p>I want to modify the agents structure (for example the length of the legs) in the <code>bipedal_walker</code> environment. </p>

<p>Is there anyway to do that?</p>
",1477383,,6529212,,2018-12-03 12:06:11,2018-12-05 22:21:55,How to modify the agent in an openai gym environment?,<reinforcement-learning><openai-gym>,1,0,,,,CC BY-SA 4.0,
920,53574578,1,53901768,,2018-12-01 20:01:39,,0,31,"<p>I built a custom Open AI Gym environment in which I have 13 different actions and and 33 observation items. During an episode every action can be used, but it can be used only once otherwise the episode ends.  Thus the maximum lenght of an episode is 13.</p>

<p>I tried to train several neuronal network for this, but so far the NN did not learned it well and it ends much prior the 13rd step. The last layer of the NN  is a softmax layer with 13 neurons.</p>

<p>Do you have any idea how an NN would look like which could learn to choose from 13 actions one-by-one?</p>

<p>Kind regards,
Ferenc</p>
",898160,,,,,2018-12-23 06:44:23,Reinforce learning - how to teach a neuronal network avoid actions already chosen during the episode?,<keras><reinforcement-learning><openai-gym>,2,0,,,,CC BY-SA 4.0,
929,69442971,1,69443797,,2021-10-04 22:21:24,,12,23130,"<p>I am trying to run an OpenAI Gym environment however I get the following error:</p>
<pre><code>import gym
env = gym.make('Breakout-v0')
</code></pre>
<p>ERROR</p>
<pre><code>/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/ale_py/gym/environment.py:11: DeprecationWarning: Importing atari-py roms won't be supported in future releases of ale-py.
  import ale_py.roms as roms
A.L.E: Arcade Learning Environment (version +a54a328)
[Powered by Stella]
Traceback (most recent call last):
  File &quot;/Users/username/Desktop/OpenAI Gym Stuff/OpenAI_Exp.py&quot;, line 2, in &lt;module&gt;
    env = gym.make('Breakout-v0')
  File &quot;/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/gym/envs/registration.py&quot;, line 200, in make
    return registry.make(id, **kwargs)
  File &quot;/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/gym/envs/registration.py&quot;, line 105, in make
    env = spec.make(**kwargs)
  File &quot;/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/gym/envs/registration.py&quot;, line 75, in make
    env = cls(**_kwargs)
  File &quot;/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/ale_py/gym/environment.py&quot;, line 123, in __init__
    self.seed()
  File &quot;/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/ale_py/gym/environment.py&quot;, line 171, in seed
    raise error.Error(
gym.error.Error: Unable to find game &quot;Breakout&quot;, did you import Breakout with ale-import-roms?
</code></pre>
",15525066,,4819376,,2021-10-05 00:58:22,2022-05-30 19:51:12,Error in importing environment OpenAI Gym,<python><openai-gym>,5,5,0,,,CC BY-SA 4.0,
935,53712923,1,53752962,,2018-12-10 20:10:27,,0,191,"<p>I am attempting to create a custom environment for reinforcement learning with openAI gym. I need to represent all possible values that the environment will see in a variable called <code>observation_space</code>. There are 3 possible actions for the agent to use called <code>action_space</code></p>

<p>To be more specific the <code>observation_space</code> is a temperature sensor which will see possible ranges from 50 to 150 degrees and I think I can represent all of this by:</p>

<p>EDIT, I had the action_space numpy array wrong</p>

<pre><code>import numpy as np
action_space = np.array([ 0,  1,  2])
observation_space = np.arange(50,150,1)
</code></pre>

<p>Is there a better method that I could use for the <code>observation_space</code> where I could bin the data? IE, make 20 bins 50-55, 55-60, 60-65, etc...</p>

<p>I think what I have will work but seems sort of cumbersome... And I am sure there is a better practice as there is not a lot of wisdom on my end this subject. This will print out a Q table:</p>

<pre><code>action_size = action_space.shape[0]
state_size = observation_space.shape[0]

qtable = np.zeros((state_size, action_size))
print(qtable)
</code></pre>
",8372455,,,,,2018-12-12 23:30:08,python binning data openAI gym,<python><numpy><reinforcement-learning><openai-gym>,1,0,0,,,CC BY-SA 4.0,
940,53656280,1,53721828,,2018-12-06 16:57:57,,0,1360,"<p>I am attempting to create a Q table following <a href=""https://gist.github.com/simoninithomas/baafe42d1a665fb297ca669aa2fa6f92#file-q-learning-with-frozenlake-ipynb"" rel=""nofollow noreferrer"">this blog post</a> that I found on Medium.com. In my <a href=""https://stackoverflow.com/questions/45068568/is-it-possible-to-create-a-new-gym-environment-in-openai"">step function</a> (python class) for creating a custom open AI Gym environment, my action space <code>self.action_space</code> would be 3 possible actions and the possible observations would be a values 50 to 150 in increments of 1 <code>self.observation_space</code>.</p>

<pre><code>    #possible actions from more_heat less heat functions
    self.action_space = np.array([ 0,  1,  2])

    #possible deviation values from temp - setpoint
    self.observation_space = np.arange(50,150,1)
</code></pre>

<p>The blog post that I am following creates the Q table by this code below which I think is just creating an array of zeros based on the sizes.</p>

<pre><code>action_size = env.action_space.n
state_size = env.observation_space.n

qtable = np.zeros((state_size, action_size))
print(qtable)
</code></pre>

<p>But when I attempt to print the Q table, I get this error:
<code>TypeError: only integer scalar arrays can be converted to a scalar index</code></p>

<p>Any words from the wise on what I am doing wrong would be greatly appreciated!</p>

<p>Same error just running this code:</p>

<pre><code>import numpy as np
action_space = np.array([0,1,2])
observation_space = np.arange(50,150,1)

action_size = action_space
state_size = observation_space

qtable = np.zeros((state_size, action_size))
print(qtable)
</code></pre>
",8372455,,4685471,,2018-12-06 19:46:33,2018-12-11 10:12:08,integer scalar arrays can be converted to a scalar index,<python><reinforcement-learning><openai-gym>,1,2,,,,CC BY-SA 4.0,
941,69520829,1,69534749,,2021-10-11 03:50:28,,8,6421,"<p>I'm running into this error when trying to run a command from docker a docker container on google compute engine.</p>
<p>Here's the stacktrace:</p>
<pre><code>Traceback (most recent call last):
  File &quot;train.py&quot;, line 16, in &lt;module&gt;
    from stable_baselines.ppo1 import PPO1
  File &quot;/home/selfplay/.local/lib/python3.6/site-packages/stable_baselines/__init__.py&quot;, line 3, in &lt;module&gt;
    from stable_baselines.a2c import A2C
  File &quot;/home/selfplay/.local/lib/python3.6/site-packages/stable_baselines/a2c/__init__.py&quot;, line 1, in &lt;module&gt;
    from stable_baselines.a2c.a2c import A2C
  File &quot;/home/selfplay/.local/lib/python3.6/site-packages/stable_baselines/a2c/a2c.py&quot;, line 3, in &lt;module&gt;
    import gym
  File &quot;/home/selfplay/.local/lib/python3.6/site-packages/gym/__init__.py&quot;, line 13, in &lt;module&gt;
    from gym.envs import make, spec, register
  File &quot;/home/selfplay/.local/lib/python3.6/site-packages/gym/envs/__init__.py&quot;, line 10, in &lt;module&gt;
    _load_env_plugins()
  File &quot;/home/selfplay/.local/lib/python3.6/site-packages/gym/envs/registration.py&quot;, line 269, in load_env_plugins
    context = contextlib.nullcontext()
AttributeError: module 'contextlib' has no attribute 'nullcontext'
</code></pre>
",2130841,,2130841,,2021-10-11 04:36:09,2022-08-25 08:56:44,OpenAI Gym - AttributeError: module 'contextlib' has no attribute 'nullcontext',<python><pip><virtual-machine><google-compute-engine><openai-gym>,3,3,0,,,CC BY-SA 4.0,
942,53836136,1,53850600,,2018-12-18 15:20:47,,8,6151,"<p>I'm trying to get some insights into reinforcement learning while using openAI gym as a learning environment. I do this by reading the book <em>Hands-on reinforcement learning with Python</em>. In this book, some code is provided. Often, the code doesn't work, because I have to unwrap it first, as shown in: <a href=""https://stackoverflow.com/questions/52040325/openai-gym-env-p-attributeerror-timelimit-object-has-no-attribute-p/53819171#53819171"">openai gym env.P, AttributeError &#39;TimeLimit&#39; object has no attribute &#39;P&#39;</a></p>

<p>However, I personally am still interested in the WHY of this unwrapping. Why do you need to unwrap? What does this do exactly? And why isn't it coded like that in the book? Is it outdated software as Giuliov assumed?</p>

<p>Thanks in advance.</p>
",10802225,,,,,2018-12-19 11:49:01,Why unwrap an openAI gym?,<python-3.x><reinforcement-learning><openai-gym>,1,0,0,,,CC BY-SA 4.0,
948,54022606,1,54583789,,2019-01-03 12:47:18,,1,2837,"<p>Aside from <a href=""https://gym.openai.com/docs/#environments"" rel=""nofollow noreferrer"">openAI's doc</a>, I hadn't been able to find a more detailed documentation.</p>

<p>I need to know the correct way to create:</p>

<ol>
<li><p>An action space which has <code>1..n</code> possible actions. (currently using Discrete action space)</p></li>
<li><p>An observation space that has <code>2^n</code> states - A state for every possible combination of actions that has been taken.
I would like a one-hot representation of the action vector - 1 for <code>action was already taken</code>, 0 for <code>action still hadn't been taken</code></p></li>
</ol>

<p>How do I do that with openAI's Gym?</p>

<p>Thanks</p>
",913098,,,,,2019-09-16 04:39:03,OpenAI Gym - How to create one-hot observation space?,<python><reinforcement-learning><openai-gym>,2,0,0,,,CC BY-SA 4.0,
950,54047654,1,58108015,,2019-01-04 23:54:42,,5,7728,"<p>I'm running a reinforcement learning program in a gym environment(BipedalWalker-v2) implemented in tensorflow. I've set the random seed of the environment, tensorflow and numpy manually as follows</p>
<pre><code>os.environ['PYTHONHASHSEED']=str(42)
random.seed(42)
np.random.seed(42)
tf.set_random_seed(42)

env = gym.make('BipedalWalker-v2')
env.seed(0)

config = tf.ConfigProto(intra_op_parallelism_threads=1, inter_op_parallelism_threads=1)
config.gpu_options.allow_growth = True
sess = tf.Session(config=config)
# run the graph with sess
</code></pre>
<p>However, I get different results every time I run my program (without changing any code). Why are the results not consistent and what should I do if I want to obtain the same result?</p>
<hr />
<h2>Update:</h2>
<p>The only places that I can think of may introduce randomness (other than the neural networks) are</p>
<ol>
<li>I use <code>tf.truncated_normal</code> to generate random noise <code>epsilon</code> so as to implement noisy layer</li>
<li>I use <code>np.random.uniform</code> to randomly select samples from replay buffer</li>
</ol>
<p>I also spot that the scores I get are pretty consistent at the first 10 episodes, but then begin to differ. Other things such as losses also show a similar trend but are not the same in numeric.</p>
<h2>Update 2</h2>
<p>I've also set &quot;PYTHONHASHSEED&quot; and use single-thread CPU as @jaypops96 described, but still cannot reproduce the result. Code has been updated in the above code block</p>
",7850499,,-1,,2020-06-20 09:12:55,2022-05-25 13:38:45,Tensorflow: Different results with the same random seed,<python><tensorflow><random><random-seed><openai-gym>,3,7,0,,,CC BY-SA 4.0,
955,69873630,1,69873671,,2021-11-07 15:06:59,,2,3495,"<p>I'm trying to learn reinforcement learning, doing coding on Jupyter notebook. But when I try to install stable baselines I'm getting an error even though I've installed it and upgraded it several times. I'm attaching the screenshots as well. Appreciate any help.<a href=""https://i.stack.imgur.com/Bn3LU.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/Bn3LU.png"" alt=""enter image description here"" /></a></p>
<p><a href=""https://i.stack.imgur.com/X5lg3.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/X5lg3.png"" alt=""enter image description here"" /></a></p>
",13816376,,,,,2021-11-13 06:14:33,ModuleNotFoundError: No module named 'stable_baselines3',<python><pip><jupyter-notebook><openai-gym>,2,1,,,,CC BY-SA 4.0,
956,55341996,1,55350702,,2019-03-25 16:09:01,,-1,451,"<p>I have installed clang in windows system and Gym Retro by</p>

<pre><code>pip3 install gym-retro
</code></pre>

<p>When I try to run </p>

<pre><code>env = retro.make(game='SonicTheHedgehog-Genesis', state='GreenHillZone.Act1', record='.')
</code></pre>

<p>I got an error:</p>

<pre><code>FileNotFoundError: Game not found: SonicTheHedgehog-Genesis. Did you make sure to import the ROM?
</code></pre>

<p>Any help will be deeply appreciated.</p>
",11255732,,11255732,,2019-03-26 12:04:51,2019-03-26 12:04:51,some problems about open ai gym retro,<openai-gym>,1,0,,,,CC BY-SA 4.0,
960,69841193,1,69841730,,2021-11-04 14:53:01,,1,396,"<p>Edit: Problem Solved. Solution below.</p>
<p>Attempting to build a RL model to handle a task.
There are two inputs: x and y, both are measured on an int scale of 1 to 100.
Based on these two inputs there should be an output (action to take on, discrete(5)) and confidence.</p>
<p>Also, I'm very new to this territory. Please, feel free to ask me anything or correct me on something that seems downright dumb/wrong.</p>
<p>Here's my program (imports haven't been cleaned up....):</p>
<pre><code>from abc import ABC
import gym
from tensorflow import keras
from gym import Env
from gym.spaces import Discrete, Box
import random
import numpy as np
from tensorflow.keras.models import Sequential
from tensorflow.keras import layers, losses, metrics
from tensorflow.keras.layers import Dense, Flatten, Input
from tensorflow.keras.optimizers import Adam
import os
from rl.agents import DQNAgent
from rl.policy import BoltzmannQPolicy
from rl.memory import SequentialMemory
os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'

steps = 10000
episodes = 100
score_requirement = 1000

class PlantEnv(Env, ABC):
    def __init__(self):
        # Actions = water: 0=(none), 1=(3 seconds), 2=(4 seconds), 3=(5 seconds), 4=(6 seconds)
        self.action_space = Discrete(5)

        # Starting Moisture
        moisture = 20 + random.randint(-10, 10)
        # Starting Chance of Rain
        chance_of_rain = 50 + random.randint(-50, 50)

        # Observations
        self.observation_space = Box(low=np.array([0, 0]), high=np.array([100, 100]), dtype=np.int)
        self.state = moisture, chance_of_rain

        # Number of water steps left
        self.water_length = steps

    def step(self, action):
        # Action section
        water = 0

        if action == 1:
            water = 2
        elif action == 2:
            water = 3
        elif action == 3:
            water = 4
        elif action == 4:
            water = 5

        moisture, chance_of_rain = self.state

        moisture += (water * 5)
        self.water_length -= 1

        # Reward Section
        reward = 0
        if 40 &lt;= moisture &lt;= 60:
            reward = 2
        # If moisture is dry or wet
        elif 60 &lt; moisture &lt;= 80 or 20 &lt;= moisture &lt; 40:
            reward = 0.5
        # If moisture is really dry or really wet
        elif 80 &lt; moisture &lt;= 100 or 0 &lt;= moisture &lt; 20:
            reward = -1
        # If moisture is really dry or really wet
        elif 100 &lt; moisture or moisture &lt; 0:
            reward = -2

        # Check if shower is done
        if self.water_length &lt;= 0:
            done = True
        else:
            done = False

        moistureLoss = random.randint(15, 25)
        moisture -= moistureLoss
        chance_of_rain = 50 + random.randint(-50, 50)
        xfactor = chance_of_rain + random.randint(-50, 50)
        if xfactor &gt; 100:
            moisture += (10 + random.randint(0, 15))

        # Set placeholder for info
        info = {}

        # Save current state
        self.state = moisture, chance_of_rain

        # Return step information
        return self.state, reward, done, info

    def reset(self):
        # Reset test environment
        # Set starting moisture
        moisture = 50 + random.randint(-10, 10)
        # Set starting chance of rain array
        chance_of_rain = 50 + random.randint(-50, 50)
        self.state = moisture, chance_of_rain
        # Reset Test time
        self.water_length = steps
        return self.state


def build_model():
    model = Sequential()
    model.add(Flatten(input_shape=(1, 4)))
    model.add(Dense(24, activation='relu'))
    model.add(Dense(24, activation='relu'))
    model.add(Dense(2, activation='linear'))
    return model


def build_agent(model):
    policy = BoltzmannQPolicy()
    memory = SequentialMemory(limit=50000, window_length=1)
    dqn = DQNAgent(model=model, memory=memory, policy=policy, nb_actions=2,
                   nb_steps_warmup=10, target_model_update=1e-2)
    return dqn


# Create environment
env = PlantEnv()

accepted_scores = []
training_data = []
scores = []
good_episodes = 0

# Create episodes and initiate simulation
for episode in range(1, episodes + 1):
    observation = env.reset()
    done = False
    score = 0
    history = []
    prev_observation = []

    while not done:
        action = env.action_space.sample()
        if observation[0] &gt; 100:
            action = 0
        elif observation[0] &lt; 0:
            action = 4
        observation, reward, done, info = env.step(action)
        score += reward
        if len(prev_observation) &gt; 0:
            history.append([prev_observation, action])
        prev_observation = observation

    if score &gt;= score_requirement:
        good_episodes += 1
        accepted_scores.append(score)
        for data in history:
            if data[1] == 1:
                output = [1]
            else:
                output = [0]

            training_data.append([data[0], output])

    scores.append(score)

if len(accepted_scores) &gt; 0:
    print(&quot;Average accepted score: &quot;, np.mean(accepted_scores))
    print(&quot;Median accepted score : &quot;, np.median(accepted_scores))
print(&quot;Episodes above accepted score of {}: {}/{}\n&quot;.format(score_requirement, good_episodes, episodes))

model = build_model()
model.summary()

dqn = build_agent(model)
dqn.compile(Adam(learning_rate=1e-3), metrics=['mae'])
dqn.fit(env, nb_steps=50000, visualize=False, verbose=1)
</code></pre>
<p>The first model gives this error when trying to dqn.fit:
<strong>ValueError: Error when checking input: expected dense_input to have 2 dimensions, but got array with shape (1, 1, 2)</strong></p>
<p>The second model gives this error when trying to build_agent:
<strong>AttributeError: 'list' object has no attribute 'shape'</strong></p>
<p>Any ideas as to what I'm doing wrong or how to go about correcting it would be a massive help. I feel 95% confident that I have my environment setup correctly.</p>
<p>I initially went with the first model just to see if I could get the program to compile and work. Then, after further research, I built the second model because I understood that it was capable of giving me an action with a confidence rating. Getting errors at both turns.</p>
",16855685,,16855685,,2021-11-04 16:03:46,2021-11-04 16:03:46,"ValueError: Error when checking input: expected dense_input to have 2 dimensions, but got array with shape (1, 1, 2)",<python><tensorflow><keras><neural-network><openai-gym>,1,1,,,,CC BY-SA 4.0,
968,55531883,1,55532101,,2019-04-05 09:08:43,,2,936,"<p>I'm trying to create a game environement in pyhton 3.6.8 using gym-retro.
It's a stable game name <strong>SpaceInvaders-Atari2600</strong> located in ""..retro\data\stable\""</p>

<p>The basic command is : </p>

<pre><code>env = retro.make(game='SpaceInvaders-Atari2600')
</code></pre>

<p>but that isn't working for me and i got the error : </p>

<p><em>raise FileNotFoundError('Game not found: %s. Did you make sure to import the ROM?' % game)
FileNotFoundError: Game not found: SpaceInvaders-Atari2600. Did you make sure to import the ROM?</em></p>

<p><strong>But</strong> the problem is that the ROM is here, in the folder SpaceInvaders-Atari2600 there is a file named <em>""rom.sha""</em></p>

<pre class=""lang-py prettyprint-override""><code>
import gym
import retro    

#Environment creation
env = retro.make(game='SpaceInvaders-Atari2600');
# don't work for me ...

# when i try this : 

for game in retro.data.list_games():
   print(game)

# I see the game SpaceInvaders-Atari2600
</code></pre>

<p>I'm desperated please if someone got an idea on how to create this environement...</p>

<p><strong>I'm totally new on python i never used it before this week, i have to use this language to test Q-learning training for a neural network so i can do stupid mistakes with syntaxe</strong></p>
",10168321,,,,,2021-06-15 20:14:39,Impossible to create an environment with a specific game (gym retro),<python><python-3.x><openai-gym>,2,0,,,,CC BY-SA 4.0,
975,55679861,1,55680940,,2019-04-14 20:41:51,,0,508,"<p>I am trying to run  vanilla policy gradient algorithm and render the Open AI environment <code>""CartPole-v1""</code>.</p>

<p>The code for the algorithm is given below, and runs well without any errors. The Jupyer notebook for this code may be found <a href=""https://github.com/geohot/ai-notebooks/blob/master/rl_vpg_cartpole.ipynb"" rel=""nofollow noreferrer"">here</a>.</p>

<pre><code>en%pylab inline
import tensorflow as tf
import tensorflow.keras.backend as K
import numpy as np
import gym
from tqdm import trange
from tensorflow.keras.models import Model
from tensorflow.keras.optimizers import Adam, SGD
from tensorflow.keras.layers import *

env = gym.make(""CartPole-v1"")
env.observation_space, env.action_space

x = in1 = Input(env.observation_space.shape)
x = Dense(32)(x)
x = Activation('tanh')(x)
x = Dense(env.action_space.n)(x)
x = Lambda(lambda x: tf.nn.log_softmax(x, axis=-1))(x)
m = Model(in1, x)
def loss(y_true, y_pred):
  # y_pred is the log probs of the actions
  # y_true is the action mask weighted by sum of rewards
  return -tf.reduce_sum(y_true*y_pred, axis=-1)
m.compile(Adam(1e-2), loss)
m.summary()
lll = []

# this is like 5x faster than calling m.predict and picking in numpy
pf = K.function(m.layers[0].input, tf.random.categorical(m.layers[-1].output, 1)[0])

tt = trange(40)
for epoch in tt:
  X,Y = [], []
  ll = []
  while len(X) &lt; 8192:
    obs = env.reset()
    acts, rews = [], []
    while True:
      # pick action
      #act_dist = np.exp(m.predict_on_batch(obs[None])[0])
      #act = np.random.choice(range(env.action_space.n), p=act_dist)

      # pick action (fast!)
      act = pf(obs[None])[0]

      # save this state action pair
      X.append(np.copy(obs))
      acts.append(act)

      # take the action
      obs, rew, done, _ = env.step(act)
      rews.append(rew)

      if done:
        for i, act in enumerate(acts):
          act_mask = np.zeros((env.action_space.n))
          act_mask[act] = np.sum(rews[i:])
          Y.append(act_mask)
        ll.append(np.sum(rews))
        break

  loss = m.train_on_batch(np.array(X), np.array(Y))
  lll.append((np.mean(ll), loss))
  tt.set_description(""ep_rew:%7.2f    loss:%7.2f"" % lll[-1])
  tt.refresh()

plot([x[0] for x in lll], label=""Mean Episode Reward"")
plot([x[1] for x in lll], label=""Epoch Loss"")
plt.legend()
</code></pre>

<p><a href=""https://i.stack.imgur.com/MQ1sI.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/MQ1sI.png"" alt=""enter image description here""></a></p>

<p>When I try to render the environment I get an IndexError:</p>

<pre><code>obs = env.reset()
rews = []
while True:
  env.render()
  pred, act = [x[0] for x in pf(obs[None])]
  obs, rew, done, _ = env.step(np.argmax(pred))
  rews.append(rew)
  time.sleep(0.05)
  if done:
    break
print(""ran %d steps, got %f reward"" % (len(rews), np.sum(rews)))
</code></pre>

<blockquote>
  <p> in (.0)
        3 while True:
        4   env.render()
  ----> 5   pred, act = [x[0] for x in pf(obs[None])]
        6   obs, rew, done, _ = env.step(np.argmax(pred))
        7   rews.append(rew)</p>
  
  <p>IndexError: invalid index to scalar variable.</p>
</blockquote>

<p>I read that this happens when you try to index a <code>numpy</code> scalar such as <code>numpy.int64</code> or <code>numpy.float64</code>, however I am not sure where the error is stemming from and how I should go about solving this issue. Any help or suggestions would be appreciated.</p>
",7389018,,,,,2019-04-17 20:15:57,Reinforcement Learning - VPG: invalid index to scalar variable Index Error,<python><tensorflow><openai-gym>,1,1,,,,CC BY-SA 4.0,
983,55782658,1,55821571,,2019-04-21 12:18:58,,0,313,"<p>It's another pip installation error (those drive me crazy). I am on Windows 8.1 with python 3.7. I was attempting to install gym-retro through pip (I already have gym installed) and kept encountering a subprocess error. I figured it was because I did not have cmake or something, so I went on the cmake website and installed cmake, and checked the ""add to PATH"" option. After that I attempted to install it again. Same issue. I restarted my command prompt as well. I tried to find help online, but nothing seemed to help. I resigned to running the command</p>

<pre><code>pip install gym-retro &gt; output.txt
</code></pre>

<p>to gather the output and post it here. I stripped it down so it would not be painful to look at.</p>

<pre><code>Building wheels for collected packages: gym-retro
  Building wheel for gym-retro (setup.py): started
  Building wheel for gym-retro (setup.py): finished with status 'error'
  Complete output from command c:\users\evgeny\appdata\local\programs\python\python37-32\python.exe -u -c ""import setuptools, tokenize;__file__='C:\\Users\\Evgeny\\AppData\\Local\\Temp\\pip-install-t3ctdvi3\\gym-retro\\setup.py';f=getattr(tokenize, 'open', open)(__file__);code=f.read().replace('\r\n', '\n');f.close();exec(compile(code, __file__, 'exec'))"" bdist_wheel -d C:\Users\Evgeny\AppData\Local\Temp\pip-wheel-q1w5imfc --python-tag cp37:
  use_scm_version False
  running bdist_wheel
  running build
  running build_py
  creating build
  creating build\lib.win32-3.7
  creating build\lib.win32-3.7\retro
  copying retro\enums.py -&gt; build\lib.win32-3.7\retro
  copying retro\retro_env.py -&gt; build\lib.win32-3.7\retro
  copying retro\__init__.py -&gt; build\lib.win32-3.7\retro
  creating build\lib.win32-3.7\retro\data
  copying retro\data\__init__.py -&gt; build\lib.win32-3.7\retro\data
  creating build\lib.win32-3.7\retro\data\stable
  copying retro\data\stable\__init__.py -&gt; build\lib.win32-3.7\retro\data\stable

&lt;/skipped lots of text here/&gt;

    running build_ext
    -- Submodules seem to be missing. Attempting to clone now.
    CMake Error at CMakeLists.txt:13 (message):
      Failed to check out submodules.  Aborting.


    -- Configuring incomplete, errors occurred!
    Traceback (most recent call last):
      File ""&lt;string&gt;"", line 1, in &lt;module&gt;
      File ""C:\Users\Evgeny\AppData\Local\Temp\pip-install-t3ctdvi3\gym-retro\setup.py"", line 92, in &lt;module&gt;
        **kwargs
      File ""c:\users\evgeny\appdata\local\programs\python\python37-32\lib\site-packages\setuptools\__init__.py"", line 131, in setup
        return distutils.core.setup(**attrs)
      File ""c:\users\evgeny\appdata\local\programs\python\python37-32\lib\distutils\core.py"", line 148, in setup
        dist.run_commands()
      File ""c:\users\evgeny\appdata\local\programs\python\python37-32\lib\distutils\dist.py"", line 966, in run_commands
        self.run_command(cmd)
      File ""c:\users\evgeny\appdata\local\programs\python\python37-32\lib\distutils\dist.py"", line 985, in run_command
        cmd_obj.run()
      File ""c:\users\evgeny\appdata\local\programs\python\python37-32\lib\site-packages\setuptools\command\install.py"", line 61, in run
        return orig.install.run(self)
      File ""c:\users\evgeny\appdata\local\programs\python\python37-32\lib\distutils\command\install.py"", line 545, in run
        self.run_command('build')
      File ""c:\users\evgeny\appdata\local\programs\python\python37-32\lib\distutils\cmd.py"", line 313, in run_command
        self.distribution.run_command(command)
      File ""c:\users\evgeny\appdata\local\programs\python\python37-32\lib\distutils\dist.py"", line 985, in run_command
        cmd_obj.run()
      File ""c:\users\evgeny\appdata\local\programs\python\python37-32\lib\distutils\command\build.py"", line 135, in run
        self.run_command(cmd_name)
      File ""c:\users\evgeny\appdata\local\programs\python\python37-32\lib\distutils\cmd.py"", line 313, in run_command
        self.distribution.run_command(command)
      File ""c:\users\evgeny\appdata\local\programs\python\python37-32\lib\distutils\dist.py"", line 985, in run_command
        cmd_obj.run()
      File ""C:\Users\Evgeny\AppData\Local\Temp\pip-install-t3ctdvi3\gym-retro\setup.py"", line 53, in run
        subprocess.check_call([cmake_exe, '.', '-G', 'Unix Makefiles', build_type, pyext_suffix, pylib_dir, python_executable])
      File ""c:\users\evgeny\appdata\local\programs\python\python37-32\lib\subprocess.py"", line 341, in check_call
        raise CalledProcessError(retcode, cmd)
    subprocess.CalledProcessError: Command '['C:\\Users\\Evgeny\\AppData\\Local\\Programs\\Python\\Python37-32\\Scripts\\cmake.exe', '.', '-G', 'Unix Makefiles', '', '-DPYEXT_SUFFIX:STRING=.cp37-win32.pyd', '-DPYLIB_DIRECTORY:PATH=build\\lib.win32-3.7', '-DPYTHON_EXECUTABLE:STRING=c:\\users\\evgeny\\appdata\\local\\programs\\python\\python37-32\\python.exe']' returned non-zero exit status 1.
</code></pre>
",7991269,,7991269,,2019-04-21 12:46:03,2019-04-24 02:32:34,Unable to install gym-retro: subprocess.CalledProcessError for cmake,<python-3.x><cmake><openai-gym>,1,0,,,,CC BY-SA 4.0,
990,55835318,1,56028322,,2019-04-24 17:11:33,,2,2467,"<p>I have succeeded in creating a custom OpenAI gym environment on my computer following this tutorial: <a href=""https://web.archive.org/web/20181128171840/https://medium.com/@apoddar573/making-your-own-custom-environment-in-gym-c3b65ff8cdaa"" rel=""nofollow noreferrer"">https://web.archive.org/web/20181128171840/https://medium.com/@apoddar573/making-your-own-custom-environment-in-gym-c3b65ff8cdaa</a></p>

<p>Then I've uploaded my package to colab as a zip file and installed it:</p>

<pre><code>!unzip /content/gym-foo.zip
!pip install -e /content/gym-foo
</code></pre>

<p>After that I've tried using my custom environment:</p>

<pre><code>import gym
import gym_foo
gym.make(""gym_foo-v0"")
</code></pre>

<p>This actually works on my computer, but on google colab it gives me:</p>

<pre><code>ModuleNotFoundError: No module named 'gym_foo'
</code></pre>

<p>Whats going on? How can I use my custom environment on google colab?</p>
",2100065,,,,,2019-07-01 16:02:13,How to create and use a custom OpenAI gym environment on google colab?,<python><package><google-colaboratory><openai-gym>,1,1,,,,CC BY-SA 4.0,
995,71437246,1,71509145,,2022-03-11 10:39:37,,0,1019,"<p>Is there a way to model action masking for continuous action spaces? I want to model economic problems with reinforcement learning. These problems often have continuous action and state spaces. In addition, the state often influences what actions are possible and, thus, the allowed actions change from step to step.</p>
<p>Simple example:</p>
<p>The agent has a wealth (continuous state) and decides about spending (continuous action). The next periods is then wealth minus spending. But he is restricted by the budget constraint. He is not allowed to spend more than his wealth. What is the best way to model this?</p>
<p>What I tried:
For discrete actions it is possible to use <a href=""https://towardsdatascience.com/action-masking-with-rllib-5e4bec5e7505"" rel=""nofollow noreferrer"">action masking</a>. So in each time step, I provided the agent with information which action is allowed and which not. I also tried to do it with contiuous action space by providing lower and upper bound on allowed actions and clip the actions smapled from actor network (e.g. DDPG).</p>
<p>I am wondering if this is a valid thing to do (it works in a simple toy model) because I did not find any RL library that implements this. Or is there a smarter way/best practice to include the information about allowed actions to the agent?</p>
",13792866,,,,,2022-03-17 08:28:34,Action masking for continuous action space in reinforcement learning,<reinforcement-learning><openai-gym><policy-gradient-descent><sac>,1,1,,,,CC BY-SA 4.0,
997,71498764,1,71502739,,2022-03-16 14:22:30,,1,453,"<p>Below is a stripped-down version of the problem I'm hitting with memory management in relation to using the Python interpreter from C++.</p>
<p>The code as it is below will run properly, but its memory footprint will gradually grow over time.   I added a line to manually invoke the Python garbage collection; this didn't solve the issue.</p>
<p>What do I need to change with this code to prevent the growing memory leak?</p>
<p>[edit]: As per the suggestion from below, I've cut down the pythonTest function even further.  All it does is create an environment, reset it, and close it. The memory leak persists.</p>
<p>I'm using Python 3.10.2 on Windows 10. C++ is being compiled by Visual Studio to the C++14 standard. I have OpenAI-Gym version 0.22.0 installed.</p>
<pre><code>void pythonTest(PyObject* inModule)
{
    // Section 1: Get the make function:
    PyObject* pMakeFunc = PyObject_GetAttrString(inModule, &quot;make&quot;);

    PyObject* pMakeArgs = PyTuple_New(1);
    PyTuple_SetItem(pMakeArgs, 0, PyUnicode_FromString(&quot;LunarLanderContinuous-v2&quot;));

    // Section 2: Get the environment and its functions:
    PyObject* pEnv = PyObject_CallObject(pMakeFunc, pMakeArgs);
    PyObject* pEnvReset = PyObject_GetAttrString(pEnv, &quot;reset&quot;);
    PyObject* pEnvStep = PyObject_GetAttrString(pEnv, &quot;step&quot;);
    PyObject* pEnvClose = PyObject_GetAttrString(pEnv, &quot;close&quot;);
    PyObject* pEnvRender = PyObject_GetAttrString(pEnv, &quot;render&quot;);

    // Section 3: Reset the environment to get the initial observation:
    PyObject* pInitialObsArray = PyObject_CallNoArgs(pEnvReset);
    PyObject* pInitialObsListFunc = PyObject_GetAttrString(pInitialObsArray, &quot;tolist&quot;);
    PyObject* pInitialObsList = PyObject_CallNoArgs(pInitialObsListFunc);

    // Clear section 3:
    Py_CLEAR(pInitialObsList);
    Py_CLEAR(pInitialObsListFunc);
    Py_CLEAR(pInitialObsArray);

    // Clear section 2: Close the environment, first:
    PyObject_CallNoArgs(pEnvClose);
    Py_CLEAR(pEnvRender);
    Py_CLEAR(pEnvClose);
    Py_CLEAR(pEnvStep);
    Py_CLEAR(pEnvReset);

    Py_CLEAR(pEnv);

    // Clear section 1:
    Py_CLEAR(pMakeArgs);
    Py_CLEAR(pMakeFunc);
}

int main()
{
    Py_Initialize();

    // Get gym module:
    PyObject* pGymName = PyUnicode_FromString(&quot;gym&quot;);
    PyObject* pModule = PyImport_Import(pGymName);

    // Get garbage collection module and collect function:
    PyObject* pgcName = PyUnicode_FromString(&quot;gc&quot;);
    PyObject* pgcModule = PyImport_Import(pgcName);
    PyObject* pgcFunction = PyObject_GetAttrString(pgcModule, &quot;collect&quot;);

    for (int k = 0; k &lt; 1000000; ++k)
    {        
        pythonTest(pModule);
        
        // Manually invoke the garbage collection:
        PyObject* pGCReturn = PyObject_CallNoArgs(pgcFunction);
        auto objectsCollected = PyLong_AsLong(pGCReturn);
        std::cout &lt;&lt; &quot;Iteration &quot; &lt;&lt; k &lt;&lt; &quot; objects collected: &quot; 
            &lt;&lt; objectsCollected &lt;&lt; std::endl;

        Py_CLEAR(pGCReturn);
    }

    Py_CLEAR(pgcFunction);
    Py_CLEAR(pgcModule);
    Py_CLEAR(pgcName);

    Py_CLEAR(pModule);
    Py_CLEAR(pGymName);

    Py_Finalize();

    return 0;
}
</code></pre>
",5209191,,5209191,,2022-03-16 17:41:07,2022-03-16 18:44:13,How can I prevent this memory leak?,<python><c++><c++14><openai-gym><python-3.10>,2,4,,,,CC BY-SA 4.0,
1004,71656396,1,72443481,,2022-03-29 04:20:59,,0,685,"<p>I run multiple episodes, but only want to record for specific ones. More specifically, I'd like to have an input acting as trigger. Currently, my code is this:</p>
<pre><code>env = gym.make(&quot;HalfCheetah-v3&quot;)

env = gym.wrappers.RecordVideo(env, 'video')

env.reset()
for t in range(200):
    action = env.action_space.sample()
    observation, reward, done, info = env.step(action)

env.close()
env.reset()
#I only one to record this run
for t in range(200):
    action = env.action_space.sample()
    observation, reward, done, info = env.step(action)
env.close()
</code></pre>
<p>In that case, there are two episodes, but I'd like to only record the second one. Or in general, I'd like to have a input where I can select which episodes are recorded and which episodes are not recorded.</p>
<p>The function <a href=""https://github.com/openai/gym/blob/master/gym/wrappers/record_video.py"" rel=""nofollow noreferrer"">RecordVideo</a> has an argument which is step_trigger, which is of the form: <code>step_trigger: Callable[[int], bool] = None</code>, but I don't really know how to use it.</p>
<p>I found this other question <a href=""https://stackoverflow.com/questions/54100582/saving-a-video-gif-file-for-the-open-ai-gymtaxi-environment"">here</a>, and they use <code>lambda id_episode:True</code>. I tried that, but didn't make a difference because I probably wasn't using it correctly. I'd appreciate any help.</p>
",12627448,,,,,2022-05-31 07:04:35,how to set a trigger for when to record video open ai gym,<python><openai-gym><callable-object>,1,0,,,,CC BY-SA 4.0,
1006,71575887,1,71615697,,2022-03-22 16:48:36,,1,79,"<p>I built a simulation model where trucks collect garbage containers based on their fill level.
I used OpenAi Gym and Tensorflow/keras to create my Deep Reinforcement Learning model...
But my training has a very high loss...
Where did I go wrong? Thanks in advance</p>
<p>this is the Env</p>
<pre><code>class Marltf(Env):
    def __init__(self):
       
        self.i= 0
        self.containers1 = Container(3,3)
        self.containers2 = Container(1,3)
        self.containers3 = Container(3,1)
        self.containers4 = Container(5,6)
        self.containers5 = Container(8,6)
        self.containers6 = Container(10,10)
        self.containers7 = Container(11,11)
        self.containers8 = Container(7,12) 
        self.passo = 0
        self.containers2.lv = 2
        self.containers3.lv = 4
        self.containers5.lv = 4
        self.containers6.lv = 1
        self.containers8.lv = 2
        self.shower_length= 300
        
        self.containers = [self.containers1,self.containers2,self.containers3,self.containers4, self.containers5, self.containers6, self.containers7, self.containers8]
        self.positions ={}
        self.capacities ={}
        self.camions= []
        b = 0
        for cont in self.containers:
            b += cont.lv
        reward = 0
        nCamionFloat = 0
        while b &gt; 6:
          b +=-10
          nCamionFloat +=1
        nCamionInt = int(nCamionFloat)
       
        for ic in range(nCamionInt):
          self.camions.append(Camion(1,1,None,ic))


        for cam in self.camions:
          
          self.positions[cam.name] = cam.position  
          self.capacities[cam.name] = 10
        
        
        self.frames = []
        self.cnt=0  


        self.mapp = Map(15,15,self.camions,self.containers)

        self.state = (15*15)/5
        self.action_space = gym.spaces.Discrete(4)
        self.observation_space = Box(low = np.array([0]), high= np.array([51]))

    def step(self, action):
      
        moves = {0: (-1, 0),1: (1, 0),2: (0, -1),3: (0, 1)}
        
        done = False
       
        ic = 0   
        for cam in self.camions: 
            cam.position = (self.positions[ic][0],self.positions[ic][1])            
            cam.capacity = self.capacities[ic] 
            
            self.state += -5
            

        mossa = moves[action]
        x=self.camions[self.i].position
        reward = 0
        nuovaposizione = [mossa[0] + x[0],mossa[1] +x[1]]
        self.shower_length -= 1 
        if self.mapp.mapp[nuovaposizione[0],nuovaposizione[1]] == -1:
          reward += -5
          self.state += -5
        
        else:
            self.mapp.mapp[x[0],x[1]] = 0
            self.camions[self.i].position=nuovaposizione
            self.mapp.mapp[nuovaposizione[0],nuovaposizione[1]] = 9
            self.positions.update({self.camions[self.i].name : nuovaposizione})
           
            
            
            
            reward += -1
            self.state = -2
            

        for contain in self.containers:
                  if self.camions[self.i].position[0] == contain.position[0] and camion.position[1] == contain.position[1] :
                        
                        if contain.lv ==3 and self.camions[self.i].capacity &gt;=3:
                            self.camions[self.i].reward += 100
                            self.camions[self.i].capacity += -3
                            self.capacities.update({self.camions[self.i].name : self.camions[self.i].capacity})
                            reward +=20
                            
                            self.state +=20
                         
                            contain.lv=0

                        elif contain.lv == 2 and self.camions[self.i].capacity &gt;=2:
                            self.camions[self.i].reward += 50
                            self.camions[self.i].capacity += -2
                            self.capacities.update({self.camions[self.i].name : self.camions[self.i].capacity})
                            self.state +=10
                          
                            reward += 50
                            
                            contain.lv=0

                        elif contain.lv == 1 and self.camions[self.i].capacity &gt;=1:
                            
                            reward += 10
                            self.camions[self.i].reward +=5
                            self.camions[self.i].capacity += -1
                            self.capacities.update({self.camions[self.i].name : self.camions[self.i].capacity})
                            contain.lv=0
                            self.state+=1
                           
                        elif contain.lv==4 and self.camions[self.i].capacity &gt;=4:
                            reward +=50
                            self.camions[self.i].reward +=50
                            self.camions[self.i].capacity += -4
                            self.capacities.update({self.camions[self.i].name : self.camions[self.i].capacity})
                            self.state +=50
                            contain.lv=0
                           
                          
                        elif contain.lv==0 and self.camions[self.i].capacity &gt;=4:
                            reward += -20
                            self.camions[self.i].reward +=-20
                            self.camions[self.i].capacity += 0
                            self.state += -20
                            contain.lv=0
                         
                        
                  if self.camions[self.i].capacity &lt;=2:
                              self.camions[self.i].positions=(1,1)
                              self.positions.update({self.camions[self.i].name : (1,1)})

                              self.camions[self.i].capacity = 10
                              self.capacities.update({self.camions[self.i].name : self.camions[self.i].capacity})

                  
                  

                          

        if self.i ==1:
                      self.i= 0              
                      self.i = 0
                      self.i = 0
        elif self.i ==0:
                      self.i= 1


        if self.shower_length &lt;= 0: 
            done = True
        else:
            done = False
        

        self.passo +=1
       
        
        
        
        

        info = {}
        
        return self.state,reward,done,info



    def render(self, mode=&quot;human&quot;):
           
            BLACK = (0, 0, 0)
            WHITE = (200, 200, 200)
            
            WINDOW_HEIGHT = len(self.mapp.mapp[0]) *50
            WINDOW_WIDTH = len(self.mapp.mapp[0]) *50
           
            whiteC=pygame.image.load('white.jpg')
            whiteC=pygame.transform.scale(whiteC,(50, 50))
           
            greenC=pygame.image.load('green.jpg')
            greenC=pygame.transform.scale(greenC,(50, 50))
            
            yellowC=pygame.image.load('yellow.jpg')
            yellowC=pygame.transform.scale(yellowC,(50, 50))

            orangeC=pygame.image.load('orange.jpg')
            orangeC=pygame.transform.scale(orangeC,(50, 50))

            redC=pygame.image.load('red.jpg')
            redC=pygame.transform.scale(redC,(50, 50))

            
            gT=pygame.image.load('greenCamion.jpg')
            gT=pygame.transform.scale(gT,(50, 50))

            yT=pygame.image.load('yellowCamion.jpg')
            yT=pygame.transform.scale(yT,(50, 50))

            rT=pygame.image.load('redCamion.jpg')
            rT=pygame.transform.scale(rT,(50, 50))
            
           
            
            
            global SCREEN, CLOCK
            pygame.init()
            SCREEN = pygame.display.set_mode((WINDOW_WIDTH, WINDOW_HEIGHT))
            CLOCK = pygame.time.Clock()
            SCREEN.fill(BLACK)
            
            pygame.draw.rect(SCREEN, WHITE, pygame.Rect( 10, 0, 50, 50))
            blockSize = 50 #Set the size of the grid block
            
            for i in range(0,len(self.mapp.mapp[0])):
              for j in range(0,len(self.mapp.mapp[0])):
                      a=i*50
                      b=j*50
                    
                      if self.mapp.mapp[i][j] == -1:
                        pygame.draw.rect(SCREEN, WHITE, pygame.Rect( a, b, 50, 50))

            for c in self.camions :
              if c.capacity &gt; 6:
                SCREEN.blit(gT, (c.position[0]*50, c.position[1]*50))
              
              if c.capacity &gt; 3 and c.capacity &lt;= 6:
                SCREEN.blit(yT, (c.position[0]*50, c.position[1]*50))     
              
              if c.capacity &lt;= 3:
                SCREEN.blit(rT, (c.position[0]*50, c.position[1]*50))
            
            
            for contain in self.containers :
              if contain.lv == 0:
                 SCREEN.blit(whiteC,(contain.position[0]*50 , contain.position[1]*50))
              
              elif contain.lv == 1:
                  SCREEN.blit(greenC,(contain.position[0]*50 , contain.position[1]*50))
              
              elif contain.lv == 2:
                  SCREEN.blit(yellowC,(contain.position[0]*50 , contain.position[1]*50))
              
              elif contain.lv == 3:
                 SCREEN.blit(orangeC,(contain.position[0]*50 , contain.position[1]*50))
              
              if contain.lv == 4:
                 SCREEN.blit(redC,(contain.position[0]*50 , contain.position[1]*50))
              
                
            
            for x in range(0, WINDOW_WIDTH, blockSize):
                for y in range(0, WINDOW_HEIGHT, blockSize):
                    rect = pygame.Rect(x, y, blockSize, blockSize)
                    pygame.draw.rect(SCREEN, WHITE, rect, 1)

            pygame.display.flip()

            view = pygame.surfarray.array3d(SCREEN)
            view = view.transpose([1, 0, 2])

            img_bgr = cv2.cvtColor(view, cv2.COLOR_RGB2BGR)
           
            
            
            
            
            pygame.image.save(SCREEN, f&quot;screenshot{self.cnt}.png&quot;)
            self.cnt +=1
            pygame.event.get()
                    


       
    def reset(self):
        self.state = (15*15)/4
        self.shower_length = 300
        
        self.containers1.lv=3
        self.containers2.lv=1
        self.containers7.lv = 2 
        self.containers3.lv = 4
        self.containers5.lv = 4
        self.containers6.lv = 1
        self.containers8.lv = 2
        self.passo = 0
        self.positions ={}
        self.capacities ={}
        self.camions= []
        b = 0
        for cont in self.containers:
            b += cont.lv
        reward = 0
        nCamionFloat = 0
        while b &gt; 6:
          b +=-10
          nCamionFloat +=1
        nCamionInt = int(nCamionFloat)
      
        for ic in range(nCamionInt):
          self.camions.append(Camion(1,1,None,ic))


        for cam in self.camions:
          
          self.positions[cam.name] = cam.position  
          self.capacities[cam.name] = 10
        
        self.shower_length =60
        self.cnt=0  
        self.i = 0
            
            
        




        containers = [    containers1,    containers2,    containers3,    containers4]
        containers.append(    containers1)


</code></pre>
<pre><code>states = env.observation_space.shape
actions = env.action_space.n
b = env.action_space.sample()
</code></pre>
<p>My model</p>
<pre><code>def build_model(states,actions):
  model = tf.keras.Sequential([
      keras.layers.Dense(64, input_shape=states),
      keras.layers.LeakyReLU(0.24,),
      keras.layers.Dense(64),
      keras.layers.LeakyReLU(0.24,),
      keras.layers.Dense(32),
      keras.layers.LeakyReLU(0.24,),
  
      keras.layers.Dense(16),
      keras.layers.LeakyReLU(0.24,),
      keras.layers.Dense(8),
      keras.layers.LeakyReLU(0.24,),
      
      keras.layers.Dense(actions, activation='linear'),
      
])
  return model
</code></pre>
<pre><code>

model = build_model(states, actions)
model.compile(loss='mse',  metrics=['accuracy'])

</code></pre>
<pre><code>def build_agent(model, actions):
      policy = GreedyQPolicy()
      memory = SequentialMemory(limit=10000, window_length=1)
      dqn = DQNAgent(model=model, memory=memory, policy=policy,nb_actions=actions, nb_steps_warmup=10, target_model_update=1e-2)
      
      return dqn
</code></pre>
<pre><code>

dqn = build_agent(model, actions)
dqn.compile(tf.keras.optimizers.Adadelta(
    learning_rate=0.1, rho=0.95, epsilon=1e-07, name='Adadelta'), metrics= [&quot;accuracy&quot;] 
)

a =dqn.fit(env, nb_steps=5000, visualize=True, verbose=2,)

</code></pre>
<p>the loss starts from 50 and reaches 200</p>
",9224530,,,,,2022-03-25 10:49:03,Why does my model not learn? Very high loss,<tensorflow><keras><deep-learning><reinforcement-learning><openai-gym>,2,1,,,,CC BY-SA 4.0,
1007,71625570,1,71626567,,2022-03-26 05:33:44,,1,1449,"<p>I am trying to use TaxiEnvironment of OpenAI Gym. I have written the following lines of code and I am getting the following error.</p>
<pre><code>import numpy as np
import gym
import random
env = gym.make(&quot;Taxi-v3&quot;)
env.render()
</code></pre>
<p>Error:</p>
<pre><code>AttributeError                            Traceback (most recent call last)
C:\Users\KESABC~1\AppData\Local\Temp/ipykernel_11956/4159949162.py in &lt;module&gt;
      1 env = gym.make(&quot;Taxi-v3&quot;)
----&gt; 2 env.render()

~\anaconda3\lib\site-packages\gym\core.py in render(self, mode, **kwargs)
    284 
    285     def render(self, mode=&quot;human&quot;, **kwargs):
--&gt; 286         return self.env.render(mode, **kwargs)
    287 
    288     def close(self):

~\anaconda3\lib\site-packages\gym\core.py in render(self, mode, **kwargs)
    284 
    285     def render(self, mode=&quot;human&quot;, **kwargs):
--&gt; 286         return self.env.render(mode, **kwargs)
    287 
    288     def close(self):

~\anaconda3\lib\site-packages\gym\envs\toy_text\taxi.py in render(self, mode)
    220         out = self.desc.copy().tolist()
    221         out = [[c.decode(&quot;utf-8&quot;) for c in line] for line in out]
--&gt; 222         taxi_row, taxi_col, pass_idx, dest_idx = self.decode(self.s)
    223 
    224         def ul(x):

AttributeError: 'TaxiEnv' object has no attribute 's'
</code></pre>
<p>What should I do to remove the error?</p>
",11175542,,11175542,,2022-03-26 05:42:09,2022-03-26 08:47:54,AttributeError: 'TaxiEnv' object has no attribute 's',<python><jupyter-notebook><anaconda><openai-gym>,1,4,,,,CC BY-SA 4.0,
1008,71626453,1,71907547,,2022-03-26 08:30:46,,2,638,"<p>I am trying to fine the shortest route between two nodes using reinforcement learning. I am not sure what environment to use. I have found this <a href=""https://pypi.org/project/route-gym/#description"" rel=""nofollow noreferrer"">particular environment</a> and am not sure if I am going in the right direction. Can anybody please help. Can anybody please suggest a few python OpenAI gym environments I can use.</p>
",11175542,,4420967,,2022-03-26 12:52:50,2022-05-06 14:58:28,Which OpenAI gym environment should be used for solve the shortest route problem?,<python><reinforcement-learning><openai-gym>,1,0,0,,,CC BY-SA 4.0,
1024,71700563,1,71723001,,2022-04-01 00:35:03,,0,450,"<p>I am very sure that I followed the correct steps to register my custom environment in the AI Gym. But I face a problem when <strong>one __ init__.py file is not recognizing a folder and gives no module found error.</strong>  I use Anaconda Jupyterlab through OneDrive so that it is synced and I work from any device.</p>
<p>The path is C-&gt; Users-&gt; myname-&gt;OneDrive-&gt; My_code-&gt;gym_mycode</p>
<ol>
<li>gym_mycode--&gt; envs folder and a first __ init__.py file</li>
<li>Inside envs--&gt;another second __ init__.py, custom_env, and some other files</li>
</ol>
<p>Contents of my first __ init__ are:</p>
<pre><code>from gym.envs.registration import register
register(id=&quot;PyABCD-v0&quot;, entry_point=&quot;gym_mycode.envs:CustomEnv_class&quot;)
</code></pre>
<p>Contents of my second __ init__ inside the envs folder are:</p>
<pre><code>from gym_mycode.envs.custom_env import CustomEnv_class
</code></pre>
<p>This second one gives me the error  No module named 'gym_mycode'. How is it possible that it is not recognizing the gym_mycode folder? Is it because I am operating this whole thing inside OneDrive and not some Anaconda specific folder?</p>
<p>I run the first init first and then the second init inside the envs. Hope this order of running is correct.</p>
<p><strong>Edit</strong> As asked, below is the current dir and traceback.</p>
<p><code>os.getcwd()</code> is <code>C:\Users\HP\OneDrive\My_code\gym_mycode\envs</code></p>
<pre><code>ModuleNotFoundError                       Traceback (most recent call last)
Input In [23], in &lt;cell line: 6&gt;()
      3 import sys
      4 sys.path.append('C:\\Users\\HP\\OneDrive\\My_code\\gym_mycode')
----&gt; 6 from gym_mycode.envs.custom_env import CustomEnv_class

ModuleNotFoundError: No module named 'gym_mycode'
</code></pre>
",15993130,,1832058,,2022-04-03 04:27:27,2022-04-03 04:27:27,AI Gym environment: no module found error despite all steps correct,<python><module><jupyter-notebook><jupyter-lab><openai-gym>,1,4,,,,CC BY-SA 4.0,
1028,71786530,1,71831080,,2022-04-07 17:24:49,,0,188,"<p>I am trying to train a custom environment using PPO via Stable-Baselines3 and OpenAI Gym. For some reason the rollout statistics are not being reported for this custom environment when I try to train the PPO model.</p>
<p>The code that I am using is below ( I have not included the  code for the CustomEnv for brevity):</p>
<pre><code>env = CustomEnv(mode = &quot;discrete&quot;)
env = Monitor(env, log_dir)
model = PPO(&quot;MlpPolicy&quot;, env, verbose=1, tensorboard_log = log_dir)

timesteps = 5000
for i in range(3):
  model.learn(total_timesteps = timesteps, reset_num_timesteps = False, tb_log_name = &quot;PPO&quot;)
  model.save(f&quot;{models_dir}/car_model_{timesteps * i}&quot;)
</code></pre>
<p>Below is an image demonstrating the output from the above code (on the right of the image), and the left side of the image demonstrates the usual output from a dummy environment that I am using for debugging.</p>
<p><a href=""https://i.stack.imgur.com/o2RXh.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/o2RXh.png"" alt=""On the left we have the usual output from model.learn() applied to a dummy environment, with rollout statistics being reported. On the right is my custom environment, where only 'time' and 'train' statistics are being reported."" /></a></p>
<p>I have already tried adding the line of code:</p>
<pre><code>env = Monitor(env, log_dir)
</code></pre>
<p>But that doesnt change the output.</p>
",17183483,,,,,2022-04-11 16:15:31,Rollout summary statistics not being monitored for CustomEnv using Stable-Baselines3,<reinforcement-learning><openai-gym><stable-baselines><openai>,1,0,,,,CC BY-SA 4.0,
1030,71933644,1,71950845,,2022-04-20 03:07:42,,0,463,"<p>I am working with OpenAI gym on colab and using the following code to render the videos within Jupyter notebook based on this tutorial: <a href=""https://colab.research.google.com/drive/1flu31ulJlgiRL1dnN2ir8wGh9p7Zij2t"" rel=""nofollow noreferrer"">https://colab.research.google.com/drive/1flu31ulJlgiRL1dnN2ir8wGh9p7Zij2t</a></p>
<pre><code>import gym
from gym import logger as gymlogger
from gym.wrappers import Monitor
gymlogger.set_level(40) #error only
import numpy as np
import random
import matplotlib
import matplotlib.pyplot as plt
import seaborn as sns
%matplotlib inline
import math
import glob
import io
import base64

from IPython.display import HTML
from IPython import display as ipythondisplay, display

import tensorflow as tf
from tensorflow.keras import Sequential
from tensorflow.keras.layers import Dense, ConvLSTM2D, Flatten, Conv2D
from tf_agents.agents.dqn import dqn_agent
from tf_agents.replay_buffers import tf_uniform_replay_buffer
from tf_agents.specs import tensor_spec

display = Display(visible=0, size=(1400, 900))
display.start()

def show_video():
  mp4list = glob.glob('video/*.mp4')
  if len(mp4list) &gt; 0:
    mp4 = mp4list[0]
    video = io.open(mp4, 'r+b').read()
    encoded = base64.b64encode(video)
    ipythondisplay.display(HTML(data='''&lt;video alt='test' autoplay 
                loop controls style='height: 400px;'&gt;
                &lt;source src='data:video/mp4;base64,{0}' type='video/mp4' /&gt;
             &lt;/video&gt;'''.format(encoded.decode('ascii'))))
  else: 
    print('Could not find video')
    

def wrap_env(env):
  env = Monitor(env, './video', force=True)
  return env

env, spec = query_environment('MsPacman-v0')
env = wrap_env(env)

observation = env.reset()

while True:
    env.render()
    #your agent goes here
    action = env.action_space.sample() 
    observation, reward, done, info = env.step(action) 
    if done: 
        break;
            
env.close()
show_video()
</code></pre>
<p>On my M1 Mac, I cannot run set up the display with these two lines below due to missing xvfb. All my packages on the M1 Mac are running natively.</p>
<pre><code>display = Display(visible=0, size=(1400, 900))
display.start()
</code></pre>
<p>I have tried installing xvfbwrapper and importing Xvfb then and replacing the first line as shown below but the error is can not find xvfb. I read and found that xvfb is not shipped with Macs any more so I installed it via Xquartz: <a href=""https://www.xquartz.org/"" rel=""nofollow noreferrer"">https://www.xquartz.org/</a> but it still does not work.</p>
<pre><code>from xvfbwrapper import Xvfb
display = Xvfb(width=1400, height=900)
</code></pre>
<p>Without these lines, there is still a video generated but it is in a new window and not within jupyter notebook itself. (There is also an issue that I cannot seem to close that window once it is generated but that is another issue).</p>
<p>My question is whether there is any modification I can do to make the code above work? If not, are there any other ways to render the video within the notebook itself?</p>
<p>Thank you.</p>
",15279634,,,,,2022-04-21 08:00:16,Getting xvfb to work in jupyter notebook on M1 Mac,<jupyter-notebook><apple-m1><openai-gym><xvfb>,1,0,,,,CC BY-SA 4.0,
1043,71973392,1,72001691,,2022-04-22 18:49:22,,3,3199,"<p>I'm working with RL agents, and was trying to replicate the finding of the this <a href=""https://github.com/openai/gym/issues/2656"" rel=""nofollow noreferrer"">paper</a>, wherein they make a custom parkour environment based on Gym open AI, however when trying to render this environment I run into.</p>
<pre><code>import numpy as np
import time
import gym
import TeachMyAgent.environments

env = gym.make('parametric-continuous-parkour-v0', agent_body_type='fish', movable_creepers=True)
env.set_environment(input_vector=np.zeros(3), water_level = 0.1)
env.reset()

while True:
    _, _, d, _ = env.step(env.action_space.sample())
    env.render(mode='human')
    time.sleep(0.1)

c:\users\manu dwivedi\teachmyagent\TeachMyAgent\environments\envs\parametric_continuous_parkour.py in render(self, mode, draw_lidars)
    462 
    463     def render(self, mode='human', draw_lidars=True):
--&gt; 464         from gym.envs.classic_control import rendering
    465         if self.viewer is None:
    466             self.viewer = rendering.Viewer(RENDERING_VIEWER_W, RENDERING_VIEWER_H)

ImportError: cannot import name 'rendering' from 'gym.envs.classic_control' (C:\ProgramData\Anaconda3\envs\teachagent\lib\site-packages\gym\envs\classic_control\__init__.py)

  [1]: https://github.com/flowersteam/TeachMyAgent
</code></pre>
<p>I thought this might be a problem with this custom environments and how the authors decided to render it, however, when I try just</p>
<pre><code>from gym.envs.classic_control import rendering
</code></pre>
<p>I run into the same error, github users <a href=""https://github.com/openai/gym/issues/2656"" rel=""nofollow noreferrer"">here</a> suggested this can be solved by adding <code>rendor_mode='human'</code> when calling <code>gym.make()</code> rendering, but this seems to only goes for their specific case.</p>
",12853807,,12892553,,2022-05-22 09:37:20,2022-05-22 09:37:20,ImportError: cannot import 'rendering' from 'gym.envs.classic_control',<python><reinforcement-learning><openai-gym>,2,0,,,,CC BY-SA 4.0,
1046,72089234,1,72089235,,2022-05-02 15:52:41,,1,356,"<p>So I am running into an interesting bug when writing a custom OpenAI gym environment. The code below is the minimal environment I can write that reproduces the bug:</p>
<pre class=""lang-py prettyprint-override""><code>import gym

class TestEnv(gym.Env):
    obs = 3

    def set_obs(self, obs):
        self.obs = obs

    def reset(self):
        return self.obs

    def step(self, a):
        return self.obs, 0, False, None

gym.register('TestEnv-v0', 
             entry_point=f'{__name__}:TestEnv',
             max_episode_steps=1000)

env = gym.make('TestEnv-v0')
print(env.reset())
print(env.obs)
env.obs = 5
print(env.reset())
print(env.obs)
env.set_obs(8)
print(env.reset())
print(env.obs)
</code></pre>
<pre><code>3
3
3
5
8
5
</code></pre>
<p>For some reason, there is a difference between calling <code>set_obs</code> and setting the <code>obs</code> attribute directly! I have never applied the pattern of a setter function in Python (it smells of Java too much), but here I have to use it so that the new <code>obs</code> is actually taken into account.</p>
",3554721,,,,,2022-05-02 15:52:41,Why I can't I set an attribute on an OpenAI Gym environment directly,<python><openai-gym>,1,0,,,,CC BY-SA 4.0,
1048,72428936,1,72428960,,2022-05-30 03:43:04,,0,367,"<p>I am trying to train my model, which is a breakout game in gym. I am trying to train the environment with 100000 timesteps. However, it keeps returning this error message. Can someone explain why and help me solve this?
I am a beginner in machine learning.
Here is the code and the error message below:</p>
<pre><code>import gym
from stable_baselines3 import A2C
from stable_baselines3.common.vec_env import VecFrameStack
from stable_baselines3.common.evaluation import evaluate_policy
from stable_baselines3.common.env_util import make_atari_env
import os
import numpy as np
env = make_atari_env(&quot;ALE/Breakout-v5&quot;, n_envs=4, seed=0)
env = VecFrameStack(env, n_stack=4)
log_path = os.path.join(&quot;Traning&quot;, &quot;Logs&quot;)
model = A2C(&quot;CnnPolicy&quot;, env, verbose=1, tensorboard_log=log_path)
model.learn(total_timesteps=100000)

ERROR MESSAGE:
---------------------------------------------------------------------------
AttributeError                            Traceback (most recent call last)
~\AppData\Local\Temp/ipykernel_7676/2886439321.py in &lt;module&gt;
----&gt; 1 model.learn(total_timesteps=100000)

D:\Anaconda\lib\site-packages\stable_baselines3\a2c\a2c.py in learn(self, total_timesteps, callback, log_interval, eval_env, eval_freq, n_eval_episodes, tb_log_name, eval_log_path, reset_num_timesteps)
    189     ) -&gt; &quot;A2C&quot;:
    190 
--&gt; 191         return super(A2C, self).learn(
    192             total_timesteps=total_timesteps,
    193             callback=callback,

D:\Anaconda\lib\site-packages\stable_baselines3\common\on_policy_algorithm.py in learn(self, total_timesteps, callback, log_interval, eval_env, eval_freq, n_eval_episodes, tb_log_name, eval_log_path, reset_num_timesteps)
    240         iteration = 0
    241 
--&gt; 242         total_timesteps, callback = self._setup_learn(
    243             total_timesteps, eval_env, callback, eval_freq, n_eval_episodes, eval_log_path, reset_num_timesteps, tb_log_name
    244         )

D:\Anaconda\lib\site-packages\stable_baselines3\common\base_class.py in _setup_learn(self, total_timesteps, eval_env, callback, eval_freq, n_eval_episodes, log_path, reset_num_timesteps, tb_log_name)
    427         # Avoid resetting the environment when calling ``.learn()`` consecutive times
    428         if reset_num_timesteps or self._last_obs is None:
--&gt; 429             self._last_obs = self.env.reset()  # pytype: disable=annotation-type-mismatch
    430             self._last_episode_starts = np.ones((self.env.num_envs,), dtype=bool)
    431             # Retrieve unnormalized observation for saving into the buffer

D:\Anaconda\lib\site-packages\stable_baselines3\common\vec_env\vec_transpose.py in reset(self)
    108         Reset all environments
    109         &quot;&quot;&quot;
--&gt; 110         return self.transpose_observations(self.venv.reset())
    111 
    112     def close(self) -&gt; None:

D:\Anaconda\lib\site-packages\stable_baselines3\common\vec_env\vec_frame_stack.py in reset(self)
     56         Reset all environments
     57         &quot;&quot;&quot;
---&gt; 58         observation = self.venv.reset()  # pytype:disable=annotation-type-mismatch
     59 
     60         observation = self.stackedobs.reset(observation)

D:\Anaconda\lib\site-packages\stable_baselines3\common\vec_env\dummy_vec_env.py in reset(self)
     59     def reset(self) -&gt; VecEnvObs:
     60         for env_idx in range(self.num_envs):
---&gt; 61             obs = self.envs[env_idx].reset()
     62             self._save_obs(env_idx, obs)
     63         return self._obs_from_buf()

D:\Anaconda\lib\site-packages\gym\core.py in reset(self, **kwargs)
    290 
    291     def reset(self, **kwargs):
--&gt; 292         return self.env.reset(**kwargs)
    293 
    294     def render(self, mode=&quot;human&quot;, **kwargs):

D:\Anaconda\lib\site-packages\gym\core.py in reset(self, **kwargs)
    331 class RewardWrapper(Wrapper):
    332     def reset(self, **kwargs):
--&gt; 333         return self.env.reset(**kwargs)
    334 
    335     def step(self, action):

D:\Anaconda\lib\site-packages\gym\core.py in reset(self, **kwargs)
    317 class ObservationWrapper(Wrapper):
    318     def reset(self, **kwargs):
--&gt; 319         observation = self.env.reset(**kwargs)
    320         return self.observation(observation)
    321 

D:\Anaconda\lib\site-packages\stable_baselines3\common\atari_wrappers.py in reset(self, **kwargs)
     57 
     58     def reset(self, **kwargs) -&gt; np.ndarray:
---&gt; 59         self.env.reset(**kwargs)
     60         obs, _, done, _ = self.env.step(1)
     61         if done:

D:\Anaconda\lib\site-packages\stable_baselines3\common\atari_wrappers.py in reset(self, **kwargs)
    104         &quot;&quot;&quot;
    105         if self.was_real_done:
--&gt; 106             obs = self.env.reset(**kwargs)
    107         else:
    108             # no-op step to advance from terminal/lost life state

D:\Anaconda\lib\site-packages\stable_baselines3\common\atari_wrappers.py in reset(self, **kwargs)
    152 
    153     def reset(self, **kwargs) -&gt; GymObs:
--&gt; 154         return self.env.reset(**kwargs)
    155 
    156 

D:\Anaconda\lib\site-packages\stable_baselines3\common\atari_wrappers.py in reset(self, **kwargs)
     34             noops = self.override_num_noops
     35         else:
---&gt; 36             noops = self.unwrapped.np_random.randint(1, self.noop_max + 1)
     37         assert noops &gt; 0
     38         obs = np.zeros(0)

AttributeError: 'numpy.random._generator.Generator' object has no attribute 'randint'
</code></pre>
",12585467,,13086128,,2022-05-30 04:15:21,2022-06-05 17:42:39,Why does model.learn() return a numpy error?,<python><numpy><reinforcement-learning><openai-gym><stable-baselines>,1,3,,,,CC BY-SA 4.0,
1057,72550211,1,73275034,,2022-06-08 17:55:38,,1,626,"<p>I am writing the code for Autonomous Driving using RL. I am using a stable baseline3 and an open ai gym environment. I was running the following code in the jupyter notebook and it is giving me the following error:</p>
<pre><code># Testing our model
episodes = 5 # test the environment 5 times
for episodes in range(1,episodes+1): # looping through each episodes
    bs = env.reset() # observation space
    # Taking the obs and passing it through our model
    # tells that which kind of the action is best for our work
    done = False 
    score = 0
    while not done:
        env.render()
        action, _ = model.predict(obs) # now using model here # returns model action and next 
state
        # take that action to get the best reward
        # for observation space we get the box environment
        # rather than getting random action we are using model.predict(obs) on our obs for an 
curr env to gen the action inorder to get best possible reward
        obs, reward, done, info = env.step(action)  # gies state, reward whose value is 1
        # reward is 1 for every step including the termination step
        score += reward
    print('Episode:{},Score:{}'.format(episodes,score))'''
env.close()
</code></pre>
<p>Error
<a href=""https://i.stack.imgur.com/cMkn7.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/cMkn7.png"" alt=""enter image description here"" /></a></p>
<p><a href=""https://i.stack.imgur.com/INVgZ.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/INVgZ.png"" alt=""enter image description here"" /></a></p>
<p><a href=""https://i.stack.imgur.com/Lentg.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/Lentg.png"" alt=""enter image description here"" /></a></p>
<p>The link for the code that I have written is given below:
<a href=""https://drive.google.com/file/d/1JBVmPLn-N1GCl_Rgb6-qGMpJyWvBaR1N/view?usp=sharing"" rel=""nofollow noreferrer"">https://drive.google.com/file/d/1JBVmPLn-N1GCl_Rgb6-qGMpJyWvBaR1N/view?usp=sharing</a></p>
<p>The version of python I am using is Python 3.8.13 in Anaconda Environment.
I am using Pytorch CPU version and the OS is Windows 10.
Please help me out in solving this question.</p>
",17065938,,,,,2022-08-10 12:32:14,"ValueError: At least one stride in the given numpy array is negative, and tensors with negative strides are not currently supported",<python><reinforcement-learning><openai-gym><stable-baselines>,1,0,0,,,CC BY-SA 4.0,
1066,72673692,1,72844035,,2022-06-19 01:08:37,,2,101,"<p>hello I've trained a PPO model from stabel_baselines3 on collab I saved it</p>
<pre><code>model.save(&quot;model&quot;)
</code></pre>
<p>but when I tried loading it I got the following error:</p>
<pre><code>m = PPO.load(&quot;model&quot;, env=env)
AttributeError                            Traceback (most recent call last)
/tmp/ipykernel_25649/121834194.py in &lt;module&gt;
      2 env = e.MinitaurBulletEnv(render=False)
      3 env.reset()
----&gt; 4 m2 = PPO.load(&quot;model&quot;, env=env)
      5 for episode in range(1, 6):
      6     obs = env.reset()

~/anaconda3/lib/python3.8/site-packages/stable_baselines3/common/base_class.py in load(cls, path, env, device, custom_objects, **kwargs)
    668             env = cls._wrap_env(env, data[&quot;verbose&quot;])
    669             # Check if given env is valid
--&gt; 670             check_for_correct_spaces(env, data[&quot;observation_space&quot;], data[&quot;action_space&quot;])
    671         else:
    672             # Use stored env, if one exists. If not, continue as is (can be used for predict)

~/anaconda3/lib/python3.8/site-packages/stable_baselines3/common/utils.py in check_for_correct_spaces(env, observation_space, action_space)
    217     :param action_space: Action space to check against
    218     &quot;&quot;&quot;
--&gt; 219     if observation_space != env.observation_space:
    220         raise ValueError(f&quot;Observation spaces do not match: {observation_space} != {env.observation_space}&quot;)
    221     if action_space != env.action_space:

~/anaconda3/lib/python3.8/site-packages/gym/spaces/box.py in __eq__(self, other)
    138 
    139     def __eq__(self, other):
--&gt; 140         return isinstance(other, Box) and (self.shape == other.shape) and np.allclose(self.low, other.low) and np.allclose(self.high, other.high)

AttributeError: 'Box' object has no attribute 'shape'
</code></pre>
<p>knowing that the env is a box env from pybullet</p>
<pre><code>import pybullet_envs.bullet.minitaur_gym_env as e
import gym

env = e.MinitaurBulletEnv(render=False)
env.reset()
</code></pre>
<p>additional info is that the model loaded perfectly in collab</p>
",17629981,,17629981,,2022-06-19 01:16:39,2022-07-03 05:07:48,unabel to load a ppo model,<python-3.x><google-colaboratory><openai-gym><stable-baselines><pybullet>,1,0,0,,,CC BY-SA 4.0,
1069,72529895,1,72532957,,2022-06-07 10:50:26,,0,109,"<p>I am trying to implement a DDPG agent to control the <a href=""https://www.gymlibrary.ml/environments/classic_control/pendulum/"" rel=""nofollow noreferrer"">Gym's Pendulum</a>.
Since I am new to gym, I was wondering if the state data collected via <code>env.step(action)</code> is already normalized or I should do that manually. Also, should <code>action</code> be normalized or in the [-2, 2] range?</p>
<p>Thanks</p>
",431909,,,,,2022-06-07 14:26:52,Is AI Gym's action and state data normalized?,<deep-learning><pytorch><reinforcement-learning><openai-gym>,1,0,,,,CC BY-SA 4.0,
1084,73111772,1,73192247,,2022-07-25 15:30:17,,0,377,"<p>I am having trouble in making things work with a Custom <code>ParallelEnv</code> I wrote by using <a href=""https://www.pettingzoo.ml/"" rel=""nofollow noreferrer"">PettingZoo</a>. I am using <a href=""https://github.com/Farama-Foundation/SuperSuit"" rel=""nofollow noreferrer"">SuperSuit</a>'s <code>ss.pettingzoo_env_to_vec_env_v1(env)</code> as a wrapper to Vectorize the environment and make it work with <a href=""https://github.com/DLR-RM/stable-baselines3"" rel=""nofollow noreferrer"">Stable-Baseline3</a> and documented <a href=""https://github.com/Farama-Foundation/SuperSuit#parallel-environment-vectorization"" rel=""nofollow noreferrer"">here</a>.</p>
<p>You can find attached a summary of the most relevant part of the code:</p>
<pre class=""lang-py prettyprint-override""><code>from typing import Optional
from gym import spaces
import random
import numpy as np
from pettingzoo import ParallelEnv
from pettingzoo.utils.conversions import parallel_wrapper_fn
import supersuit as ss
from gym.utils import EzPickle, seeding


def env(**kwargs):
    env_ = parallel_env(**kwargs)
    env_ = ss.pettingzoo_env_to_vec_env_v1(env_)
    #env_ = ss.concat_vec_envs_v1(env_, 1)
    return env_


petting_zoo = env


class parallel_env(ParallelEnv, EzPickle):
    metadata = {'render_modes': ['ansi'], &quot;name&quot;: &quot;PlayerEnv-Multi-v0&quot;}

    def __init__(self, n_agents: int = 20, new_step_api: bool = True) -&gt; None:
        EzPickle.__init__(
            self,
            n_agents,
            new_step_api
        )

        self._episode_ended = False
        self.n_agents = n_agents

        self.possible_agents = [
            f&quot;player_{idx}&quot; for idx in range(n_agents)]

        self.agents = self.possible_agents[:]

        self.agent_name_mapping = dict(
            zip(self.possible_agents, list(range(len(self.possible_agents))))
        )

        self.observation_spaces = spaces.Dict(
            {agent: spaces.Box(shape=(len(self.agents),),
                               dtype=np.float64, low=0.0, high=1.0) for agent in self.possible_agents}
        )

        self.action_spaces = spaces.Dict(
            {agent: spaces.Discrete(4) for agent in self.possible_agents}
        )
        self.current_step = 0

    def seed(self, seed=None):
        self.np_random, seed = seeding.np_random(seed)

    def observation_space(self, agent):
        return self.observation_spaces[agent]

    def action_space(self, agent):
        return self.action_spaces[agent]

    def __calculate_observation(self, agent_id: int) -&gt; np.ndarray:
        return self.observation_space(agent_id).sample()

    def __calculate_observations(self) -&gt; np.ndarray:
        observations = {
            agent: self.__calculate_observation(
                agent_id=agent)
            for agent in self.agents
        }
        return observations

    def observe(self, agent):
        return self.__calculate_observation(agent_id=agent)

    def step(self, actions):
        if self._episode_ended:
            return self.reset()
        observations = self.__calculate_observations()
        rewards = random.sample(range(100), self.n_agents)
        self.current_step += 1
        self._episode_ended = self.current_step &gt;= 100
        infos = {agent: {} for agent in self.agents}
        dones = {agent: self._episode_ended for agent in self.agents}
        rewards = {
            self.agents[i]: rewards[i]
            for i in range(len(self.agents))
        }
        if self._episode_ended:
            self.agents = {}  # To satisfy `set(par_env.agents) == live_agents`
        return observations, rewards, dones, infos

    def reset(self,
              seed: Optional[int] = None,
              return_info: bool = False,
              options: Optional[dict] = None,):
        self.agents = self.possible_agents[:]
        self._episode_ended = False
        self.current_step = 0
        observations = self.__calculate_observations()
        return observations

    def render(self, mode=&quot;human&quot;):
        # TODO: IMPLEMENT
        print(&quot;TO BE IMPLEMENTED&quot;)

    def close(self):
        pass
</code></pre>
<p>Unfortunately when I try to test with the following main procedure:</p>
<pre class=""lang-py prettyprint-override""><code>from stable_baselines3 import DQN, PPO
from stable_baselines3.common.env_checker import check_env
from dummy_env import dummy
from pettingzoo.test import parallel_api_test


if __name__ == '__main__':
    # Testing the parallel algorithm alone
    env_parallel = dummy.parallel_env()
    parallel_api_test(env_parallel)  # This works!

    # Testing the environment with the wrapper
    env = dummy.petting_zoo()

    # ERROR: AssertionError: The observation returned by the `reset()` method does not match the given observation space 
    check_env(env)  

    # Model initialization
    model = PPO(&quot;MlpPolicy&quot;, env, verbose=1)
    
    # ERROR: ValueError: could not broadcast input array from shape (20,20) into shape (20,)
    model.learn(total_timesteps=10_000)

</code></pre>
<p>I get the following error:</p>
<pre><code>AssertionError: The observation returned by the `reset()` method does not match the given observation space
</code></pre>
<p>If I skip <code>check_env()</code> I get the following one:</p>
<pre><code>ValueError: could not broadcast input array from shape (20,20) into shape (20,)
</code></pre>
<p>It seems like that <code>ss.pettingzoo_env_to_vec_env_v1(env)</code> is capable of splitting the parallel environment in multiple vectorized ones, but not for the <code>reset()</code> function.</p>
<p>Does anyone know how to fix this problem?</p>
<p>Plese find the <a href=""https://github.com/PieroMacaluso/dummy-env"" rel=""nofollow noreferrer"">Github Repository</a> to reproduce the problem.</p>
",7358319,,7358319,,2022-07-27 09:27:53,2022-08-01 10:17:51,Problem with PettingZoo and Stable-Baselines3 with a ParallelEnv,<reinforcement-learning><openai-gym><stable-baselines><multi-agent-reinforcement-learning><pettingzoo>,2,1,,,,CC BY-SA 4.0,
1085,73201176,1,74527912,,2022-08-02 01:43:37,,0,239,"<p>I am trying to learn reinforcement learning to train ai on custom games in python, and decided to use gym for the environment and stable-baselines3 for the training. I decided to start off with a basic tic tac toe environment. Here's my code</p>
<pre class=""lang-py prettyprint-override""><code>import gym
from gym import spaces
import numpy as np
from stable_baselines3.common.env_checker import check_env

class tictactoe(gym.Env):
    def __init__(self):
        #creating grid, action and obervation space
        self.box = [0,0,0,0,0,0,0,0,0]
        self.done=False
        self.turn = 1
        self.action_space = spaces.Discrete(9)
        self.observation_space = spaces.Discrete(9)

    def _get_obs(self):
        #returns the observation (the grid)
        return np.array(self.box)

    def iswinner(self, b, l):
        #function to check if a side has won
        return (b[1] == l and b[2] == l and b[3] == l) or (b[4] == l and b[5] == l and b[6] == l) or (b[7] == l and b[8] == l and b[9] == l) or (b[1] == l and b[4] == l and b[7] == l) or (b[7] == l and b[5] == l and b[3] == l) or (b[1] == l and b[5] == l and b[9] == l) or (b[8] == l and b[5] == l and b[2] == l) or (b[9] == l and b[6] == l and b[3] == l)
    
    def reset(self):
        #resets the env (grid, turn and done variable) and returns the observation
        self.box = [0,0,0,0,0,0,0,0,0]
        self.turn = 1
        self.done=False
        return self._get_obs()

    def step(self, action):
        #gives negative reward for illegal move (square occupied)
        if self.box[action] != 0:
            return self._get_obs(), -10, True, {} 
        #enters a value (1 or 2) in the grid and flips the turn
        self.box[action] = self.turn
        self.turn = (1 if self.turn == 2 else 2)
        reward = 0
        #checks if the game is over and sets a reward (+5 win, 0 draw)
        if self.iswinner([0]+self.box,1) and self.turn == 1: reward,self.done = 5,True
        elif 0 not in self.box: reward,self.done = 0,True
        #returns the observation (grid), reward, if the game is finished and extra information (empty dict for me)
        return self._get_obs(), reward, self.done, {}

    def render(self):
        #renders the board so it looks like a grid
        print(self.box[:3],self.box[3:6],self.box[6:],sep='\n')

#checking the env
env = tictactoe()
print(check_env(env))
</code></pre>
<p>Trying this code, I got the error <code>AssertionError: The observation returned by 'reset()' method must be an int</code>. I completely do not understand how this is supposed to work. Since my <code>reset</code> function returns the obervation from <code>_get_obs</code>. Is it trying to say that my observation must be an integer? That makes even less sense as now I have no idea how I'm supposed to do that.</p>
",19625345,,,,,2022-11-22 05:33:23,Python stable_baselines3 - AssertionError: The observation returned by `reset()` method must be an int,<python><reinforcement-learning><openai-gym><stable-baselines>,1,1,,,,CC BY-SA 4.0,
1098,73308140,1,73308824,,2022-08-10 14:30:25,,0,54,"<p>I'm trying to get the Atari environments from Petting Zoo working: <a href=""https://www.pettingzoo.ml/"" rel=""nofollow noreferrer"">https://www.pettingzoo.ml/</a>. I've installed the AutoROMs and I can see the multiple .bin files from all the environments in the directory where the ROMs are installed. However, when I run the following code</p>
<pre><code>import gym, supersuit
from pettingzoo.atari import boxing_v2

path = '/home/myname/miniconda/envs/gym/lib/python3.8/site-packages/multi_agent_ale_py/roms'
env = boxing_v2.env(obs_type='rgb_image', full_action_space=True, max_cycles=100000, auto_rom_install_path=path)
</code></pre>
<p>I get the following error:</p>
<pre><code>Traceback (most recent call last):
  File &quot;testingboxingpettingzoo.py&quot;, line 5, in &lt;module&gt;
    env = boxing_v2.env(obs_type=&quot;rgb_image&quot;, full_action_space=True, max_cycles=100000, auto_rom_install_path=path)
  File &quot;/home/myname/miniconda3/envs/gym/lib/python3.8/site-packages/pettingzoo/atari/base_atari_env.py&quot;, line 19, in env_fn
    env = raw_env_fn(**kwargs)
  File &quot;/home/myname/miniconda3/envs/gym/lib/python3.8/site-packages/pettingzoo/atari/boxing/boxing.py&quot;, line 10, in raw_env
    version_num = parent_file[0].split(&quot;_&quot;)[-1].split(&quot;.&quot;)[0]
IndexError: list index out of range
</code></pre>
<p>I'm following everything in the documentation so not sure what I'm missing here.</p>
",5020491,,17562044,,2022-08-10 14:33:58,2022-08-10 18:04:44,Errors with Atari Environments in PettingZooML,<python><openai-gym><atari-2600><pettingzoo>,1,0,,,,CC BY-SA 4.0,
1102,73195438,1,73195616,,2022-08-01 14:41:17,,5,4773,"<p>I am getting to know OpenAI's GYM (0.25.1) using Python3.10 with gym's environment set to <code>'FrozenLake-v1</code> (code below).</p>
<p>According to the <a href=""https://www.gymlibrary.ml/"" rel=""noreferrer"">documentation</a>, calling <code>env.step()</code> should return a tuple containing 4 values (observation, reward, done, info). However, when running my code accordingly, I get a ValueError:</p>
<p>Problematic code:</p>
<pre><code>observation, reward, done, info = env.step(new_action)
</code></pre>
<p>Error:</p>
<pre><code>      3 new_action = env.action_space.sample()
----&gt; 5 observation, reward, done, info = env.step(new_action)
      7 # here's a look at what we get back
      8 print(f&quot;observation: {observation}, reward: {reward}, done: {done}, info: {info}&quot;)

ValueError: too many values to unpack (expected 4)
</code></pre>
<p>Adding one more variable fixes the error:</p>
<pre><code>a, b, c, d, e = env.step(new_action)
print(a, b, c, d, e)
</code></pre>
<p>Output:</p>
<pre><code>5 0 True True {'prob': 1.0}
</code></pre>
<p>My interpretation:</p>
<ul>
<li><code>5</code> should be observation</li>
<li><code>0</code> is reward</li>
<li><code>prob: 1.0</code> is info</li>
<li>One of the <code>True</code>'s is done</li>
</ul>
<p>So what's the leftover boolean standing for?</p>
<p>Thank you for your help!</p>
<hr />
<p>Complete code:</p>
<pre><code>import gym

env = gym.make('FrozenLake-v1', new_step_api=True, render_mode='ansi') # build environment

current_obs = env.reset() # start new episode

for e in env.render():
    print(e)
    
new_action = env.action_space.sample() # random action

observation, reward, done, info = env.step(new_action) # perform action, ValueError!

for e in env.render():
    print(e)
</code></pre>
",18267127,,18267127,,2022-08-01 14:45:19,2022-10-17 08:07:08,OpenAI GYM's env.step(): what are the values?,<python><valueerror><openai-gym>,2,0,,,,CC BY-SA 4.0,
1103,73262597,1,73274546,,2022-08-06 19:10:26,,1,79,"<p>I create a custom environment for an example of trading bot (RL).</p>
<p>During the training I wanted to check results by using of TensorBoard, but what I see are only a few metrics, in particular only :</p>
<pre><code>-----------------------------------------
| time/                   |             |
|    fps                  | 711         |
|    iterations           | 2           |
|    time_elapsed         | 5           |
|    total_timesteps      | 4096        |
| train/                  |             |
|    approx_kl            | 0.011529377 |
|    clip_fraction        | 0.0534      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.09       |
|    explained_variance   | 0.0319      |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0119      |
|    n_updates            | 10          |
|    policy_gradient_loss | -0.00402    |
|    value_loss           | 0.0277      |
-----------------------------------------
</code></pre>
<p>according to this (<a href=""https://medium.com/aureliantactics/understanding-ppo-plots-in-tensorboard-cbc3199b9ba2"" rel=""nofollow noreferrer"">https://medium.com/aureliantactics/understanding-ppo-plots-in-tensorboard-cbc3199b9ba2</a>) I expected more metrics, in particular something about the reward such as <code>rollout/ep_lean_mean</code> and <code>rollout/ep_rew_mean</code></p>
<p>this is my code:</p>
<pre><code>
import gym 
from gym import spaces 
 
class customEnv(gym.Env): 
    &quot;&quot;&quot;Custom Environment that follows gym interface&quot;&quot;&quot; 
    metadata = {'render.modes': ['human']} 
 
    def __init__(self, df, initial_balance=100, lookback_window_size=50, Render_range=100,): 
        super(customEnv, self).__init__() 
        self.df = df.reset_index() 
        self.fees = .998 
        self.initial_balance = initial_balance 
        self.lookback_window_size = lookback_window_size 
        self.df_total_steps = len(self.df)-1 
        self.orders_history = deque(maxlen=self.lookback_window_size) 
        self.columns = list(self.df.columns[1:]) 
        self.Render_range = Render_range 
        # Market history contains the OHCL values for the last lookback_window_size prices 
        self.market_history = deque(maxlen=self.lookback_window_size) 
        # Define action and observation space 
        # They must be gym.spaces objects 
        # Example when using discrete actions: 
        self.action_space = spaces.Discrete(3) 
        # Example for using image as input: 
        self.observation_space = spaces.Box(low= -np.inf,high=np.inf, 
                                            shape=(self.lookback_window_size,len(self.columns) + 5), 
                                           dtype= np.float64) 
     
 
    def reset(self,env_steps_size = 0 ):
        #self.visualization = TradingGraph(Render_range=self.Render_range, Show_reward=self.Show_reward, Show_indicators=self.Show_indicators) # init visualization 
        #self.trades = deque(maxlen=self.Render_range) # limited orders memory for visualization 
        #print(&quot;RESET&quot;)
        self.balance = self.initial_balance 
        self.net_worth = self.initial_balance 
        self.prev_net_worth = self.initial_balance 
        self.crypto_held = 0 
        self.crypto_sold = 0 
        self.crypto_bought = 0 
        self.episode_orders = 0 # track episode orders count 
        self.prev_episode_orders = 0 # track previous episode orders count 
        self.rewards = deque(maxlen=self.Render_range) 
        self.env_steps_size = env_steps_size 
        self.punish_value = 0 
        self.trades = deque(maxlen=self.Render_range) 
        if env_steps_size &gt; 0: # used for training dataset 
            self.start_step = random.randint(self.lookback_window_size, self.df_total_steps - env_steps_size) 
            self.end_step = self.start_step + env_steps_size 
        else: # used for testing dataset 
            self.start_step = self.lookback_window_size 
            self.end_step = self.df_total_steps 
             
        self.current_step = self.start_step 
 
        for i in reversed(range(self.lookback_window_size)): 
            current_step = self.current_step - i 
            self.orders_history.append([self.balance, #/ self.normalize_value, 
                                        self.net_worth,# / self.normalize_value, 
                                        self.crypto_bought,# / self.normalize_value, 
                                        self.crypto_sold,# / self.normalize_value, 
                                        self.crypto_held# / self.normalize_value 
                                        ]) 
 
            # one line for loop to fill market history withing reset call 
            self.market_history.append([self.df.loc[current_step, column] for column in self.columns]) 
             
        state = np.concatenate((self.orders_history, self.market_history), axis=1) 
        #print(f&quot;END RESET: {state.shape} - {np.isnan(state).sum()}&quot;)
        return state 
    def step(self, action, production = False): 
        #print(&quot;STEP&quot;)
        self.crypto_bought = 0 
        self.crypto_sold = 0 
        self.current_step += 1 
 
        # Set the current price to a random price between open and close 
        #current_price = random.uniform( 
        #    self.df.loc[self.current_step, 'Open'], 
        #    self.df.loc[self.current_step,'Close']) 
        current_price = self.df.loc[self.current_step, 'Open'] 
        Date = self.df.loc[self.current_step, 'Date'] # for visualization 
        High = self.df.loc[self.current_step, 'High'] # for visualization 
        Low = self.df.loc[self.current_step, 'Low'] # for visualization 
 
        if action == 0: # Hold 
            pass 
 
        elif action == 1 and self.balance &gt; self.initial_balance*0.05: 
            # Buy with 100% of current balance 
            self.crypto_bought = self.balance / current_price 
            self.crypto_bought *= (1-self.fees) # substract fees 
            self.balance -= self.crypto_bought * current_price 
            self.crypto_held += self.crypto_bought 
            self.trades.append({'Date' : Date, 'High' : High, 'Low' : Low, 'total': self.crypto_bought, 'type': &quot;buy&quot;, 'current_price': current_price}) 
            self.episode_orders += 1 
 
        elif action == 2 and self.crypto_held*current_price&gt; self.initial_balance*0.05: 
            # Sell 100% of current crypto held 
            self.crypto_sold = self.crypto_held 
            self.crypto_sold *= (1-self.fees) # substract fees 
            self.balance += self.crypto_sold * current_price 
            self.crypto_held -= self.crypto_sold 
            self.trades.append({'Date' : Date, 'High' : High, 'Low' : Low, 'total': self.crypto_sold, 'type': &quot;sell&quot;, 'current_price': current_price}) 
            self.episode_orders += 1 
 
        self.prev_net_worth = self.net_worth 
        self.net_worth = self.balance + self.crypto_held * current_price 
 
        self.orders_history.append([self.balance ,#/ self.normalize_value, 
                                        self.net_worth,# / self.normalize_value, 
                                        self.crypto_bought,# / self.normalize_value, 
                                        self.crypto_sold,# / self.normalize_value, 
                                        self.crypto_held# / self.normalize_value 
                                        ]) 
 
        # Receive calculated reward 
        reward = self.get_reward() 
 
        if self.net_worth &lt;= self.initial_balance*0.9: 
            done = True 
        else: 
            done = False 
 
        obs = self.next_observation(production) 
        #print(reward,action)
        return obs, reward, done, {} 
     
     # Get the data points for the given current_step 
    def next_observation(self,production): 
        if(not production): 
            self.market_history.append([self.df.loc[self.current_step, column] for column in self.columns]) 
        obs = np.concatenate((self.orders_history, self.market_history), axis=1) 
        return obs 
 
    # Calculate reward 
    def get_reward(self): 
        if self.episode_orders &gt; 1 and self.episode_orders &gt; self.prev_episode_orders: 
            self.prev_episode_orders = self.episode_orders 
            if self.trades[-1]['type'] == &quot;buy&quot; and self.trades[-2]['type'] == &quot;sell&quot;: 
                reward = self.trades[-2]['total']*self.trades[-2]['current_price'] - self.trades[-2]['total']*self.trades[-1]['current_price'] 
                self.trades[-1][&quot;Reward&quot;] = reward
                return reward 
            elif self.trades[-1]['type'] == &quot;sell&quot; and self.trades[-2]['type'] == &quot;buy&quot;: 
                reward = self.trades[-1]['total']*self.trades[-1]['current_price'] - self.trades[-2]['total']*self.trades[-2]['current_price'] 
                self.trades[-1][&quot;Reward&quot;] = reward 
                return reward
            #elif self.trades[-1]['type'] == &quot;sell&quot; and self.trades[-2]['type'] == &quot;sell&quot;: 
            #    return -100
            #elif self.trades[-1]['type'] == &quot;buy&quot; and self.trades[-2]['type'] == &quot;buy&quot;: 
            #    return -100
            else:
                return 0
        else: 
            return 0 
 
     
    def render(self, mode='human', close=False): 
        profit = self.net_worth - self.initial_balance 
        print(f'Step: {self.current_step}') 
        print(f'Balance: {self.balance}') 
        print(f'Crypto held: {self.crypto_held}') 
        print(f'Profit: {profit}') 
        # Render the environment to the screen



from stable_baselines3 import A2C,PPO

env.reset()
model = PPO(&quot;MlpPolicy&quot;,env,verbose= 1,tensorboard_log= logdir)
TIMESTEPS = 10000
for i in range(1,10):
    model.learn(total_timesteps= TIMESTEPS*i,reset_num_timesteps=False,tb_log_name=kind)
    model.save(f&quot;{models_dir}/{TIMESTEPS*i}&quot;)
</code></pre>
<p>how I can show more metrics?</p>
<p>Thanks a lot</p>
",13401459,,8695460,,2022-08-07 04:00:19,2022-08-08 07:56:23,More metrics in Tensorboard,<python><deep-learning><reinforcement-learning><openai-gym><stable-baselines>,1,0,,,,CC BY-SA 4.0,
1104,73411966,1,73453078,,2022-08-19 04:52:27,,0,84,"<p>I am trying to use the simglucose package with OpenAI gym.  I am encountering a strange problem; when I run the code below in a certain directory (let's call it <code>problem_dir/</code>), it fails with the error below.  However, when I copy-paste the file to other locations, it runs fine (I tried about 10 other locations, including child and parent directories of <code>problem_dir/</code>).  What could cause an error like the one below, but <em><strong>only</strong></em> when the Python file is in <code>problem_dir/</code>?</p>
<p>Problematic code:</p>
<pre><code>import gym
from gym.envs.registration import register
register(
    id='simglucose-adolescent2-v0',
    entry_point='simglucose.envs:T1DSimEnv',
    kwargs={'patient_name': 'adolescent#002'}
)
env = gym.make('simglucose-adolescent2-v0') # ERROR HERE
</code></pre>
<p>The error:</p>
<pre><code>(seldonian_library_env) james@james-desktop2019-Ub18:~$ python seldonian_library_repos/Engine/seldonian/RL/environments/temp.py 
/home/james/anaconda3/envs/seldonian_library_env/lib/python3.8/site-packages/gym/envs/registration.py:17: PkgResourcesDeprecationWarning: Parameters to load are deprecated.  Call .resolve and .require separately.
  result = entry_point.load(False)
Traceback (most recent call last):
  File &quot;seldonian_library_repos/Engine/seldonian/RL/environments/temp.py&quot;, line 8, in &lt;module&gt;
    env = gym.make('simglucose-adolescent2-v0')
  File &quot;/home/james/anaconda3/envs/seldonian_library_env/lib/python3.8/site-packages/gym/envs/registration.py&quot;, line 164, in make
    return registry.make(id)
  File &quot;/home/james/anaconda3/envs/seldonian_library_env/lib/python3.8/site-packages/gym/envs/registration.py&quot;, line 122, in make
    env = spec.make()
  File &quot;/home/james/anaconda3/envs/seldonian_library_env/lib/python3.8/site-packages/gym/envs/registration.py&quot;, line 88, in make
    cls = load(self._entry_point)
  File &quot;/home/james/anaconda3/envs/seldonian_library_env/lib/python3.8/site-packages/gym/envs/registration.py&quot;, line 17, in load
    result = entry_point.load(False)
  File &quot;/home/james/anaconda3/envs/seldonian_library_env/lib/python3.8/site-packages/pkg_resources/__init__.py&quot;, line 2450, in load
    return self.resolve()
  File &quot;/home/james/anaconda3/envs/seldonian_library_env/lib/python3.8/site-packages/pkg_resources/__init__.py&quot;, line 2456, in resolve
    module = __import__(self.module_name, fromlist=['__name__'], level=0)
  File &quot;/home/james/seldonian_library_repos/Engine/seldonian/RL/environments/simglucose.py&quot;, line 62, in &lt;module&gt;
    register(
  File &quot;/home/james/anaconda3/envs/seldonian_library_env/lib/python3.8/site-packages/gym/envs/registration.py&quot;, line 161, in register
    return registry.register(id, **kwargs)
  File &quot;/home/james/anaconda3/envs/seldonian_library_env/lib/python3.8/site-packages/gym/envs/registration.py&quot;, line 154, in register
    raise error.Error('Cannot re-register id: {}'.format(id))
gym.error.Error: Cannot re-register id: simglucose-adolescent2-v0
</code></pre>
<p>I am running Ubuntu 18.04.6 and Python 3.8.13.</p>
<p><strong>Update:</strong></p>
<p>The problem has &quot;spread&quot;; it's occurring in other directories now too, but not all directories.  Also, when I change my code to try to deregister the environment before registering, as follows, I get the following new error:</p>
<p>Modified code:</p>
<pre><code>import gym
env_dict = gym.envs.registration.registry.env_specs.copy()

for env in env_dict:
    if 'simglucose' in env:
        print(f&quot;Removing {env}&quot;)
        del gym.envs.registration.registry.env_specs[env]

from gym.envs.registration import register
register(
    id='simglucose-adolescent2-v0',
    entry_point='simglucose.envs:T1DSimEnv',
    kwargs={'patient_name': 'adolescent#002'}
)

env = gym.make('simglucose-adolescent2-v0')
</code></pre>
<p>New error:</p>
<pre><code>ModuleNotFoundError: No module named 'simglucose.envs'; 'simglucose' is not a package
</code></pre>
<p>Strangely, when I run this modified code in the directories where the 1st error above is not occurring (still using the same pip/conda environment), this new error does not occur either.  I have checked for hidden files and found nothing other the the __pychache__ directory.  I tried deleting that, but it changes nothing.</p>
",3187892,,3187892,,2022-08-23 04:19:06,2022-08-23 05:31:39,Error Only When Code is in a Certain Directory: gym.error.Error: Cannot re-register id,<conda><openai-gym>,1,0,,,,CC BY-SA 4.0,
1107,73667333,1,73726592,,2022-09-09 20:44:44,,0,904,"<p>So I wanted to try some reinforcement learning, I haven't coded anything for a while.
On Jupiter Notebooks when I run this code</p>
<pre><code>import gym
env = gym.make(&quot;MountainCar-v0&quot;)
env.reset()

done = False
while not done:
    action = 2  # always go right!
    env.step(action)
    env.render()
</code></pre>
<p>it just tries to render it but can't, the hourglass on top of the window is showing but it never renders anything, I can't do anything from there.</p>
<p>Same with this code</p>
<pre><code>import gym
env_name = &quot;MountainCar-v0&quot;
env = gym.make(env_name)

env.reset()

for _ in range(200)
    action = env.action_space.sample()
    env.step(action)
    env.render()
</code></pre>
<p>Both of these don't work neither on Jupiter notebooks nor Pycharm nor terminal.
I'm on Windows.
Couldn't find anything similar to this online.
Yes I'm <strong>new</strong> to this</p>
<p>Edit-
I did this</p>
<pre><code># Install latest stable version from PyPI
!pip install -U pysdl2

# Install latest development verion from GitHub
!pip install -U git+https://github.com/py-sdl/py-sdl2.git

</code></pre>
<p>and it now says <strong>error: windlib not available</strong></p>
<p>I tried</p>
<pre><code>!pip install windlib
</code></pre>
<p>but still can't fix the error</p>
",19960775,,19960775,,2022-09-13 00:02:50,2022-09-15 06:38:15,"Open AI gym environments don't render, don't show at all",<python><openai-gym>,1,0,,,,CC BY-SA 4.0,
1116,73737008,1,73994339,,2022-09-15 20:22:14,,1,107,"<p>I am trying to make an AI agent for playing OpenAI Gym CarRacing environment and I am having trouble loading saved models. I train them, they work, I save them and load them and suddenly the car doesn't even move. I even tried downloading models from other people and when loaded, the car just doesn't move.</p>
<p>I am on Ubuntu 20.04 in VS Code in a Jupyter notebook using
gym==0.21.0, stable-baselines3==1.6.0, python==3.7.0</p>
<pre><code>import gym 
from stable_baselines3 import PPO
from stable_baselines3.common.evaluation import evaluate_policy
import os
</code></pre>
<p>I make the environment</p>
<pre><code>environment_name = &quot;CarRacing-v0&quot;
env = gym.make(environment_name)
</code></pre>
<p>I create the PPO model and make it learn for a couple thousand timesteps. Now when I evaluate the policy, the car renders as moving.</p>
<pre><code>log_path = os.path.join('Training', 'Logs')
model = PPO(&quot;CnnPolicy&quot;, env, verbose=1, tensorboard_log=log_path)
model.learn(total_timesteps=4000)
evaluate_policy(model, env, n_eval_episodes=1, render=True)
</code></pre>
<p>I save the model</p>
<pre><code>ppo_path = os.path.join('Training', 'Saved Models', 'PPO_Car_Testing')
model.save(ppo_path)
</code></pre>
<p>now I delete the model and load the saved one and when I evaluate it the car just doesn't move as if it always got action do nothing. I tried models learning for 2k timesteps up to a model which has been learning for 2 million timesteps.</p>
<pre><code>del model
model = PPO(&quot;CnnPolicy&quot;, env, verbose=1, tensorboard_log=log_path)
ppo_path_load = os.path.join('Training', 'Saved Models', 'PPO_2m_Driving_model')
model.load(ppo_path_load, env)
evaluate_policy(model, env, n_eval_episodes=1, render=True)
</code></pre>
<p>Any ideas why the models load incorrectly?</p>
",17646343,,17646343,,2022-09-16 15:36:31,2022-10-08 04:18:04,stable-baselines3 PPO model loaded but not working,<python><pytorch><openai-gym><stable-baselines><racing>,2,0,,,,CC BY-SA 4.0,
1119,73758588,1,73758682,,2022-09-17 21:17:39,,1,190,"<pre><code>import gym
env = gym.make(&quot;FrozenLake-v1&quot;)
env.reset()
env.render()
env.step(1)
env.render()
</code></pre>
<pre><code>Something went wrong with pygame. This should never happen.
  File &quot;C:\Users\ardgn\OneDrive\Belgeler\GitHub\Kutuphaneler-cheatsheets\GYMLibraryOPENAI&quot;, line 7, in &lt;module&gt;
    env.render()
</code></pre>
<p>This is the error I get when I use the above code</p>
",19865050,,19865050,,2022-09-17 21:35:22,2022-09-17 21:49:59,"Python error, no results on the internet openai-gym",<python><openai-gym>,1,2,,,,CC BY-SA 4.0,
1120,73845576,1,73850845,,2022-09-25 15:10:30,,0,271,"<p>I am trying to learn Reinforcement learning. I wanted to build a Reinforcement Learning model for autonomous driving. However, whenever I use env.render() while training the Reinforcement learning model. It gives me an assertion error. The code is as below for my model:</p>
<pre><code>import gym 
from stable_baselines3 import PPO
from stable_baselines3.common.vec_env import VecFrameStack
from stable_baselines3.common.evaluation import evaluate_policy
import os

environment_name = &quot;CarRacing-v2&quot;
env = gym.make(environment_name)

episodes = 5
for episode in range(1, episodes+1):
    state = env.reset()
    done = False
    score = 0 
    
    while not done:
        env.render()
        action = env.action_space.sample()
        n_state, reward, done, info = env.step(action)
        score+=reward
    print('Episode:{} Score:{}'.format(episode, score))
env.close()
</code></pre>
<p>Error:</p>
<pre><code>AssertionError                            Traceback (most recent call last)
&lt;ipython-input-31-c07c36362924&gt; in &lt;module&gt;
      6 
      7     while not done:
----&gt; 8         env.render()
      9         action = env.action_space.sample()
     10         n_state, reward, done, info = env.step(action)

~\Anaconda3\lib\site-packages\gym\core.py in render(self, *args, **kwargs)
    327     ) -&gt; Optional[Union[RenderFrame, List[RenderFrame]]]:
    328         &quot;&quot;&quot;Renders the environment.&quot;&quot;&quot;
--&gt; 329         return self.env.render(*args, **kwargs)
    330 
    331     def close(self):

~\Anaconda3\lib\site-packages\gym\wrappers\order_enforcing.py in render(self, *args, **kwargs)
     49                 &quot;set `disable_render_order_enforcing=True` on the OrderEnforcer wrapper.&quot;
     50             )
---&gt; 51         return self.env.render(*args, **kwargs)
     52 
     53     @property

~\Anaconda3\lib\site-packages\gym\wrappers\env_checker.py in render(self, *args, **kwargs)
     51         if self.checked_render is False:
     52             self.checked_render = True
---&gt; 53             return env_render_passive_checker(self.env, *args, **kwargs)
     54         else:
     55             return self.env.render(*args, **kwargs)

~\Anaconda3\lib\site-packages\gym\utils\passive_env_checker.py in env_render_passive_checker(env, *args, **kwargs)
    314             )
    315 
--&gt; 316     result = env.render(*args, **kwargs)
    317 
    318     # TODO: Check that the result is correct

~\Anaconda3\lib\site-packages\gym\envs\box2d\car_racing.py in render(self)
    566 
    567     def render(self):
--&gt; 568         return self._render(self.render_mode)
    569 
    570     def _render(self, mode: str):

~\Anaconda3\lib\site-packages\gym\envs\box2d\car_racing.py in _render(self, mode)
    569 
    570     def _render(self, mode: str):
--&gt; 571         assert mode in self.metadata[&quot;render_modes&quot;]
    572 
    573         pygame.font.init()

AssertionError: 
</code></pre>
<p>I do not know what the problem is but I have tried to install box2d  like this:</p>
<pre><code>!pip install gym[box2d] pyglet==1.3.2
</code></pre>
<p>Please help me with this. Thanks!!!!</p>
",16879380,,1832058,,2022-09-26 08:12:25,2022-09-28 12:39:30,"Whenever I try to use env.render() for OpenAIgym I get ""AssertionError""?",<python><reinforcement-learning><assertion><openai-gym>,1,3,,,,CC BY-SA 4.0,
1134,74060371,1,74064722,,2022-10-13 18:40:22,,2,254,"<p>I'm running Python3 (3.8.10) and am attempting a tutorial with the gym_super_mario_bros (7.3.0) and nes_py libraries. I followed various tutorials code and tried on multiple computers but get an error. I have tried to adjust some of the parameters like adding a 'truncated' variable to the list of values to return. As this is a tutorial level example I'm curious what is wrong. It looks like something with env.step(). Below is the code:</p>
<pre><code>from nes_py.wrappers import JoypadSpace
from gym_super_mario_bros.actions import SIMPLE_MOVEMENT

env = gym_super_mario_bros.make('SuperMarioBros-v0')
env = JoypadSpace(env, SIMPLE_MOVEMENT)

done = True
for step in range(1000):
        if done:
                env.reset()
        state, reward, done, info = env.step(env.action_space.sample())
        env.render()
env.close()
</code></pre>
<p>The error I get is below:</p>
<pre><code>/home/d/.local/lib/python3.8/site-packages/gym/envs/registration.py:555: UserWarning: WARN: The environment SuperMarioBros-v0 is out of date. You should consider upgrading to version `v3`.
  logger.warn(
/home/d/.local/lib/python3.8/site-packages/gym/utils/passive_env_checker.py:195: UserWarning: WARN: The result returned by `env.reset()` was not a tuple of the form `(obs, info)`, where `obs` is a observation and `info` is a dictionary containing additional information. Actual type: `&lt;class 'numpy.ndarray'&gt;`
  logger.warn(
/home/d/.local/lib/python3.8/site-packages/gym/utils/passive_env_checker.py:219: DeprecationWarning: WARN: Core environment is written in old step API which returns one bool instead of two. It is recommended to rewrite the environment with new step API.
  logger.deprecation(
Traceback (most recent call last):
  File &quot;mario.py&quot;, line 12, in &lt;module&gt;
    state, reward, done, info = env.step(env.action_space.sample())
  File &quot;/home/d/.local/lib/python3.8/site-packages/nes_py/wrappers/joypad_space.py&quot;, line 74, in step
    return self.env.step(self._action_map[action])
  File &quot;/home/d/.local/lib/python3.8/site-packages/gym/wrappers/time_limit.py&quot;, line 50, in step
    observation, reward, terminated, truncated, info = self.env.step(action)
ValueError: not enough values to unpack (expected 5, got 4)
</code></pre>
<p>Any guidance is appreciated, thank you!</p>
",16077713,,,,,2022-10-14 05:35:46,"gym_super_mario_bros (7.3.0) - ValueError: not enough values to unpack (expected 5, got 4)",<python><openai-gym>,1,0,,,,CC BY-SA 4.0,
1135,73916260,1,73998243,,2022-10-01 06:26:32,,0,428,"<p>The programs I use are colab, vscode, vscode-jupyter, kaggle, pycharm. Pyton version 3.10.7
I tried to render the cartpole environment in every program I use. I tried many different gym package versions. But I can't. I wonder which python version, which gym version, or what are the things I should use in general. (IDE vs..)</p>
<p>Speaking for vscode, this is the code I ran and the output I got.
(PACKETS =&gt; pygame=2.1.0, gym=0.26.1, gym-notices=0.0.8, python=3.10.7)
<a href=""https://i.stack.imgur.com/pDKae.png"" rel=""nofollow noreferrer"">VSCODE code</a></p>
<p>my code is working but what i want is to see this.I can't see that.
<a href=""https://i.stack.imgur.com/QN1Ho.png"" rel=""nofollow noreferrer"">i want to see</a></p>
<p>PyCharm is the same and Spyder is the same.</p>
<p>GOOGLE COLAB I am running the same code again.This is the output<a href=""https://i.stack.imgur.com/FFvIN.png"" rel=""nofollow noreferrer"">colab output</a></p>
",16720044,,16720044,,2022-10-02 12:16:05,2022-10-08 15:29:23,How can i render openai gym in windows python3(cartpole),<machine-learning><deep-learning><artificial-intelligence><reinforcement-learning><openai-gym>,1,2,,,,CC BY-SA 4.0,
1144,74198765,1,74201695,,2022-10-25 18:42:20,,0,46,"<p>Before I start, I know there are a lot of questions with the same error but none of them solved the issue for me.</p>
<p>I have a PPO implementation for playing the CarRacing-v2 environment from gym (gym==0.26.0 , tensorflow==2.10.0). I wanted to make it faster and move a bunch of code into a separate function and wrap it with tf.function, however just moving it into a different function created an error. There is no modification in the code other than just moving a part of it to a different function.</p>
<p>This is the working code I had before.</p>
<pre><code>def learn(self):
        for epoch in range(self.n_epochs):
            # print(f&quot;{epoch = }&quot;)
            state_arr, action_arr, old_prob_arr, vals_arr,\
                reward_arr, dones_arr, batches = \
                self.memory.generate_batches()

            values = vals_arr
            advantage = np.zeros(len(reward_arr), dtype=np.float32)

            # print(&quot;a_t&quot;)
            for t in range(len(reward_arr)-1):
                discount = 1
                a_t = 0
                for k in range(t, len(reward_arr)-1):
                    a_t += discount*(reward_arr[k] + self.gamma*values[k+1] * (
                        1-int(dones_arr[k])) - values[k])
                    discount *= self.gamma*self.gae_lambda
                advantage[t] = a_t

            for batch in batches:
                # do this into a tf function
                # print(&quot;batch&quot;)
                with tf.GradientTape(persistent=True) as tape:
                    states = tf.convert_to_tensor(state_arr[batch])
                    old_probs = tf.convert_to_tensor(old_prob_arr[batch])
                    actions = tf.convert_to_tensor(action_arr[batch])

                    probs = self.actor(states)
                    dist = tfp.distributions.Categorical(probs)
                    new_probs = dist.log_prob(actions)

                    critic_value = self.critic(states)

                    critic_value = tf.squeeze(critic_value, 1)

                    prob_ratio = tf.math.exp(new_probs - old_probs)
                    weighted_probs = advantage[batch] * prob_ratio
                    clipped_probs = tf.clip_by_value(prob_ratio,
                                                     1-self.policy_clip,
                                                     1+self.policy_clip)
                    weighted_clipped_probs = clipped_probs * advantage[batch]
                    actor_loss = -tf.math.minimum(weighted_probs,
                                                  weighted_clipped_probs)
                    actor_loss = tf.math.reduce_mean(actor_loss)

                    returns = advantage[batch] + values[batch]
                    # critic_loss = tf.math.reduce_mean(tf.math.pow(
                    #                                  returns-critic_value, 2))
                    critic_loss = keras.losses.MSE(critic_value, returns)

                actor_params = self.actor.trainable_variables
                actor_grads = tape.gradient(actor_loss, actor_params)
                critic_params = self.critic.trainable_variables
                critic_grads = tape.gradient(critic_loss, critic_params)
                self.actor.optimizer.apply_gradients(
                        zip(actor_grads, actor_params))
                self.critic.optimizer.apply_gradients(
                        zip(critic_grads, critic_params))

        self.memory.clear_memory()
</code></pre>
<p>and this is the code split into 2 functions and here is where the error occures</p>
<pre><code>def learn(self):
        for epoch in range(self.n_epochs):
            # print(f&quot;{epoch = }&quot;)
            state_arr, action_arr, old_prob_arr, vals_arr,\
                reward_arr, dones_arr, batches = \
                self.memory.generate_batches()

            values = vals_arr
            advantage = np.zeros(len(reward_arr), dtype=np.float32)

            # print(&quot;a_t&quot;)
            for t in range(len(reward_arr)-1):
                discount = 1
                a_t = 0
                for k in range(t, len(reward_arr)-1):
                    a_t += discount*(reward_arr[k] + self.gamma*values[k+1] * (
                        1-int(dones_arr[k])) - values[k])
                    discount *= self.gamma*self.gae_lambda
                advantage[t] = a_t

            for batch in batches:
               self.do_batch(state_arr, old_prob_arr,action_arr, batch, advantage, values)

        self.memory.clear_memory()

    @tf.function
    def do_batch(self, state_arr, old_prob_arr, action_arr, batch, advantage, values):
        with tf.GradientTape(persistent=True) as tape:
            states = tf.convert_to_tensor(state_arr[batch])
            old_probs = tf.convert_to_tensor(old_prob_arr[batch])
            actions = tf.convert_to_tensor(action_arr[batch])

            probs = self.actor(states)
            dist = tfp.distributions.Categorical(probs)
            new_probs = dist.log_prob(actions)

            critic_value = self.critic(states)

            critic_value = tf.squeeze(critic_value, 1)

            prob_ratio = tf.math.exp(new_probs - old_probs)
            weighted_probs = advantage[batch] * prob_ratio
            clipped_probs = tf.clip_by_value(prob_ratio,
                                                1-self.policy_clip,
                                                1+self.policy_clip)
            weighted_clipped_probs = clipped_probs * advantage[batch]
            actor_loss = -tf.math.minimum(weighted_probs,
                                            weighted_clipped_probs)
            actor_loss = tf.math.reduce_mean(actor_loss)

            returns = advantage[batch] + values[batch]
            # critic_loss = tf.math.reduce_mean(tf.math.pow(
            #                                  returns-critic_value, 2))
            critic_loss = keras.losses.MSE(critic_value, returns)

        actor_params = self.actor.trainable_variables
        actor_grads = tape.gradient(actor_loss, actor_params)
        critic_params = self.critic.trainable_variables
        critic_grads = tape.gradient(critic_loss, critic_params)
        self.actor.optimizer.apply_gradients(
                zip(actor_grads, actor_params))
        self.critic.optimizer.apply_gradients(
                zip(critic_grads, critic_params))
</code></pre>
<p><a href=""https://i.stack.imgur.com/2EDBQ.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/2EDBQ.png"" alt=""enter image description here"" /></a></p>
<p>the issue seems to be with batch being a tensor, however it should be a list from my code</p>
<pre><code>def generate_batches(self):
        n_states = len(self.states)
        batch_start = np.arange(0, n_states, self.batch_size)
        indices = np.arange(n_states, dtype=np.int64)
        np.random.shuffle(indices)
        batches = [indices[i:i+self.batch_size] for i in batch_start]

        return np.array(self.states),\
            np.array(self.actions),\
            np.array(self.probs),\
            np.array(self.vals),\
            np.array(self.rewards),\
            np.array(self.dones),\
            batches
</code></pre>
",17646343,,,,,2022-10-26 01:16:38,"TypeError: Only integers, slices (`:`), ellipsis (`...`), tf.newaxis (`None`) and scalar tf.int32/tf.int64 tensors are valid indices",<python><tensorflow><machine-learning><deep-learning><openai-gym>,1,0,,,,CC BY-SA 4.0,
1151,74338552,1,74342432,,2022-11-06 18:20:19,,0,38,"<p>I have created a resource group, a compute instance, a CPU cluster and a notebook as described in <a href=""https://github.com/Azure/MachineLearningNotebooks/blob/f45b8152218cbf5e40f44b983b459fe1c71c730e/how-to-use-azureml/reinforcement-learning/cartpole-on-single-compute/cartpole_sc.ipynb"" rel=""nofollow noreferrer"">Cartpool notebook</a>.</p>
<p>However, when the command <code>ray_env_build_details.wait_for_completion(show_output=True)</code> is executed I get the following error:</p>
<pre><code>Environment version is set. Attempting to register desired version. To auto-version, reset version to None.
Image Build Status: Queued

2022/11/06 17:46:18 Downloading source code...
2022/11/06 17:46:19 Finished downloading source code
2022/11/06 17:46:20 Creating Docker network: acb_default_network, driver: 'bridge'
2022/11/06 17:46:20 Successfully set up Docker network: acb_default_network
2022/11/06 17:46:20 Setting up Docker configuration...
2022/11/06 17:46:21 Successfully set up Docker configuration
2022/11/06 17:46:21 Logging in to registry: ca98820ccecd46c29745d8e4c35dca23.azurecr.io
2022/11/06 17:46:22 Successfully logged into ca98820ccecd46c29745d8e4c35dca23.azurecr.io
2022/11/06 17:46:22 Executing step ID: acb_step_0. Timeout(sec): 5400, Working directory: '', Network: 'acb_default_network'
2022/11/06 17:46:22 Scanning for dependencies...
2022/11/06 17:46:22 Output from dependency scanning: fatal: not a git repository (or any parent up to mount point /)
Stopping at filesystem boundary (GIT_DISCOVERY_ACROSS_FILESYSTEM not set).
fatal: not a git repository (or any parent up to mount point /)
Stopping at filesystem boundary (GIT_DISCOVERY_ACROSS_FILESYSTEM not set).
unexpected dockerfile format
failed to run step ID: acb_step_0: failed to scan dependencies: exit status 1

Run ID: chc failed after 5s. Error: failed during run, err: exit status 1
Image Build Status: Failed

&lt;azureml.core.environment.ImageBuildDetails at 0x7f27449317c0&gt;
</code></pre>
<p>I have tried running <code>git init</code> in a terminal window for the Notebook before running the command, but that didn't help.</p>
<p>Any guidance?</p>
",693101,,,,,2022-11-07 05:50:28,Azure ML Notebook GIT_DISCOVERY_ACROSS_FILESYSTEM,<git><docker><jupyter-notebook><azure-machine-learning-studio><openai-gym>,1,0,,,,CC BY-SA 4.0,
1166,55941463,1,55944674,,2019-05-01 19:16:54,,2,1292,"<p>I was reading this blog about <a href=""http://deeplizard.com/learn/video/wrBUkpiRvCA"" rel=""nofollow noreferrer"">Deep Q-Learning</a>. </p>

<p>1- In the <code>The input</code> section of the blog, I wanted to know how do we feed the 4 still-frames/screenshots from the game, that represent the input state, into the Policy network? Will all 4 frames be fed in one flattened tensor (where one image ends the next one starts, forming a continuous row input, in one tensor)? Or will they be fed separately one after the other into the network?</p>

<p>2- For preprocessing the images, do we avoid using the Max-pooling stage? My understanding is this process eliminate the need for spacial/position recognition in image-feature recognition. While in normal Conv-Net this is important for recognising image features regardless of where they appear in space and distance (so we us max-pooling). In Q-learning for games, the space/position of different elements on the image is important. Therefore, we remove the use of Max-pooling from the proprocessing stage. Is this correct?</p>

<p>3- Can anyone recommend a good implementation resource of Deep Q-learning, written from scratch (in Python), i.e. without the use of out-of-the-box libraries, like PyTorch, Keras and Scikit-learn ..etc, for a game, where image frame feeds from the game is required as states input. I'm thinking perhaps implementing the model from scratch gives a better control over customisation and fine tuning of the hyper-parameters. Or is it better to use out-out-of-the-box library? Any code implementation on this would be super helpful. </p>

<p>Many thanks in advance.</p>
",1766575,,,,,2019-05-02 01:42:56,DQN - How to feed the input of 4 still frames from a game as one single state input,<deep-learning><reinforcement-learning><q-learning>,1,0,,,,CC BY-SA 4.0,
1170,40896951,1,41722336,,2016-11-30 19:50:31,,1,860,"<p>Okay, so I have created a neural network Q-learner using the same idea as DeepMind's Atari algorithm (except I give raw data not pictures (yet)).</p>

<p><strong>Neural network build:</strong></p>

<ul>
<li><p>9 inputs (0 for empty spot, 1 for ""X"", -1 for ""O"")</p></li>
<li><p>1 hidden layer with 9-50 neurons (tried with different sizes, activation function sigmoid)</p></li>
<li><p>9 outputs (1 for every action, outputs Q-value, activation function sigmoid)</p></li>
<li>MSE loss function</li>
<li>Adam backprop</li>
</ul>

<p>I'm 100% confident network is built correctly because of gradient checks and lots of tests.</p>

<p><strong>Q-parameters:</strong></p>

<ul>
<li>-1 reward for lost game </li>
<li>-1 reward if move is attempted to already occupied spot (e.g. X is already in the spot where player O tries to put his ""O"")</li>
<li>0 reward for draws</li>
<li>0 reward for moves, which don't lead to terminal state</li>
<li>+1 reward for won game</li>
<li>Next state (in s,a,r,s') is the state after your own and your opponent's move. E.g. empty board and player X has first turn and puts ""X"" in upper left corner. Then player O puts ""O"" in upper right corner. Then s,a,r,s' would be s = [0,0,0,0,0,0,0,0,0], a = 0, r = 0, s' = [1,0,-1,0,0,0,0,0,0]</li>
</ul>

<p><strong>Problem</strong></p>

<p>All my Q-values go to zero if I give -1 reward when move is made to already occupied spot. If I don't do it the network doesn't learn that it shouldn't make moves to already occupied places and seems to learn arbitrary Q-values. Also my error doesn't seem to shrink.</p>

<p><strong>Solutions that didn't work</strong></p>

<ul>
<li><p>I have tried to change rewards to (0, 0.5, 1) and (0, 1) but it still didn't learn.</p></li>
<li><p>I have tried to present state as 0 for empty, 0.5 for O, 1 for X, but didn't work.</p></li>
<li><p>I have tried to give the next state straight after move is made but it didn't help.</p></li>
<li><p>I have tried with Adam and vanilla back prop, but still same results.</p></li>
<li>I have tried with batches from replay memory and stochastic gradient descent, but still the same</li>
<li>Changed sigmoid to ReLU but didn't help.</li>
<li>All kinds of things I can't recall now</li>
</ul>

<blockquote>
  <p>Project in GitHub: <a href=""https://github.com/Dopet/tic-tac-toe"" rel=""nofollow noreferrer"">https://github.com/Dopet/tic-tac-toe</a> (Sorry for
  ugly code mostly due to all of these refactorings of code, also this was supposed to be easy test to see if the algorithm works)</p>
</blockquote>

<p>Main points:</p>

<ul>
<li>TicTac class has the game itself (made using template method pattern from abstract Game class)</li>
<li>NeuralNetwork class logs some data to file called MyLogFile.log in current directory</li>
<li>Block and Combo classes are just used to create the winning situations</li>
<li>jblas-1.2.4.jar contains the DoubleMatrix libraries</li>
</ul>
",5609686,,5609686,,2016-12-01 12:18:10,2017-01-18 14:44:29,Why doesn't my neural network Q-learner doesn't learn tic-tac-toe,<machine-learning><neural-network><deep-learning><reinforcement-learning><q-learning>,2,7,,,,CC BY-SA 3.0,
1175,43804248,1,43812411,,2017-05-05 11:51:07,,1,694,"<p>I've made a simple version of Curve Fever also known as ""Achtung Die Kurve"". I want the machine to figure out how to play the game optimally. I copied and slightly modified an existing DQN from some Atari game examples that is made with Google's Tensorflow.</p>

<p>I'm tyring to figure out an appropriate reward function. Currently, I use this reward setup:</p>

<ul>
<li>0.1 for every frame it does not crash</li>
<li>-500 for every crash</li>
</ul>

<p>Is this the right approach? Do I need to tweak the values? Or do I need a completely different approach?</p>
",2390578,,1497463,,2017-05-06 16:39:43,2017-05-06 16:39:43,Reward function for learning to play Curve Fever game with DQN,<machine-learning><tensorflow><deep-learning><reinforcement-learning><q-learning>,2,4,,,,CC BY-SA 3.0,
1178,56446445,1,56450099,,2019-06-04 14:57:03,,-5,60,"<p>The goal is to create an AI to play a simple game, tracking a horizontally moving dot across the screen which increases speed until no longer tracked. </p>

<p>I would like to create an AI to behave similarly to a real test subject. I have a large amount of trials that were recorded of many months, position of dot on screen and user cursor position over time. </p>

<p>I would like to train the network on these trials so that the network behaves similarly to a real test subject and I can then obtain very large amounts of test data to observe how changing the parameters of the game affects the networks ability to track the moving dot.</p>

<p>I am interested in learning about the underlying code of neural networks and would love some advice on where to start with this project. I understand AIs can get very good at performing different tasks such as snake, or other simple games, but my goal would be to have the AI perform similarly to a real test subject. </p>
",11599610,,3119050,,2019-06-04 19:14:57,2019-06-04 19:21:38,train a neural network on real subject input/output to have it behave similarly to subject,<machine-learning><neural-network><deep-learning><artificial-intelligence><q-learning>,1,0,,,,CC BY-SA 4.0,
1179,57168356,1,57186335,,2019-07-23 16:16:17,,0,688,"<p>i am trying to learn DQN agent to play Tic Tac Toe using Keras. Issue is that my output has different shape than I expected. </p>

<p>Details: 
Input shape : <code>(BOARD_SIZE ^ 2) * 3</code> --> It is one hot encoded game board
Output shape: I expect that output will be list with size of <code>(BOARD_SIZE^2)</code> because it should number of available actions</p>

<p>Problem: 
Output has shape Size of input layer <code>[(BOARD_SIZE ^ 2) *3] * Number of actions (BOARD_SIZE^2)</code></p>

<p>I tried to look for solutions but Keras documentation is quite poor. Plz help</p>

<p>THIS IS MY MODEL</p>

<pre><code>    def create_model(self, game: GameController) -&gt; Sequential:
    input_size = (game.shape ** 2) * 3

    model = Sequential()
    model.add(Dense(input_size, input_dim=1, activation='relu'))
    model.add(Dense(int(input_size / 2), activation='relu'))
    model.add(Dense(int(input_size / 2), activation='relu'))
    model.add(Dense((game.shape ** 2), activation='linear'))
    model.compile(loss=""mean_squared_error"", optimizer=Adam(self.alpha))

    return model
</code></pre>

<p>THIS IS HOW I AM TRYING TO GET OUTPUT</p>

<pre><code>q_values = self.model.predict(processed_input)
</code></pre>

<p>THIS IS BOAD PREPROCESSING (ONE HOT ENCODING)</p>

<pre><code>def preprocess_input(self, game: GameController) -&gt; list:
    encoded_x = copy.deepcopy(game.board)
    encoded_o = copy.deepcopy(game.board)
    encoded_blank = copy.deepcopy(game.board)

    for row in range(game.shape):
        for col in range(game.shape):
            if encoded_x[row][col] == 'X':
                encoded_x[row][col] = 1
            else:
                encoded_x[row][col] = 0

            if encoded_o[row][col] == 'O':
                encoded_o[row][col] = 1
            else:
                encoded_o[row][col] = 0

            if encoded_blank[row][col] == '-':
                encoded_blank[row][col] = 1
            else:
                encoded_blank[row][col] = 0

    chained_x = list(chain.from_iterable(encoded_x))
    chained_o = list(chain.from_iterable(encoded_o))
    chained_blank = list(chain.from_iterable(encoded_blank))

    string_board = list(chain(chained_x, chained_o, chained_blank))
    board_to_int = [int(element) for element in string_board]

    return board_to_int
</code></pre>
",8584176,,8584176,,2019-07-24 13:17:33,2019-07-24 15:35:23,How to define output layer shape of DQN model in Keras,<python><keras><deep-learning><reinforcement-learning><q-learning>,1,0,0,,,CC BY-SA 4.0,
1180,45739697,1,45741073,,2017-08-17 15:48:04,,0,231,"<p>I am trying to write a deep q-learning network for a problem in AI. I have a function <code>predict()</code> that produces a tensor of shape <code>(None, 3)</code> taking in an input of shape <code>(None, 5)</code>. The 3 in <code>(None, 3)</code> corresponds to the q-value of each action that can be taken at each state. Now, in the training step, I have to call <code>predict()</code> multiple times and use the result to compute the cost and train the model. For doing this, I also have another data array available called <code>current_actions</code> which is a list containing indices of actions taken for a particular state in the previous iterations. </p>

<p>What needs to happen is <code>current_states_outputs</code> should be a tensor created from the output of <code>predict()</code> in which each row contains only one q-value(as opposed to three from the output of <code>predict()</code>) and which q-value should be selected should depend on the corresponding index of <code>current_actions</code>.</p>

<p>For example, if <code>current_states_output = [[1,2,3],[4,5,6],[7,8,9]]</code> and <code>current_actions=[0,2,1]</code>, the result after the operation should be <code>[1,6,8]</code> (updated)</p>

<p>How do I do this?</p>

<p>I have tried the following - </p>

<pre><code>    current_states_outputs = self.sess.run(self.prediction, feed_dict={self.X:current_states})
    current_states_outputs = np.array([current_states_outputs[a][current_actions[a]] for a in range(len(current_actions))])
</code></pre>

<p>I basally ran the session on <code>predict()</code> and did the required using normal python methords. But because this severs the connection of the cost from the previous layers of the graph, no training can be done.  So, I need to do this operation staying within tensorflow and keeping everything as a tensorflow tensor itself. How can I manage this?</p>
",5530553,,4023951,,2017-08-17 17:09:06,2017-08-17 17:09:06,How do I index from another array into a tensor tensorflow,<python><tensorflow><deep-learning><artificial-intelligence><q-learning>,1,0,,,,CC BY-SA 3.0,
1183,40137792,1,40138820,,2016-10-19 17:20:23,,0,675,"<p>In Q-learning, how should I represent my Reward function if my Q-function is approximated by a normal Feed-Forward Neural Network?</p>

<p>Should I represent it as discrete values ""near"", ""very near"" to the goal etc.. All I'm what concerned about, is that as long as I already moved to the neural network approximation of  the Q-function <code>Q(s, a, Î¸)</code> and not using a lookup table anymore, would I still be obliged to build a Reward table as well?</p>
",3340234,,4185106,,2017-05-16 20:16:15,2017-05-16 20:16:15,Reward function with a neural network approximated Q-function,<machine-learning><tensorflow><deep-learning><reinforcement-learning><q-learning>,1,0,,,,CC BY-SA 3.0,
1184,51069380,1,51364829,,2018-06-27 18:33:12,,-3,106,"<p>In the Q-learning algorithm used in Reinforcement Learning with replay, one would use a data structure in which it stores previous experience that is used in training (a basic example would be a tuple in Python). For a complex state space, I would need to train the agent in a very large number of different situations to obtain a NN that correctly approximates the Q-values. The experience data will occupy more and more memory and thus I should impose a superior limit for the number of experience to be stored, after which the computer should drop the experience from memory.</p>

<p>Do you think FIFO (first in first out) would be a good way of manipulating the data vanishing procedure in the memory of the agent (that way, after reaching the memory limit I would discard the oldest experience, which may be useful for permitting the agent to adapt quicker to changes in the medium)? How could I compute a good maximum number of experiences in the memory to make sure that Q-learning on the agent's NN converges towards the Q function approximator I need (I know that this could be done empirically, I would like to know if an analytical estimator for this limit exists)?</p>
",9978354,,9978354,,2018-06-28 08:23:28,2018-07-16 15:03:13,Deep Q learning Replay method Memory Vanishing,<python><machine-learning><deep-learning><reinforcement-learning><q-learning>,1,2,0,,,CC BY-SA 4.0,
1188,53018226,1,53085844,,2018-10-27 02:26:14,,3,1078,"<p>I was trying to implement a q-learning algorithms in Keras. According to the articles i found these lines of code.     </p>

<pre><code>for state, action, reward, next_state, done in sample_batch:
        target = reward
        if not done:
            #formula
          target = reward + self.gamma * np.amax(self.brain.predict(next_state)[0])
        target_f = self.brain.predict(state)
        #shape (1,2)
        target_f[0][action] = target
        print(target_f.shape)
        self.brain.fit(state, target_f, epochs=1, verbose=0)
    if self.exploration_rate &gt; self.exploration_min:
        self.exploration_rate *= self.exploration_decay
</code></pre>

<p>Variable <code>sample_batch</code> is the array that contains sample <code>state, action, reward, next_state, done</code> from collected data.
I also found the following q-learning formula <a href=""https://i.stack.imgur.com/XBsvH.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/XBsvH.png"" alt=""Formula""></a></p>

<p>Why is there no <code>-</code> sign in the equation(code)? I found out that <code>np.amax</code> returns the maximum of an array or maximum along an axis. When i call <code>self.brain.predict(next_state)</code>, I get <code>[[-0.06427538 -0.34116858]]</code>. So it plays the role of prediction  in this equation? As we go forward <code>target_f</code> is the predicted output for the current state and then we also append to it the reward with this step. Then, we train model on current <code>state</code>(<code>X</code>) and <code>target_f</code>(<code>Y</code>). I have a few questions. What is the role of <code>self.brain.predict(next_state)</code> and why there is no minus? Why do we predict twice on one model? Ex <code>self.brain.predict(state) and self.brain.predict(next_state)[0]</code></p>
",,user9900027,,user9900027,2018-10-27 14:31:55,2018-10-31 14:33:27,Reinforcement Learning with Keras model,<python><keras><deep-learning><reinforcement-learning><q-learning>,1,0,0,,,CC BY-SA 4.0,
1193,67630194,1,67630959,,2021-05-21 02:43:52,,1,725,"<pre><code>import pyautogui as pg
from PIL import ImageGrab, ImageOps
import numpy as np
import tensorflow as tf
from tensorflow import keras
from time import sleep
from tensorflow.keras.layers import *
import cv2
import random

box = (0,259,953,428)

def bigjump():
    pg.keyDown('space')
    sleep(0.1)
    pg.keyUp('space')
    
def smalljump():
    pg.press('space')
    
def duck():
    pg.keyDown('down')
    sleep(0.4)
    pg.keyUp('down')
    
def noting():
    sleep(0.1)

screen = ImageGrab.grab(bbox = box)
screen = ImageOps.grayscale(screen)

screen_n = np.array(screen)/255

model = keras.models.Sequential()

model.add(Conv2D(32,(8,8),strides=(2,2)))
model.add(MaxPooling2D(pool_size = (2,2)))
model.add(Activation('relu'))
model.add(Conv2D(64,(4,4),strides=(2,2),padding= 'same'))
model.add(MaxPooling2D(pool_size = (2,2)))
model.add(Activation('relu'))
model.add(Conv2D(64,(3,3),strides=(1,1),padding= 'same'))
model.add(MaxPooling2D(pool_size=(2,2)))
model.add(Activation('relu'))
model.add(Flatten())
model.add(Dense(512,activation='relu'))
model.add(Dense(4))

model.compile(optimizer = 'adam',loss = 'mean_square_error',metrics =['accurate']) 

model.build((169,953))
model.summary()

done = 0
count = 0

discount = 0.99

max_episodes = 1000

state_list = list()
q_list = list()

replay_mem = list()

replay_size = 32
replay_limit = 2048

for episode in range(max_episodes):
    while not done:
        
        count += 1
        e = 1 / ((episode / 10) + 1)
    
        screen = ImageGrab.grab(bbox = box)
        screen_n = np.array(ImageOps.grayscale(screen))/255
        
        q = model.predict(screen_n)
    
        if np.random.rand(1)&lt;e:
            action = random.randint(1,4)
        else:
            action = np.argmax(q)

        if action == 1:
            bigjump()
        elif action == 2:
            smalljump()
        elif action == 3:
            duck()
        elif action == 4:
            noting()
        
        screen_next = ImageGrab.grab(bbox = box)
        screen_next_n = np.array(ImageOps.grayscale(screen_next))/255
        
        if screen_next_n == screen_n:
            done = 1
            reward = -100
        else:
            reward = 1
            
        replay_mem.append([screen_n,screen_next_n,action,done,reward])
        
        if len(replay_mem)&lt;replay_size:
            replay_size = len(replay_mem)
        
        for sample in random.sample(replay_mem,replay_size):
            screen_n,screen_next_n,action_dont,reward = sample
            if done:
                q[0,action] = -100
            else:
                q_next = model.predict(screen_next_n)
                q[0,action] = reward + discount*argmax(q_next)
            
            state_list.append(screen_n)
            q_list.append(q)
        hist = model.fit(state_list,q_list,epochs = 5,verbose =0)
</code></pre>
<p>I am making Q-learning Algorithm to play Chrome dino</p>
<p>I capture screen and convert to binary image and convert to numpy array</p>
<p>And i use model.predict to find q-value but error apear</p>
<p>is there any method for solving this?</p>
<p>ValueError: Input 0 of layer sequential_5 is incompatible with the layer: : expected min_ndim=4, found ndim=2. Full shape received: [None, 953]</p>
",14784305,,14784305,,2021-05-22 04:10:03,2021-05-22 04:10:03,"ValueError: Input 0 of layer sequential_5 is incompatible with the layer: : expected min_ndim=4, found ndim=2. Full shape received: [None, 953]",<python><tensorflow><keras><deep-learning><q-learning>,1,0,0,,,CC BY-SA 4.0,
1196,27370495,1,27379795,,2014-12-09 02:23:45,,4,2810,"<p>I have little background knowledge of Machine Learning, so please forgive me if my question seems silly.</p>

<p>Based on what I've read, the best model-free reinforcement learning algorithm to this date is Q-Learning, where each state,action pair in the agent's world is given a q-value, and at each state the action with the highest q-value is chosen. The q-value is then updated as follows:</p>

<p>Q(s,a) = (1-Î±)Q(s,a) + Î±(R(s,a,s') + (max_a' * Q(s',a'))) where Î± is the learning rate.</p>

<p>Apparently, for problems with high dimensionality, the number of states become astronomically large making q-value table storage infeasible. </p>

<p>So the practical implementation of Q-Learning requires using Q-value approximation via generalization of states aka features. For example if the agent was Pacman then the features would be:</p>

<ul>
<li>Distance to closest dot</li>
<li>Distance to closest ghost</li>
<li>Is Pacman in a tunnel?</li>
</ul>

<p>And then instead of q-values for every single state you would only need to only have q-values for every single feature.</p>

<p>So my question is:</p>

<p><strong>Is it possible for a reinforcement learning agent to create or generate additional features?</strong></p>

<p>Some research I've done:</p>

<p><a href=""https://stackoverflow.com/a/26601252/800735"">This post</a> mentions A Geramifard's iFDD method </p>

<ul>
<li><a href=""http://www.icml-2011.org/papers/473_icmlpaper.pdf"" rel=""nofollow noreferrer"">http://www.icml-2011.org/papers/473_icmlpaper.pdf</a></li>
<li><a href=""http://people.csail.mit.edu/agf/Files/13RLDM-GQ-iFDD+.pdf"" rel=""nofollow noreferrer"">http://people.csail.mit.edu/agf/Files/13RLDM-GQ-iFDD+.pdf</a></li>
</ul>

<p>which is a way of ""discovering feature dependencies"", but I'm not sure if that is feature generation, as the paper assumes that you start off with a set of binary features.</p>

<p>Another paper that I found was apropos is <a href=""http://arxiv.org/pdf/1312.5602v1.pdf"" rel=""nofollow noreferrer"">Playing Atari with Deep Reinforcement Learning</a>, which ""extracts high level features using a range of neural network architectures"".</p>

<p>I've read over the paper but still need to flesh out/fully understand their algorithm. Is this what I'm looking for?</p>

<p>Thanks</p>
",800735,,-1,,2017-05-23 12:25:02,2014-12-09 13:13:11,"In Q-learning with function approximation, is it possible to avoid hand-crafting features?",<machine-learning><deep-learning><q-learning><function-approximation>,1,0,0,,,CC BY-SA 3.0,
1198,48943191,1,48944634,,2018-02-23 07:34:50,,2,423,"<p>I am current working on making a Deep q-network and i a bit confused about how my Q-network knows which reward i give it.</p>

<p>For example I have this state action function with policy and temporal difference:</p>

<p><a href=""https://i.stack.imgur.com/lBasw.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/lBasw.png"" alt=""state action fucntions""></a></p>

<p>and then I have my Q-network:</p>

<p><a href=""https://i.stack.imgur.com/PCBfD.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/PCBfD.png"" alt=""Q-network""></a></p>

<p>Where I input my states and I get 4 different q values in the same observation. Theory wise <strong>how do I reward my Q-network</strong> because my only inputs are the state but not the reward.</p>

<p>I hope one can explain me this!</p>
",5950877,,3625036,,2018-02-23 11:28:25,2018-02-23 11:28:25,How does neural network know which reward it got from action?,<neural-network><deep-learning><reinforcement-learning><q-learning>,1,1,0,,,CC BY-SA 3.0,
1199,47378493,1,47569565,,2017-11-19 15:23:13,,0,749,"<p>I'm trying to gain an intuitive understanding of deep reinforcement learning. In deep Q-networks (DQN) we store all actions/environments/rewards in a memory array and at the end of the episode, ""replay"" them through our neural network. This makes sense because we are trying to build out our rewards matrix and see if our episode ended in reward, scale that back through our matrix. </p>

<p>I would think the sequence of actions that led to the reward state is what is important to capture - this sequence of actions (and not the actions independently) are what led us to our reward state.</p>

<p>In the <a href=""https://arxiv.org/abs/1312.5602"" rel=""nofollow noreferrer"">Atari-DQN paper by Mnih</a> and many tutorials since we see the practice of random sampling from the memory array and training. So if we have a memory of:</p>

<blockquote>
  <p>(action a, state 1) --> (action b, state 2) --> (action c, state 3) --> (action d, state 4) --> reward!</p>
</blockquote>

<p>We may train a mini-batch of:</p>

<blockquote>
  <p>[(action c state 3), (action b, state 2), reward!]</p>
</blockquote>

<p>The reason given is:</p>

<blockquote>
  <p>Second, learning directly from consecutive samples is inefficient, due to the strong correlations
  between the samples; randomizing the samples breaks these correlations and therefore reduces the
  variance of the updates.</p>
</blockquote>

<p>or from this <a href=""http://pytorch.org/tutorials/intermediate/reinforcement_q_learning.html"" rel=""nofollow noreferrer"">pytorch tutorial</a>:</p>

<blockquote>
  <p>By sampling from it randomly, the transitions that build up a batch are decorrelated. It has been shown that this greatly stabilizes and improves the DQN training procedure.</p>
</blockquote>

<p>My intuition would tell me the sequence is what is most important in reinforcement learning. Most episodes have a delayed reward so most action/states do not have a reward (and are not ""reinforced""). The only way to bring a portion of the reward to these previous states is to retroactively break the reward out across the sequence (through the future_reward in the Q algorithm of <em>reward + reward * learning_rate(future_reward)</em>)</p>

<p>A random sampling of the memory bank breaks our sequence, how does that help when you are trying to back-fill a Q (reward) matrix?</p>

<p>Perhaps this is more similar to a Markov model where every state should be considered independent? Where is the error in my intuition?</p>

<p>Thank you!</p>
",2942295,,,,,2017-11-30 09:07:20,Why random sample from replay for DQN?,<neural-network><deep-learning><reinforcement-learning><q-learning>,1,2,0,,,CC BY-SA 3.0,
1205,42829488,1,42876405,,2017-03-16 09:05:05,,0,737,"<p>I'm trying to write a DQL algorithm and I'm trying to run the following graph on tensorflow</p>

<pre><code>class DQN:

def __init__(self, env, n_hidden, learning_rate):

    self.image_input = tf.placeholder(shape=[None, 128,128,3], dtype=tf.float32)
    self.conv1 = tf.contrib.layers.convolution2d(inputs=self.image_input, num_outputs=32, 
                                                 kernel_size=[8,8], stride=[4,4], padding=""VALID"")
    self.conv2 = tf.contrib.layers.convolution2d(inputs=self.conv1, num_outputs=64, 
                                                 kernel_size=[4,4], stride=[2,2], padding=""VALID"")
    self.conv3 = tf.contrib.layers.convolution2d(inputs=self.conv2, num_outputs=64, 
                                                 kernel_size=[3,3], stride=[1,1], padding=""VALID"")
    self.conv4 = tf.contrib.layers.convolution2d(inputs=self.conv3, num_outputs=512, 
                                                 kernel_size=[7,7], stride=[1,1], padding=""VALID"")

    self.conv_out = tf.contrib.layers.flatten(self.conv4)
    self.weights_1 = tf.Variable(tf.random_normal([18432, env.action_space.n], stddev=0.35), name=""fully1_w"")
    self.bias_1 = tf.Variable(tf.zeros(env.action_space.n), name=""fully1_b"")
    self.q_out = tf.add(tf.matmul(self.conv_out, self.weights_1), self.bias_1, name=""q_out"")
    self.predict = tf.argmax(self.q_out, 1)

    self.target_q = tf.placeholder(shape=[None],dtype=tf.float32)
    self.actions = tf.placeholder(shape=[None],dtype=tf.int32)
    self.actions_onehot = tf.one_hot(self.actions,env.action_space.n,dtype=tf.float32)
    self.q_value = tf.reduce_sum(tf.multiply(self.q_out, self.actions_onehot), reduction_indices=1)

    self.td_error = tf.square(self.target_q - self.q_value)
    self.loss = tf.reduce_mean(self.td_error)
    self.trainer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate)
    self.grads_and_vars = self.trainer.compute_gradients(self.loss)
    self.trainer.apply_gradients(self.grads_and_vars)
</code></pre>

<p>And here is the training procedure:</p>

<pre><code>tf.reset_default_graph()
main_qf = DQN(env, n_hidden=10, learning_rate=1.0)

init = tf.global_variables_initializer()
saver = tf.train.Saver()
trainables = tf.trainable_variables()

target_ops = update_target_graph(trainables,tau, mode=""periodically"")
grads=[]
experience_buffer = ExperienceReplay(exp_size)
total_rewards = np.zeros(num_episodes)
losses = np.zeros(num_episodes)

with tf.Session() as session:
state = env.reset()
session.run(init)
update_target(target_ops, session)

for _iter in range(num_episodes):
    state = env.reset()
    # play ===================================================================================
    done = False
    img = process_image(env.render(mode=""rgb_array""))
    episode = []
    while not done:
        #e-greedy
        if np.random.rand() &lt; epsilon:
            action = np.random.choice(range(env.action_space.n))
        else:
            feed_dict = {main_qf.image_input: img[None,:,:,:]}
            action = session.run(main_qf.predict, feed_dict=feed_dict)[0]

        new_state, reward, done, _ = env.step(action)
        new_img = process_image(env.render(mode=""rgb_array""))

        experience_buffer.add((img, action, new_img,reward, done))
        # update results =========================================================================
        total_rewards[_iter] += reward


    # Adjust params (epsilon)  ===============================================================

    if epsilon &gt;= min_epsilon:
        epsilon -= decay

    # train ==================================================================================
    prev_state, actions, new_state, rewards, is_terminal = experience_buffer.sample(batch_size)

    q_function = session.run([main_qf.q_out], feed_dict={
                                                        main_qf.image_input:prev_state})

    q_target = session.run([main_qf.predict], feed_dict={
                                                        main_qf.image_input:new_state})
    q_target = rewards + gamma * q_target * is_terminal

    loss, weights, grad  = session.run([main_qf.loss,main_qf.weights_1, main_qf.grads_and_vars], feed_dict={
                                                        main_qf.image_input : prev_state,
                                                        main_qf.target_q : q_target,
                                                        main_qf.actions : actions
        })

    losses[_iter] = loss
    update_target(target_ops, session)
</code></pre>

<p>But for some reason, I do not understand the training procedure is not updating the weights of the network. I tried to fetch the gradients to check if  I had vanishing gradients (getting grads_and_vars), but this is not the case, The gradients have big values. I also tried to manually assign values to the variables (by calling main_qf.weights1.assing(val)) but it also doesn't work.</p>

<p>Is it something in the composition of my graph? Or in the way, I'm running the session? I'm completely lost on this one. </p>
",3193886,,,,,2017-03-18 15:46:40,Deep Q_learning - Tensorflow - Weights won't change,<tensorflow><deep-learning><reinforcement-learning><q-learning>,1,0,,,,CC BY-SA 3.0,
1207,53660662,1,53697265,,2018-12-06 22:29:01,,4,3303,"<p>In the context of Double Q or Deuling Q Networks, I am not sure if I fully understand the difference.  Especially with V. What exactly is V(s)?  How can a state have an inherent value? </p>

<p>If we are considering this in the context of trading stocks lets say, then how would we define these three variables?</p>
",9831372,,712995,,2018-12-09 22:22:01,2018-12-09 22:22:01,"What exactly is the difference between Q, V (value function) , and reward in Reinforcement Learning?",<machine-learning><deep-learning><reinforcement-learning><q-learning>,1,1,0,,,CC BY-SA 4.0,
1209,51425688,1,51426358,,2018-07-19 14:48:43,,2,1680,"<p>I am trying to use deep reinforcement learning with keras to train an agent to learn how to play the <a href=""https://gym.openai.com/envs/LunarLander-v2/"" rel=""nofollow noreferrer"">Lunar Lander OpenAI gym environment</a>. The problem is that my model is not converging. Here is my code:</p>

<pre><code>import numpy as np
import gym

from keras.models import Sequential
from keras.layers import Dense
from keras import optimizers

def get_random_action(epsilon):
    return np.random.rand(1) &lt; epsilon

def get_reward_prediction(q, a):
    qs_a = np.concatenate((q, table[a]), axis=0)
    x = np.zeros(shape=(1, environment_parameters + num_of_possible_actions))
    x[0] = qs_a
    guess = model.predict(x[0].reshape(1, x.shape[1]))
    r = guess[0][0]
    return r

results = []
epsilon = 0.05
alpha = 0.003
gamma = 0.3
environment_parameters = 8
num_of_possible_actions = 4
obs = 15
mem_max = 100000
epochs = 3
total_episodes = 15000

possible_actions = np.arange(0, num_of_possible_actions)
table = np.zeros((num_of_possible_actions, num_of_possible_actions))
table[np.arange(num_of_possible_actions), possible_actions] = 1

env = gym.make('LunarLander-v2')
env.reset()

i_x = np.random.random((5, environment_parameters + num_of_possible_actions))
i_y = np.random.random((5, 1))

model = Sequential()
model.add(Dense(512, activation='relu', input_dim=i_x.shape[1]))
model.add(Dense(i_y.shape[1]))

opt = optimizers.adam(lr=alpha)

model.compile(loss='mse', optimizer=opt, metrics=['accuracy'])

total_steps = 0
i_x = np.zeros(shape=(1, environment_parameters + num_of_possible_actions))
i_y = np.zeros(shape=(1, 1))

mem_x = np.zeros(shape=(1, environment_parameters + num_of_possible_actions))
mem_y = np.zeros(shape=(1, 1))
max_steps = 40000

for episode in range(total_episodes):
    g_x = np.zeros(shape=(1, environment_parameters + num_of_possible_actions))
    g_y = np.zeros(shape=(1, 1))
    q_t = env.reset()
    episode_reward = 0

    for step_number in range(max_steps):
        if episode &lt; obs:
            a = env.action_space.sample()
        else:
            if get_random_action(epsilon, total_episodes, episode):
                a = env.action_space.sample()
            else:
                actions = np.zeros(shape=num_of_possible_actions)

                for i in range(4):
                    actions[i] = get_reward_prediction(q_t, i)

                a = np.argmax(actions)

        # env.render()
        qa = np.concatenate((q_t, table[a]), axis=0)

        s, r, episode_complete, data = env.step(a)
        episode_reward += r

        if step_number is 0:
            g_x[0] = qa
            g_y[0] = np.array([r])
            mem_x[0] = qa
            mem_y[0] = np.array([r])

        g_x = np.vstack((g_x, qa))
        g_y = np.vstack((g_y, np.array([r])))

        if episode_complete:
            for i in range(0, g_y.shape[0]):
                if i is 0:
                    g_y[(g_y.shape[0] - 1) - i][0] = g_y[(g_y.shape[0] - 1) - i][0]
                else:
                    g_y[(g_y.shape[0] - 1) - i][0] = g_y[(g_y.shape[0] - 1) - i][0] + gamma * g_y[(g_y.shape[0] - 1) - i + 1][0]

            if mem_x.shape[0] is 1:
                mem_x = g_x
                mem_y = g_y
            else:
                mem_x = np.concatenate((mem_x, g_x), axis=0)
                mem_y = np.concatenate((mem_y, g_y), axis=0)

            if np.alen(mem_x) &gt;= mem_max:
                for l in range(np.alen(g_x)):
                    mem_x = np.delete(mem_x, 0, axis=0)
                    mem_y = np.delete(mem_y, 0, axis=0)

        q_t = s

        if episode_complete and episode &gt;= obs:
            if episode%10 == 0:
                model.fit(mem_x, mem_y, batch_size=32, epochs=epochs, verbose=0)

        if episode_complete:
            results.append(episode_reward)
            break
</code></pre>

<p>I am running tens of thousands of episodes and my model still won't converge. It will begin to reduce average change in policy over ~5000 episodes while increasing the average reward, but then it goes off the deep end and the average reward per episode actually goes <em>down</em> after that. I've tried messing with the hyperparameters, but I haven't gotten anywhere with that. I'm trying to model my code after the <a href=""https://deepmind.com/research/dqn/"" rel=""nofollow noreferrer"">DeepMind DQN paper</a>.</p>
",10100281,,10100281,,2018-07-19 15:00:57,2018-07-26 13:57:28,Model for OpenAI gym's Lunar Lander not converging,<neural-network><keras><deep-learning><reinforcement-learning><q-learning>,2,0,,,,CC BY-SA 4.0,
1212,54173261,1,54173355,,2019-01-13 21:11:48,,0,734,"<p>I am training an agent with DQN. The reward is increasing and the loss is decreasing. It is a good sign I have great results. However, I have a little doubt because the loss decreased and suddenly jump to a very high value </p>

<p>Here is the first 20 epochs </p>

<pre><code>===============================
Reward: 0.0 Steps: 0.0 Update: 1 Time: 1.2 Episodes: 1
Loss: 19796.0547
===============================
Reward: 13243.5 Steps: 100.0 Update: 3 Time: 5.33 Episodes: 2
Loss: 19431.1680
===============================
Reward: 13507.0 Steps: 100.0 Update: 6 Time: 5.56 Episodes: 3
Loss: 19586.0059
===============================
Reward: 13469.5 Steps: 100.0 Update: 9 Time: 5.96 Episodes: 4
Loss: 19398.0176
===============================
Reward: 13923.5 Steps: 100.0 Update: 12 Time: 6.34 Episodes: 5
Loss: 19539.2090
===============================
Reward: 13382.0 Steps: 100.0 Update: 15 Time: 6.57 Episodes: 6
Loss: 19461.4648
===============================
Reward: 14326.0 Steps: 100.0 Update: 18 Time: 6.89 Episodes: 7
Loss: 19103.9668
===============================
Reward: 15041.0 Steps: 100.0 Update: 21 Time: 7.16 Episodes: 8
Loss: 19470.4160
===============================
Reward: 15718.5 Steps: 100.0 Update: 24 Time: 7.52 Episodes: 9
Loss: 19668.2324
===============================
Reward: 14925.5 Steps: 100.0 Update: 27 Time: 8.0 Episodes: 10
Loss: 19771.4648
===============================
Reward: 15555.0 Steps: 100.0 Update: 30 Time: 8.12 Episodes: 11
Loss: 19788.6621
===============================
Reward: 14711.0 Steps: 100.0 Update: 33 Time: 8.52 Episodes: 12
Loss: 19724.0176
===============================
Reward: 15329.5 Steps: 100.0 Update: 36 Time: 9.03 Episodes: 13
Loss: 19551.4707
===============================
Reward: 15748.0 Steps: 100.0 Update: 39 Time: 9.17 Episodes: 14
Loss: 19516.3770
===============================
Reward: 15666.5 Steps: 100.0 Update: 42 Time: 9.39 Episodes: 15
Loss: 19426.6973
===============================
Reward: 15593.5 Steps: 100.0 Update: 45 Time: 9.85 Episodes: 16
Loss: 19327.2832
===============================
Reward: 15614.0 Steps: 100.0 Update: 48 Time: 10.13 Episodes: 17
Loss: 19158.5488
===============================
Reward: 15874.5 Steps: 100.0 Update: 51 Time: 10.47 Episodes: 18
Loss: 19061.7402
===============================
Reward: 15575.5 Steps: 100.0 Update: 54 Time: 10.68 Episodes: 19
Loss: 18895.0918
===============================
Reward: 15949.5 Steps: 100.0 Update: 57 Time: 11.01 Episodes: 20
Loss: 18741.6094
</code></pre>

<p>After <code>37 epochs</code>, the reward reached <code>~17000</code> and the loss decreased to <code>15694</code>. </p>

<p>Here where can notice the big jump in the loss. It does it 3 times over 100 episodes</p>

<pre><code>Reward: 16366.0 Steps: 100.0 Update: 117 Time: 17.44 Episodes: 40
Loss: 15099.0156
===============================
Reward: 15909.5 Steps: 100.0 Update: 120 Time: 17.9 Episodes: 41
Loss: 14892.0322
===============================
Reward: 16744.5 Steps: 100.0 Update: 123 Time: 17.87 Episodes: 42
Loss: 14705.1650
===============================
Reward: 16613.5 Steps: 100.0 Update: 126 Time: 18.39 Episodes: 43
Loss: 14518.6943
===============================
Reward: 16422.0 Steps: 100.0 Update: 129 Time: 18.8 Episodes: 44
Loss: 19189.0879
===============================
Reward: 16820.5 Steps: 100.0 Update: 132 Time: 19.27 Episodes: 45
Loss: 28676.2344
===============================
Reward: 16513.5 Steps: 100.0 Update: 135 Time: 19.66 Episodes: 46
Loss: 28341.6875
===============================
Reward: 16878.5 Steps: 100.0 Update: 138 Time: 20.08 Episodes: 47
Loss: 27986.1465
</code></pre>

<p>I expected that the loss keeps decreasing or stabilizing. How can I explain the jump in the loss? How can I avoid it?</p>
",10872297,,,,,2019-01-13 21:24:34,Loss decreased and jump suddenly,<deep-learning><reinforcement-learning><q-learning>,1,0,,,,CC BY-SA 4.0,
1215,55800211,1,55909274,,2019-04-22 19:46:53,,1,95,"<p>I have implemented DQN with experience replay.Input is 50x50x1. With a batch size of 4, input would become (4,50,50,1). Total output actions are 10. If batch size is 4, output would be (4,10). I want to know how would i select the max q-value out of this (4,10) vector. Thanks in advance</p>
",11348030,,712995,,2019-10-12 08:16:16,2019-10-12 08:16:16,How to select the action with highest Q value,<deep-learning><action><reinforcement-learning><q-learning>,1,0,,,,CC BY-SA 4.0,
1223,58559415,1,58560622,,2019-10-25 13:23:42,,2,162,"<p>For standard Q-Learning combined with a neural network things are more or less easy. 
One stores <em>(s,a,r,sâ€™)</em> during interaction with the environment and use </p>

<p><code>target = Qnew(s,a) = (1 - alpha) * Qold(s,a) + alpha * ( r + gamma * max_{aâ€™} Qold(sâ€™, aâ€™)  )</code></p>

<p>as target values for the neural network approximation the Q-function. So the input of the ANN is <em>(s,a)</em> and the output is the scalar <em>Qnew(s,a)</em>. Deep Q-Learning papers/tutorials change the structure of the Q-function. Instead of providing a single Q-value for the pair <em>(s,a)</em> it should now provide the Q-Values of all possible actions for a state <em>s</em>, so it is <em>Q(s)</em> instead of <em>Q(s,a)</em>. </p>

<p>There comes my problem. The data base filled with <em>(s,a,r,sâ€™)</em> does for a specific state <em>s</em> does not contain the reward for all actions. Only for some, maybe just one action. So how to setup the target values for the network <code>Q(s) = [Q(a_1), â€¦. , Q(a_n) ]</code> without having all rewards for the state <em>s</em> in the database? I have seen different loss functions/target values but all contain the reward. </p>

<p>As you see; I am puzzled. Does someone help me? There are a lot of tutorials on the web but this step is in general poorly descried and even less motivated looking at the theoryâ€¦ </p>
",8389476,,5228890,,2019-10-25 14:22:58,2019-10-25 19:12:47,Setting up target values for Deep Q-Learning,<machine-learning><deep-learning><reinforcement-learning>,1,1,,,,CC BY-SA 4.0,
1231,38300193,1,38302645,,2016-07-11 06:10:30,,0,235,"<p>I've been reading up on deep reinforcement learning such as here:</p>

<p><a href=""https://www.nervanasys.com/demystifying-deep-reinforcement-learning/"" rel=""nofollow"">https://www.nervanasys.com/demystifying-deep-reinforcement-learning/</a></p>

<p>It will be a while before I understand all the math but that doesn't stop me from using the libraries. Anyway, I know that in a convolutional ANN, if you want to perform learning on images, you have to preprocess the images otherwise the computing power required to compute the neural network is astronomical. Does this degrade the quality of the net in any ways? If so how?</p>

<p>For instance, lets say you had enough computing power to feed the network every pixel of every high quality image in a video stream for the purpose of learning how to reach goals. Would that make the net far more adept at achieving its goal? Would it broaden the types of goals the net could accomplish, possibly giving it the ability to better generalize?</p>

<p>I'm also thinking of this in the context of computer vision where you might have a robot reasoning about its environment in order to learn to perform tasks. It seems that preprocessing the imagery it receives would be akin to handicapping it with extremely poor sight.</p>
",1390723,,,,,2019-04-16 07:22:16,Does Preprocessing In Deep Q/Reinforcement Learning Lessen Accuracy?,<computer-vision><neural-network><deep-learning><reinforcement-learning>,1,0,,,,CC BY-SA 3.0,
1233,60955922,1,61006203,,2020-03-31 16:59:01,,1,182,"<p>I'm new in DRL. Starting from this code <a href=""https://github.com/jaromiru/cwcf"" rel=""nofollow noreferrer"">https://github.com/jaromiru/cwcf</a>, I would like to substitute the MLP used for the q function approximation with a CNN, but I don't know how to do. Can anybody help me? Thanks</p>
",13172307,,,,,2020-04-03 06:38:42,Building a deep reinforcement learning with a cnn q - approximation,<python><pytorch><reinforcement-learning><conv-neural-network>,1,0,,,,CC BY-SA 4.0,
1238,43536573,1,43537544,,2017-04-21 07:31:59,,0,49,"<p><a href=""https://github.com/yenchenlin/DeepLearningFlappyBird/blob/master/deep_q_network.py#L82"" rel=""nofollow noreferrer"">https://github.com/yenchenlin/DeepLearningFlappyBird/blob/master/deep_q_network.py#L82</a></p>

<p>I have spend a lot of time to understand it.</p>

<p>Why use <code>tf.multiply</code>?</p>

<p>I can not find the math that support this multiply operation.</p>
",2455061,,,,,2017-04-21 08:23:00,Can not understand this line of a popular deep Q learning program,<machine-learning><deep-learning><reinforcement-learning>,1,1,,,,CC BY-SA 3.0,
1240,55473950,1,55478993,,2019-04-02 11:41:05,,3,5035,"<p>Batch size mean the number of samples trained in neural work in supervised learning, however, what is the meaning of batch size meaning in the background of reinforcement learning? Does it refer to samples too? If so, what is the samples meaning in the background of reinforcement learning?</p>
",10668594,,,,,2019-04-02 15:56:33,What is the meaning of batch size in the background of deep reinforcement learning?,<reinforcement-learning><batchsize>,1,0,0,,,CC BY-SA 4.0,
1243,50989406,1,51409138,,2018-06-22 13:45:26,,0,220,"<p>I have been playing around with the <a href=""https://selfdrivingcars.mit.edu/deeptraffic/"" rel=""nofollow noreferrer"">MIT DeepTraffic Challenge</a>
Also <a href=""https://selfdrivingcars.mit.edu/"" rel=""nofollow noreferrer"">watching the lecture and reading the slides</a></p>

<p>After getting a General understanding of the architecture I was wondering what exactly the reward function given by the Environment is.</p>

<ol>
<li>Is it the same as the Input of the gridcell (max. drivable Speed)?</li>
<li>And are they using Reward Clipping, or not?</li>
</ol>

<p>I also found this javascript <a href=""https://github.com/karpathy/convnetjs/blob/master/build/deepqlearn.js"" rel=""nofollow noreferrer"">Codebase</a>, which does not really help my understanding either.</p>
",5763590,,5763590,,2018-07-19 21:11:11,2018-07-20 18:12:55,Reward Function in MIT Deep Traffic Challenge?,<machine-learning><reinforcement-learning><reward>,1,0,0,,,CC BY-SA 4.0,
1246,47685341,1,47697583,,2017-12-06 23:41:56,,3,897,"<p>The neural network I trained is the critic network for deep reinforcement learning. The problem is when one of the layer's activation is set to be relu or elu, the output would be nan after some training step, while the output is normal if the activation is tanh. And the code is as follows(based on tensorflow):</p>



<pre class=""lang-py prettyprint-override""><code>with tf.variable_scope('critic'):

        self.batch_size = tf.shape(self.tfs)[0]

        l_out_x = denseWN(x=self.tfs, name='l3', num_units=self.cell_size, nonlinearity=tf.nn.tanh, trainable=True,shape=[det*step*2, self.cell_size])

        l_out_x1 = denseWN(x=l_out_x, name='l3_1', num_units=32, trainable=True,nonlinearity=tf.nn.tanh, shape=[self.cell_size, 32])
        l_out_x2 = denseWN(x=l_out_x1, name='l3_2', num_units=32, trainable=True,nonlinearity=tf.nn.tanh,shape=[32, 32])
        l_out_x3 = denseWN(x=l_out_x2, name='l3_3', num_units=32, trainable=True,shape=[32, 32])

        self.v = denseWN(x=l_out_x3, name='l4', num_units=1,  trainable=True, shape=[32, 1])
</code></pre>

<p>Here is the code for basic layer construction:</p>

<pre class=""lang-py prettyprint-override""><code>def get_var_maybe_avg(var_name, ema,  trainable, shape):
    if var_name=='V':
        initializer = tf.contrib.layers.xavier_initializer()
        v = tf.get_variable(name=var_name, initializer=initializer, trainable=trainable, shape=shape)
    if var_name=='g':
        initializer = tf.constant_initializer(1.0)
        v = tf.get_variable(name=var_name, initializer=initializer, trainable=trainable, shape=[shape[-1]])
    if var_name=='b':
        initializer = tf.constant_initializer(0.1)
        v = tf.get_variable(name=var_name, initializer=initializer, trainable=trainable, shape=[shape[-1]])
    if ema is not None:
        v = ema.average(v)
    return v

def get_vars_maybe_avg(var_names, ema, trainable, shape):
    vars=[]
    for vn in var_names:
        vars.append(get_var_maybe_avg(vn, ema, trainable=trainable, shape=shape))
    return vars

def denseWN(x, name, num_units, trainable, shape, nonlinearity=None, ema=None, **kwargs):
    with tf.variable_scope(name):
        V, g, b = get_vars_maybe_avg(['V', 'g', 'b'], ema, trainable=trainable, shape=shape)
        x = tf.matmul(x, V)
        scaler = g/tf.sqrt(tf.reduce_sum(tf.square(V),[0]))
        x = tf.reshape(scaler,[1,num_units])*x + tf.reshape(b,[1,num_units])
        if nonlinearity is not None:
            x = nonlinearity(x)
        return x
</code></pre>

<p>Here is the code to train the network:</p>

<pre class=""lang-py prettyprint-override""><code>self.tfdc_r = tf.placeholder(tf.float32, [None, 1], 'discounted_r')
self.advantage = self.tfdc_r - self.v
l1_regularizer = tf.contrib.layers.l1_regularizer(scale=0.005, scope=None)
self.weights = tf.trainable_variables()
regularization_penalty_critic = tf.contrib.layers.apply_regularization(l1_regularizer, self.weights)
self.closs = tf.reduce_mean(tf.square(self.advantage))
self.optimizer = tf.train.RMSPropOptimizer(0.0001, 0.99, 0.0, 1e-6)
self.grads_and_vars = self.optimizer.compute_gradients(self.closs)
self.grads_and_vars = [[tf.clip_by_norm(grad,5), var] for grad, var in self.grads_and_vars if grad is not None]
self.ctrain_op = self.optimizer.apply_gradients(self.grads_and_vars, global_step=tf.contrib.framework.get_global_step())
</code></pre>
",8955454,,712995,,2017-12-07 16:36:30,2017-12-07 16:36:30,"For deep learning, With activation relu the output becomes NAN during training while is normal with tanh",<machine-learning><tensorflow><neural-network><deep-learning><reinforcement-learning>,1,0,0,,,CC BY-SA 3.0,
1248,67738099,1,70704158,,2021-05-28 11:36:39,,1,236,"<pre><code>I recently set up my MAC M1 Air to implement deep reinforcement learning. 
But, when I started following this tutorial - Deep Reinforcement Learning Tutorial for Python https://www.youtube.com/watch?v=cO5g5qLrLSo&amp;list=PLgNJO2hghbmjlE6cuKMws2ejC54BTAaWV&amp;index=2, I got errors with DQN Agent as
 

In this part of the code.
Build Agent with Keras-RL
  from rl.agents import DQNAgent
  from rl.policy import BoltzmannQPolicy
  from rl.memory import SequentialMemory
  def build_agent(model, actions):
      policy = BoltzmannQPolicy()
      memory = SequentialMemory(limit=50000, window_length=1)
      dqn = DQNAgent(model=model, memory=memory, policy=policy, 
              nb_actions=actions, nb_steps_warmup=10, 
      target_model_update=1e-2)
      return dqn
  dqn = build_agent(model, actions)
  dqn.compile(Adam(lr=1e-3), metrics=['mae'])
  dqn.fit(env, nb_steps=50000, visualize=False, verbose=1)
</code></pre>
<p>TypeError: Keras symbolic inputs/outputs do not implement <code>__len__</code>. You may be trying to pass Keras symbolic inputs/outputs to a TF API that does not register to dispatch, preventing Keras from automatically converting the API call to a lambda layer in the Functional Model. This error will also get raised if you try asserting a symbolic input/output directly.</p>
<pre><code>I searched a lot for it and they are asking to downgrade TensorFlow for its compatibility with Keras-rl2. But, I don't see any such version available for the MAC silicon chip. Can someone please guide me, I have been stuck on it for far too long.




Any help is appreciated.
</code></pre>
",12987236,,12987236,,2021-06-01 09:40:46,2022-03-29 14:28:12,Setting up deep reinforcement learning environment on my MAC M1 Air,<tensorflow><keras><deep-learning><reinforcement-learning><apple-m1>,1,2,,,,CC BY-SA 4.0,
1250,67777471,1,67813891,,2021-05-31 16:36:13,,1,166,"<p>All descriptions of links referenced in the question below are from 2021/05/31.</p>
<p>I have trained a deep Q network following the version of the <a href=""https://www.tensorflow.org/agents/tutorials/1_dqn_tutorial"" rel=""nofollow noreferrer"">TF Agents tutorial</a> on a custom problem.  Now I would like to feed it some hand-crafted observations to see what actions it recommends.  I have some utility functions for creating these feature vectors that I use in my PyEnvironment. However, I am not sure how to convert these bits to feed into the network.</p>
<p>What I would like to have is something like the following:</p>
<ol>
<li>Feed in an initial state, and see the recommended action from the network.</li>
<li>Manally alter the state, and see what the network recomends, next.</li>
<li>And so on...</li>
</ol>
<p>My environment has a stochastic component, so I want to manually modify the environment state rather than have the agent explicitly take a path through the environment.</p>
<p>To make progress on this question, I have been examining this <a href=""https://www.tensorflow.org/agents/tutorials/3_policies_tutorial"" rel=""nofollow noreferrer"">tutorial on policies</a>. It looks like, my use case might be similar to the section &quot;Random TF Policy&quot; or the one below on &quot;Actor policies&quot;.  However, in my use case I have a loaded agent and have Python (non TF) observation, time specs, and action specs. What is the ideal approach to drive my network to produce actions from these components?</p>
<p>Here is something I have tried:</p>
<pre><code>saved_policy = tf.compat.v2.saved_model.load(policy_dir)
# get_feat_vector returns an numpy.ndarray
observation = tf.convert_to_tensor(state.get_feat_vector(), dtype=tf.float32)
time_step = ts.restart(observation)
action_step = saved_policy.action(time_step)

</code></pre>
<p>and the associated error message:</p>
<pre><code>File &quot;/home/---/.local/lib/python3.8/site-packages/tensorflow/python/saved_model/function_deserialization.py&quot;, line 267, in restored_function_body
    raise ValueError(
ValueError: Could not find matching function to call loaded from the SavedModel. Got:
  Positional arguments (2 total):
    * TimeStep(step_type=&lt;tf.Tensor 'time_step:0' shape=() dtype=int32&gt;, reward=&lt;tf.Tensor 'time_step_1:0' shape=() dtype=float32&gt;, discount=&lt;tf.Tensor 'time_step_2:0' shape=() dtype=float32&gt;, observation=&lt;tf.Tensor 'time_step_3:0' shape=(170,) dtype=float32&gt;)
    * ()
  Keyword arguments: {}

Expected these arguments to match one of the following 2 option(s):

Option 1:
  Positional arguments (2 total):
    * TimeStep(step_type=TensorSpec(shape=(None,), dtype=tf.int32, name='step_type'), reward=TensorSpec(shape=(None,), dtype=tf.float32, name='reward'), discount=TensorSpec(shape=(None,), dtype=tf.float32, name='discount'), observation=TensorSpec(shape=(None, 170), dtype=tf.float32, name='observation'))
    * ()
  Keyword arguments: {}

Option 2:
  Positional arguments (2 total):
    * TimeStep(step_type=TensorSpec(shape=(None,), dtype=tf.int32, name='time_step/step_type'), reward=TensorSpec(shape=(None,), dtype=tf.float32, name='time_step/reward'), discount=TensorSpec(shape=(None,), dtype=tf.float32, name='time_step/discount'), observation=TensorSpec(shape=(None, 170), dtype=tf.float32, name='time_step/observation'))
    * ()
  Keyword arguments: {}
</code></pre>
",30636,,30636,,2021-06-02 15:16:07,2021-06-04 01:58:22,TF Agents: How to feed faked observations in to a trained deep Q network model to examine which actions it chooses?,<tensorflow><reinforcement-learning>,1,0,,,,CC BY-SA 4.0,
1255,50001520,1,50166602,,2018-04-24 12:19:24,,2,214,"<p>Deep Reinforcement Learning can be very useful in applying it to real-world problems which have highly dynamic nature. Few examples can be listed as is finance, healthcare etc. But when it comes to these kinds of problems it is hard to have a simulated environment. So what are the possible things to do?</p>
",5915270,,,,,2018-05-04 03:14:20,How to apply model free deep reinforcement learning when the access to the real environment is hard?,<reinforcement-learning>,1,5,0,,,CC BY-SA 3.0,
1256,68835417,1,68891183,,2021-08-18 15:39:09,,0,317,"<p>I have a policy that I read from disk using the function SavedModelPyTFEagerPolicy.  For troubleshooting the environment definitions, I would like to examine the predicted value of different states.</p>
<p>I have had success using <a href=""https://stackoverflow.com/questions/67777471/tf-agents-how-to-feed-faked-observations-in-to-a-trained-deep-q-network-model-t"">these instructions</a> to extract the actions from the policy for test cases.  Is there a function that will allow me to extract the predicted values associated with those actions?</p>
",30636,,,,,2021-08-23 10:41:42,TF-Agents Deep Q Learning: How to extract predicted value for state/action pair?,<tensorflow><reinforcement-learning>,1,0,,,,CC BY-SA 4.0,
1258,70340806,1,70463589,,2021-12-13 20:55:01,,0,81,"<p>I am try to run DRL on a low speed environment and sequential learning is making me upset. is there anyway to speed up the learning process? I tried some offline deep reinforcement learning but I still need higher speed (if possible).</p>
",10038877,,4685471,,2021-12-13 23:45:28,2021-12-23 14:38:10,parallelized deep reinforcement learning,<machine-learning><parallel-processing><gpu><reinforcement-learning>,1,1,,,,CC BY-SA 4.0,
1261,73723103,1,73811374,,2022-09-14 20:59:04,,0,57,"<p>I have a deep sarsa algorithm which work great on Pytorch on lunar-lander-v2 and I would use with Keras/Tensorflow. It use mini-batch of size 64 which are used 128 time to train at each episode.</p>
<p>There are the results I get. As you can see, it work great with Pytorch but not with Keras / Tensorflow... So I think I do not correctly implement the training function is Keras/Tensorflow (code is below).</p>
<p>It seems that loss is oscillating in Keras because epsilon go to early to slow value but it work very great in Pytorch...</p>
<p>Do you see something that could explain why it do not work in Keras/Tensorflow please? Thanks a lot for your help and any idea that could help me...</p>
<p><a href=""https://i.stack.imgur.com/1b6kh.jpg"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/1b6kh.jpg"" alt=""enter image description here"" /></a></p>
<p><strong>Network information:</strong></p>
<p>It use Adam optimizer, and a network with two layers : 256 and 128, with relu on each:</p>
<pre><code>class Q_Network(nn.Module):
def __init__(self, state_dim , action_dim):
    super(Q_Network, self).__init__()
    self.x_layer = nn.Linear(state_dim, 256)
    self.h_layer = nn.Linear(256, 128)
    self.y_layer = nn.Linear(128, action_dim)
    print(self.x_layer)

def forward(self, state):
    xh = F.relu(self.x_layer(state))
    hh = F.relu(self.h_layer(xh))
    state_action_values = self.y_layer(hh)
    return state_action_values
</code></pre>
<p>For keras/Tensorflwo I use this one:</p>
<pre><code>def CreationModele(dimension): 
  entree_etat = keras.layers.Input(shape=(dimension))

  sortie = keras.layers.Dense(units=256, activation='relu')(entree_etat)
  sortie = keras.layers.Dense(units=128, activation='relu')(sortie)
  sortie = keras.layers.Dense(units=4)(sortie)

  modele = keras.Model(inputs=entree_etat,outputs=sortie)
  return modele
</code></pre>
<p><strong>Training code</strong></p>
<p>In Pytorch, the training is done by:</p>
<pre><code>def update_Sarsa_Network(self, state, next_state, action, next_action, reward, ends):

    actions_values = torch.gather(self.qnet(state), dim=1, index=action.long())

    next_actions_values = torch.gather(self.qnet(next_state), dim=1, index=next_action.long())

    next_actions_values = reward + (1.0 - ends) * (self.discount_factor * next_actions_values)

    q_network_loss = self.MSELoss_function(actions_values, next_actions_values.detach())
    self.qnet_optim.zero_grad()
    q_network_loss.backward()
    self.qnet_optim.step()
    return q_network_loss
</code></pre>
<p>And in Keras/Tensorflow by:</p>
<pre><code>mse = keras.losses.MeanSquaredError(
    reduction=keras.losses.Reduction.SUM)

@tf.function
def train(model, batch_next_states_tensor, batch_next_actions_tensor, batch_reward_tensor, batch_end_tensor, batch_states_tensor, batch_actions_tensor, optimizer, gamma):
  with tf.GradientTape() as tape:
    # EStimation des valeurs des actions courantes
    actions_values = model(batch_states_tensor)                                                          # (mini_batch_size,4)
    actions_values = tf.linalg.diag_part(tf.gather(actions_values,batch_actions_tensor,axis=1))         # (mini_batch_size,)
    actions_values = tf.expand_dims(actions_values,-1)                                                  # (mini_batch_size,1)

    # EStimation des valeurs des actions suivantes
    next_actions_values = model(batch_next_states_tensor)                                                          # (mini_batch_size,4)
    next_actions_values = tf.linalg.diag_part(tf.gather(next_actions_values,batch_next_actions_tensor,axis=1))   # (mini_batch_size,)
    cibles = batch_reward_tensor + (1.0 - batch_end_tensor)*gamma*tf.expand_dims(next_actions_values,-1)         # (mini_batch_size,1)

    error = mse(cibles, actions_values)
    
  grads = tape.gradient(error, model.trainable_variables)
  optimizer.apply_gradients(zip(grads, model.trainable_variables))
  
  return error
</code></pre>
<p><strong>Error function and Optimizer code</strong></p>
<p>The optimizer is Adam in Pytorch and Tensorflow with lr=0.001. In Pytorch:</p>
<pre><code>def __init__(self, state_dim, action_dim):
    self.qnet = Q_Network(state_dim, action_dim)
    self.qnet_optim = torch.optim.Adam(self.qnet.parameters(), lr=0.001)
    self.discount_factor = 0.99
    self.MSELoss_function = nn.MSELoss(reduction='sum')
    self.replay_buffer = ReplayBuffer()
    pass
</code></pre>
<p>In Keras / Tensorflow:</p>
<pre><code>alpha = 1e-3

# Initialise le modÃ¨le
modele_Keras = CreationModele(8)

optimiseur_Keras = keras.optimizers.Adam(learning_rate=alpha)
</code></pre>
",7927646,,214143,,2022-09-14 22:08:08,2022-09-25 09:38:11,Problem with Deep Sarsa algorithm which work with pytorch (Adam optimizer) but not with keras/Tensorflow (Adam optimizer),<tensorflow><keras><pytorch><reinforcement-learning><sarsa>,1,1,,,,CC BY-SA 4.0,
1264,61058333,1,61168375,,2020-04-06 11:05:10,,6,754,"<p>I have a custom environment in keras-rl with the following configurations in the constructor</p>

<pre><code>def __init__(self, data):

    #Declare the episode as the first episode
    self.episode=1

    #Initialize data      
    self.data=data

    #Declare low and high as vectors with -inf values 
    self.low = numpy.array([-numpy.inf])
    self.high = numpy.array([+numpy.inf])

    self.observation_space = spaces.Box(self.low, self.high, dtype=numpy.float32)

    #Define the space of actions as 3 (I want them to be 0, 1 and 2)
    self.action_space = spaces.Discrete(3) 

    self.currentObservation = 0

    self.limit = len(data)      

    #Initiates the values to be returned by the environment
    self.reward = None
</code></pre>

<p>As you can see, my agent will perform 3 actions, depending on the action, a different reward will be calculated in the function step() below:</p>

<pre><code>def step(self, action):

    assert self.action_space.contains(action)

    #Initiates the reward
    self.reward=0

    #get the reward 
    self.possibleGain = self.data.iloc[self.currentObservation]['delta_next_day']

    #If action is 1, calculate the reward 
    if(action == 1):
        self.reward = self.possibleGain-self.operationCost

    #If action is 2, calculate the reward as negative     
    elif(action==2):
        self.reward = (-self.possibleGain)-self.operationCost

    #If action is 0, no reward     
    elif(action==0):
        self.reward = 0

    #Finish episode 
    self.done=True 

    self.episode+=1   
    self.currentObservation+=1

    if(self.currentObservation&gt;=self.limit):
        self.currentObservation=0

    #Return the state, reward and if its done or not
    return self.getObservation(), self.reward, self.done, {}
</code></pre>

<p>The problem is the fact that, if I print the actions at every episode, they are 0, 2, and 4. I want them to be 0, 1 and 2. How can I force the agent to recognize only these 3 actions with keras-rl?</p>
",2163392,,,,,2020-04-12 07:52:39,Define action values in keras-rl,<python><keras><reinforcement-learning><keras-rl>,1,1,,,,CC BY-SA 4.0,
1265,56446182,1,56446304,,2019-06-04 14:43:06,,1,630,"<p>I am using a deep reinforcement learning approach to predict time series behavior. I am quite a newbie on that so my question is more conceptual than a computer programming one. My colleague has given me the following chart, with training, validation and testing accuracy of time series data classification using deep reinforcement learning.</p>

<p><a href=""https://i.stack.imgur.com/W1AMY.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/W1AMY.png"" alt=""enter image description here""></a></p>

<p>From this graph, it's possible to see that both validation and testing accuracies are random, so, of course, the agent is overfitting.</p>

<p>But what makes me more surprised (maybe because of lack of knowledge, and that is why I am here to ask you), is how my colleague trains his agent. In the X-axis of this chart, you can find the ""epoch"" number (or iteration). In other words, the agent is fitted (or trained) several times 
as in the code below:</p>

<pre><code>#initiating the agent

self.agent = DQNAgent(model=self.model, policy=self.policy, 
nb_actions=self.nbActions, memory=self.memory, nb_steps_warmup=200, 
target_model_update=1e-1, 
enable_double_dqn=True,enable_dueling_network=True)

#Compile the agent with the Adam optimizer and with the mean absolute error metric

self.agent.compile(Adam(lr=1e-3), metrics=['mae'])

#there will be 100 iterations, I will fit and test the agent 100 times
for i in range(0,100):
    #delete previous environments and create new ones         
    del(trainEnv)       
    trainEnv = SpEnv(parameters)
    del(validEnv)
    validEnv=SpEnv(parameters)
    del(testEnv)
    testEnv=SpEnv(parameters)

   #Reset the callbacks used to show the metrics while training, validating and testing
   self.trainer.reset()
   self.validator.reset()
   self.tester.reset()

   ####TRAINING STEP#### 
   #Reset the training environment
   trainEnv.resetEnv()
   #Train the agent
   self.agent.fit(trainEnv,nb_steps=floor(self.trainSize.days-self.trainSize.days*0.2),visualize=False,verbose=0)
   #Get metrics from the train callback  
   (metrics)=self.trainer.getInfo()
   #################################

   ####VALIDATION STEP####
   #Reset the validation environment
   validEnv.resetEnv()
   #Test the agent on validation data
   self.agent.test(validEnv,other_parameters)
   #Get the info from the validation callback
   (metrics)=self.validator.getInfo()
   ####################################             

   ####TEST STEP####
   #Reset the testing environment
   testEnv.resetEnv()
   #Test the agent on testing data            
   self.agent.test(testEnv,nb_episodes=floor(self.validationSize.days-self.validationSize.days*0.2),visualize=False,verbose=0)
   #Get the info from the testing callback
   (metrics)=self.tester.getInfo()
</code></pre>

<p>What is strange to me according to the chart and the code is that the agent is fitted several times, independently from each other, but the training accuracy increases with the time. It seems that previous experiences are helping the agent to increase the training accuracy. But how can that be possible if the environments are reset and the agent has simply fitted again? is there any backpropagation of error from previous fittings that are helping the agent to increase its accuracy in the next fittings?</p>
",2163392,,,,,2019-06-24 08:26:00,Deep Reinforcement Learning Training Accuracy,<machine-learning><deep-learning><artificial-intelligence><reinforcement-learning><keras-rl>,2,0,,,,CC BY-SA 4.0,
1267,56182236,1,56182586,,2019-05-17 08:09:51,,1,563,"<p>I am reading through the DQN implementation in <a href=""https://github.com/keras-rl/keras-rl"" rel=""nofollow noreferrer"">keras-rl</a> <code>/rl/agents/dqn.py</code> and see that in the <code>compile()</code> step essentially 3 keras models are instantiated:</p>

<ul>
<li><code>self.model</code> : provides q value predictions  </li>
<li><code>self.trainable_model</code> : same as <code>self.model</code> but has the loss function we want to train  </li>
<li><code>self.target_model</code> : target model which provides the q targets and is updated every <code>k</code> steps with the weights from <code>self.model</code></li>
</ul>

<p>The only model on which <code>train_on_batch()</code> is called is <code>trainable_model</code>, however - and this is what I don't understand - this also updates the weights of <code>model</code>. </p>

<p>In the definition of <code>trainable_model</code> one of the output tensors <code>y_pred</code> is referencing the output from <code>model</code>:  </p>

<pre><code>        y_pred = self.model.output
        y_true = Input(name='y_true', shape=(self.nb_actions,))
        mask = Input(name='mask', shape=(self.nb_actions,))
        loss_out = Lambda(clipped_masked_error, output_shape=(1,), name='loss')([y_true, y_pred, mask])
        ins = [self.model.input] if type(self.model.input) is not list else self.model.input
        trainable_model = Model(inputs=ins + [y_true, mask], outputs=[loss_out, y_pred])
</code></pre>

<p>When <code>trainable_model.train_on_batch()</code> is called, <b>BOTH</b> the weights in <code>trainable_model</code> and in <code>model</code> change. I am surprised because even though the two models reference the same output tensor object (<code>trainable_model.y_pred = model.output</code>), the instantiation of <code>trainable_model = Model(...)</code>  should also instantiate a new set of weights, no?</p>

<p>Thanks for the help!</p>
",5379182,,,,,2019-05-17 08:32:06,keras rl - dqn model update,<python><tensorflow><keras><keras-rl>,1,0,,,,CC BY-SA 4.0,
1275,57104436,1,57237953,,2019-07-19 01:24:20,,3,730,"<p>Getting:</p>

<pre><code>    assert q_values.shape == (len(state_batch), self.nb_actions)
AssertionError
q_values.shape &lt;class 'tuple'&gt;: (1, 1, 10)
(len(state_batch), self.nb_actions) &lt;class 'tuple'&gt;: (1, 10)
</code></pre>

<p>which is from the keras-rl library of the sarsa agent:</p>

<blockquote>
  <p>rl.agents.sarsa.SARSAAgent#compute_batch_q_values</p>
</blockquote>

<pre><code>    batch = self.process_state_batch(state_batch)
    q_values = self.model.predict_on_batch(batch)
    assert q_values.shape == (len(state_batch), self.nb_actions)
</code></pre>

<p>Here is my code:</p>

<pre><code>class MyEnv(Env):

    def __init__(self):
        self._reset()

    def _reset(self) -&gt; None:
        self.i = 0

    def _get_obs(self) -&gt; List[float]:
        return [1] * 20

    def reset(self) -&gt; List[float]:
        self._reset()
        return self._get_obs()



    model = Sequential()
    model.add(Dense(units=20, activation='relu', input_shape=(1, 20)))
    model.add(Dense(units=10, activation='softmax'))
    logger.info(model.summary())

    policy = BoltzmannQPolicy()
    agent = SARSAAgent(model=model, nb_actions=10, policy=policy)

    optimizer = Adam(lr=1e-3)
    agent.compile(optimizer, metrics=['mae'])

    env = MyEnv()
    agent.fit(env, 1, verbose=2, visualize=True)
</code></pre>

<p>Was wondering if someone can explain to me how the dimensions should be set up and how it works with the libraries? I'm putting in a list of 20 inputs, and want an output of 10.</p>
",184379,,,,,2019-07-28 13:03:43,How does the dimensions work when training a keras model?,<python><keras><keras-rl>,2,0,,,,CC BY-SA 4.0,
1277,59682542,1,59745136,,2020-01-10 13:21:03,,14,9178,"<p>I am trying to implement a DQL model on one game of openAI gym. But it's giving me following error.</p>

<blockquote>
  <p>TypeError: len is not well defined for symbolic Tensors.
  (activation_3/Identity:0) Please call <code>x.shape</code> rather than <code>len(x)</code>
  for shape information.</p>
</blockquote>

<p>Creating a gym environment:</p>

<pre><code>ENV_NAME = 'CartPole-v0'

env = gym.make(ENV_NAME)
np.random.seed(123)
env.seed(123)
nb_actions = env.action_space.n
</code></pre>

<p>My model looks like this:</p>

<pre><code>model = Sequential()
model.add(Flatten(input_shape=(1,) + env.observation_space.shape))
model.add(Dense(16))
model.add(Activation('relu'))
model.add(Dense(nb_actions))
model.add(Activation('linear'))
print(model.summary())
</code></pre>

<p>Fitting that model to DQN model from keral-rl as follows:</p>

<pre><code>policy = EpsGreedyQPolicy()
memory = SequentialMemory(limit=50000, window_length=1)
dqn = DQNAgent(model=model, nb_actions=nb_actions, memory=memory, nb_steps_warmup=10, target_model_update=0.001, policy=policy)
dqn.compile(Adam(lr=1e-3), metrics=['mse', 'mae'])
dqn.fit(env, nb_steps=5000, visualize=False, verbose=3)
</code></pre>

<p>The error is from this line:</p>

<pre><code>dqn = DQNAgent(model=model, nb_actions=nb_actions, memory=memory, nb_steps_warmup=10, target_model_update=0.001, policy=policy)
</code></pre>

<p>I am using keras-rl==0.4.2 and tensorflow==2.1.0. Based on other answers, I also tried tensorflow==2.0.0-beta0 but it doesn't solve the error. </p>

<p>Can someone please explain to me why I am facing this error? and how to solve it?</p>

<p>Thank you.</p>
",7791036,,7791036,,2020-01-11 09:30:03,2021-04-05 16:04:24,TypeError: len is not well defined for symbolic Tensors. (activation_3/Identity:0) Please call `x.shape` rather than `len(x)` for shape information,<python><tensorflow><keras><reinforcement-learning><keras-rl>,2,2,,,,CC BY-SA 4.0,
1279,63555612,1,63571706,,2020-08-24 06:14:50,,1,1962,"<p>To fit a classification model in R, have been using <code>library(KerasR)</code>. To control learning rate and <a href=""https://cran.r-project.org/web/packages/kerasR/kerasR.pdf"" rel=""nofollow noreferrer"">KerasR</a> says</p>
<pre><code>compile(optimizer=Adam(lr = 0.001, beta_1 = 0.9, beta_2 = 0.999, epsilon = 1e-08, decay = 0, clipnorm = -1, clipvalue = -1), loss      = 'binary_crossentropy', metrics   =  c('categorical_accuracy') )
</code></pre>
<p>But it is given me an error like this</p>
<blockquote>
<p>Error in modules$keras.optimizers$Adam(lr = lr, beta_1 = beta_2,
beta_2 = beta_2,  :    attempt to apply non-function</p>
</blockquote>
<p>I also used <code>keras_compile</code> still getting the same error.
I can change optimizer in compile but the largest learning rate is 0.01, I want to try 0.2.</p>
<pre><code>model &lt;- keras_model_sequential()

model %&gt;% layer_dense(units = 512, activation = 'relu',  input_shape =  ncol(X_train)) %&gt;% 
  layer_dropout(rate = 0.2) %&gt;% 
  layer_dense(units = 128, activation = 'relu')%&gt;%
  layer_dropout(rate = 0.1) %&gt;% 
  layer_dense(units = 2, activation = 'sigmoid')%&gt;%
compile( 
  optimizer = 'Adam', 
  loss      = 'binary_crossentropy',
  metrics   =  c('categorical_accuracy') 
)
</code></pre>
",11307637,,11307637,,2020-08-25 02:06:03,2020-08-25 03:44:41,How to control learning rate in KerasR in R,<r><neural-network><keras-rl>,1,2,,,,CC BY-SA 4.0,
1288,60171887,1,60173136,,2020-02-11 15:06:25,,1,1562,"<p>Currently train keras on tensorflow model with default setting - float32.</p>

<p>Post training the network is quantized: cast weights to float16. This improves performance by ~x3 while keeping the same accuracy.</p>

<p>I was trying to train from start using float16 and failed miserably. I cannot find any link that explain if that is possible and if not why is it not possible.</p>
",9764924,,,,,2020-02-11 16:42:56,Is it possible to train with tensorflow 1 using float16?,<python><tensorflow><precision><keras-rl>,1,2,,,,CC BY-SA 4.0,
1290,59861818,1,59863973,,2020-01-22 14:22:21,,0,1028,"<p>I am trying to implement a DQN agent using Keras-rl. The problem is that when I define my model I need to use an LSTM layer in the architecture:</p>

<pre><code>model = Sequential()
model.add(Flatten(input_shape=(1, 8000)))
model.add(Reshape(target_shape=(200, 40)))
model.add(LSTM(20))
model.add(Dense(3, activation='softmax'))
return model
</code></pre>

<p>Executing the rl-agent I obtain the following error:</p>

<pre><code>RuntimeError: Attempting to capture an EagerTensor without building a function.
</code></pre>

<p>Which is related to the use of the LSTM and to the following line of code:</p>

<pre><code>tf.compat.v1.disable_eager_execution()
</code></pre>

<p>Using a Dense layer instead of an LSTM:</p>

<pre><code>model = Sequential()
model.add(Flatten(input_shape=(1, 8000)))
model.add(Dense(20))
model.add(Dense(3, activation='softmax'))
return model
</code></pre>

<p>and maintaining eager execution disabled I don't have the previously reported error. If I delete the disabling of the eager execution with the LSTM layer I have other errors.</p>

<p>Can anyone help me to understand the reason of the error?</p>
",9993865,,7395911,,2020-04-06 17:09:33,2020-05-03 11:29:39,Keras LSTM layers in Keras-rl,<keras><tensorflow2.0><reinforcement-learning><keras-rl>,3,0,,,,CC BY-SA 4.0,
1291,47140642,1,47140848,,2017-11-06 15:43:39,,1,1970,"<p>I have found the <a href=""https://github.com/matthiasplappert/keras-rl/blob/master/examples/cem_cartpole.py#L46"" rel=""nofollow noreferrer"">keras-rl/examples/cem_cartpole.py</a> example and I would like to understand, but I don't find documentation.</p>

<p>What does the line</p>

<pre><code>memory = EpisodeParameterMemory(limit=1000, window_length=1)
</code></pre>

<p>do? What is the <code>limit</code> and what is the <code>window_length</code>? Which effect does increasing either / both parameters have?</p>
",562769,,,,,2019-04-13 12:29:40,What does the EpisodeParameterMemory of keras-rl do?,<reinforcement-learning><keras-rl>,2,0,,,,CC BY-SA 3.0,
1292,58156384,1,58277912,,2019-09-29 14:58:58,,2,1720,"<p>I am implementing a RL agent with policy gradient method. I define a dense network for actor and another dense network for critic. For example, my critic network is:</p>

<pre><code>state_input = Input(shape=(self.num_states,))
x = Dense(self.hidden_size, activation='tanh')(state_input)
for _ in range(self.num_layers - 1):
    x = Dense(self.hidden_size, activation='tanh')(x)

out_value = Dense(1)(x)

model = Model(inputs=[state_input], outputs=[out_value])
model.compile(optimizer=SGD(lr=self.learning_rate), loss='mse')
</code></pre>

<p>In training phase I'm calling tensorboard:</p>

<pre><code>from keras.callbacks import TensorBoard

tensorboard = TensorBoard(log_dir=""/logs/{}"".format(time()), 
                          histogram_freq=1, batch_size=32, 
                          write_graph=True, write_grads=True,
                          write_images=True, embeddings_freq=0, 
                          embeddings_layer_names=None, 
                          embeddings_metadata=None, 
                          embeddings_data=None, update_freq='epoch')

critic_loss = self.critic.fit([obs, advantage, old_prediction], [action], 
                                 batch_size=self.batch_size,
                                 shuffle=True, epochs=self.epochs, verbose=False,
                                 callbacks=[tensorboard_actor])
</code></pre>

<p>But I'm getting this error:</p>

<pre><code>TypeError: 'module' object is not callable
</code></pre>
",9704382,,9704382,,2019-09-29 15:03:36,2019-10-07 22:29:39,TypeError: 'module' object is not callable Tensorboard in Keras,<python><tensorflow><keras><tensorboard><keras-rl>,1,1,,,,CC BY-SA 4.0,
1295,52734928,1,52735507,,2018-10-10 07:33:13,,5,12671,"<p>I am trying to understand some code from a reinforcement learning algorithm. In order to do that I am trying to print the value of a tensor.</p>

<p>I made a simple piece of code to show what I mean.</p>

<pre><code>import tensorflow as tf
from keras import backend as K

x = K.abs(-2.0)
tf.Print(x,[x], 'x')
</code></pre>

<p>The goal is to have the value '2' printed(the absolute value of -2). But I only get back the following:</p>

<pre><code>Using TensorFlow backend.

Process finished with exit code 0
</code></pre>

<p>Nothing, how can I print the value '2' just like a print('...') statement would do?</p>
",2253003,,3924118,,2020-01-01 16:57:21,2020-04-09 15:04:35,TensorFlow's Print is not printing,<python><keras><keras-rl>,3,6,0,,,CC BY-SA 4.0,
1300,65897335,1,65937743,,2021-01-26 07:29:22,,3,611,"<p>I have implemented a CNN-based regression model that uses a data generator to use the huge amount of data I have. Training and evaluation work well, but there's an issue with the prediction. If for example I want to predict values from a test dataset of 50 samples, I use model.predict with a batch size of 5. The problem is that model.predict returns 5 values repeated 10 times, instead of 50 different values . The same thing happens if I change to batch size to 1, it will return one value 50 times.</p>
<p>To solve this issue, I used a full batch size (50 in my example), and it worked. But I can't I use this method on my whole test data because it's too huge.</p>
<p>Do you have any other solution, or what is the problem in my approach?</p>
<p><strong>My data generator code:</strong></p>
<pre class=""lang-py prettyprint-override""><code>import numpy as np
import keras

class DataGenerator(keras.utils.Sequence):
    'Generates data for Keras'
    def __init__(self, list_IDs, data_X, data_Z, target_y batch_size=32, dim1=(120,120),
                 dim2 = 80, n_channels=1, shuffle=True):
        'Initialization'
        self.dim1 = dim1
        self.dim2 = dim2
        self.batch_size = batch_size
        self.data_X = data_X
        self.data_Z = data_Z
        self.target_y = target_y
        self.list_IDs = list_IDs
        self.n_channels = n_channels
        self.shuffle = shuffle
        self.on_epoch_end()

    def __len__(self):
        'Denotes the number of batches per epoch'
        return int(np.floor(len(self.list_IDs) / self.batch_size))

    def __getitem__(self, index):
        'Generate one batch of data'
        # Generate indexes of the batch
        indexes = self.indexes[index*self.batch_size:(index+1)*self.batch_size]

        # Find list of IDs
        list_IDs_temp = [self.list_IDs[k] for k in range(len(indexes))]

        # Generate data
        ([X, Z], y) = self.__data_generation(list_IDs_temp)

        return ([X, Z], y)

    def on_epoch_end(self):
        'Updates indexes after each epoch'
        self.indexes = np.arange(len(self.list_IDs))
        if self.shuffle == True:
            np.random.shuffle(self.indexes)

    def __data_generation(self, list_IDs_temp):
        'Generates data containing batch_size samples' # X : (n_samples, *dim, n_channels)
        # Initialization
        X = np.empty((self.batch_size, *self.dim1, self.n_channels))
        Z = np.empty((self.batch_size, self.dim2))
        y = np.empty((self.batch_size))

        # Generate data
        for i, ID in enumerate(list_IDs_temp):
            # Store sample
            X[i,] = np.load('data/' + data_X + ID + '.npy')
            Z[i,] = np.load('data/' + data_Z + ID + '.npy')

            # Store target
            y[i] = np.load('data/' + target_y + ID + '.npy')

</code></pre>
<p><strong>How I call model.predict()</strong></p>
<pre><code>predict_params = {'list_IDs': 'indexes', 
                  'data_X': 'images',
                  'data_Z': 'factors',
                  'target_y': 'True_values'
                  'batch_size': 5,
                  'dim1': (120,120),
                  'dim2': 80, 
                  'n_channels': 1,
                  'shuffle'=False}

# Prediction generator
prediction_generator = DataGenerator(test_index, **predict_params)

predition_results = model.predict(prediction_generator, steps = 1, verbose=1)
</code></pre>
",13624042,,13624042,,2021-01-27 03:59:52,2021-01-28 13:09:00,Keras data generator predict same number of values,<tensorflow><keras><deep-learning><predict><keras-rl>,3,2,,,,CC BY-SA 4.0,
1303,55119291,1,55119429,,2019-03-12 10:30:23,,0,247,"<p>I'm trying to use that Object as <a href=""https://towardsdatascience.com/lunar-landings-from-demonstrations-27b553b00ce2"" rel=""nofollow noreferrer"">This Blog</a> uses in <a href=""https://github.com/jakegrigsby/lunarDQfD"" rel=""nofollow noreferrer"">His Code</a> but when I do <code>from rl.agents.dqn import DQfDAgent</code> it returns me and error <code>ImportError: cannot import name 'DQfDAgent'</code>.</p>

<p>I've done a <code>dir(rl.agents.dqn)</code> and there is no <code>DQfDAgent</code> object so, how has the man of the blog done it?</p>

<p><strong>(Update 1)</strong></p>

<p>I've already done this:</p>

<pre><code>pip install -e git+git://github.com/jakegrigsby/keras-rl.git#egg=keras-rl
</code></pre>

<p>but it returns this error:</p>

<pre><code>Command ""git clone -q git://github.com/jakegrigsby/keras-rl.git C:\Users\myuser\src\keras-rl"" failed with error code 128 in None
</code></pre>
",11067209,,11067209,,2019-03-13 11:25:12,2019-03-13 11:25:12,Where Can I find the implemented DQfDAgent?,<python><git><pip><keras-rl>,2,0,,,,CC BY-SA 4.0,
1307,67908668,1,67916231,,2021-06-09 17:02:00,,1,221,"<p>I am training a DDPG agent on my custom  environment that I wrote using openai gym. I am getting error during training the model.</p>
<p>When I search for a solution on web, I found that some people who faced similar issue were able to resolve it by initializing the variable.</p>
<pre><code>For example by using:
tf.global_variable_initialzer()
</code></pre>
<p>But I am using tensorflow version 2.5.0 which does not have this method. Which means there should be some other way to solve this error. But I am unable to find the solution.</p>
<p>Here are the libraries that I used with there versions</p>
<pre><code>tensorflow: 2.5.0
gym:        0.18.3
numpy:      1.19.5
keras:      2.4.3
keras-rl2:  1.0.5          DDPG agent comes from this library
</code></pre>
<p>Error/Stacktrace:</p>
<pre><code>Training for 1000 steps ...
Interval 1 (0 steps performed)
   17/10000 [..............................] - ETA: 1:04 - reward: 256251545.0121
C:\Users\vchou\anaconda3\envs\AdSpendProblem\lib\site-packages\keras\engine\training.py:2401: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  warnings.warn('`Model.state_updates` will be removed in a future version. '
  100/10000 [..............................] - ETA: 1:03 - reward: 272267266.5754
C:\Users\vchou\anaconda3\envs\AdSpendProblem\lib\site-packages\tensorflow\python\keras\engine\training.py:2426: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  warnings.warn('`Model.state_updates` will be removed in a future version. '
---------------------------------------------------------------------------
FailedPreconditionError                   Traceback (most recent call last)
&lt;ipython-input-17-0938aa6056e8&gt; in &lt;module&gt;
      1 # Training
----&gt; 2 ddpgAgent.fit(env, 1000, verbose=1, nb_max_episode_steps = 100)

~\anaconda3\envs\AdSpendProblem\lib\site-packages\rl\core.py in fit(self, env, nb_steps, action_repetition, callbacks, verbose, visualize, nb_max_start_steps, start_step_policy, log_interval, nb_max_episode_steps)
    191                     # Force a terminal state.
    192                     done = True
--&gt; 193                 metrics = self.backward(reward, terminal=done)
    194                 episode_reward += reward
    195 

~\anaconda3\envs\AdSpendProblem\lib\site-packages\rl\agents\ddpg.py in backward(self, reward, terminal)
    279                     state0_batch_with_action = [state0_batch]
    280                 state0_batch_with_action.insert(self.critic_action_input_idx, action_batch)
--&gt; 281                 metrics = self.critic.train_on_batch(state0_batch_with_action, targets)
    282                 if self.processor is not None:
    283                     metrics += self.processor.metrics

~\anaconda3\envs\AdSpendProblem\lib\site-packages\keras\engine\training_v1.py in train_on_batch(self, x, y, sample_weight, class_weight, reset_metrics)
   1075       self._update_sample_weight_modes(sample_weights=sample_weights)
   1076       self._make_train_function()
-&gt; 1077       outputs = self.train_function(ins)  # pylint: disable=not-callable
   1078 
   1079     if reset_metrics:

~\anaconda3\envs\AdSpendProblem\lib\site-packages\keras\backend.py in __call__(self, inputs)
   4017       self._make_callable(feed_arrays, feed_symbols, symbol_vals, session)
   4018 
-&gt; 4019     fetched = self._callable_fn(*array_vals,
   4020                                 run_metadata=self.run_metadata)
   4021     self._call_fetch_callbacks(fetched[-len(self._fetches):])

~\anaconda3\envs\AdSpendProblem\lib\site-packages\tensorflow\python\client\session.py in __call__(self, *args, **kwargs)
   1478       try:
   1479         run_metadata_ptr = tf_session.TF_NewBuffer() if run_metadata else None
-&gt; 1480         ret = tf_session.TF_SessionRunCallable(self._session._session,
   1481                                                self._handle, args,
   1482                                                run_metadata_ptr)

FailedPreconditionError: Could not find variable dense_5_1/kernel. This could mean that the variable has been deleted. In TF1, it can also mean the variable is uninitialized. Debug info: container=localhost, status=Not found: Resource localhost/dense_5_1/kernel/class tensorflow::Var does not exist.
     [[{{node ReadVariableOp_21}}]]
</code></pre>
<p>The actor and critic networks are as follows:</p>
<pre><code>ACTOR NETWORK
Model: &quot;sequential&quot;
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
flatten (Flatten)            (None, 10)                0         
_________________________________________________________________
dense (Dense)                (None, 32)                352       
_________________________________________________________________
activation (Activation)      (None, 32)                0         
_________________________________________________________________
dense_1 (Dense)              (None, 32)                1056      
_________________________________________________________________
activation_1 (Activation)    (None, 32)                0         
_________________________________________________________________
dense_2 (Dense)              (None, 32)                1056      
_________________________________________________________________
activation_2 (Activation)    (None, 32)                0         
_________________________________________________________________
dense_3 (Dense)              (None, 10)                330       
_________________________________________________________________
activation_3 (Activation)    (None, 10)                0         
=================================================================
Total params: 2,794
Trainable params: 2,794
Non-trainable params: 0
_________________________________________________________________
None
</code></pre>
<pre><code>CRITIC NETWORK
Model: &quot;model&quot;
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
observation_input (InputLayer)  [(None, 1, 10)]      0                                            
__________________________________________________________________________________________________
action_input (InputLayer)       [(None, 10)]         0                                            
__________________________________________________________________________________________________
flatten_1 (Flatten)             (None, 10)           0           observation_input[0][0]          
__________________________________________________________________________________________________
concatenate (Concatenate)       (None, 20)           0           action_input[0][0]               
                                                                 flatten_1[0][0]                  
__________________________________________________________________________________________________
dense_4 (Dense)                 (None, 32)           672         concatenate[0][0]                
__________________________________________________________________________________________________
activation_4 (Activation)       (None, 32)           0           dense_4[0][0]                    
__________________________________________________________________________________________________
dense_5 (Dense)                 (None, 32)           1056        activation_4[0][0]               
__________________________________________________________________________________________________
activation_5 (Activation)       (None, 32)           0           dense_5[0][0]                    
__________________________________________________________________________________________________
dense_6 (Dense)                 (None, 32)           1056        activation_5[0][0]               
__________________________________________________________________________________________________
activation_6 (Activation)       (None, 32)           0           dense_6[0][0]                    
__________________________________________________________________________________________________
dense_7 (Dense)                 (None, 1)            33          activation_6[0][0]               
__________________________________________________________________________________________________
activation_7 (Activation)       (None, 1)            0           dense_7[0][0]                    
==================================================================================================
Total params: 2,817
Trainable params: 2,817
Non-trainable params: 0
__________________________________________________________________________________________________
None
</code></pre>
<p>Here is the code for DDPG agent</p>
<pre><code># Create DDPG agent
ddpgAgent = DDPGAgent(
    nb_actions = nb_actions,
    actor = actor,
    critic = critic,
    critic_action_input = action_input,
    memory = memory,
    nb_steps_warmup_critic = 100,
    nb_steps_warmup_actor = 100,
    random_process = random_process,
    gamma = 0.99,
    target_model_update = 1e-3
)

ddpgAgent.compile(Adam(learning_rate=0.001, clipnorm=1.0), metrics=['mae'])
</code></pre>
",15915226,,,,,2021-06-10 07:00:42,"FailedPreconditionError while using DDPG RL algorithm, in python, with keras, keras-rl2",<python><keras><tensorflow2.0><reinforcement-learning><keras-rl>,1,0,,,,CC BY-SA 4.0,
1311,57813278,1,57813489,,2019-09-05 21:38:27,,1,143,"<p>I have been working for the last four days to try and create a simple working neural network(NN) that learns. I started off with the tower of Hanoi but that was quite tricky (doable with a Q-table)and no-one has really got good examples online so I decided to do it instead for snake game where there are lots of examples and tutorials. Long story short i have made a new super simple game where you have [0,0,0,0] and by picking 0, 1, 2, or 3 you change a 0 to a 1 or vice versa. So picking 1 would give an output of [0,1,0,0] and picking 1 again goes back to [0,0,0,0].  Very easy</p>

<p>Despite the game being very easy I'm very much struggling to go from concept to practical as I have no education in coding.</p>

<p>The final goal right now is to get the code below to be able to complete the game more than once. (it currently has run about 600 times and not once completed the 4 step problem) </p>

<p>Current network architecture is 4 inputs 4 nodes in 1st hidden layer and 4 outputs and I would like to keep it this way even if the hidden layer is redundant just so I can learn how to do it correctly for other problems.</p>

<p>If you cant be bothered to read the code and I don't blame you, ill put my mental psudocode here: </p>

<ol>
<li>setup variables, placeholders and import libraries</li>
<li>run program 200 times to give it a chance to learn and each run has 20 turns</li>
<li>run through the NN with ""states"" as the input and get out the output defined as ""output"" for future use</li>
<li>game code</li>
<li>the new reward for this specific game would just be the new set of states as (it has just occurred to me that this is the wrong way round ([0,1,0,0] for states should have rewards [1,0,1,1]) but i have already tried flipping it and it it still didn't work so that isn't the issue)</li>
<li>my thinking was that i could get the next Q value by just running the new states through the NN</li>
<li>this equation is taken directly from any deep q-learning tutorial on the internet and i think that maybe I have got this or one of the components for this wrong.</li>
<li>run the gradient decent optimisation function</li>
</ol>

<pre class=""lang-py prettyprint-override""><code>import tensorflow as tf             ## importing libraries
import random
import numpy as np

epsilon = 0.1                       ## create non tf variables
y = 0.4
memory = []
memory1 = []

input_ = tf.placeholder(tf.float32, [None, 4], name='input_') 
W1 = tf.Variable(tf.random_normal([4, 4], stddev=0.03), name='W1') 
b1 = tf.Variable(tf.random_normal([4]), name='b1')    
hidden_out = tf.add(tf.matmul(input_, W1), b1, name='hidden_out')   ## W for weights
hidden_out = tf.nn.relu(hidden_out)                                 ## b for bias'

W2 = tf.Variable(tf.random_normal([4, 4], stddev=0.03), name='W2')
b2 = tf.Variable(tf.random_normal([4]), name='b2')
Qout = tf.add(tf.matmul(hidden_out, W2), b2, name='Qout')
sig_out = tf.sigmoid(Qout, name='out')


Q_target = tf.placeholder(shape=(None,4), dtype=tf.float32)
loss = tf.reduce_sum(tf.square(Q_target - Qout))
optimiser = tf.train.GradientDescentOptimizer(learning_rate=y).minimize(loss)

init_op = tf.global_variables_initializer()

with tf.compat.v1.Session() as sess:
    sess.run(init_op)
    for epoch in range(200):         ## run game 200 times
        states = [0,0,0,0]
        for _ in range(20):          ## 20 turns to do the correct 4 moves
            if _ == 19:
                memory1.append(states)
            output = np.argmax(sess.run(sig_out, feed_dict={input_: [states]}))
            ## sig_out is the output put through a sigmoid function
            if random.random() &lt; epsilon:       ## this is the code for the game 
                output = random.randint(0,3)    ## ...
            if states[output] == 0:             ## ...
                states[output] = 1              ## ...
            else:                               ## ...
                states[output] = 0              ## ...
            reward = states     
            Qout1 = sess.run(sig_out, feed_dict={input_: [states]})
            target = [reward + y*np.max(Qout1)]
            sess.run([optimiser,loss], feed_dict={input_: [states], Q_target: target})
</code></pre>

<p>I haven't got any error messages in a while with this, the actual result would ideally be [1,1,1,1] every time.</p>

<p>Thanks in advance for all of your help</p>

<p>p.s. i couldn't think of an objective title for this, sorry</p>
",11186260,,,,,2019-09-05 22:05:08,How do you create Deep Q-Learning neural network to solve simple games like snake?,<python><tensorflow><machine-learning><neural-network><deep-learning>,1,0,,,,CC BY-SA 4.0,
1312,50803778,1,50810239,,2018-06-11 18:13:24,,2,234,"<p>Let's say we have a bot that has some money and some shares. The input is a list of prices for the last 30 days. It doesn't use an RNN and the prices are entered all at the same time. The output is a continuous action where a positive number is to buy and a negative number is to sell the amount of the stock. How can I restrict the action space so that it is clipped between how many shares it has(the lower bound) and how much money it has(the upper bound)?</p>

<p>Should I have it clipped or just penalize an illegal action? Which option would create the best results?</p>
",8022643,,,,,2018-06-12 06:08:04,How would I clip a continuous action in an actor-critic agent?,<artificial-intelligence><reinforcement-learning>,2,0,,,,CC BY-SA 4.0,
1325,42606589,1,42620948,,2017-03-05 09:22:52,,5,3945,"<p>I cannot understand what the fundamental difference between on-policy methods (like <code>A3C</code>) and off-policy methods (like <code>DDPG</code>) is. As far as I know, off-policy methods can learn the optimal policy regardless of the behavior policy. It can learn by observing any trajectory in the environment. Therefore, can I say off-policy methods are better than on-policy methods?</p>

<p>I have read the <a href=""https://webdocs.cs.ualberta.ca/~sutton/book/ebook/node65.html"" rel=""noreferrer"">cliff-walking example</a> showing the difference between <code>SARSA</code> and <code>Q-learning</code>. It says that <code>Q-learning</code> would learn the optimal policy to walk along the cliff, while <code>SARSA</code> would learn to choose a safer way when using the <code>epsilon-greedy</code> policy. But since <code>Q-learning</code> have already told us that the optimal policy, why don't we just follow that policy instead of keep exploring?</p>

<p>Plus, are there situations for the two kinds of learning methods that one is better than the other? in which case would one prefer on-policy algorithms?</p>
",4622058,,,,,2017-03-06 08:36:37,Are off-policy learning methods better than on-policy methods?,<reinforcement-learning><q-learning>,1,0,0,,,CC BY-SA 3.0,
1326,45542902,1,48705375,,2017-08-07 08:50:59,,2,516,"<p>I am trying to train an agent on <a href=""https://github.com/mwydmuch/ViZDoom"" rel=""nofollow noreferrer"">ViZDoom</a> platform on the deadly_corridor scenario with A3C algorithm and TensorFlow on TITAN X GPU server, however, the performance collapsed after training about 2+ days. As you can see in the following picture.</p>

<p><a href=""https://i.stack.imgur.com/O9l3A.jpg"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/O9l3A.jpg"" alt=""enter image description here""></a></p>

<p>There are 6 demons in the corridor and the agent should kill at least 5 demons to get to the destination and get the vest.</p>

<p>Here is the code of the newtwork</p>

<pre class=""lang-py prettyprint-override""><code>with tf.variable_scope(scope):
    self.inputs = tf.placeholder(shape=[None, *shape, 1], dtype=tf.float32)
    self.conv_1 = slim.conv2d(activation_fn=tf.nn.relu, inputs=self.inputs, num_outputs=32,
                              kernel_size=[8, 8], stride=4, padding='SAME')
    self.conv_2 = slim.conv2d(activation_fn=tf.nn.relu, inputs=self.conv_1, num_outputs=64,
                              kernel_size=[4, 4], stride=2, padding='SAME')
    self.conv_3 = slim.conv2d(activation_fn=tf.nn.relu, inputs=self.conv_2, num_outputs=64,
                              kernel_size=[3, 3], stride=1, padding='SAME')
    self.fc = slim.fully_connected(slim.flatten(self.conv_3), 512, activation_fn=tf.nn.elu)

    # LSTM
    lstm_cell = tf.contrib.rnn.BasicLSTMCell(cfg.RNN_DIM, state_is_tuple=True)
    c_init = np.zeros((1, lstm_cell.state_size.c), np.float32)
    h_init = np.zeros((1, lstm_cell.state_size.h), np.float32)
    self.state_init = [c_init, h_init]
    c_in = tf.placeholder(tf.float32, [1, lstm_cell.state_size.c])
    h_in = tf.placeholder(tf.float32, [1, lstm_cell.state_size.h])
    self.state_in = (c_in, h_in)
    rnn_in = tf.expand_dims(self.fc, [0])
    step_size = tf.shape(self.inputs)[:1]
    state_in = tf.contrib.rnn.LSTMStateTuple(c_in, h_in)
    lstm_outputs, lstm_state = tf.nn.dynamic_rnn(lstm_cell,
                                                 rnn_in,
                                                 initial_state=state_in,
                                                 sequence_length=step_size,
                                                 time_major=False)
    lstm_c, lstm_h = lstm_state
    self.state_out = (lstm_c[:1, :], lstm_h[:1, :])
    rnn_out = tf.reshape(lstm_outputs, [-1, 256])

    # Output layers for policy and value estimations
    self.policy = slim.fully_connected(rnn_out,
                                       cfg.ACTION_DIM,
                                       activation_fn=tf.nn.softmax,
                                       biases_initializer=None)
    self.value = slim.fully_connected(rnn_out,
                                      1,
                                      activation_fn=None,
                                      biases_initializer=None)
    if scope != 'global' and not play:
        self.actions = tf.placeholder(shape=[None], dtype=tf.int32)
        self.actions_onehot = tf.one_hot(self.actions, cfg.ACTION_DIM, dtype=tf.float32)
        self.target_v = tf.placeholder(shape=[None], dtype=tf.float32)
        self.advantages = tf.placeholder(shape=[None], dtype=tf.float32)

        self.responsible_outputs = tf.reduce_sum(self.policy * self.actions_onehot, axis=1)

        # Loss functions
        self.policy_loss = -tf.reduce_sum(self.advantages * tf.log(self.responsible_outputs+1e-10))
        self.value_loss = tf.reduce_sum(tf.square(self.target_v - tf.reshape(self.value, [-1])))
        self.entropy = -tf.reduce_sum(self.policy * tf.log(self.policy+1e-10))

        # Get gradients from local network using local losses
        local_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope)
        value_var, policy_var = local_vars[:-2] + [local_vars[-1]], local_vars[:-2] + [local_vars[-2]]
        self.var_norms = tf.global_norm(local_vars)

        self.value_gradients = tf.gradients(self.value_loss, value_var)
        value_grads, self.grad_norms_value = tf.clip_by_global_norm(self.value_gradients, 40.0)

        self.policy_gradients = tf.gradients(self.policy_loss, policy_var)
        policy_grads, self.grad_norms_policy = tf.clip_by_global_norm(self.policy_gradients, 40.0)

        # Apply local gradients to global network
        global_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, 'global')
        global_vars_value, global_vars_policy = \
            global_vars[:-2] + [global_vars[-1]], global_vars[:-2] + [global_vars[-2]]

        self.apply_grads_value = optimizer.apply_gradients(zip(value_grads, global_vars_value))
        self.apply_grads_policy = optimizer.apply_gradients(zip(policy_grads, global_vars_policy))
</code></pre>

<p>And the optimizer is </p>

<pre><code>optimizer = tf.train.RMSPropOptimizer(learning_rate=1e-5)
</code></pre>

<p>And here are some summaries of the gradients and norms</p>

<p><a href=""https://i.stack.imgur.com/kM1QY.jpg"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/kM1QY.jpg"" alt=""enter image description here""></a></p>

<p>Help some one can help me to tackle this problem.</p>
",5046896,,,,,2018-02-09 12:02:12,"Reinforcement learning, why the performance collapsed?",<deep-learning><reinforcement-learning>,1,0,0,,,CC BY-SA 3.0,
1327,65668160,1,66160672,,2021-01-11 13:53:12,,1,639,"<p>I am making a comparison between both kind of algorithms against the CartPole environment. Having the imports as:</p>
<pre><code>import ray
from ray import tune
from ray.rllib import agents
ray.init() # Skip or set to ignore if already called
</code></pre>
<p>Running this works perfectly:</p>
<pre><code>experiment = tune.run(
    agents.ppo.PPOTrainer,
    config={
        &quot;env&quot;: &quot;CartPole-v1&quot;,
        &quot;num_gpus&quot;: 1,
        &quot;num_workers&quot;: 0,
        &quot;num_envs_per_worker&quot;: 50,
        &quot;rollout_fragment_length&quot;: 100,
        &quot;train_batch_size&quot;: 5000,
        &quot;sgd_minibatch_size&quot;: 500,
        &quot;num_sgd_iter&quot;: 10,
        &quot;entropy_coeff&quot;: 0.01,
        &quot;lr_schedule&quot;: [
              [0, 0.0005],
              [10000000, 0.000000000001],
        ],
        &quot;lambda&quot;: 0.95,
        &quot;kl_coeff&quot;: 0.5,
        &quot;clip_param&quot;: 0.1,
        &quot;vf_share_layers&quot;: False,
    },
    metric=&quot;episode_reward_mean&quot;,
    mode=&quot;max&quot;,
    stop={&quot;training_iteration&quot;: 100},
    checkpoint_at_end=True,
)
</code></pre>
<p>But when I do the same with the A2C Agent:</p>
<pre><code>experiment = tune.run(
    agents.a3c.A2CTrainer,
    config={
        &quot;env&quot;: &quot;CartPole-v1&quot;,
        &quot;num_gpus&quot;: 1,
        &quot;num_workers&quot;: 0,
        &quot;num_envs_per_worker&quot;: 50,
        &quot;rollout_fragment_length&quot;: 100,
        &quot;train_batch_size&quot;: 5000,
        &quot;sgd_minibatch_size&quot;: 500,
        &quot;num_sgd_iter&quot;: 10,
        &quot;entropy_coeff&quot;: 0.01,
        &quot;lr_schedule&quot;: [
              [0, 0.0005],
              [10000000, 0.000000000001],
        ],
        &quot;lambda&quot;: 0.95,
        &quot;kl_coeff&quot;: 0.5,
        &quot;clip_param&quot;: 0.1,
        &quot;vf_share_layers&quot;: False,
    },
    metric=&quot;episode_reward_mean&quot;,
    mode=&quot;max&quot;,
    stop={&quot;training_iteration&quot;: 100},
    checkpoint_at_end=True,
)
</code></pre>
<p>It returns this exception:</p>
<pre><code>---------------------------------------------------------------------------
TuneError                                 Traceback (most recent call last)
&lt;ipython-input-9-6680e67f9343&gt; in &lt;module&gt;()
     23     mode=&quot;max&quot;,
     24     stop={&quot;training_iteration&quot;: 100},
---&gt; 25     checkpoint_at_end=True,
     26 )

/usr/local/lib/python3.6/dist-packages/ray/tune/tune.py in run(run_or_experiment, name, metric, mode, stop, time_budget_s, config, resources_per_trial, num_samples, local_dir, search_alg, scheduler, keep_checkpoints_num, checkpoint_score_attr, checkpoint_freq, checkpoint_at_end, verbose, progress_reporter, loggers, log_to_file, trial_name_creator, trial_dirname_creator, sync_config, export_formats, max_failures, fail_fast, restore, server_port, resume, queue_trials, reuse_actors, trial_executor, raise_on_failed_trial, callbacks, ray_auto_init, run_errored_only, global_checkpoint_period, with_server, upload_dir, sync_to_cloud, sync_to_driver, sync_on_checkpoint)
    432     if incomplete_trials:
    433         if raise_on_failed_trial:
--&gt; 434             raise TuneError(&quot;Trials did not complete&quot;, incomplete_trials)
    435         else:
    436             logger.error(&quot;Trials did not complete: %s&quot;, incomplete_trials)

TuneError: ('Trials did not complete', [A2C_CartPole-v1_6acda_00000])
</code></pre>
<p>Can anybody tell me whats going on? I don't know if it has something to do with the versions of the libraries that I'm using or I've coded something wrong. Is this a common issue?</p>
",11067209,,,,,2021-02-11 18:29:34,RLLib tunes PPOTrainer but not A2CTrainer,<python><reinforcement-learning><ray><rllib>,1,0,,,,CC BY-SA 4.0,
1331,63218710,1,63238112,,2020-08-02 16:55:49,,3,6050,"<p>I have following code for train function for training an A3C.
I am stuck with following error.</p>
<pre><code>RuntimeError(&quot;grad can be implicitly created only for scalar outputs&quot;)
</code></pre>
<p>at line <code>(policy_loss + 0.5 * value_loss).backward()</code></p>
<p>Here is my code, can someone please help to check what is wrong with this code.</p>
<pre><code>def train(rank, params, shared_model, optimizer,ticker):
torch.manual_seed(params.seed + rank) # shifting the seed with rank to asynchronize each training agent
print(ticker)
try:
    ohlcv = pd.read_csv(ticker + '.csv')
    data = ohlcv.copy()
    data['rsi'] = ab.RSI(data['Close'],14)
    data['adx'] = ab.ADX(data,20)
    data = ab.BollBnd(data,20)
    data['BBr'] = data['Close']/data['BB_dn']
    data['macd_signal'] = ab.MACD(data)
    data['macd_r'] = data['macd_signal']/data['Close']
    data['ema20'] = ab.EMA(np.asarray(data['Close']), 20)
    data['ema20_r'] = data['ema20']/data['Close']
    data['Close'] = data['Close']/data['Close'].max()
    data = data.iloc[:,[4,7,8,13,15,17]]
    data = data.dropna()
    data = torch.DoubleTensor(np.asarray(data))
    env = ENV(state_dim, action_dim, data)
    
    model = ActorCritic(env.observation_space, env.action_space)
    state = env.reset()
    done = True
    episode_length = 0
    while True:
        episode_length += 1
        model.load_state_dict(shared_model.state_dict())
        if done:
            cx = Variable(torch.zeros(1, state_dim)) # the cell states of the LSTM are reinitialized to zero
            hx = Variable(torch.zeros(1, state_dim)) # the hidden states of the LSTM are reinitialized to zero
        else:
            cx = Variable(cx.data)
            hx = Variable(hx.data)
        values = []
        log_probs = []
        rewards = []
        entropies = []
        for step in range(params.num_steps):
            value, action_values, (hx, cx) = model((Variable(state.unsqueeze(0)), (hx, cx)))
            prob = F.softmax(action_values,-1)
            log_prob = F.log_softmax(action_values,-1)
            entropy = -(log_prob * prob).sum(1)
            entropies.append(entropy)
            action = prob.multinomial(num_samples = action_dim).data
            log_prob = log_prob.gather(1, Variable(action))
            values.append(value)
            log_probs.append(log_prob)
            state, reward, done = env.step(action)
            done = (done or episode_length &gt;= params.max_episode_length)
            reward = max(min(reward, 1), -1) # clamping the reward between -1 and +1
            if done:
                episode_length = 0
                state = env.reset()
            rewards.append(reward)
            if done:
                break
        R = torch.zeros(1, 1)
        if not done: # if we are not done:
            value, _, _ = model((Variable(state.unsqueeze(0)), (hx, cx)))
            R = value.data
        values.append(Variable(R))
        policy_loss = torch.zeros(1, 1)
        value_loss = torch.zeros(1, 1) 
        R = Variable(R)
        gae = torch.zeros(1, 1)
        for i in reversed(range(len(rewards))):
            R = params.gamma * R + rewards[i]
            advantage = R - values[i]
            print(&quot;advantage:&quot;,advantage)
            value_loss = value_loss + 0.5 * advantage.pow(2) # computing the value loss
            TD = rewards[i] + params.gamma * values[i + 1].data - values[i].data # computing the temporal difference
            gae = gae * params.gamma * params.tau + TD # gae = sum_i (gamma*tau)^i * TD(i) with gae_i = gae_(i+1)*gamma*tau + (r_i + gamma*V(state_i+1) - V(state_i))
            print(&quot;gae:&quot;,gae)
            policy_loss = policy_loss - log_probs[i] * Variable(gae) - 0.01 * entropies[i] # computing the policy loss
            print(&quot;policy_loss:&quot;,policy_loss)
        optimizer.zero_grad() # initializing the optimizer
        los = policy_loss + 0.5 * value_loss
        print(&quot;los&quot;,los.shape)
        (policy_loss + 0.5 * value_loss).backward()
        torch.nn.utils.clip_grad_norm(model.parameters(), 40) # clamping the values 
        ensure_shared_grads(model, shared_model) 
        optimizer.step() # running the optimization step
except Exception as e:
    print(e)
    traceback.print_exc()
    var = traceback.format_exc()
</code></pre>
<p>Below are the outputs:-</p>
<pre><code>advantage: tensor([[-1.0750]], grad_fn=&lt;ThSubBackward&gt;)
gae: tensor([[-1.0750]])
policy_loss: tensor([[-25.8590, -26.1414, -25.9023, -25.2628]], grad_fn=&lt;ThSubBackward&gt;)
los torch.Size([1, 4])
</code></pre>
<p>RuntimeError: grad can be implicitly created only for scalar outputs
PS E:\ML\Breakout_a3c\Code_With_Comments&gt;</p>
",2783767,,2783767,,2020-08-02 22:43:16,2020-08-03 23:08:16,"RuntimeError(""grad can be implicitly created only for scalar outputs"")",<python><pytorch><reinforcement-learning>,1,1,,,,CC BY-SA 4.0,
1336,62405053,1,62407299,,2020-06-16 09:15:02,,0,443,"<p>i try to create a simplified rl4j example based on the existing Gym and Malmo examples.
Given is a sine wave and the AI should say if we are on top of the wave, on bottom or somewhere else(noop).</p>

<p>The SineRider is the ""Game"",
State is the value of the sine function(Just one double)</p>

<p>The problem is it never calls the step function in SineRider to get a reward. What do i wrong?</p>

<p>Kotlin:</p>

<pre><code>package aiexample

import org.deeplearning4j.gym.StepReply
import org.deeplearning4j.rl4j.learning.sync.qlearning.QLearning
import org.deeplearning4j.rl4j.learning.sync.qlearning.discrete.QLearningDiscreteDense
import org.deeplearning4j.rl4j.mdp.MDP
import org.deeplearning4j.rl4j.network.dqn.DQNFactoryStdDense
import org.deeplearning4j.rl4j.space.DiscreteSpace
import org.deeplearning4j.rl4j.space.Encodable
import org.deeplearning4j.rl4j.space.ObservationSpace
import org.nd4j.linalg.api.ndarray.INDArray
import org.nd4j.linalg.factory.Nd4j
import org.nd4j.linalg.learning.config.Adam
import kotlin.math.sin

object Example {
    var ql: QLearning.QLConfiguration = QLearning.QLConfiguration(
            123,  //Random seed
            1000,  //Max step By epoch
            8000,  //Max step
            1000,  //Max size of experience replay
            32,  //size of batches
            100,  //target update (hard)
            0,  //num step noop warmup
            0.05,  //reward scaling
            0.99,  //gamma
            10.0,  //td-error clipping
            0.1f,  //min epsilon
            2000,  //num step for eps greedy anneal
            true //double DQN
    )

    var net: DQNFactoryStdDense.Configuration = DQNFactoryStdDense.Configuration.builder()
            .l2(0.01).updater(Adam(1e-2)).numLayer(3).numHiddenNodes(16).build()


    @JvmStatic
    fun main(args: Array&lt;String&gt;) {
        simpleSine()
    }

    private fun simpleSine() {
        val mdp = Env.create()
        val dql = QLearningDiscreteDense(mdp, net, ql)
        dql.train()
        mdp.close()
    }
}


class Action(val name:String) {
    companion object {
        val noop = Action(""noop"")
        val top = Action(""top"")
        val bottom = Action(""bottom"")
    }
}




class State(private val inputs: DoubleArray): Encodable {
    override fun toArray(): DoubleArray {
        return inputs
    }
}


class SineObservationSpace: ObservationSpace&lt;State&gt; {
    override fun getLow(): INDArray {
        return Nd4j.create(doubleArrayOf(-1.0))
    }

    override fun getHigh(): INDArray {
        return Nd4j.create(doubleArrayOf(1.0))
    }

    override fun getName(): String {
        return ""Discrete""
    }

    override fun getShape(): IntArray {
        return intArrayOf(1)
    }
}




class SineRider{
    companion object {
        val actions = mapOf(
                0 to Action.noop,
                1 to Action.top,
                2 to Action.bottom)
    }

    var i = 0.0

    fun step(action:Int): Double{
        val act = actions[action]
        if(act == Action.top){
            return if(i &gt; 0.9) 1.0 else -1.0
        }

        if(act == Action.bottom){
            return if(i &lt; -0.9) 1.0 else -1.0
        }

        if(act == Action.noop){
            return if(i &lt; 0.9 &amp;&amp; i &gt; -0.9) 1.0 else -1.0
        }

        return 0.0
    }

    fun reset(){

    }

    fun next(){
        i += 0.1
    }

    fun state(): State {
        val sine = sin(i)
        next()
        return State(arrayOf(sine).toDoubleArray())
    }
}



class Env(private val sineRider: SineRider) : MDP&lt;State, Int, DiscreteSpace&gt; {
    private val actionSpace = DiscreteSpace(3)
    private var done = false

    override fun getObservationSpace(): ObservationSpace&lt;State&gt; {
        return SineObservationSpace()
    }

    override fun getActionSpace(): DiscreteSpace {
        return actionSpace
    }

    override fun step(action: Int): StepReply&lt;State&gt; {
        val reward = sineRider.step(action)
        val state = sineRider.state()
        return StepReply(state, reward, true, null)
    }

    override fun isDone(): Boolean {
        return true
    }

    override fun reset(): State? {
        done = false
        sineRider.reset()
        return sineRider.state()
    }

    override fun close() {

    }

    override fun newInstance(): Env {
        return create()
    }

    companion object {
        fun create() : Env {
            val sinRider = SineRider()
            return Env(sinRider)
        }
    }
}

</code></pre>
",13200972,,,,,2020-06-16 11:23:50,Simple Reinforcement Learning example,<kotlin><reinforcement-learning><deeplearning4j><dl4j>,1,1,,,,CC BY-SA 4.0,
1338,56153309,1,56157184,,2019-05-15 15:51:59,,1,135,"<p>My friend and I are training a DDQN for learning 2D soccer. I trained the model about 40.000 episodes but it tooks 6 days. Is there a way for training this model concurrently?</p>

<p>For example, I have 4 core and 4 thread and each thread trains the model 10.000 times concurrently. Therefore, time to training 40.000 episodes are reduced 6 days to 1,5 days like parallelism of for loop.</p>

<p>EDIT : If we train a model 10.000 episodes in 4 threads separately, would forming a new model consisting of the average of those trained models give the effect of training 40.000 episodes or would it be a model that was trained 10.000 episodes but a better one?</p>
",9822055,,9822055,,2019-05-15 17:21:38,2019-05-15 20:30:22,Training DDQN concurrently,<neural-network><reinforcement-learning>,1,4,,,,CC BY-SA 4.0,
1342,56868307,1,56924915,,2019-07-03 10:42:26,,1,1114,"<p>I am new to reinforcement learning and I read about these two algorithms Actor Critic and DDQN. I found that both of these gives fairly good results. But because two algos are totally different so I want to know that where I should prefer actor critic and where DDQN should be preferred. Also what are the advantages and disadvantages of actor critic over DDQN.</p>
",8791362,,8791362,,2019-07-03 21:08:54,2019-07-07 18:22:16,Advantage and disadvantages of using Actor Critic over DDQN,<machine-learning><reinforcement-learning>,1,1,0,,,CC BY-SA 4.0,
1347,55943678,1,55948320,,2019-05-01 22:51:50,,1,928,"<p>I am trying to implement clipped PPO algorithm for classical control task like keeping room temperature, charge of battery, etc. within certain limits. So far I've seen the implementations in game environments only. My question is the game environments and classical control problems are different when it comes to the implementation of the clipped PPO algorithm? If they are, help and tips on how to implement the algorithm for my case are appreciated.</p>
",10437878,,,,,2019-05-02 08:21:25,How to implement Proximal Policy Optimization (PPO) Algorithm for classical control problems?,<python><keras><reinforcement-learning>,1,0,,,,CC BY-SA 4.0,
1350,68342446,1,68342485,,2021-07-12 05:48:11,,-1,358,"<p>I am writing an RL program with keras.
The program builds a NN model and then uses that model to predict the situation.
```def _build_model(self):</p>
<pre><code>    model = Sequential()
    model.add(Dense(24,input_dim=self.state_size, activation='relu'))
    model.add(Dense(18, activation='relu'))
    model.add(Dense(self.action_size, activation ='linear'))
    model.compile(loss='mse', optimizer=Adam(lr=self.learning_rate))

    return model```
</code></pre>
<p>I use action = model.predict(state) in each episode.
In the end I use keras.save(name) to save the model before exiting the program.
As I mentioned, I use model.predict() in each episode of the game to select an action and it returns an action out of 8 actions[0 to 7]</p>
<p>But if I write a piece of code and use keras.models.load_model(name) and then call
action = model.predict(state), it returns an array of 8 elements which are probability of each action, not the selected action that  expect.</p>
<p>I need to add that if I don't want to save the model to deploy and use it later, it is working fine. All the results and outputs sound logical but if I decide to save it for later to just put it in the new environment and it takes action, I cannot as I explained above.
How can I resolve this issue?
Thanks in advance for your help.</p>
",11421621,,,,,2021-07-12 13:01:53,calling predict on the loaded model in keras returns the probaility value of each action not the prediction,<tensorflow><keras><reinforcement-learning>,1,1,,,,CC BY-SA 4.0,
1352,58693786,1,58693947,,2019-11-04 12:48:20,,1,10961,"<p>I am working on an RL problem and I created a class to initialize the model and other parameters. The code is as follows:</p>

<pre><code>class Agent:
    def __init__(self, state_size, is_eval=False, model_name=""""):
        self.state_size = state_size
        self.action_size = 20 # measurement, CNOT, bit-flip
        self.memory = deque(maxlen=1000)
        self.inventory = []
        self.model_name = model_name
        self.is_eval = is_eval
        self.done = False

        self.gamma = 0.95
        self.epsilon = 1.0
        self.epsilon_min = 0.01
        self.epsilon_decay = 0.995


    def model(self):
        model = Sequential()
        model.add(Dense(units=16, input_dim=self.state_size, activation=""relu""))
        model.add(Dense(units=32, activation=""relu""))
        model.add(Dense(units=8, activation=""relu""))
        model.add(Dense(self.action_size, activation=""softmax""))
        model.compile(loss=""categorical_crossentropy"", optimizer=Adam(lr=0.003))
        return model

    def act(self, state):
        options = self.model.predict(state)
        return np.argmax(options[0]), options
</code></pre>

<p>I want to run it for only one iteration, hence I create an object and I pass a vector of length <code>16</code> like this:</p>

<pre><code>agent = Agent(density.flatten().shape)
state = density.flatten()
action, probs = agent.act(state)
</code></pre>

<p>However, I get the following error:</p>

<pre><code>AttributeError                       Traceback (most recent call last) &lt;ipython-input-14-4f0ff0c40f49&gt; in &lt;module&gt;
----&gt; 1 action, probs = agent.act(state)

&lt;ipython-input-10-562aaf040521&gt; in act(self, state)
     39 #             return random.randrange(self.action_size)
     40 #         model = self.model()
---&gt; 41         options = self.model.predict(state)
     42         return np.argmax(options[0]), options
     43 

AttributeError: 'function' object has no attribute 'predict'
</code></pre>

<p>What's the issue? I checked some other people's codes as well, like <a href=""https://github.com/Alexander-H-Liu/Policy-Gradient-and-Actor-Critic-Keras/blob/master/agent_dir/agent_pg.py"" rel=""nofollow noreferrer"">this</a> and I think mine is also very similar.</p>

<p>Let me know.</p>

<p><strong>EDIT:</strong></p>

<p>I changed the argument in <code>Dense</code> from <code>input_dim</code> to <code>input_shape</code> and <code>self.model.predict(state)</code> to <code>self.model().predict(state)</code>.</p>

<p>Now when I run the NN for one input data of shape <code>(16,1)</code>, I get the following error:</p>

<blockquote>
  <p>ValueError: Error when checking input: expected dense_1_input to have
  3 dimensions, but got array with shape (16, 1)</p>
</blockquote>

<p>And when I run it with shape <code>(1,16)</code>, I get the following error:</p>

<blockquote>
  <p>ValueError: Error when checking input: expected dense_1_input to have
  3 dimensions, but got array with shape (1, 16)</p>
</blockquote>

<p>What should I do in this case?</p>
",4595522,,4595522,,2019-11-05 07:26:55,2019-11-05 07:42:54,AttributeError: 'function' object has no attribute 'predict'. Keras,<python-3.x><keras><deep-learning><reinforcement-learning><attributeerror>,2,6,0,,,CC BY-SA 4.0,
1360,61626080,1,61641475,,2020-05-06 01:58:29,,0,171,"<p>I has been trying to understand this machine learning problem for many days now and it really confuses me, I need some help.</p>

<p>I am trying to train a neural network whose input is an image, and which generates another image as output (it is not a very large image, it is 8x8 pixels). And I have an arbitrary <em>fancy_algorithm()</em> ""black box"" function that receives the input and prediction of the network (the two images) and outputs a float number that tells how good the output of the network was (calculates a loss). My problem is that I want to train THIS neural network but using the loss generated by the black box algorithm. This problem is confusing me, I researched a lot and I didn't find much about it, it seems like reinforcement learning, but at the same time I'm not sure because itâ€™s not like an agent, but it has some kind of reinforcement at the same time.</p>

<p>In case you need more details to help me just ask. Thanks in advance!</p>
",,user13357341,,user13357341,2020-05-06 03:04:25,2020-05-06 17:31:15,"In Keras, can I use an arbitrary algorithm as a loss function for a network?",<python><tensorflow><keras><neural-network><reinforcement-learning>,1,9,,,,CC BY-SA 4.0,
1361,37973005,1,37974356,,2016-06-22 15:55:46,,7,50436,"<p>I'm going through <a href=""http://outlace.com/Reinforcement-Learning-Part-3/"" rel=""noreferrer"">this reinforcement learning tutorial</a>
and It's been really great so far but could someone please explain what</p>

<pre><code>newQ = model.predict(new_state.reshape(1,64), batch_size=1)
</code></pre>

<p>and </p>

<pre><code>model.fit(X_train, y_train, batch_size=batchSize, nb_epoch=1, verbose=1)
</code></pre>

<p>mean?</p>

<p>As in what do the arguments <code>bach_size</code>, <code>nb_epoch</code> and <code>verbose</code> do?
I know neural networks so explaining in terms of that would be helpful.</p>

<p>You could also send me a link where the documentation of these functions can be found.</p>
",6461720,,1643939,,2016-06-22 16:57:09,2017-12-13 14:42:10,What do model.predict() and model.fit() do?,<python><deep-learning><keras><reinforcement-learning>,1,0,0,,,CC BY-SA 3.0,
1362,44747216,1,44750593,,2017-06-25 14:13:23,,3,626,"<p>I am in the process of making a tank game in pygame where you move a tank around walls and shoot other tanks.</p>

<p>I am trying to find a way to make a neural network, probably reinforcement learning for the enemies so that the game will make decisions on which tanks should move where, if they should shoot etc by passing attributes from each object.</p>

<pre><code>Attributes:
Enemy -&gt; x,y,width,height,speed,health and other items  
Wall -&gt; x,y,width,height  
Bullet -&gt; x,y,width,height,speed  
Player -&gt; x,y,width,height,speed,health  
</code></pre>

<p>I was planning to use the keras python module to create a neural network, however I cannot find a way to set it up so that the input data is the correct shape and size as there will be a variable number of walls and bullets.</p>

<p>What I would like to do :</p>

<pre><code>action = Network.predict(state)
</code></pre>

<p>where<br>
    <code>state = (Enemy, Player, Tuple_of_Wall_Data, Tuple_of_Bullet_Data)</code>  </p>

<p>and action is an option on where the enemy should move in the form<br>
<code>action = (Direction,Should_Shoot)</code></p>

<p>TLDR
My question is , how would I set up a Neural network input layer so it can take (1 enemy , 1 player , multiple walls, multiple bullets) and train the neural network to give the enemy a direction and if it should fire using reinforcement learning ?</p>
",8212080,,8212080,,2017-06-25 20:47:55,2017-06-25 21:02:04,Python game Neural network. How to setup inputs,<python><machine-learning><pygame><keras><reinforcement-learning>,1,5,,,,CC BY-SA 3.0,
1363,43382046,1,43406410,,2017-04-13 01:19:14,,2,76,"<p>I am attempting to use reinforcement learning to choose the closest point to the origin out of a given set of points repeatedly, until a complex (and irrelevant) end condition is reached. (This is a simplification of my main problem.)</p>

<p>A 2D array containing possible points is passed to the reinforcement learning algorithm, which makes a choice as to which point it thinks is the most ideal.</p>

<pre><code>A [1,  10]
B [100, 0]
C [30, 30]
D [5,   7]
E [20, 50]
</code></pre>

<p>In this case, <code>D</code> would be the true best choice. (The algorithm should ideally output <code>3</code>, from the range <code>0</code> to <code>4</code>.)</p>

<p>However, whenever I train the algorithm, it seems to not learn what the ""concept"" is, but instead just that choosing, say, <code>C</code> is <em>usually</em> the best choice, so it should always choose that.</p>

<pre><code>import numpy as np
import rl.core as krl


class FindOriginEnv(krl.Env):

    def observe(self):
        return np.array([
            [np.random.randint(100), np.random.randint(100)] for _ in range(5)
        ])

    def step(self, action):

        observation = self.observe()

        done = np.random.rand() &lt; 0.01  # eventually

        reward = 1 if done else 0

        return observation, reward, done, {}

    # ...
</code></pre>

<p>What should I modify about my algorithm such that it will actually learn about the goal it is trying to accomplish?</p>

<ul>
<li>Observation shape?</li>
<li>Reward function?</li>
<li>Action choices?</li>
</ul>

<p>Keras code would be appreciated, but is not required; a purely algorithmic explanation would also be extremely helpful.</p>
",7504176,,,,,2017-04-14 06:32:43,Choose closest point to origin with reinforcement learning,<machine-learning><keras><reinforcement-learning>,1,0,,,,CC BY-SA 3.0,
1372,65291261,1,65303458,,2020-12-14 15:01:00,,3,231,"<p>I know this is a silly question, but I cannot find a good way to put it.</p>
<p>I've worked with TensorFlow and TFAgents, and am now moving to Ray RLlib. Looking at all the RL frameworks/libraries, I got confused about the difference between the two below:</p>
<ul>
<li>frameworks such as Keras, TensorFlow, PyTorch</li>
<li>RL implementation libraries such as TFAgents, RLlib, OpenAi Baseline, Tensorforce, KerasRL, etc</li>
</ul>
<p>For example, there are Keras codes in TensorFlow and Ray RLlib supports both TensorFlow and PyTorch. How are they all related?</p>
<p>My understanding so far is that Keras allows to make neural networks and TensorFlow is more of a math library for RL (I don't have enough understanding about PyTorch). And libraries like TFAgents and RLlib use frameworks like Keras and TensorFlow to implement existing RL algorithms so that programmers can utilize them with ease.</p>
<p>Can someone please explain how they are interconnected/different? Thank you very much.</p>
",14622788,,,,,2020-12-15 09:48:38,What is the difference between Neural Network Frameworks and RL Algorithm Libraries?,<tensorflow><keras><pytorch><reinforcement-learning><ray>,1,0,0,,,CC BY-SA 4.0,
1374,65359792,1,65373575,,2020-12-18 15:36:14,,1,439,"<p>I'm recently learning deep reinforcement learning and I wanted to apply what I learned to a problem from gym using Keras.</p>
<p>During training I realized that it is too slow, after checking the reason I saw that &quot;fit&quot; function takes so much time.</p>
<p>Running each episode takes 3-4 minutes.</p>
<p>Is there something wrong at what I'm doing? Or can you suggest an improvement?</p>
<pre><code>import pandas as pd
import numpy as np
import tensorflow as tf
import tensorflow.keras as keras
from tensorflow.keras.optimizers import Adam
from collections import deque
import random
import gym
import datetime

class DQN():
    def __init__(self, env):
        self.env = env
        self.memory = deque(maxlen=2000)

        self.gamma = 0.98
        self.epsilon = 1
        self.epsilon_min = 0.01
        self.epsilon_decay = 0.998
        self.learning_rate = 0.001

        self.model = self.create_model()
        self.target_model = self.create_model()

    def create_model(self):
        model = keras.Sequential()
        state_shape = self.env.observation_space.shape
        model.add(keras.layers.Dense(48, activation=&quot;relu&quot;, input_dim=state_shape[0]))
        model.add(keras.layers.Dense(24, activation=&quot;relu&quot;))
        model.add(keras.layers.Dense(self.env.action_space.n, activation=&quot;relu&quot;))
        model.compile(loss=&quot;mse&quot;, optimizer=Adam(lr=self.learning_rate))
        return model

    def remember(self, state, action, reward, new_state, done):
        self.memory.append([state, action, reward, new_state, done])
    
    def replay(self):
        batch_size = 32
        if len(self.memory) &lt; batch_size:
            return
        
        samples = random.sample(self.memory, batch_size)
        #Â states, actions, rewards, states_, dones = samples
        #Â targets = self.target_model.predict(states)
        # _states = [i for i in range(len(samples))]
        # targets = [[0 for j in range(self.env.action_space.n)] for i in range(len(samples))]
        _states = np.zeros((len(samples), 8))
        targets = np.zeros((len(samples), self.env.action_space.n))

        for i, sample in enumerate(samples):
            state, action, reward, new_state, done = sample
            _states[i] = state
            #Â target = self.target_model.predict(state)
            if done:
                targets[i][action] = reward
            else:
                Q_future = max(self.target_model.predict(new_state)[0])
                targets[i][action] = reward + Q_future*self.gamma

        self.model.fit(_states, targets, epochs=1, verbose=0)

         
             
        # for sample in samples:
        #     state, action, reward, new_state, done = sample
        #     target = self.target_model.predict(state)
        #     if done:
        #         target[0][action] = reward
        #     else:
        #         Q_future = max(self.target_model.predict(new_state)[0])
        #         target[0][action] = reward + Q_future*self.gamma

        #         start_time = datetime.datetime.now()
        #         self.model.fit(state, target, epochs=1, verbose=0)
        #         end_time = datetime.datetime.now()
        #         print(&quot;--fit--&quot;)
        #         print(end_time-start_time)

            

    def target_train(self):
        weights = self.model.get_weights()
        target_weights = self.target_model.get_weights()
        for i in range(len(target_weights)):
            target_weights[i] = weights[i]
        self.target_model.set_weights(target_weights)
    
    def act(self, state):
        self.epsilon *= self.epsilon_decay
        self.epsilon = max(self.epsilon_min, self.epsilon)
        if np.random.random() &lt; self.epsilon:
            return self.env.action_space.sample()
        return np.argmax(self.model.predict(state)[0])

    def save_model(self, fn):
        self.model.save(fn)

    def act_eval(self, state):
        return np.argmax(self.model.predict(state)[0])

    def evaluation(self, n_eval=10):
        total_reward = 0
        for _ in range(n_eval):
            self.env.reset()
            cur_state = self.env.reset().reshape(1,8)
            done = False
            while not done:
                action = self.act_eval(cur_state)
                new_state, reward, done, _ = self.env.step(action)
                total_reward += reward
                cur_state = new_state.reshape(1,8)
        
        return total_reward / n_eval



def main():
    save_path = &quot;policies/&quot;
    env = gym.make(&quot;LunarLander-v2&quot;)
    
    trials = 2000
    trial_len = 500

    update_target_network = 500
    agent = DQN(env=env)
    for trial in range(trials):
        cur_state = env.reset().reshape(1,8)
        time_step_cntr = 0


        # check execution durations
        dur_replay = 0
        dur_step = 0
        dur_act = 0


        for step in range(trial_len):
            print(&quot;Trial {0}, step {1}&quot;.format(trial, step))
            action = agent.act(cur_state) # 



            new_state, reward, done, _ = env.step(action) # 

            new_state = new_state.reshape(1,8)
            agent.remember(cur_state, action, reward, new_state, done)

            # learn from experience
            agent.replay() # 

            # after &quot;update_target_network&quot; steps, update target network
            if time_step_cntr % update_target_network == 0:
                agent.target_train()
            time_step_cntr += 1

            cur_state = new_state
            if done:
                break
        
        #Â print(&quot;Duration replay {0}, duration act {1}, duration step {2}&quot;.format(dur_replay, dur_act, dur_step))
        
        # at each N steps, evaluate
        print(&quot;Evaluation over 10 episodes&quot;, agent.evaluation())

        
        print(&quot;Trial #{0} completed.&quot;.format(trial))
        # # print the progress
        # if trial % 100 == 0:
        #     print(&quot;Trial #{0} completed.&quot;.format(trial))

        #Â save the model
        #Â if trial % 20 == 0:
        agent.save_model(save_path + str(trial) + &quot;__.model&quot;)

    agent.save_model(save_path + &quot;_final&quot; + &quot;__.model&quot;)

if __name__ == &quot;__main__&quot;:
    main()
</code></pre>
",6738104,,11341120,,2020-12-18 17:01:30,2020-12-19 19:16:39,Keras fit takes so much time,<tensorflow><machine-learning><keras><deep-learning><reinforcement-learning>,1,0,0,,,CC BY-SA 4.0,
1381,54654641,1,54663971,,2019-02-12 16:30:54,,1,334,"<p>I'm playing around with making a self driving car in a pc game.  I was thinking of using reinforcement learning, and giving the car a location on the map to get to.  The reward would be a function of the distance from the waypoint, and something very negative if the car crashes.</p>

<p>I can't really wrap my head around how to add the waypoint in to the system though.  I'm using the camera input from the car as the input to the model, and I can calculate the reward based on its current position and the waypoint... but I don't always want the car to drive to the same spot... I want to give it a waypoint and have it drive there without crashing into anything.</p>

<p>How do I incorporate the waypoint and current position into the state / model?</p>
",7953712,,134077,,2019-04-22 23:10:45,2019-04-22 23:10:45,reinforcement learning - drive to waypoint,<keras><reinforcement-learning><q-learning><deepdrive>,1,0,,,,CC BY-SA 4.0,
1384,60753759,1,60755822,,2020-03-19 08:57:07,,2,1034,"<p>My goal is to develop a <strong><em>DQN-agent</em></strong> that will choose its action based on a certain strategy/policy. I previously worked with OpenAi gym-environments, but now I wanted to create my own RL environment.</p>

<p>At this stage, the <em>agent</em> shall either choose a random action or choose his action based on the predictions given by a deep neural network (defined in the class <em>DQN</em>). </p>

<p>So far, I have setup both the <strong>neural net model and my environment</strong>. The NN shall receive states as its input. These states represent 11 possible scalar values ranging from 9.5 to 10.5 (9.5, 9.6, ..., 10.4, 10.5). Since we're dealing with RL, the agent generates its data during the training process. The output should be either 0 and 1 corresponding to the recommended action.</p>

<p>Now, I would like to feed my agent a <strong>scalar value</strong>: e.g. a sample state of x = 10 and let him decide upon the action to take (Agent.select_action() is called), I encounter an issue related to the input shape/input dimension.</p>

<p>Here's the code:
<strong>1. DQN Class:</strong></p>

<pre><code>class DQN():

    def __init__(self, state_size, action_size, lr):
        self.state_size = state_size
        self.action_size = action_size
        self.lr = lr

        self.model = Sequential()
        self.model.add(Dense(128, input_dim=self.state_size, activation='relu'))
        self.model.add(Dense(128, activation='relu'))
        self.model.add(Dense(self.action_size, activation='linear'))

        self.model.compile(optimizer=Adam(lr=self.lr), loss='mse')

        self.model.summary()


    def model_info(self):
        model_description = '\n\n---Model_INFO Summary: The model was passed {} state sizes,\
            \n {} action sizes and a learning rate of {} -----'\
                            .format(self.state_size, self.action_size, self.lr)
        return model_description

    def predict(self, state):
        return self.model.predict(state)

    def train(self, state, q_values):
        self.state = state
        self.q_values = q_values
        return self.model.fit(state, q_values, verbose=0)

    def load_weights(self, path):
        self.model.load_weights(path)

    def save_weights(self, path):
        self.model.save_weights(path)
</code></pre>

<p><strong>2. Agent Class:</strong></p>

<pre><code>NUM_EPISODES = 100
MAX_STEPS_PER_EPISODE = 100
EPSILON = 0.5 
EPSILON_DECAY_RATE = 0.001
EPSILON_MIN = 0.01
EPSILON_MAX = 1
DISCOUNT_FACTOR = 0.99
REPLAY_MEMORY_SIZE = 50000
BATCH_SIZE = 50
TRAIN_START = 100
ACTION_SPACE = [0, 1]
STATE_SIZE = 11 
LEARNING_RATE = 0.01

class Agent():
    def __init__(self, num_episodes, max_steps_per_episode, epsilon, epsilon_decay_rate, \
        epsilon_min, epsilon_max, discount_factor, replay_memory_size, batch_size, train_start):
        self.num_episodes = NUM_EPISODES
        self.max_steps_per_episode = MAX_STEPS_PER_EPISODE
        self.epsilon = EPSILON
        self.epsilon_decay_rate = EPSILON_DECAY_RATE
        self.epsilon_min = EPSILON_MIN
        self.epsilon_max = EPSILON_MAX
        self.discount_factor = DISCOUNT_FACTOR
        self.replay_memory_size = REPLAY_MEMORY_SIZE
        self.replay_memory = deque(maxlen=self.replay_memory_size)
        self.batch_size = BATCH_SIZE
        self.train_start = TRAIN_START
        self.action_space = ACTION_SPACE
        self.action_size = len(self.action_space)
        self.state_size = STATE_SIZE
        self.learning_rate = LEARNING_RATE
        self.model = DQN(self.state_size, self.action_size, self.learning_rate)

    def select_action(self, state):
        random_value = np.random.rand()
        if random_value &lt; self.epsilon:
            print('random_value = ', random_value)       
            chosen_action = random.choice(self.action_space) # = EXPLORATION Strategy
            print('Agent randomly chooses the following EXPLORATION action:', chosen_action)       
        else: 
            print('random_value = {} is greater than epsilon'.format(random_value))       
            state = np.float32(state) # Transforming passed state into numpy array
            prediction_by_model = self.model.predict(state) 
            chosen_action = np.argmax(prediction_by_model[0]) # = EXPLOITATION strategy
            print('NN chooses the following EXPLOITATION action:', chosen_action)       
        return chosen_action

if __name__ == ""__main__"":
    agent_test = Agent(NUM_EPISODES, MAX_STEPS_PER_EPISODE, EPSILON, EPSILON_DECAY_RATE, \
        EPSILON_MIN, EPSILON_MAX, DISCOUNT_FACTOR, REPLAY_MEMORY_SIZE, BATCH_SIZE, \
            TRAIN_START)
    # Test of select_action function:
    state = 10 
    state = np.array(state)
    print(state.shape)
    print(agent_test.select_action(state))
</code></pre>

<p>Here's the traceback error I get when running this code:</p>

<pre><code>**ValueError**: Error when checking input: expected dense_209_input to have 2 dimensions, but got array with shape ()
</code></pre>

<p>I am unsure why the error regarding 2 dimensions occurs since I have configured the NN in the DQN class to receive only 1 dimension.</p>

<p>I have already read through similar questions on stackoverflow (<a href=""https://stackoverflow.com/questions/57345443/keras-sequential-model-input-shape"">Keras Sequential model input shape</a>, <a href=""https://stackoverflow.com/questions/51925937/keras-model-input-shape-wrong"">Keras model input shape wrong</a>, <a href=""https://stackoverflow.com/questions/44747343/keras-input-explanation-input-shape-units-batch-size-dim-etc"">Keras input explanation: input_shape, units, batch_size, dim, etc</a>). However, I was not yet able to adapt the suggestions to my use case.</p>

<p>Do you have any suggestions or hints? Thank you for your help!</p>
",11836870,,,,,2020-03-19 11:14:33,Keras model: Input shape dimension error for RL agent,<python><machine-learning><keras><reinforcement-learning><valueerror>,1,0,0,,,CC BY-SA 4.0,
1390,70808035,1,70936907,,2022-01-21 21:57:30,,1,1162,"<p>I have two different problems occurs at the same time.</p>
<p>I am having dimensionality problems with MaxPooling2d and having same dimensionality problem with DQNAgent.</p>
<p>The thing is, I can fix them seperately but cannot at the same time.</p>
<p><strong>First Problem</strong></p>
<p>I am trying to build a CNN network with several layers. After I build my model, when I try to run it, it gives me an error.</p>
<pre><code>!pip install PyOpenGL==3.1.* PyOpenGL-accelerate==3.1.*
!pip install tensorflow gym keras-rl2 gym[atari] keras pyvirtualdisplay 

from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Flatten, Convolution2D, MaxPooling2D, Activation
from keras_visualizer import visualizer 
from tensorflow.keras.optimizers import Adam
</code></pre>
<pre><code>env = gym.make('Boxing-v0')
height, width, channels = env.observation_space.shape
actions = env.action_space.n
</code></pre>
<pre><code>input_shape = (3, 210, 160, 3)   ## input_shape = (batch_size, height, width, channels)
</code></pre>
<pre><code>def build_model(height, width, channels, actions):
  model = Sequential()
  model.add(Convolution2D(32, (8,8), strides=(4,4), activation=&quot;relu&quot;, input_shape=input_shape, data_format=&quot;channels_last&quot;))
  model.add(MaxPooling2D(pool_size=(2, 2), data_format=&quot;channels_last&quot;))
  model.add(Convolution2D(64, (4,4), strides=(1,1), activation=&quot;relu&quot;))
  model.add(MaxPooling2D(pool_size=(2, 2), data_format=&quot;channels_last&quot;))
  model.add(Convolution2D(64, (3,3), activation=&quot;relu&quot;))
  model.add(Flatten())
  model.add(Dense(512, activation=&quot;relu&quot;))
  model.add(Dense(256, activation=&quot;relu&quot;))
  model.add(Dense(actions, activation=&quot;linear&quot;))
  return model
</code></pre>
<pre><code>model = build_model(height, width, channels, actions)
</code></pre>
<p>It gives below error:</p>
<blockquote>
<p>ValueError: Input 0 of layer &quot;max_pooling2d_12&quot; is incompatible with the layer: expected ndim=4, found ndim=5. Full shape received: (None, 3, 51, 39, 32)</p>
</blockquote>
<p><strong>Second Problem</strong></p>
<p>My <code>input_shape</code> is <code>(3, 210, 160, 3)</code>. I am using the first 3 on purpose due to I have to specify the <code>batch_size</code> before. If I do not specify it before and pass it as <code>(210, 160, 3)</code> to the <code>build_model</code> function, below <code>build_agent</code> function gives me an another error:</p>
<pre><code>def build_agent(model, actions):
  policy = LinearAnnealedPolicy(EpsGreedyQPolicy(), attr=&quot;eps&quot;, value_max=1., value_min=.1, value_test=.2, nb_steps=10000)
  memory = SequentialMemory(limit=1000, window_length=3)
  dqn = DQNAgent(model=model, memory=memory, policy=policy,
                 enable_dueling_network=True, dueling_type=&quot;avg&quot;,
                 nb_actions=actions, nb_steps_warmup=1000)
  return dqn
</code></pre>
<pre><code>dqn = build_agent(model, actions)
dqn.compile(Adam(learning_rate=1e-4))

dqn.fit(env, nb_steps=10000, visualize=False, verbose=1)
</code></pre>
<blockquote>
<p>ValueError: Error when checking input: expected conv2d_11_input to have 4 dimensions, but got array with shape (1, 3, 210, 160, 3)</p>
</blockquote>
<p>Deleting batch size number in the model construction phase, removes the MaxPooling2D incompatibility error but throws DQNAgent dimensionality error. Adding the batch size to the model construction phase removes DQNAgent dimensionality error but throws the MaxPooling2D incompatibility error.</p>
<p>I am really stucked.</p>
",13574152,,13574152,,2022-01-21 22:32:32,2022-02-01 07:31:52,"ValueError: Input 0 of layer ""max_pooling2d"" is incompatible with the layer: expected ndim=4, found ndim=5. Full shape received: (None, 3, 51, 39, 32)",<python><tensorflow><keras><artificial-intelligence><reinforcement-learning>,1,16,,,,CC BY-SA 4.0,
1391,70199324,1,70199445,,2021-12-02 12:18:42,,1,87,"<p>I want to build a reinforcement learning model with keras which needs to have two outputs. can it be done the same way that the Keras library does or is it even doable?</p>
<p>this is what I want to do</p>
<pre><code>inp = Input(shape=(input_layer_size, ))
x = Dense(hidden_layer_size, activation=&quot;relu&quot;)(inp)
for i in range(nb_hidden_layer):
    x = Dense(hidden_layer_size, activation=&quot;relu&quot;)(x)
a1 = Dense(1, activation='sigmoid')(x)
a2 = Dense(1, activation='sigmoid')(x)

</code></pre>
",17523967,,,,,2021-12-02 12:27:11,keras-rl model with multiple outputs,<python><tensorflow><keras><reinforcement-learning>,1,1,,,,CC BY-SA 4.0,
1392,54688502,1,54688983,,2019-02-14 10:43:03,,5,871,"<p>My project partner and I are currently facing a problem in our latest university project.
Our mission is to implement a neural network that plays the game Pong. We are giving the ball position the ball speed and the position of the paddles to our network and have three outputs: UP DOWN DO_NOTHING. After a player has 11 points we train the network with all states, the made decisions and the reward of the made decisions (see reward_cal()). The problem we are facing is, that the loss is constantly staying at a specific value only depending on the learning rate. Because of this the network always makes the same decision even though we reward it as terribly wrong. </p>

<p>Please help us find out what we did wrong we are thankful for every advise! Below is our code pls feel free to ask if there are any questions. We are pretty new to this topic so pls don't be rude if there is something completly stupid :D</p>

<p>this is our code:</p>

<pre><code>import sys, pygame, time
import numpy as np
import random
from os.path import isfile
import keras
from keras.optimizers import SGD
from keras.layers import Dense
from keras.layers.core import Flatten


pygame.init()
pygame.mixer.init()

#surface of the game
width = 400
height = 600
black = 0, 0, 0 #RGB value
screen = pygame.display.set_mode((width, height), 0, 32)
#(Resolution(x,y), flags, colour depth)
font = pygame.font.SysFont('arial', 36, bold=True)
pygame.display.set_caption('PyPong') #title of window

#consts for the game
acceleration = 0.0025 # ball becomes faster during the game
mousematch = 1
delay_time = 0
paddleP = pygame.image.load(""schlaeger.gif"")
playerRect = paddleP.get_rect(center = (200, 550))
paddleC = pygame.image.load(""schlaeger.gif"")
comRect = paddleC.get_rect(center=(200,50))
ball = pygame.image.load(""ball.gif"")
ballRect = ball.get_rect(center=(200,300))

#Variables for the game
pointsPlayer = [0]
pointsCom = [0]
playermove = [0, 0]
speedbar = [0, 0]
speed = [6, 6]
hitX = 0

#neural const
learning_rate = 0.01
number_of_actions = 3
filehandler = open('logfile.log', 'a')
filename = sys.argv[1]

#neural variables
states, action_prob_grads, rewards, action_probs = [], [], [], []

reward_sum = 0
episode_number = 0
reward_sums = []




pygame.display.flip()


def pointcontrol(): #having a look at the points in the game and restart()
     if pointsPlayer[0] &gt;= 11:
        print('Player Won ', pointsPlayer[0], '/', pointsCom[0])
        restart(1)
        return 1
     if pointsCom[0] &gt;= 11:
        print('Computer Won ', pointsPlayer[0], '/', pointsCom[0])
        restart(1)
        return 1
     elif pointsCom[0] &lt; 11 and pointsPlayer[0] &lt; 11:
        restart(0)
        return 0

def restart(finished): #resetting the positions and the ball speed and
(if point limit was reached) the points
     ballRect.center = 200,300
     comRect.center = 200,50
     playerRect.center = 200, 550
     speed[0] = 6
     speed[1] = 6
     screen.blit(paddleC, comRect)
     screen.blit(paddleP, playerRect)
     pygame.display.flip()
     if finished:
         pointsPlayer[0] = 0
         pointsCom[0] = 0

def reward_cal(r, gamma = 0.99): #rewarding every move
     discounted_r = np.zeros_like(r) #making zero array with size of
reward array
     running_add = 0
     for t in range(r.size - 1, 0, -1): #iterating beginning in the end
         if r[t] != 0: #if reward -1 or 1 (point made or lost)
             running_add = 0
         running_add = running_add * gamma + r[t] #making every move
before the point the same reward but a little bit smaller
         discounted_r[t] = running_add #putting the value in the new
reward array
     #e.g r = 000001000-1 -&gt; discounted_r = 0.5 0.6 0.7 0.8 0.9 1 -0.7
-0.8 -0.9 -1 values are not really correct just to make it clear
     return discounted_r


#neural net
model = keras.models.Sequential()
model.add(Dense(16, input_dim = (8), kernel_initializer =
'glorot_normal', activation = 'relu'))
model.add(Dense(32, kernel_initializer = 'glorot_normal', activation =
'relu'))
model.add(Dense(number_of_actions, activation='softmax'))
model.compile(loss = 'categorical_crossentropy', optimizer = 'adam')
model.summary()

if isfile(filename):
     model.load_weights(filename)

# one ball movement before the AI gets to make a decision
ballRect = ballRect.move(speed)
reward_temp = 0.0
if ballRect.left &lt; 0 or ballRect.right &gt; width:
    speed[0] = -speed[0]
if ballRect.top &lt; 0:
    pointsPlayer[0] += 1
    reward_temp = 1.0
    done = pointcontrol()
if ballRect.bottom &gt; height:
    pointsCom[0] += 1
    done = pointcontrol()
    reward_temp = -1.0
if ballRect.colliderect(playerRect):
    speed[1] = -speed[1]
if ballRect.colliderect(comRect):
    speed[1] = -speed[1]
if speed[0] &lt; 0:
    speed[0] -= acceleration
if speed[0] &gt; 0:
    speed[0] += acceleration
if speed[1] &lt; 0:
    speed[1] -= acceleration
if speed[1] &gt; 0 :
    speed[1] += acceleration

while True: #game
     for event in pygame.event.get():
          if event.type == pygame.QUIT:
                pygame.quit()
                sys.exit()

     state = np.array([ballRect.center[0], ballRect.center[1], speed[0],
speed[1], playerRect.center[0], playerRect.center[1], comRect.center[0],
comRect.center[1]])
     states.append(state)
     action_prob = model.predict_on_batch(state.reshape(1, 8))[0, :]

     action_probs.append(action_prob)
     action = np.random.choice(number_of_actions, p=action_prob)
     if(action == 0): playermove = [0, 0]
     elif(action == 1): playermove = [5, 0]
     elif(action == 2): playermove = [-5, 0]
     playerRect = playerRect.move(playermove)

     y = np.array([-1, -1, -1])
     y[action] = 1
     action_prob_grads.append(y-action_prob)

     #enemy move
     comRect = comRect.move(speedbar)
     ballY = ballRect.left+5
     comRectY = comRect.left+30
     if comRect.top &lt;= (height/1.5):
        if comRectY - ballY &gt; 0:
           speedbar[0] = -7
        elif comRectY - ballY &lt; 0:
           speedbar[0] = 7
     if comRect.top &gt; (height/1.5):
        speedbar[0] = 0

     if(mousematch == 1):
          done = 0
          reward_temp = 0.0
          ballRect = ballRect.move(speed)
          if ballRect.left &lt; 0 or ballRect.right &gt; width:
                speed[0] = -speed[0]
          if ballRect.top &lt; 0:
                pointsPlayer[0] += 1
                done = pointcontrol()
                reward_temp = 1.0
          if ballRect.bottom &gt; height:
                pointsCom[0] += 1
                done = pointcontrol()
                reward_temp = -1.0
          if ballRect.colliderect(playerRect):
                speed[1] = -speed[1]
          if ballRect.colliderect(comRect):
                speed[1] = -speed[1]
          if speed[0] &lt; 0:
                speed[0] -= acceleration
          if speed[0] &gt; 0:
                speed[0] += acceleration
          if speed[1] &lt; 0:
                speed[1] -= acceleration
          if speed[1] &gt; 0 :
                speed[1] += acceleration
          rewards.append(reward_temp)

          if (done):
              episode_number += 1
              reward_sums.append(np.sum(rewards))
              if len(reward_sums) &gt; 40:
                  reward_sums.pop(0)
              s = 'Episode %d Total Episode Reward: %f , Mean %f' % (
episode_number, np.sum(rewards), np.mean(reward_sums))
              print(s)
              filehandler.write(s + '\n')
              filehandler.flush()

              # Propagate the rewards back to actions where no reward
was given.
              # Rewards for earlier actions are attenuated
              rewards = np.vstack(rewards)

              action_prob_grads = np.vstack(action_prob_grads)
              rewards = reward_cal(rewards)

              X = np.vstack(states).reshape(-1, 8)

              Y = action_probs + learning_rate * rewards * y


              print('loss: ', model.train_on_batch(X, Y))

              model.save_weights(filename)

              states, action_prob_grads, rewards, action_probs = [], [], [], []

              reward_sum = 0

          screen.fill(black)
          screen.blit(paddleP, playerRect)
          screen.blit(ball, ballRect)
          screen.blit(paddleC, comRect)
          pygame.display.flip()
          pygame.time.delay(delay_time)
</code></pre>

<p>this is our output:</p>

<pre><code>pygame 1.9.4 Hello from the pygame community. https://www.pygame.org/contribute.html Using TensorFlow backend.
    _________________________________________________________________ 

Layer (type)                 Output Shape              Param #   
    ================================================================= 

dense_1 (Dense)              (None, 16)                144       
    _________________________________________________________________ 

dense_2 (Dense)              (None, 32)                544       
    _________________________________________________________________ 

dense_3 (Dense)              (None, 3)                 99        
    ================================================================= 

Total params: 787 Trainable params: 787 Non-trainable params: 0
    _________________________________________________________________ 2019-02-14 11:18:10.543401: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 AVX512F FMA 2019-02-14 11:18:10.666634: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1432] Found device 0 with properties:  name: GeForce GTX 1080 Ti major: 6 minor: 1 memoryClockRate(GHz): 1.6705 pciBusID: 0000:17:00.0 totalMemory:
    10.92GiB freeMemory: 10.76GiB 2019-02-14 11:18:10.775144: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1432] Found device 1 with properties:  name: GeForce GTX 1080 Ti major: 6 minor: 1 memoryClockRate(GHz): 1.6705 pciBusID: 0000:65:00.0 totalMemory:
    10.91GiB freeMemory: 10.73GiB 2019-02-14 11:18:10.776037: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1511] Adding visible gpu devices: 0, 1 2019-02-14 11:18:11.176560: I tensorflow/core/common_runtime/gpu/gpu_device.cc:982] Device interconnect StreamExecutor with strength 1 edge matrix: 2019-02-14 11:18:11.176590: I tensorflow/core/common_runtime/gpu/gpu_device.cc:988]      0 1  2019-02-14 11:18:11.176596: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1001] 0:   N Y  2019-02-14 11:18:11.176600: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1001] 1:   Y N  2019-02-14 11:18:11.176914: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 10403 MB memory) -&gt; physical GPU (device: 0, name: GeForce GTX 1080 Ti, pci bus id: 0000:17:00.0, compute capability: 6.1) 2019-02-14 11:18:11.177216: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:1 with 10382 MB memory) -&gt; physical GPU (device: 1, name: GeForce GTX 1080 Ti, pci bus id: 0000:65:00.0, compute capability: 6.1) 


Computer Won  0 / 11 Episode 1 Total Episode Reward: -11.000000 , Mean -11.000000 

loss:  0.254405 


Computer Won  0 / 11 Episode 2 Total Episode Reward: -11.000000 , Mean -11.000000 

loss:  0.254304 


Computer Won  0 / 11 Episode 3 Total Episode Reward: -11.000000 , Mean -11.000000 

loss:  0.254304 


Computer Won  0 / 11 Episode 4 Total Episode Reward: -11.000000 , Mean -11.000000 

loss:  0.254304 


Computer Won  0 / 11 Episode 5 Total Episode Reward: -11.000000 , Mean -11.000000 

loss:  0.254304 


Computer Won  0 / 11 Episode 6 Total Episode Reward: -11.000000 , Mean -11.000000 

loss:  0.254304
</code></pre>
",11061454,,3327376,,2019-02-14 10:56:19,2019-02-14 13:05:11,neural network does not learn (loss stays the same),<python><tensorflow><keras><neural-network><reinforcement-learning>,1,1,,,,CC BY-SA 4.0,
1393,70756617,1,70769329,,2022-01-18 13:47:19,,0,76,"<p>For startes: this question does not ask for help regarding reinforcement learning (RL), RL is only used as an example.</p>
<p>The Keras documentation contains an example <a href=""https://keras.io/examples/rl/actor_critic_cartpole/"" rel=""nofollow noreferrer"">actor-critic reinforcement learning implementation</a> using <a href=""https://www.tensorflow.org/api_docs/python/tf/GradientTape"" rel=""nofollow noreferrer"">Gradient Tape</a>. Basically, they've created a model with two separate outputs: one for the actor (<code>n</code> actions) and one for the critic (<code>1</code> reward). The following lines describe the backpropagation process (found somewhere in the code example):</p>
<pre><code># Backpropagation
loss_value = sum(actor_losses) + sum(critic_losses)
grads = tape.gradient(loss_value, model.trainable_variables)
optimizer.apply_gradients(zip(grads, model.trainable_variables))
</code></pre>
<p>Despite the fact that the actor and critic losses are calculated differently, they sum up those two losses to obtain the final loss value used for calculating the gradients.</p>
<p>When looking at this code example, one question came to my mind: Is there a way to calculate the gradients of the output layer with respect to the corresponding losses, i.e. calculate the gradients of the first <code>n</code> output nodes based on the actor loss and the gradient of the last output node using the critic loss? For my understanding, this would be much more convenient than adding both losses (different!) and updating the gradients based on this cumulative approach. Do you agree?</p>
",10607799,,,,,2022-01-19 10:54:42,Keras GradientType: Calculating gradients with respect to the output node,<python><tensorflow><keras><reinforcement-learning><gradienttape>,1,0,,,,CC BY-SA 4.0,
1397,59592009,1,59599208,,2020-01-04 15:13:44,,1,96,"<p>I am trying to train a Neural Net on playing Tic Tac Toe via Reinforcement Learning with <code>Keras</code>, <code>Python</code>.
Currently the Net gets an Input of the current board:</p>

<pre><code>    array([0,1,0,-1,0,1,0,0,0])
</code></pre>

<pre><code>1 = X 
-1 = O
0 = an empty field
</code></pre>

<p>If the Net won a game it gets a reward for every action(Output) it did.    <code>[0,0,0,0,1,0,0,0,0]</code>
If the Net loses I want to train it with a bad reward.    <code>[0,0,0,0,-1,0,0,0,0]</code></p>

<p>But currently I get a lot of    <code>0.000e-000</code>    accuracies.</p>

<p>Can I train a ""bad reward"" at all? Or if can't do it with <code>-1</code> how should I do it instead?</p>

<p>Thanks in advance.</p>
",10929070,,7338544,,2020-01-04 15:21:22,2020-01-05 15:43:34,How to train a bad reward with a classifying Neural Net?,<python><keras><reinforcement-learning><reward>,1,0,,,,CC BY-SA 4.0,
1398,47976845,1,47979477,,2017-12-26 10:02:02,,1,313,"<p>I'm doing an AI with reinforcement learning and i'm getting weird results, the loss shows like this:
Tensorflow loss: <a href=""https://imgur.com/a/Twacm"" rel=""nofollow noreferrer"">https://imgur.com/a/Twacm</a></p>

<p>And while it's training, after each game, it's playing against a random player and after a player with a weighted matrix, but it goes up and down:
results: <a href=""https://imgur.com/a/iGuu2"" rel=""nofollow noreferrer"">https://imgur.com/a/iGuu2</a></p>

<p>Basically i'm doing a reinforcement learning agent that learns to play Othello. Using E-greedy, Experience replay and deep networks using Keras over Tensorflow. Tried different architectures like sigmoid, relu and in the images shown above, tanh. All them have similar loss but the results are a bit different.
In this exemple the agent is learning from 100k professional games.
Here is the architecture, with default learning rate as 0.005:</p>

<pre><code>model.add(Dense(units=200,activation='tanh',input_shape=(64,)))
model.add(Dense(units=150,activation='tanh'))
model.add(Dense(units=100,activation='tanh'))
model.add(Dense(units=64,activation='tanh'))
optimizer = Adam(lr=lr, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.0)
model.compile(loss=LOSS,optimizer=optimizer)
</code></pre>

<p>Original code: <a href=""https://github.com/JordiMD92/thellia/tree/keras"" rel=""nofollow noreferrer"">https://github.com/JordiMD92/thellia/tree/keras</a></p>

<p>So, why i get these results?
Now my input is 64 neurons (8*8 matrix), with 0 void square, 1 black square and -1 white square. Is it bad to use negative inputs?</p>
",2335427,,,,,2017-12-26 13:44:19,Tensorflow loss is already low,<python><tensorflow><keras><reinforcement-learning><othello>,1,0,,,,CC BY-SA 3.0,
1400,61574690,1,61574737,,2020-05-03 12:46:39,,2,2138,"<p>Sorry if this is a 'nooby' question, but I really don't know how to solve it. I've installed keras and a lot of other stuff for deep learning with Ananconda, but now I want to try to make something with Reinforcement Learning. So I've read that I need to install keras-rl, and I installed it as follows:</p>

<pre><code>git clone https://github.com/matthiasplappert/keras-rl.git
cd keras-rl
python setup.py install
</code></pre>

<p>I have also installed gym:</p>

<pre><code>pip install gym
</code></pre>

<p>But when I write in Anaconda Spyder 4 (4.1.2):</p>

<pre><code>from rl.agents.dqn import DQNAgent
from rl.policy import EpsGreedyQPolicy
from rl.memory import SequentialMemory
</code></pre>

<p>... it says that ' ModuleNotFoundError: No module named 'rl' '. What's wrong? Thanks in advance for any answer! </p>
",9040471,,,,,2020-05-04 12:07:49,Anaconda how to import keras-rl,<python><keras><anaconda><spyder><reinforcement-learning>,1,0,,,,CC BY-SA 4.0,
1401,47840527,1,48791563,,2017-12-15 22:10:26,,15,17914,"<p>I am trying to use huber loss in a keras model (writing DQN), but I am getting bad result, I think I am something doing wrong. My is code is below.</p>

<pre><code>model = Sequential()
model.add(Dense(output_dim=64, activation='relu', input_dim=state_dim))
model.add(Dense(output_dim=number_of_actions, activation='linear'))
loss = tf.losses.huber_loss(delta=1.0)
model.compile(loss=loss, opt='sgd')
return model
</code></pre>
",7643343,,,,,2019-09-16 17:33:15,Using Tensorflow Huber loss in Keras,<python><tensorflow><keras><reinforcement-learning>,4,0,0,,,CC BY-SA 3.0,
1406,65521485,1,65526528,,2020-12-31 13:24:54,,1,1400,"<p>I want to have adaptive learning rate based on time steps instead of epochs unlike most of the schedulers are based. I have a model as:</p>
<pre><code>import tensorflow as tf
from tensorflow.keras.models import Model
from tensorflow.keras import layers
from tensorflow.keras.optimizers import Adam

class DQNagent:

    def __init__(self, state_size, action_size):
        self.state_size = state_size
        self.action_size = action_size
        self.model = self.build_model()    # original model
        self.target_model = self.build_model()  # target model
        self.lr = 1e-2

    def build_model(self):
        
        x_in = layers.Input(shape=(self.step_size, self.state_size))
        x_out = layers.Dense(20, activation='relu')(x_in)
        output = layers.Dense(self.action_size, activation='linear')(x_out)
        
        self.learning_rate = CustomSchedule()
 
        opt = tf.keras.optimizers.Adam(self.learning_rate)
        model = Model(inputs=x_in, outputs=output_y, name=&quot;DQN&quot;)
        model.compile(loss=['mse'], optimizer=opt)
    

        return model
</code></pre>
<p>and I want to make a scheduler as something like this:</p>
<pre><code>class CustomSchedule:
  def __init__(self, lr=1e-2):
    super(CustomSchedule, self).__init__()
    self.lr = lr
    self.t = 0

  def __call__(self):
    self.t +=1
    if self.t % 100 ==0:
        self.lr /= 10

    return self.lr
</code></pre>
<p>and my main code without declaration of everything has something like this:</p>
<pre><code>dqn = DQNagent(state_size, action_size)

for step in range(1000):
    states_all = np.array([[[0, 0, 1],[1,0,1], [0, -1, 1], [1,-1,1]]])
    Q_values =  dqn.model.predict(state_all)[0]
    
    # training
    batch = memory.sample(batch_size)
    batch_states = utils.get_states_user(batch) # assuming I have generated states using this method
    
    Q_states = dqn.model.predict(batch_states) # assuming I have sampled batch states
    
    dqn.model.fit(batch_states, Q_states, verbose =0)
</code></pre>
<p>I want to schedule my learning rate in a way that if my lets say <code>step%100==0</code> the learning rate is decreased as <code>learning_rate/10</code>. Seems like for the <code>CustomSchedule</code> class that I have created, I will have to recompiled the <code>model</code> which doesn't seem efficient to save and load weights. Is there any other way I can do this?</p>
<h2>EDITS:</h2>
<p>I have edited my code as following @FedericoMalerba <a href=""https://stackoverflow.com/a/65526528/2252047"">answer</a></p>
<p>Created a <code>decay_func</code> as:</p>
<pre><code>def decay_func(step, lr):

    return lr/10**(step/100)
</code></pre>
<p>then I added followings changes to my <code>DQNAgent</code> class:</p>
<pre><code>class DQNAgent():
     def __init__(self, state_size, action_size):
     self.lr = 1e-2
     self.t_step = tf.Variable(0, trainable=False, name='Step', dtype=tf.int64)
     self.decaying_lr = partial(decay_func, step=self.step, lr=self.lr)
     
    def __call__(self):
        self.step.assign_add(1)
        return self.step
</code></pre>
<p>and called <code>dqn()</code> in my main code for every step. The callable <code>decaying_lr</code> is is passed to the optimiser in <code>build_model()</code> as
<code>opt = tf.keras.optimizers.Adam(self.decaying_lr)</code></p>
",2252047,,2252047,,2021-01-02 20:32:59,2021-01-02 20:32:59,Exponential decay learning rate based on batches instead of epochs,<python><tensorflow><machine-learning><keras><reinforcement-learning>,2,0,,,,CC BY-SA 4.0,
1410,49673515,1,49675186,,2018-04-05 13:23:44,,2,392,"<p>I'm new to learning about Neural Networks and AI. For my college project I'm trying to make an agent drive a car towards a target placed in a random position on a plane. This is my model code:</p>

<pre><code>def CreateModel(self):
    model = Sequential()
    model.add(Conv2D(40, kernel_size=(7, 9), strides=(1, 1), input_shape=self.input_shape, activation='relu'))
    model.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2)))
    model.add(Conv2D(70, kernel_size=(5, 5), strides=(1, 1), activation='relu'))
    model.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2)))
    model.add(Conv2D(90, kernel_size=(4, 5), strides=(1, 1), activation='relu'))
    model.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2)))
    model.add(Activation('relu'))
    model.add(Flatten())
    model.add(Dense(768))
    model.add(Activation('relu'))
    model.add(Dense(1024))
    model.add(Activation('relu'))
    model.add(Dense(512))
    model.add(Activation('relu'))
    model.add(Dense(5))
    model.add(Activation('softmax'))
    model.compile(loss=""mean_squared_error"", optimizer=keras.optimizers.Adam(lr=self.learning_rate))
    return model

def __init__(self, load='', epsilon=0.8):
    self.input_shape = (90,120,3)

    # Training hyperparameters
    self.gamma           = 0.99
    self.epsilon         = epsilon
    self.epsilon_min     = 0.3
    self.epsilon_decay   = 0.9998
    self.learning_rate   = 0.001
    self.tau             = 0.05 
    self.memory_size     = 2000

    self.memory = deque(maxlen=self.memory_size)

    if (load != ''):
        print('|| ------------  Init model load: {0}'.format(load))
        self.model = load_model(load)
        self.target_model = load_model(load)
    else:
        print('|| ------------  No load. Initializing')
        self.model = self.CreateModel()
        self.target_model = self.CreateModel()



def Act(self, state):
    self.epsilon *= self.epsilon_decay
    self.epsilon = max(self.epsilon_min, self.epsilon)

    print(""|| ----  Epsilon: {0}"".format(self.epsilon))

    if np.random.random() &lt; self.epsilon:
        return np.random.randint(low=0,high=5)

    return (self.model.predict(state)).argmax(axis=1)[0]
</code></pre>

<p>It takes a single 120 x 90 3 channel image as input and outputs a vector of Q-values for the 5 possible actions that can be taken based on the image. I read this research paper(<a href=""https://link.springer.com/content/pdf/10.1007%2Fs00521-017-3241-z.pdf"" rel=""nofollow noreferrer"">https://link.springer.com/content/pdf/10.1007%2Fs00521-017-3241-z.pdf</a>) paper and modelled my network based on it. I've been training it for quite a while but the results have been disappointing. 
I'd like to incorporate an LSTM cell to process a sequence of 10 consecutive frames instead of 1 and output the same single 5 elements array for each sequence. I tried to understand the Keras LSTM and TimeDistributed layers, but can't understand how to make the corresponding changes to my model. Please help me out or direct me to a page that addresses this topic in a newbie way?</p>
",5125161,,,,,2018-04-05 14:43:52,Add lstm cell to neural network for reinforcement learning,<python-3.x><machine-learning><keras><reinforcement-learning>,1,0,,2018-04-05 15:19:54,,CC BY-SA 3.0,
1411,65237345,1,65239050,,2020-12-10 15:22:05,,0,178,"<p><strong>TLDR</strong></p>
<p>The input shape of each sample for my DoubleDuelingDQN is (169, 3). The output of that DDDQN shall be of shape (3) for 3 corresponding actions. Currently, when I call</p>
<pre><code>next_qs_list = self.target_network(next_states).numpy()
</code></pre>
<p>..the output shape is (64, 169, 3) for batch_size=64. My assumption is, that the output shape is wrong and should be (64, 3).
My NN is currently configured like below (where it's call() function returns the wrong shape) - How would I need to build my network to return the correct shape (3) instead of (169,3)?:</p>
<pre><code>class DuelingDeepQNetwork(keras.Model):
    def __init__(self, n_actions, neurons_1, neurons_2, neurons_3=None):
        super(DuelingDeepQNetwork, self).__init__()
        self.dens_1 = keras.layers.Dense(neurons_1, activation='relu', input_dim=(169,31,))  # Here I added input_dim which is not present in my LunarLander Agent
        self.dens_2 = keras.layers.Dense(neurons_2, activation='relu')
        if neurons_3:
            self.dens_3 = keras.layers.Dense(neurons_3, activation='relu')
        self.V = keras.layers.Dense(1, activation=None)  # Value layer
        self.A = keras.layers.Dense(n_actions, activation=None)  # Advantage layer

    def call(self, state):
        x = self.dens_1(state)
        x = self.dens_2(x)
        if self.dens_3:
            x = self.dens_3(x)
        V = self.V(x)
        A = self.A(x)
        Q = V + (A - tf.math.reduce_mean(A, axis=1, keepdims=True))
        return Q

    def advantage(self, state):
        x = self.dens_1(state)
        x = self.dens_2(x)
        if self.dens_3:
            x = self.dens_3(x)
        A = self.A(x)
        return A
</code></pre>
<p>Updated error message:</p>
<pre><code>ValueError: non-broadcastable output operand with shape (3,) doesn't match the broadcast shape (3,3)
</code></pre>
<p>is raised on the last line of:</p>
<pre><code>for idx, done in enumerate(dones):
    target_qs_list[idx, actions[idx]] = rewards[idx]
    tmp1 = self.gamma * next_qs_list[idx, max_actions[idx]]
    target_qs_list[idx, actions[idx]] += tmp1 * (1-int(dones[idx]))
</code></pre>
<p><strong>Initial Post</strong>:</p>
<p>I have (kind of) finished my Custom RL Environment respecting the OpenAI Gym concept. Basically the environment is a TimeSeries of OHLCV Cryptoprices and <strong>env.reset()</strong> returns a windows of shape (169, 31) - 169 TimeSteps and 31 Features. With <strong>env.step()</strong> the agent's observation window wanders one TimeStep on. I want start with 3 possible actions (do nothing / buy / sell)</p>
<pre><code>self.action_space = spaces.Discrete(3)
self.observation_space = spaces.Box(low=-np.inf, high=np.inf, shape=(self.HISTORY_LENGTH+1, self.df_sample.shape[1]), dtype=np.float32)
# (169, 31)
</code></pre>
<p>Now I am failing on the migration of my existing DQN-Agent from LunarLander-v2 (created following multiple Tutorials on Youtube and Medium). I assume that my DQNetwork and/or MemoryBuffer are not formatted correctly. I begin with filling up my memory with 1,000 samples on random actions. Then Training begins and on with the <strong>agent.learn()</strong> call, the following <em>Error</em> is raised, which I am unable to interpret and which is the reason for this question for help.</p>
<pre><code>TypeError: Only integers, slices (`:`), ellipsis (`...`), tf.newaxis (`None`) and scalar tf.int32/tf.int64 tensors are valid indices, got &lt;tf.Tensor: shape=(3,), dtype=int64, numpy=array([144, 165,   2])&gt;
</code></pre>
<p><em>(deleted obsolete text)</em>
To be exactly this update loop raises the error:</p>
<pre><code>for idx, done in enumerate(dones):
    target_qs_list[idx, actions[idx]] = rewards[idx] + self.gamma * next_qs_list[idx, max_actions[idx]] * (1-int(dones[idx]))
</code></pre>
<p>Since my debugging skills and knowledge on python+keras+TF have found an end here, I appreciate any help.</p>
<p>Here is the code of my Agent. If more code or information is needed, I will happily provide more info.</p>
<pre><code>class ReplayBuffer():
    def __init__(self, max_mem_size, dims):
        # self.memory = max_mem_size
        self.state_memory = np.zeros((max_mem_size, *dims), dtype=np.float32)  # Here I added &quot;*&quot; to deflate the no 2D observation (169, 31)
        self.action_memory = np.zeros(max_mem_size, dtype=np.int32)
        self.reward_memory = np.zeros(max_mem_size, dtype=np.float32)
        self.new_state_memory = np.zeros((max_mem_size, *dims), dtype=np.float32)  # Here I added &quot;*&quot; to deflate the no 2D observation (169, 31)
        self.done_memory = np.zeros(max_mem_size, dtype=np.int32)
        self.max_mem_size = max_mem_size
        self.mem_counter = 0
        self.index = 0

    def store_transition(self, transition):
        '''
        :param transition: Tuple of transition data (state, action, reward, new_state, done)
        :return: Nothing
        '''
        self.state_memory[self.index] = transition[0]
        self.action_memory[self.index] = transition[1]
        self.reward_memory[self.index] = transition[2]
        self.new_state_memory[self.index] = transition[3]
        self.done_memory[self.index] = transition[4]
        self.mem_counter += 1
        if self.index &lt; self.max_mem_size - 1:
            self.index += 1
        else:
            self.index = 0

    def get_sample_batch(self, batch_size, replace=False):
        '''
        :param batch_size: Number of samples for batch
        :param replace: Wether or not double entries are allowed in returned batch
        :return: Tuples of transition data (state, action, reward, new_state, done)
        '''
        max_size = min(self.mem_counter, self.max_mem_size)
        batch_ids = np.random.default_rng().choice(max_size, batch_size, replace)
        states = self.state_memory[batch_ids]
        actions = self.action_memory[batch_ids]
        rewards = self.reward_memory[batch_ids]
        new_states = self.new_state_memory[batch_ids]
        dones = self.done_memory[batch_ids]
        return states, actions, rewards, new_states, dones


class DuelingDeepQNAgent():
    def __init__(self, lr, gamma, env, batch_size=64, mem_size=1_000_000, update_target_every=50):

        self.n_actions = env.action_space.n
        self.input_dims = env.observation_space.shape  #env.observation_space.shape[0]
        self.action_space = [i for i in range(self.n_actions)]
        self.gamma = gamma
        self.epsilon = 1.0
        self.batch_size = batch_size
        self.memory = ReplayBuffer(max_mem_size=mem_size, dims=self.input_dims)
        self.update_target_every = update_target_every
        self.update_target_counter = 0
        self.learn_step_counter = 0

        # Main model - gets trained every single step()
        self.q_network = DuelingDeepQNetwork(n_actions=self.n_actions, neurons_1=256, neurons_2=256, neurons_3=128)
        self.target_network = DuelingDeepQNetwork(n_actions=self.n_actions, neurons_1=256, neurons_2=256, neurons_3=128)
        self.q_network.compile(optimizer=Adam(learning_rate=lr), loss='mse')
        self.target_network.compile(optimizer=Adam(learning_rate=lr), loss='mse')

    def store_transition(self, transition):
        self.memory.store_transition(transition)

    def choose_action(self, observation):
        if np.random.random() &lt; self.epsilon:
            action = np.random.choice(self.action_space)
        else:
            state = np.array([observation])  # Add in an extra dimension -&gt; quasi hinzufÃ¼gen einer &quot;batch-dimension&quot;

            q_values = self.q_network.advantage(state)
            action = tf.math.argmax(q_values, axis=1).numpy()[0]
        return action

    def learn(self):
        if self.memory.mem_counter &lt; self.batch_size:
            return
        if self.update_target_counter % self.update_target_every == 0:
            self.target_network.set_weights(self.q_network.get_weights())

        current_states, actions, rewards, next_states, dones = self.memory.get_sample_batch(self.batch_size)

        current_qs_list = self.q_network(current_states)
        next_qs_list = self.target_network(next_states)
        target_qs_list = current_qs_list.numpy()  # ??? From Tensor to Numpy?!
        max_actions = tf.math.argmax(self.q_network(next_states), axis=1)

        # According to Phil: improve on my solution here....
        for idx, done in enumerate(dones):
            target_qs_list[idx, actions[idx]] = rewards[idx] + self.gamma * next_qs_list[idx, max_actions[idx]] * (1-int(dones[idx]))
        self.q_network.fit(current_states, target_qs_list, batch_size=self.batch_size, verbose=0)
        self.learn_step_counter += 1

    
</code></pre>
",7463215,,7463215,,2020-12-10 16:59:48,2020-12-10 17:09:14,"How to configure Dueling Double DQN input_shape for samples with a shape of (169, 3) each?",<python><keras><tensorflow2.0><reinforcement-learning><agent>,1,0,,,,CC BY-SA 4.0,
1413,58110466,1,62046758,,2019-09-26 06:03:32,,1,157,"<p>So, I'm trying to create an implementation of AlphaZero using keras. However, I am not too sure about MCTS. My understanding and coding of Monte Carlo Tree Search is as follows:</p>

<pre><code>
class MCTS(object):
    def __init__(self, action_size, movesets, nsims, ndepth):
        self.nsims = nsims
        self.ndepth = ndepth
        self.movesets = movesets
        self.action_size = action_size

    def evaluate_and_act(self, agent, stateizer, critic, state):
        sims = []

        print(""Beginning monte carlo tree search"")

        true_state = state
        for i in range(self.nsims):
            random_walk = []
            for j in range(self.ndepth):
                random_actions = []
                print(""Searching depth"", j, ""of simulation"", i)

                for k in range(self.movesets):
                    rand_move = np.random.choice(self.action_size)
                    rand_move_matrix = cp.add(cp.zeros((1, self.action_size)), .0001)
                    rand_move_matrix[0][rand_move] = critic.predict(state, batch_size=64)[0][0]
                    random_actions.append(cp.asnumpy(rand_move_matrix))
                random_action_concat = np.concatenate(random_actions, -1)
                state = stateizer.predict(cp.asnumpy(random_action_concat), batch_size=64)
                random_walk.append(random_actions)
            sims.append(random_walk)
            state = true_state

        best_reward = -1000000.0
        for walk in sims:

            sum_reward = np.sum(walk)
            if sum_reward &gt;= best_reward:
                best_walk = walk
                best_reward = sum_reward

        return best_walk[0]
</code></pre>

<p>It seems like I don't need the policy network at all in this implementation, just the critic. Can someone please help me with understanding whether or not my implementation is correct, and why it's incorrect in terms of AlphaZero? Thanks.  </p>
",10515022,,,,,2020-05-27 15:23:32,Understanding monte carlo tree search,<python><keras><reinforcement-learning>,1,0,,,,CC BY-SA 4.0,
1415,46455176,1,46458768,,2017-09-27 18:48:59,,2,1112,"<p>Following this <a href=""http://edersantana.github.io/articles/keras_rl/"" rel=""nofollow noreferrer"">example</a> and this <a href=""https://www.intelnervana.com/demystifying-deep-reinforcement-learning/"" rel=""nofollow noreferrer"">article</a> on reinforcement learning. I finally manage to create a similar Q-learning that learns to play another game environment. The only problem i have is with the last output layer of the neural network which represents the input actions of the game.</p>

<p>The mechanics/logic of the game environment are not relevant to this question, but the game environment require 2 types of inputs at the same time for each given action:</p>

<ol>
<li>Input number 1 represents a single key press between 3 possible 
keys. So basically the layer should output 3 classes where the probabilities sum is 1. I will then pick the class with the highest 
probability out of these three.</li>
<li>Input number 2 represents a percentage ranging from 0 to 1. And should be independent of the first three classes.</li>
</ol>

<p>I really do not see how can i create this last output layer so that it has a total of 4 output classes. The first 3 classes should give probabilities between each other with a total sum of 1. And the last class should be independent of the first three and should ranging from 0 to 1. </p>

<p>Can somebody point me in the right direction on how to achieve this? How do i structure such a layer?</p>

<p>I am thinking of something like this for the first input:</p>

<pre><code>model.add(Dense(output_dim=3))
model.add(Activation(""softmax""))
model.compile(loss='categorical_crossentropy', optimizer=""adam"")
</code></pre>

<p>and then for the second input something like this</p>

<pre><code>model.add(Dense(output_dim=1))
model.add(Activation(""sigmoid""))
model.compile(loss='binary_crossentropy', optimizer='adam')
</code></pre>

<p>but how do i combine them into one output layer?</p>

<p>Maybe you have another structure in mind? </p>
",2480410,,2480410,,2017-09-27 21:00:43,2017-09-27 23:37:03,Last output layer with multiple classes. Keras backed by Tensorflow,<tensorflow><neural-network><deep-learning><keras><reinforcement-learning>,2,1,0,,,CC BY-SA 3.0,
1416,45060530,1,47339348,,2017-07-12 14:28:53,,1,677,"<p>A project i am working on has a reinforcement learning stage using the <a href=""http://www-anw.cs.umass.edu/~barto/courses/cs687/williams92simple.pdf"" rel=""nofollow noreferrer"">REINFORCE</a> algorithm. The used model has a final softmax activation layer and because of that a negative learning rate is used as a replacement for negative rewards. I have some doubts about this process and can't find much literature on using a negative learning rate.  </p>

<p>Does reinforement learning work with switching learning rate between positive and negative? and if not what would be a better approach, get rid of softmax or has keras a nice option for this?</p>

<p>Loss function:</p>

<pre class=""lang-py prettyprint-override""><code>def log_loss(y_true, y_pred):
    '''
    Keras 'loss' function for the REINFORCE algorithm, 
    where y_true is the action that was taken, and updates 
    with the negative gradient will make that action more likely. 
    We use the negative gradient because keras expects training data
    to minimize a loss function.
    '''
    return -y_true * K.log(K.clip(y_pred, K.epsilon(), 1.0 - K.epsilon()))
</code></pre>

<p>Switching learning rate:</p>

<pre class=""lang-py prettyprint-override""><code>K.set_value(optimizer.lr, lr * (+1 if won else -1))
learner_net.train_on_batch(np.concatenate(st_tensor, axis=0),
                           np.concatenate(mv_tensor, axis=0))
</code></pre>

<p><strong>Update, test results</strong></p>

<p>I ran a test with only positive reinforcement samples, omitting all negative examples and thus the negative learning rate. Winning rate is rising, it is improving and i can safely assume using a negative learning rate <strong>is not correct</strong>.<br>
anybody any thoughts on how we should implement it?</p>

<p><strong>Update, model explanation</strong></p>

<p>We are trying to recreate <a href=""https://storage.googleapis.com/deepmind-media/alphago/AlphaGoNaturePaper.pdf"" rel=""nofollow noreferrer"">AlphaGo as described by DeepMind</a>, the slow policy net:</p>

<blockquote>
  <p>For the first stage of the training pipeline, we build on prior work
  on predicting expert moves in the game of Go using supervised
  learning13,21â€“24. The SL policy network pÏƒ(a|â€Šs) alternates between convolutional
  layers with weights Ïƒ, and rectifier nonlinearities. A final softmax
  layer outputs a probability distribution over all legal moves a. </p>
</blockquote>
",3796822,,3796822,,2017-07-15 16:48:16,2017-11-16 21:26:59,Keras reinforcement training with softmax,<keras><reinforcement-learning><softmax>,1,3,0,,,CC BY-SA 3.0,
1420,59585026,1,59803495,,2020-01-03 20:35:02,,3,2275,"<p>I'm learning about Action-Critic Reinforcement Learning techniques, in particular A2C algorithm.</p>

<p>I've found a good description of a simple version of the algorithm (i.e. without experience replay, batching or other tricks) with implementation here: <a href=""https://link.medium.com/yi55uKWwV2"" rel=""nofollow noreferrer"">https://link.medium.com/yi55uKWwV2</a>. The complete code from that article is <a href=""https://github.com/andy-psai/MountainCar_ActorCritic/blob/master/RL%20Blog%20FINAL%20MEDIUM%20code%2002_12_19.ipynb"" rel=""nofollow noreferrer"">available on GitHub</a>.</p>

<p>I think I understand ok-ish what's happening here, but to make sure I actually do, I'm trying to reimplement it from scratch using higher-level tf.keras APIs. Where I'm getting stuck is how do I implement training loop correctly, and how do I formulate actor's loss function.</p>

<ol>
<li>What is the correct way to pass action and advantage into the loss function? </li>
<li>Actor's loss function involves computing probability of the action taken given to normal distribution. How can I ensure that mu and sigma of the normal distribution during loss function computation actually match the ones were during prediction?</li>
<li>The way it is in the original, the actor's loss function doesn't care about y_pred, it only does about action that was chosen while interacting with the environment. This seems to be wrong, but I'm not sure how.</li>
</ol>

<p>The code I have so far: <a href=""https://gist.github.com/nevkontakte/beb59f29e0a8152d99003852887e7de7"" rel=""nofollow noreferrer"">https://gist.github.com/nevkontakte/beb59f29e0a8152d99003852887e7de7</a></p>

<p>Edit: I suppose some of my confusion stems from a poor understanding of magic behind gradient computation in Keras/TensorFlow, so any pointers there would be appreciated.</p>
",759134,,759134,,2020-01-04 13:17:30,2020-01-18 18:14:02,A2C algorithm in tf.keras: actor loss function,<python><tensorflow><keras><reinforcement-learning>,2,2,,,,CC BY-SA 4.0,
1423,58509177,1,58509842,,2019-10-22 17:04:23,,0,24,"<p>I'm want to build DQNAgent but I have a problem whit the data or with the NN(I'm not sure).  I tried to solve it by changing the shape of the array but I'm always getting the same error:<strong>ValueError: setting an array element with a sequence.</strong></p>

<p><strong>This is the data:</strong></p>

<p>state(array,shape:(2,2)):</p>

<pre><code>       [4499.74073719,  121.58564876],
       [4669.91329184,   42.37631835]])
  array([[-2000.        ,   290.01270128]])]
 [array([[-4370.,   800.],
       [ -635.,   800.]]) -12.0]] 
</code></pre>

<p>Label(Q value):</p>

<pre><code>array([0.23,1,3,0.1234])
</code></pre>

<p>The programe need to prodict the q_value baice on the data.
<strong>This is the code:</strong></p>

<pre><code>    model=Sequential()
    model.add(Flatten(input_shape=(2,2)))
    model.add(Dense(24,  activation='relu'))
    model.add(Dense(24, activation='relu'))
    model.add(Dense(acton_size, activation='linear'))
    model.compile(loss='mse',optimizer=Adam(lr=learning_rate))
    return model
</code></pre>

<pre><code>    r_locs, i_locs, c_locs, ang, score=Game_step(random.randint(0,4))
    state=np.array([r_locs, i_locs, c_locs, ang])
    state=np.reshape(state,[2,2])
</code></pre>

<p><strong>ERROR:</strong></p>

<pre><code>return array(a, dtype, copy=False, order=order)
ValueError: setting an array element with a sequence.
</code></pre>

<p>Any idea how to solve this?
<strong>THANKS</strong>ðŸ˜</p>
",11724935,,,,,2019-10-22 17:52:42,Error: Setting an Array Element with a Sequence. Keras Neural Network,<python><arrays><keras><neural-network><reinforcement-learning>,1,0,,,,CC BY-SA 4.0,
1426,58949680,1,58953694,,2019-11-20 08:22:32,,2,67,"<p>I'm trying to apply reinforcement learning to a round-based game environment. Each round I get a (self-contained / nearly markovian) state and have to provide an action to progress in the world. Because there exist some long-term strategies (develop resource ""A"", wait few rounds for development, use resource ""A""), I'm thinking of using an LSTM layer in my neural net. During training I can feed sequences of rounds into the network to train the LSTM; however, during the testing phase I'm only able to provide the current state (this is a hard requirement).</p>

<p>I'm wondering whether LSTMs are a viable option here or if they are not suitable for this usage, because I can only provide one state during testing / deployment.</p>
",5176889,,,,,2019-11-21 08:57:10,Using LSTMs to predict from single-element sequence,<tensorflow><machine-learning><keras><lstm><reinforcement-learning>,1,0,0,,,CC BY-SA 4.0,
1427,59093814,1,59094012,,2019-11-28 17:21:57,,1,264,"<p>I'm trying to implement a DQN agent, so a Deep Reinforcement Learning solution.</p>

<p>I should decrease the learning rate after some iterations, without changing the model weights or anything else. In RL problems, the ''fit'' is done after a certain number of new events are collected, and each ''fit'' only has 1 single epoch, so the decaying rates that </p>

<p>at the moment, the only solution I found is doing the following:</p>

<pre><code>if(time%1000==0):
    learning_rate=learning_rate*0.75
    mainQN_temp=QNetwork(hidden_size=hidden_size, learning_rate=learning_rate)
    mainQN_temp.model.load_weights(""./save/dqn-angle3-""+str(t)+"".h5"")
    mainQN=mainQN_temp



class QNetwork:
    def __init__(self, learning_rate=0.01, state_size=4,
                 action_size=5, hidden_size=32):

        # some layers in here

    self.optimizer = Adam(lr=learning_rate)
    self.model.compile(loss='mse', optimizer=self.optimizer)
</code></pre>

<p>which is the most inefficient thing possible. I tried referencing things like mainQN.optimizer.lr with no luck.</p>
",1834153,,10133797,,2019-11-28 17:38:52,2019-11-28 17:45:13,Can I change dynamically the learning rate of a Neural Network in Keras?,<keras><deep-learning><reinforcement-learning>,1,0,,,,CC BY-SA 4.0,
1429,59823099,1,59823326,,2020-01-20 12:15:40,,0,262,"<p>iÂ´m new to reinforcement learning, and was trying to use LSTM for reinforcement learning for a space invaders agent. 
I tried to use the network found in this <a href=""https://arxiv.org/pdf/1507.06527.pdf"" rel=""nofollow noreferrer"">paper</a> but I kept having trouble:</p>

<p>-If i use conv2D the dimensions with the LSTM dont fit and i get this error:</p>

<blockquote>
  <p>ValueError: Input 0 is incompatible with layer conv_lst_m2d_1:
  expected ndim=5, found ndim=4</p>
</blockquote>

<p>This is the code:</p>

<pre><code>    self.model = Sequential()
    self.model.add(Conv2D(32,kernel_size=8,strides=4,activation='relu',input_shape=(None,84,84,1)))
    self.model.add(Conv2D(64,kernel_size=4,strides=2,activation='relu'))
    self.model.add(Conv2D(64,kernel_size=3, strides=1,activation='relu'))
    self.model.add(ConvLSTM2D(512, kernel_size=(3,3), padding='same', return_sequences=False))
    self.model.add(Dense(4, activation='relu'))
    self.model.compile(loss='mse', optimizer=Adam(lr=0.0001))
    self.model.summary()
</code></pre>

<p>-And if I use Conv3D that outputs a 5D tensor I cant use one image as an input:</p>

<blockquote>
  <p>ValueError: Error when checking input: expected conv3d_1_input to have
  5 dimensions, but got array with shape (1, 84, 84, 1)</p>
</blockquote>

<p>Code: </p>

<pre><code>    self.model.add(Conv3D(32,kernel_size=8,strides=4,activation='relu',input_shape=(None,84,84,1)))
    self.model.add(Conv3D(64,kernel_size=4,strides=2,activation='relu'))
    self.model.add(Conv3D(64,kernel_size=3, strides=1,activation='relu'))
    self.model.add(ConvLSTM2D(512, kernel_size=(3,3), padding='same', return_sequences=False))
    self.model.add(Dense(4, activation='relu'))
    self.model.compile(loss='mse', optimizer=Adam(lr=0.0001))
    self.model.summary()
</code></pre>

<p>(edit)</p>

<p>Network summary(of the second network): </p>

<pre><code>_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
conv3d_1 (Conv3D)            (None, None, 20, 20, 32)  16416     
_________________________________________________________________
conv3d_2 (Conv3D)            (None, None, 9, 9, 64)    131136    
_________________________________________________________________
conv3d_3 (Conv3D)            (None, None, 7, 7, 64)    110656    
_________________________________________________________________
conv_lst_m2d_1 (ConvLSTM2D)  (None, 7, 7, 512)         10618880  
_________________________________________________________________
dense_1 (Dense)              (None, 7, 7, 4)           2052      
=================================================================
</code></pre>

<p>And data input shape is: <code>(84, 84, 1)</code> </p>
",12747547,,8036123,,2020-01-20 14:34:30,2020-01-20 20:56:40,LSTM network for space-invaders RL (Keras),<python><machine-learning><keras><lstm><reinforcement-learning>,2,3,,,,CC BY-SA 4.0,
1431,65732431,1,65732504,,2021-01-15 07:59:13,,0,299,"<p>I write some tensorflow code about Deep Successor Representation (DSQ) reinforcement learning:</p>
<pre class=""lang-py prettyprint-override""><code>class RL_Brain():
    def __init__(self, n_features, n_action, memory_size=10, batch_size=32, gamma=0.9, phi_size=15):
        self.n_features = n_features
        self.n_actions = n_action
        self.memory_size = memory_size
        self.replay_buffer = np.zeros((self.memory_size, n_features * 2 + 2), np.float)
        self.count = 0
        self.batch_size = batch_size
        self.gamma = gamma
        self.phi_size = phi_size
        self.epsilon = 0.9  # é»˜è®¤æœ‰0.1çš„éšæœºåº¦
        self.model, self.mus_model = self.build_model()
        self.opt = Adam()

    def build_model(self):
        input_state = Input(shape=(self.n_features,), name='input')
        input_phi = Input(shape=(self.phi_size,), name='input_phi')

        layer1 = Dense(32, 'relu', name='encode/layer1')(input_state)
        layer2 = Dense(32, 'relu', name='encode/layer2')(layer1)
        layer3 = Dense(10, 'relu', name='encode/layer3')(layer2)
        phi = Dense(15, 'relu', name='phi')(layer3)
        decoder1 = Dense(10, 'relu', name='decode/layer1')(phi)
        decoder2 = Dense(32, 'relu', name='decode/layer2')(decoder1)
        decoder3 = Dense(32, 'relu', name='decode/layer3')(decoder2)
        s_hat = Dense(self.n_features, name='output_s_hat')(decoder3)

        stop_grad_phi = tf.stop_gradient(phi)
        R = Dense(1, name='R', use_bias=False)(stop_grad_phi)
        mus = []
        for i in range(self.n_actions):
            mu = Dense(10, 'relu', name='mu/m%s/layer1' % i)(input_phi)
            mu = Dense(10, 'relu', name='mu/m%s/layer2' % i)(mu)
            mu = Dense(15, 'relu', name='mu/m%s/layer3' % i)(mu)
            m = Model(inputs=input_phi, outputs=mu)
            mus.append(m)

        outputs = [phi, R, s_hat]
        model = Model(inputs=input_state, outputs=outputs)
        return model, mus
    def learn(self):
        # choices = np.random.choice(self.count if self.count &lt; self.memory_size else self.memory_size, self.batch_size, replace=True)
        states = np.expand_dims(self.replay_buffer[(self.count-1) % self.memory_size, :self.n_features], 0)
        states_ = np.expand_dims(self.replay_buffer[(self.count-1) % self.memory_size, -self.n_features:], 0)
        r = np.expand_dims(self.replay_buffer[(self.count-1) % self.memory_size, self.n_features + 1], 0)
        a = self.replay_buffer[(self.count-1) % self.memory_size, self.n_features]
        o_phi_t, o_r, o_s_hat = self.model(states)  # æ¨¡åž‹è¾“å‡ºçš„phi, reward, s_hat
        print(o_r)
        # Training auto-encoder loss and reward loss.
        with tf.GradientTape() as tape:
            loss1 = tf.keras.losses.mean_squared_error(states, self.model(states)[2])
            loss2 = tf.keras.losses.mean_squared_error(r, self.model(states)[1])
            loss = loss1 + loss2
-----&gt;      self.opt.minimize(loss, self.model.trainable_variables, tape=tape)

        o_phi_t_, _, __ = self.model(states_)
        mus_ = tf.squeeze(tf.stack([self.mus_model[i](o_phi_t_) for i in range(self.n_actions)]))
        w = tf.Variable(self.model.get_layer('R').get_weights()[0])
        q = tf.matmul(mus_, w)
        max_q_action_index = tf.argmax(tf.squeeze(q)).numpy()
        # Training M loss
        # =========
        with tf.GradientTape() as tape:
            loss = tf.keras.losses.mean_squared_error(o_phi_t + self.gamma * mus_[max_q_action_index],self.mus_model[max_q_action_index](o_phi_t))
----&gt;       self.opt.minimize(loss, self.mus_model[action_index].trainable_variables, tape=tape)

</code></pre>
<p>When I run the <code>learn</code> function, I get the following error:</p>
<blockquote>
<p>Traceback (most recent call last):
File &quot;/Users/wangheng/workspace/pycharmworkspace/MLAlgorithm/reinforcement_learning/SR/dsr_brain_keras.py&quot;, line 67, in learn
self.opt.minimize(loss, self.model.trainable_variables, tape=tape)
File &quot;/Users/wangheng/app/anaconda3/lib/python3.8/site-packages/tensorflow/python/keras/optimizer_v2/optimizer_v2.py&quot;, line 496, in minimize
grads_and_vars = self._compute_gradients(
File &quot;/Users/wangheng/app/anaconda3/lib/python3.8/site-packages/tensorflow/python/keras/optimizer_v2/optimizer_v2.py&quot;, line 543, in _compute_gradients
with tape:
File &quot;/Users/wangheng/app/anaconda3/lib/python3.8/site-packages/tensorflow/python/eager/backprop.py&quot;, line 858, in <strong>enter</strong>
self._push_tape()
File &quot;/Users/wangheng/app/anaconda3/lib/python3.8/site-packages/tensorflow/python/eager/backprop.py&quot;, line 869, in _push_tape
raise ValueError(&quot;Tape is still recording, This can happen if you try to &quot;
ValueError: Tape is still recording, This can happen if you try to re-enter an already-active tape.</p>
</blockquote>
<p>I guess it may be some error occurred in <code>self.opt.minimize()</code> but I do not know how to solve it.</p>
",14990784,,4685471,,2021-01-15 12:05:55,2021-01-15 12:05:55,"ValueError: Tape is still recording, This can happen if you try to re-enter an already-active tape",<python><tensorflow><keras><reinforcement-learning>,1,0,,,,CC BY-SA 4.0,
1432,67311128,1,67343177,,2021-04-29 04:40:17,,1,107,"<p>I am trying to build a NN model for my Policy Gradient (deep reinforcement learning) agent by using Keras Functional API. What I intend to do is to mask invalid actions by reducing their probability distribution to zero in the logit layer:</p>
<pre class=""lang-py prettyprint-override""><code>def __build_policy_network(self):
    inputs = keras.layers.Input(shape=(self.input_dim,))
    advantages = keras.layers.Input(shape=(1,))
    valid_actions = keras.layers.Input(shape=(3,))
    dense_1 = keras.layers.Dense(units=self.fc1_size, activation=&quot;relu&quot;, kernel_initializer=&quot;he_uniform&quot;)(inputs)
    dense_2 = keras.layers.Dense(units=self.fc2_size, activation=&quot;relu&quot;, kernel_initializer=&quot;he_uniform&quot;)(dense_1)
    probs_logits = keras.layers.Dense(units=self.nb_actions, activation='softmax')(dense_2)
    masked_probs = keras.layers.Multiply()([probs_logits, valid_actions])
    probs = keras.layers.Lambda(lambda x: x / keras.backend.sum(x, axis=1))(masked_probs)
    
       def custom_loss(y_true, y_pred):
           out = keras.backend.clip(y_pred, 1e-8, 1 - 1e-8)
           log_lik = y_true * keras.backend.log(out)
           return keras.backend.sum(-log_lik * advantages)
    
     policy = keras.models.Model([inputs, advantages], [probs])
     policy.compile(optimizer=keras.optimizers.Adam(lr=self.alpha), loss=custom_loss)
     predict = keras.models.Model([inputs, valid_actions], [probs])
     return policy, predict
</code></pre>
<p>However, I run into the infamous error <code>ValueError: Graph disconnected: cannot obtain value for tensor Tensor(&quot;input_3:0&quot;, shape=(None, 3), dtype=float32) at layer &quot;multiply&quot;.</code> When I comment out either of the <code>advantages</code> or <code>valid_actions</code> input layers (and of course, removing their corresponding lines) I can successfully run the code. I should mention that <code>valid_actions</code> input layer is only passed to mask invalid probabilities and is not required for loss calculation.</p>
<p>I really appreciate it if someone can help me with this.</p>
<p>Thanks in advance for your time</p>
",3156071,,3156071,,2021-04-29 11:45:21,2021-05-01 05:31:42,Difficulty in Connecting Layers with Keras: Graph Disconnected,<tensorflow><keras><deep-learning><reinforcement-learning>,2,0,,,,CC BY-SA 4.0,
1437,60998288,1,60999317,,2020-04-02 18:13:42,,0,2358,"<p>I'm trying to implement Temporal attention in a Reinforcement Learning problem using Stable baselines however, I keep getting the mentioned error in the customer policy. I am using TensorFlow version 1.14.
While using an LSTMCell along with RNN class from TensorFlow in my policy.py, I am also initializing a wrapper for attention but I keep getting the following error.</p>

<pre><code>Traceback (most recent call last):
  File ""run.py"", line 60, in &lt;module&gt;
    trainedModel = model_training(featureMatrix, config['env_name'], config['number_of_cpus'], config['total_training_timesteps'], config['policy'])
  File ""/code/src/util/utils.py"", line 88, in model_training
    trained_model = trained_model.train()
  File ""/code/src/util/model/model_training.py"", line 103, in train
    tensorboard_log=self.tensorboard_path).learn(total_timesteps=self.total_training_timesteps, callback=self.callback)
  File ""/venv/lib/python3.7/site-packages/stable_baselines/acktr/acktr.py"", line 119, in __init__
    self.setup_model()
  File ""/venv/lib/python3.7/site-packages/stable_baselines/acktr/acktr.py"", line 148, in setup_model
    1, n_batch_step, reuse=False, **self.policy_kwargs)
  File ""/code/src/util/policy/policy.py"", line 97, in __init__
    rnn = tf.keras.layers.RNN(self._build_rnn_cell())
  File ""/code/src/util/policy/policy.py"", line 165, in _build_rnn_cell
    return tf.keras.layers.StackedRNNCells([self._build_single_cell() for _ in range(3)])
  File ""/code/src/util/policy/policy.py"", line 165, in &lt;listcomp&gt;
    return tf.keras.layers.StackedRNNCells([self._build_single_cell() for _ in range(3)])
  File ""/code/src/util/policy/policy.py"", line 158, in _build_single_cell
    128,
  File ""/code/src/util/policy/attention_wrapper.py"", line 123, in __init__
    super(TemporalPatternAttentionCellWrapper, self).__init__(_reuse=reuse)
TypeError: __init__() missing 1 required positional argument: 'units'
</code></pre>

<p>My policy.py is as follows:</p>

<pre><code>class CustomPolicy(ActorCriticPolicy):
    def __init__(self, sess, ob_space, ac_space, n_env, n_steps, n_batch, reuse=False, **kwargs):
        super(CustomPolicy, self).__init__(sess, ob_space, ac_space, n_env, n_steps, n_batch, reuse=reuse, scale=True)

        with tf.variable_scope(""model"", reuse=reuse):
            rnn = tf.keras.layers.RNN(self._build_rnn_cell())

            feature_layer = rnn(self.processed_obs)

            pi_layers = Sequential([
                Dense(128, input_shape = (256,), 
                      kernel_regularizer=regularizers.l2(0.01), activity_regularizer=regularizers.l1(0.01)),
                Activation('relu'),
                Dense(128, kernel_regularizer=regularizers.l2(0.01), activity_regularizer=regularizers.l1(0.01))
            ])

            pi_latent = pi_layers(feature_layer)

            vf_layers = Sequential([
                Dense(32, input_shape = (256,),
                      kernel_regularizer=regularizers.l2(0.01), activity_regularizer=regularizers.l1(0.01)),
                Activation('relu'),
                Dense(32, kernel_regularizer=regularizers.l2(0.01), activity_regularizer=regularizers.l1(0.01))
            ])

            vf_latent = vf_layers(feature_layer)            
            temp_value_fn = Dense(1, input_shape=(32,))
            value_fn = temp_value_fn(vf_latent)

            self._proba_distribution, self._policy, self.q_value = \
                self.pdtype.proba_distribution_from_latent(pi_latent, vf_latent, init_scale=0.01)

        self._value_fn = value_fn
        self._setup_init()


    def step(self, obs, state=None, mask=None, deterministic=False):
        if deterministic:
            action, value, neglogp = self.sess.run([self.deterministic_action, self.value_flat, self.neglogp],
                                                   {self.obs_ph: obs})
        else:
            action, value, neglogp = self.sess.run([self.action, self.value_flat, self.neglogp],
                                                   {self.obs_ph: obs})
        return action, value, self.initial_state, neglogp

    def proba_step(self, obs, state=None, mask=None):
        return self.sess.run(self.policy_proba, {self.obs_ph: obs})

    def value(self, obs, state=None, mask=None):
        return self.sess.run(self.value_flat, {self.obs_ph: obs})

    def _build_single_cell(self):
        cell = tf.keras.layers.LSTMCell(256)
        cell = TemporalPatternAttentionCellWrapper(
            cell,
            128,
        )
        return cell

    def _build_rnn_cell(self):
        return tf.keras.layers.StackedRNNCells([self._build_single_cell() for _ in range(3)])
</code></pre>

<p>and my Attention Wrapper is as follows:</p>

<pre><code>class TemporalPatternAttentionCellWrapper(tf.keras.layers.LSTMCell):
    def __init__(self,
                 cell,
                 attn_length,
                 units=256,
                 attn_size=None,
                 attn_vec_size=None,
                 input_size=None,
                 state_is_tuple=True,
                 reuse=None):
        """"""Create a cell with attention.
        Args:
            cell: an RNNCell, an attention is added to it.
            attn_length: integer, the size of an attention window.
            attn_size: integer, the size of an attention vector. Equal to
                cell.output_size by default.
            attn_vec_size: integer, the number of convolutional features
                calculated on attention state and a size of the hidden layer
                built from base cell state. Equal attn_size to by default.
            input_size: integer, the size of a hidden linear layer, built from
                inputs and attention. Derived from the input tensor by default.
            state_is_tuple: If True, accepted and returned states are n-tuples,
                where `n = len(cells)`. By default (False), the states are all
                concatenated along the column axis.
            reuse: (optional) Python boolean describing whether to reuse
                variables in an existing scope. If not `True`, and the existing
                scope already has the given variables, an error is raised.
        Raises:
            TypeError: if cell is not an RNNCell.
            ValueError: if cell returns a state tuple but the flag
                `state_is_tuple` is `False` or if attn_length is zero or less.
        """"""
        super(TemporalPatternAttentionCellWrapper, self).__init__(_reuse=reuse)
        if nest.is_sequence(cell.state_size) and not state_is_tuple:
            raise ValueError(""Cell returns tuple of states, but the flag ""
                             ""state_is_tuple is not set. State size is: %s"" %
                             str(cell.state_size))
        if attn_length &lt;= 0:
            raise ValueError(""attn_length should be greater than zero, got %s""
                             % str(attn_length))
        if not state_is_tuple:
            logging.warn(
                ""%s: Using a concatenated state is slower and will soon be ""
                ""deprecated.    Use state_is_tuple=True."", self)
        if attn_size is None:
            attn_size = 2880
        if attn_vec_size is None:
            attn_vec_size = attn_size
        self._state_is_tuple = state_is_tuple
        self._cell = cell
        self._attn_vec_size = attn_vec_size
        self._input_size = input_size
        self._attn_size = attn_size
        self._attn_length = attn_length
        self._reuse = reuse
        self._attention_mech = TemporalPatternAttentionMechanism()


    @property
    def state_size(self):
        size = (self._cell.state_size, self._attn_size,
                self._attn_size * self._attn_length)
        if self._state_is_tuple:
            return size
        else:
            return sum(list(size))

    @property
    def output_size(self):
        return self._attn_size

    def call(self, inputs, state):
        """"""Long short-term memory cell with attention (LSTMA).""""""
        print(""TPA Wrapper called"")
        if self._state_is_tuple:
            state, attns, attn_states = state
        else:
            states = state
            state = tf.slice(states, [0, 0], [-1, self._cell.state_size])
            attns = tf.slice(states, [0, self._cell.state_size],
                             [-1, self._attn_size])
            attn_states = tf.slice(
                states, [0, self._cell.state_size + self._attn_size],
                [-1, self._attn_size * self._attn_length])
        attn_states = tf.reshape(attn_states,
                                 [-1, self._attn_length, self._attn_size])
        input_size = self._input_size
        if input_size is None:
            input_size = inputs.get_shape().as_list()[1]

        temp_inputs = Dense(input_size, input_shape = (5760,), use_bias=True)

        inputs = temp_inputs(tf.concat([inputs, attns], 1))

        lstm_output, new_state = self._cell(inputs)

        if self._state_is_tuple:
            new_state_cat = tf.concat(nest.flatten(new_state), 1)
        else:
            new_state_cat = new_state
        new_attns, new_attn_states = self._attention_mech(
            new_state_cat, attn_states, self._attn_size, self._attn_length,
            self._attn_vec_size)

        with tf.variable_scope(""attn_output_projection""):
            temp_output = Sequential([
                Dense(self._attn_size, input_shape = (2880,),
                      use_bias=True),
            ])

            output = dense(tf.concat([lstm_output, new_attns], 1))

        new_attn_states = tf.concat(
            [new_attn_states, tf.expand_dims(output, 1)], 1)
        new_attn_states = tf.reshape(new_attn_states,
                                     [-1, self._attn_length * self._attn_size])
        new_state = (new_state, new_attns, new_attn_states)
        if not self._state_is_tuple:
            new_state = tf.concat(list(new_state), 1)

        return output, new_state

</code></pre>

<p>The error occurs in the line</p>

<pre><code>super(TemporalPatternAttentionCellWrapper, self).__init__(_reuse=reuse) in the init function of the wrapper.
</code></pre>

<p>Any help would be greatly appreciated and please let me know if more information is needed.</p>
",5610841,,,,,2020-04-02 19:13:51,TypeError: __init__() missing 1 required positional argument: 'units' in LSTMCell,<python><tensorflow><keras><deep-learning><reinforcement-learning>,1,2,,,,CC BY-SA 4.0,
1441,47643678,1,47655115,,2017-12-04 23:30:37,,3,1514,"<p>I am trying to write my own DQN in Python, using keras. I think my logic is correct. I am trying it on the CartPole environment, but the rewards are not increasing after 50,000 episodes. Any help will be appreciated. Currently I am not looking to the dueling or the Double DQN part. </p>

<pre><code>class ReplayBuffer:
        def __init__(self, size=100000):
            self.buffer=deque(maxlen=size)

        def sample(self, sample_size):
            return random.sample(self.buffer, sample_size)

        def add_to_buffer(self, experience):
            self.buffer.append(experience)

    def generator(number):
        return(i for i in range(number))

    def epsilon_greedy_policy(q_values, epsilon):
        number_of_actions =len(q_values)
        action_probabilites = np.ones(number_of_actions, dtype=float)*epsilon/number_of_actions
        best_action = np.argmax(q_values)
        action_probabilites[best_action]+= (1-epsilon)
        return np.random.choice(number_of_actions, p=action_probabilites)

    class DQNAgent:
        def __init__(self, env, model, gamma):
            self.env=env
            self.model=model
            self.replay_buffer=ReplayBuffer()
            self.gamma=gamma
            self.state_dim=env.observation_space.shape[0]

        def train_model(self, training_data, training_label):
            self.model.fit(training_data, training_label, batch_size=32, verbose=0)

        def predict_one(self, state):
            return self.model.predict(state.reshape(1, self.state_dim)).flatten()

        def experience_replay(self, experiences):
            import pdb; pdb.set_trace()
            states, actions, rewards, next_states=zip(*[[experience[0], experience[1], experience[2], experience[3]] for experience in experiences])
            states=np.asarray(states)
            place_holder_state=np.zeros(self.state_dim)
            next_states_ = np.asarray([(place_holder_state if next_state is None else next_state) for next_state in next_states])
            q_values_for_states=self.model.predict(states)
            q_values_for_next_states=self.model.predict(next_states_)
            for x in generator(len(experiences)):
                y_true=rewards[x]
                if next_states[x].any():
                    y_true +=self.gamma*(np.amax(q_values_for_next_states[x]))
                q_values_for_states[x][actions[x]]=y_true
            self.train_model(states, q_values_for_states)

        def fit(self, number_of_epsiodes, batch_size):
            for _ in generator(number_of_epsiodes):
                total_reward=0
                state=env.reset()
                while True:
                    #self.env.render()
                    q_values_for_state=self.predict_one(state)
                    action=epsilon_greedy_policy(q_values_for_state, 0.1)
                    next_state, reward, done, _=env.step(action)
                    self.replay_buffer.add_to_buffer([state, action, reward, next_state])
                    state = next_state
                    total_reward += reward
                    if len(self.replay_buffer.buffer) &gt; 50:
                        experience=self.replay_buffer.sample(batch_size)
                        self.experience_replay(experience)
                    if done:
                       break
                print(""Total reward:"", total_reward)


    env = gym.make('CartPole-v0')
    model=create_model(env.observation_space.shape[0], env.action_space.n)
    agent=DQNAgent(env, model, 0.99)
    agent.fit(100000, 32)'
</code></pre>
",1388783,,754136,,2017-12-05 05:35:39,2017-12-05 17:08:36,DQN not working Properly,<python><keras><reinforcement-learning>,2,0,0,,,CC BY-SA 3.0,
1444,64863256,1,64873414,,2020-11-16 17:59:44,,0,511,"<p>here is my code and written in keras</p>
<pre><code>import keras
from keras import layers
from keras import Sequential
from keras.layers import Dense, Flatten
import numpy as np
from keras.engine.topology import Input
from keras.engine.training import Model

class _Model:
  def __init__(self,state,n_dims, n_action):
    self.n_action=n_action
    self.n_dims=n_dims
    self.state=state
    self.model=self.build_model()
  def build_model(self):
    inx=model=Input((10,16))
    model=Flatten()(model)
    model=Dense(512, activation=None)(model)
    model=Dense(512, activation=None)(model)
    p_model=Dense(self.n_action, activation='sigmoid')(model)
    v_model=Dense(1, activation='tanh')(model)
    _model=Model(inx,[p_model,v_model])
    losses = ['categorical_crossentropy', 'mean_squared_error']
    _model.compile(loss=losses, optimizer='adam')
    print(_model.summary())
    return _model
  def predict(self,state):
    return self.model.predict(state)
  def train(self, state, action_probability, leaf_value):
    batch_size=11
    state=np.array(state)
    action_probability=np.array(action_probability)
    leaf_value=np.array(leaf_value)
    self.model.fit(state, [action_probability, leaf_value],batch_size=batch_size,verbose=1)
    loss=self.model.evaluate(state, [action_probability, leaf_value],batch_size=batch_size,verbose=0)
    return loss[0]

state=[ 4321432141243124,
        1423123424143213,
        4321432143213421,
        4321431241324323,
        1243121234214334,
        4123123421342314,
        4321432434212412,
        4121432121121343,
        4123413412412321,
        4123413412413431,
]
m=_Model(state,16,4)
m.train(state,[0.1,0.5,0.4,0.7],0.4)
</code></pre>
<p>This implementation is for alphago zero. I'm trying to implement the model that has two outputs. The two values (P, v)  that p is action probability and v is winning probability.
what's the problem? what is matter with this code?
The error says the list is out of index but I don't know what list. Should I change the input_shape?</p>
<pre><code>---------------------------------------------------------------------------
IndexError                                Traceback (most recent call last)
&lt;ipython-input-9-c5b4b76d5be5&gt; in &lt;module&gt;()
     48 ]
     49 m=_Model(state,16,4)
---&gt; 50 m.train(state,[0.1,0.5,0.4,0.7],0.4)

6 frames
&lt;ipython-input-9-c5b4b76d5be5&gt; in train(self, state, action_probability, leaf_value)
     32     action_probability=np.array(action_probability)
     33     leaf_value=np.array(leaf_value)
---&gt; 34     self.model.fit(state, [action_probability, leaf_value],batch_size=batch_size,verbose=1)
     35     loss=self.model.evaluate(state, [action_probability, leaf_value],batch_size=batch_size,verbose=0)
     36     return loss[0]

/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py in _method_wrapper(self, *args, **kwargs)
    106   def _method_wrapper(self, *args, **kwargs):
    107     if not self._in_multi_worker_mode():  # pylint: disable=protected-access
--&gt; 108       return method(self, *args, **kwargs)
    109 
    110     # Running inside `run_distribute_coordinator` already.

/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py in fit(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)
   1061           use_multiprocessing=use_multiprocessing,
   1062           model=self,
-&gt; 1063           steps_per_execution=self._steps_per_execution)
   1064 
   1065       # Container that configures and calls `tf.keras.Callback`s.

/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/data_adapter.py in __init__(self, x, y, sample_weight, batch_size, steps_per_epoch, initial_epoch, epochs, shuffle, class_weight, max_queue_size, workers, use_multiprocessing, model, steps_per_execution)
   1115         use_multiprocessing=use_multiprocessing,
   1116         distribution_strategy=ds_context.get_strategy(),
-&gt; 1117         model=model)
   1118 
   1119     strategy = ds_context.get_strategy()

/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/data_adapter.py in __init__(self, x, y, sample_weights, sample_weight_modes, batch_size, epochs, steps, shuffle, **kwargs)
    273     inputs = pack_x_y_sample_weight(x, y, sample_weights)
    274 
--&gt; 275     num_samples = set(int(i.shape[0]) for i in nest.flatten(inputs))
    276     if len(num_samples) &gt; 1:
    277       msg = &quot;Data cardinality is ambiguous:\n&quot;

/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/data_adapter.py in &lt;genexpr&gt;(.0)
    273     inputs = pack_x_y_sample_weight(x, y, sample_weights)
    274 
--&gt; 275     num_samples = set(int(i.shape[0]) for i in nest.flatten(inputs))
    276     if len(num_samples) &gt; 1:
    277       msg = &quot;Data cardinality is ambiguous:\n&quot;

/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/tensor_shape.py in __getitem__(self, key)
    885       else:
    886         if self._v2_behavior:
--&gt; 887           return self._dims[key].value
    888         else:
    889           return self._dims[key]

IndexError: list index out of range
</code></pre>
",14649896,,14649896,,2020-11-16 18:53:13,2020-11-17 10:26:34,list index out of range error when I want fit a model in keras,<python><keras><deep-learning><reinforcement-learning><keras-layer>,1,2,,,,CC BY-SA 4.0,
1448,68978463,1,68979613,,2021-08-30 02:45:50,,0,413,"<p>I am making reinforcement learning for CartPole and i meet this problem</p>
<pre><code>model = keras.models.Sequential()
model.add(Dense(8,activation = 'relu'))
model.add(Dense(2,activation = 'linear')
</code></pre>
<p>this is my model</p>
<pre><code>state = env.reset()
print(state)


output:
[-0.00315391 -0.0150189   0.01804181  0.02032083]
</code></pre>
<p>And this is what i got for prediction of my model</p>
<pre><code>model.predict(state)

output:
[[-0.00028523  0.00031606]
 [-0.00135828  0.00150507]
 [ 0.00500827 -0.01125371]
 [ 0.00564091 -0.01267526]]
</code></pre>
<p>Why <code>Dense(2,activation='linear')</code> receiving 2D array?</p>
<p>I was expecting output with (2,1) shape but why model is receiving (2,4) shape?</p>
<p>I found that output has relationship between output node of model and input shape</p>
",14784305,,2423278,,2021-08-30 06:02:17,2021-08-30 06:13:44,Dense layer give me 2D array,<tensorflow><keras><artificial-intelligence><reinforcement-learning>,2,0,,,,CC BY-SA 4.0,
1458,65743558,1,65749350,,2021-01-15 21:07:47,,3,2051,"<p>I am trying to learn a custom environment using the TFAgents package. I am following the Hands-on-ML book (<a href=""https://github.com/ageron/handson-ml2/blob/master/18_reinforcement_learning.ipynb"" rel=""nofollow noreferrer"">Code in colab see cell 129</a>). My aim is to use DQN agent on a custom-written grid world environment.</p>
<p>Grid-World environment:</p>
<pre><code>class MyEnvironment(tf_agents.environments.py_environment.PyEnvironment):

def __init__(self, discount=1.0):
    super().__init__()
    self.discount = discount

    self._action_spec = tf_agents.specs.BoundedArraySpec(shape=(), dtype=np.int32, name=&quot;action&quot;, minimum=0, maximum=3)
    self._observation_spec = tf_agents.specs.BoundedArraySpec(shape=(4, 4), dtype=np.int32, name=&quot;observation&quot;, minimum=0, maximum=1)


def action_spec(self):
    return self._action_spec

def observation_spec(self):
    return self._observation_spec

def _reset(self):
    self._state = np.zeros(2, dtype=np.int32)
    obs = np.zeros((4, 4), dtype=np.int32)
    obs[self._state[0], self._state[1]] = 1
    return tf_agents.trajectories.time_step.restart(obs)

def _step(self, action):
    self._state += [(-1, 0), (+1, 0), (0, -1), (0, +1)][action]
    reward = 0
    obs = np.zeros((4, 4), dtype=np.int32)
    done = (self._state.min() &lt; 0 or self._state.max() &gt; 3)
    if not done:
        obs[self._state[0], self._state[1]] = 1
    if done or np.all(self._state == np.array([3, 3])):
        reward = -1 if done else +10
        return tf_agents.trajectories.time_step.termination(obs, reward)
    else:
        return tf_agents.trajectories.time_step.transition(obs, reward, self.discount)
</code></pre>
<p>And the Q network is:</p>
<pre><code>tf_env = MyEnvironment()

preprocessing_layer = keras.layers.Lambda(lambda obs: tf.cast(obs, np.float32) / 255.)
conv_layer_params=[(32, (2, 2), 1)]
fc_layer_params=[512]
q_net = QNetwork(
    tf_env.observation_spec(),
    tf_env.action_spec(),
    preprocessing_layers=preprocessing_layer,
    conv_layer_params=conv_layer_params,
    fc_layer_params=fc_layer_params)
</code></pre>
<p>And finally, the DQN agent is</p>
<pre><code>train_step = tf.Variable(0)
update_period = 4 # train the model every 4 steps
optimizer = keras.optimizers.RMSprop(lr=2.5e-4, rho=0.95, momentum=0.0, epsilon=0.00001, centered=True)
epsilon_fn = keras.optimizers.schedules.PolynomialDecay(initial_learning_rate=1.0, decay_steps=250000 // update_period, end_learning_rate=0.01)


agent = DqnAgent(tf_env.time_step_spec(),
    tf_env.action_spec(),
    q_network=q_net,
    optimizer=optimizer,
    target_update_period=2000, # &lt;=&gt; 32,000 ALE frames
    td_errors_loss_fn=keras.losses.Huber(reduction=&quot;none&quot;),
    gamma=0.99, # discount factor
    train_step_counter=train_step,
    epsilon_greedy=lambda: epsilon_fn(train_step))  
agent.initialize()
</code></pre>
<p>Directly, running the code gave me the following error trace:</p>
<p>/usr/local/lib/python3.6/dist-packages/gin/config.py in gin_wrapper(*args, **kwargs)
1067       scope_info = &quot; in scope '{}'&quot;.format(scope_str) if scope_str else ''
1068       err_str = err_str.format(name, fn_or_cls, scope_info)
-&gt; 1069       utils.augment_exception_message_and_reraise(e, err_str)
1070
1071   return gin_wrapper</p>
<pre><code>/usr/local/lib/python3.6/dist-packages/gin/utils.py in augment_exception_message_and_reraise(exception, message)
     39   proxy = ExceptionProxy()
     40   ExceptionProxy.__qualname__ = type(exception).__qualname__
---&gt; 41   raise proxy.with_traceback(exception.__traceback__) from None
     42 
     43 

/usr/local/lib/python3.6/dist-packages/gin/config.py in gin_wrapper(*args, **kwargs)
   1044 
   1045     try:
-&gt; 1046       return fn(*new_args, **new_kwargs)
   1047     except Exception as e:  # pylint: disable=broad-except
   1048       err_str = ''

/usr/local/lib/python3.6/dist-packages/tf_agents/agents/dqn/dqn_agent.py in __init__(self, time_step_spec, action_spec, q_network, optimizer, observation_and_action_constraint_splitter, epsilon_greedy, n_step_update, boltzmann_temperature, emit_log_probability, target_q_network, target_update_tau, target_update_period, td_errors_loss_fn, gamma, reward_scale_factor, gradient_clipping, debug_summaries, summarize_grads_and_vars, train_step_counter, name)
    216     tf.Module.__init__(self, name=name)
    217 
--&gt; 218     self._check_action_spec(action_spec)
    219 
    220     if epsilon_greedy is not None and boltzmann_temperature is not None:

/usr/local/lib/python3.6/dist-packages/tf_agents/agents/dqn/dqn_agent.py in _check_action_spec(self, action_spec)
    293 
    294     # TODO(oars): Get DQN working with more than one dim in the actions.
--&gt; 295     if len(flat_action_spec) &gt; 1 or flat_action_spec[0].shape.rank &gt; 0:
    296       raise ValueError(
    297           'Only scalar actions are supported now, but action spec is: {}'

AttributeError: 'tuple' object has no attribute 'rank'
  In call to configurable 'DqnAgent' (&lt;class 'tf_agents.agents.dqn.dqn_agent.DqnAgent'&gt;)
</code></pre>
<p><strong>What I have tried:</strong> Following the suggestions <a href=""https://github.com/tensorflow/agents/issues/363"" rel=""nofollow noreferrer"">here</a> The modified</p>
<pre><code>self._action_spec = tf_agents.specs.BoundedArraySpec(shape=(), dtype=np.int32, name=&quot;action&quot;, minimum=0, maximum=3)
self._observation_spec = tf_agents.specs.BoundedArraySpec(shape=(4, 4), dtype=np.int32, name=&quot;observation&quot;, minimum=0, maximum=1)
</code></pre>
<p>to:</p>
<pre><code>self._action_spec = tf_agents.specs.BoundedTensorSpec(
    shape=(), dtype=np.int32, name=&quot;action&quot;, minimum=0, maximum=3)
self._observation_spec = tf_agents.specs.BoundedTensorSpec(
    shape=(4, 4), dtype=np.int32, name=&quot;observation&quot;, minimum=0, maximum=1)
</code></pre>
<p>However, this resulted in:</p>
<pre><code>---------------------------------------------------------------------------
TypeError                                 Traceback (most recent call last)
&lt;ipython-input-53-ce737b2b13fd&gt; in &lt;module&gt;()
     21 
     22 
---&gt; 23 agent = DqnAgent(tf_env.time_step_spec(),
     24     tf_env.action_spec(),
     25     q_network=q_net,

1 frames
/usr/local/lib/python3.6/dist-packages/tf_agents/environments/py_environment.py in time_step_spec(self)
    147       the step_type, reward, discount, and observation structure.
    148     &quot;&quot;&quot;
--&gt; 149     return ts.time_step_spec(self.observation_spec(), self.reward_spec())
    150 
    151   def current_time_step(self) -&gt; ts.TimeStep:

/usr/local/lib/python3.6/dist-packages/tf_agents/trajectories/time_step.py in time_step_spec(observation_spec, reward_spec)
    388           'Expected observation and reward specs to both be either tensor or '
    389           'array specs, but saw spec values {} vs. {}'
--&gt; 390           .format(first_observation_spec, first_reward_spec))
    391   if isinstance(first_observation_spec, tf.TypeSpec):
    392     return TimeStep(

TypeError: Expected observation and reward specs to both be either tensor or array specs, but saw spec values BoundedTensorSpec(shape=(4, 4), dtype=tf.int32, name='observation', minimum=array(0, dtype=int32), maximum=array(1, dtype=int32)) vs. ArraySpec(shape=(), dtype=dtype('float32'), name='reward')
</code></pre>
<p>I understand the reward is the issue: So, added an extra line</p>
<pre><code>self._reward_spec = tf_agents.specs.TensorSpec((1,), np.dtype('float32'), 'reward')
</code></pre>
<p>but still resulted in the same error. Is there anyway I can solve this:</p>
",3701747,,,,,2021-06-02 22:36:28,Custom environment using TFagents,<tensorflow><keras><reinforcement-learning>,1,0,0,,,CC BY-SA 4.0,
1461,66268742,1,66366863,,2021-02-18 21:58:12,,2,868,"<p>My problem seems to be very common.</p>
<p>I am doing some reinforcement learning using a vanilla policy gradient method. The environment is just a simple one period game where the state and action spaces are the real line. The agent is a neural network with two output heads that I build manually using dense layers from Keras, e.g. my first hidden layer would be</p>
<pre><code>layers.Dense(NH[0], activation =&quot;relu&quot;, \
             kernel_initializer=initializers.GlorotNormal())(inputs)
</code></pre>
<p>where NH contains a list of number of neurons for hidden layers. The outputs are the mean and standard deviation for my gaussian policy. I don't if this part matters, but I included it nonetheless.</p>
<p>The environment is simple: the state is a normal variable, the action is some real scalar, and there is just one period. I run the policy a bunch of times, collect the resulting batch and use the tools from tf.GradientTape() to update the network on the basis of a custom loss function. I have no problem running that code thousands of times to see the algorithm learn.</p>
<p>The real problem is that I'd like to run the learning process multiple times, each time re-initializing the network weights randomly to have distributions for the history of rewards, but if I run all of this in a loop the computer freezes rapidly. Apparently, this is a very common problem with Keras and Tensorflow, one that people have been complaining about for years and it is still a problem... Now, I have tried the usual solutions. <a href=""https://stackoverflow.com/questions/58137677/keras-model-training-memory-leak"">Here</a>, people suggested adding something like the following at the end of the loop so that before I reinitialize the network I get a clean slate.</p>
<pre><code>keras.backend.clear_session()
gc.collect()
del actor
</code></pre>
<p>This doesn't solve the problem. Then, I saw someone gave a function that went a little further</p>
<pre><code>def reset_keras(model):

# Clear model, if possible
try:
    del model
except:
    pass

# Garbage collection
gc.collect()

# Clear and close tensorflow session
session = K.get_session() # Get session
K.clear_session()         # Clear session
session.close()           # Close session

# Reset all tensorflow graphs
tf.compat.v1.reset_default_graph()
</code></pre>
<p>And that doesn't work either. I also tried moving around the order of the first three commands and it doesn't work either...</p>
<p>Anyone has any idea how to solve the problem? It would also be useful to know <strong>why</strong> this happens. I'd also like to know how to profile memory usage here so that I don't have to wait 4 hours to learn the computer is freezing again with the new solution.</p>
<p>In fact, if you have a minimal working example where you can demonstrate the code doesn't lead to exploding memory use, I would be very much disposed to re-code the whole damn thing from scratch to stop the problem. As a side note, why haven't the developers solve this issue? It's the only package on both R and Python where this has ever happened to me...</p>
<p>EDIT
As asked, I provide a minimal working example of the issue. I made up a quick game: it's a moving target where the optimal action is to play some multiple of the state value which yields a reward of 0.</p>
<p>I wrote down an actor class and used a simple linear regression as a critic which may be turned off. If you look at the memory usage, it is climbing... That game won't crash my computer unless I play it a lot more, but it shows that memory usage increases.</p>
<pre><code>import numpy      as np
import psutil

import tensorflow                    as tf
import tensorflow.keras              as keras
import tensorflow.keras.layers       as layers
import tensorflow.keras.initializers as initializers

import tensorflow.python.keras.backend as kb
import matplotlib.pyplot as plt

BATCH    = 10
MC_DRAWS = 2000
M        = 10

# Training options
LR = 0.01
def display_memory():
    print( f'{round(psutil.virtual_memory().used/2**30, 2)} GB' )

class Actor:

    def __init__(self):
        self.nn    = self.make_actor()
        self.batch = BATCH
        self.opt   = keras.optimizers.Adam( learning_rate = LR )

    def make_actor(self):
        inputs = layers.Input( shape=(1) )
        hidden = layers.Dense(5, activation='relu', 
                              kernel_initializer=initializers.GlorotNormal() )(inputs)
        mu     = layers.Dense(1, activation='linear',
                          kernel_initializer=initializers.GlorotNormal() )(hidden)
        sigma  = layers.Dense(1, activation='softplus',
                          kernel_initializer=initializers.GlorotNormal() )(hidden)
    
        nn  = keras.Model(inputs=inputs, outputs=[mu, sigma])
    
        return nn

    def update_weights(self, state, action, reward):

        # Get proper format
        state  = tf.constant(state,  dtype='float32', shape=(self.batch,1))
        action = tf.constant(action, dtype='float32', shape=(self.batch,1))
        reward = tf.constant(reward, dtype='float32', shape=(self.batch,1))
    
        # Update Policy Network Parameters
        with tf.GradientTape() as tape:   
            # Compute Gaussian loss
            loss_value = self.custom_loss(state, action, reward)
            loss_value = tf.math.reduce_mean( loss_value, keepdims=True )
        
            # Compute gradients
            grads = tape.gradient(loss_value, self.nn.trainable_variables)
 
            # Apply gradients to update network weights
            self.opt.apply_gradients(zip(grads, self.nn.trainable_variables))
        
    def custom_loss(self, state, action, reward):
        # Obtain mean and standard deviation
        nn_mu, nn_sigma = self.nn(state)
    
        # Gaussian pdf
        pdf_value = tf.exp(-0.5 *((action - nn_mu) / (nn_sigma))**2) *\
                    1/(nn_sigma*tf.sqrt(2 *np.pi))
                    
        # Log probabilities
        log_prob  = tf.math.log( pdf_value + 1e-5 )
    
        # Compute loss
        loss_actor = -reward * log_prob
    
        return loss_actor

class moving_target_game:

    def __init__(self):
        self.action_range = [-np.inf, np.inf]
        self.state_range  = [1, 2]
        self.reward_range = [-np.inf, 0]

    def draw(self):
        return np.random.ranint(low  = self.state_range[0],
                            high = self.state_range[1])

    def get_reward(self, action, state):
        return -(5*state - action)**2

class Critic:  
    def __init__(self):
    
        self.order      = 3
        self.projection = None

    def predict(self, state, reward):
    
        # Enforce proper format
        x = np.array( state ).reshape(-1,1)
        y = np.array( reward ).reshape(-1,1)
    
        # Make regression matrix
        X = np.ones( shape = x.shape )
        for i in range( self.order ):
            X = np.hstack( (X, x**(i+1)) )
        
        # Prediction
        xt = x.transpose()
        P  = x @ np.linalg.inv( xt @ x  ) @ xt
        Py = P @ y
    
        self.projection = P
    
        return Py

#%% Moving Target Game with Actor and Actor-Critic

do_actor_critic = True

display_memory()

history    = np.zeros( shape=(MC_DRAWS, M) )
env        = moving_target_game()

for m in range(M):

    # New Actor Network
    actor  = Actor()

    if do_actor_critic:
        critic = Critic()

    for i in range(MC_DRAWS):
    
        state_tape  = []
        action_tape = []
        reward_tape = []
    
        for j in range(BATCH):
        
            # Draw state
            state = env.draw()
            s     = tf.constant([state], dtype='float32')
        
            # Take action
            mu, sigma = actor.nn( s )
            a         = tf.random.normal([1], mean=mu, stddev=sigma)
        
            # Reward
            r = env.get_reward( state, a )
        
            # Collect results
            action_tape.append( float(a)     )
            reward_tape.append( float(r)     )
            state_tape.append(  float(state) )
        
            del (s, a, mu, sigma)
    
        # Update network weights
        history[i,m] = np.mean( reward_tape )
    
        if do_actor_critic:
            # Update critic
            value = critic.predict(state_tape, reward_tape)
            # Benchmark reward
            mod = np.array(reward_tape).reshape(-1,1) - value
            # Update actor
            actor.update_weights(state_tape, action_tape, mod)
        else:
            actor.update_weights(state_tape, action_tape, reward_tape)

    del actor
    kb.clear_session()

    if do_actor_critic:
        del critic
    
    print( f'Average Reward on last: {np.mean(reward_tape)} ' )
    display_memory()

plt.plot( history )
</code></pre>
",8589444,,8589444,,2021-02-21 00:11:13,2021-02-25 10:36:19,How do you prevent memory usage to explode when using Keras in a loop,<python><tensorflow><keras><reinforcement-learning>,1,5,,,,CC BY-SA 4.0,
1465,52277003,1,52288098,,2018-09-11 13:24:53,,3,9151,"<p>Look at the following example</p>

<pre><code># encoding: utf-8
import numpy as np
import pandas as pd
import random
import math
from keras import Sequential
from keras.layers import Dense, Activation
from keras.optimizers import Adam, RMSprop
from keras.callbacks import LearningRateScheduler

X = [i*0.05 for i in range(100)]

def step_decay(epoch):
    initial_lrate = 1.0
    drop = 0.5
    epochs_drop = 2.0
    lrate = initial_lrate * math.pow(drop, 
    math.floor((1+epoch)/epochs_drop))
    return lrate

def build_model():
    model = Sequential()
    model.add(Dense(32, input_shape=(1,), activation='relu'))
    model.add(Dense(1, activation='linear'))
    adam = Adam(lr=0.5)
    model.compile(loss='mse', optimizer=adam)
    return model

model = build_model()
lrate = LearningRateScheduler(step_decay)
callback_list = [lrate]

for ep in range(20):
    X_train = np.array(random.sample(X, 10))
    y_train = np.sin(X_train)
    X_train = np.reshape(X_train, (-1,1))
    y_train = np.reshape(y_train, (-1,1))
    model.fit(X_train, y_train, batch_size=2, callbacks=callback_list, 
              epochs=1, verbose=2)
</code></pre>

<p>In this example, the <code>LearningRateSchedule</code> does not change the learning rate at all because in each iteration of <code>ep</code>, <code>epoch=1</code>. Thus the learning rate is just const (1.0, according to <code>step_decay</code>). In fact, instead of setting <code>epoch</code>>1 directly, I have to do outer loop as shown in the example, and insider each loop, I just run 1 epoch. (This is the case when I implement deep reinforcement learning, instead of supervised learning).</p>

<p>My question is how to set an exponentially decay learning rate in my example and how to get the learning rate in each iteration of <code>ep</code>.</p>
",8601361,,,,,2019-09-30 10:58:13,How to implement exponentially decay learning rate in Keras by following the global steps,<neural-network><keras><deep-learning><reinforcement-learning>,2,0,0,,,CC BY-SA 4.0,
1466,52396257,1,52396304,,2018-09-19 00:22:34,,1,139,"<p>I was following <a href=""https://medium.com/applied-data-science/how-to-build-your-own-alphazero-ai-using-python-and-keras-7f664945c188"" rel=""nofollow noreferrer"">How to build your own AlphaZero AI using Python and Keras</a></p>

<p>The git is <a href=""https://github.com/AppliedDataSciencePartners/DeepReinforcementLearning"" rel=""nofollow noreferrer"">here</a>
In run.ipynb, this part of the code:</p>

<pre><code>memory.clear_stmemory()

if len(memory.ltmemory) &gt;= config.MEMORY_SIZE:
</code></pre>

<p>The post didn't explain much on it.
What are <code>memory.ltmemory</code> and <code>memory.stmemory</code> used for?</p>
",3618363,,,,,2018-09-19 00:30:28,"stmemory and ltmemory in ""How to build your own AlphaZero AI using Python and Keras""",<python><machine-learning><keras><deep-learning><reinforcement-learning>,1,0,0,,,CC BY-SA 4.0,
1468,52237609,1,52238710,,2018-09-08 17:16:22,,3,2599,"<p>I've created a neural network of the following form in <code>keras</code>:</p>

<pre><code>from keras.layers import Dense, Activation, Input
from keras import Model

input_dim_v = 3
hidden_dims=[100, 100, 100]

inputs = Input(shape=(input_dim_v,))
net = inputs

for h_dim in hidden_dims:
    net = Dense(h_dim)(net)
    net = Activation(""elu"")(net)

outputs = Dense(self.output_dim_v)(net)
model_v = Model(inputs=inputs, outputs=outputs)
model_v.compile(optimizer='adam', loss='mean_squared_error', metrics=['mse'])
</code></pre>

<p>Later, I train it on single examples using <code>model_v.train_on_batch(X[i],y[i])</code>. </p>

<p>To test, whether the neural network is becoming a better function approximator, I wanted to evaluate the model on the accumulated <code>X</code> and <code>y</code> (in my case, <code>X</code> and <code>y</code> grow over time) periodically. However, when I call <code>model_v.evaluate(X, y)</code>, only the characteristic progress bars appear in the console, but neither the loss value nor the mse-metric (which are the same in this case) are printed. </p>

<p>How can I change that?</p>
",8125119,,,,,2018-09-13 15:53:09,keras model.evaluate() does not show loss,<python><neural-network><keras><reinforcement-learning>,1,0,,,,CC BY-SA 4.0,
1472,54053848,1,54057629,,2019-01-05 16:21:26,,2,1460,"<p>I'm learning the Deep Reinforcement learning
 framework Chainer.</p>

<p>I've followed a tutorial and gotten the following code:</p>

<pre><code>def train_dddqn(env):

    class Q_Network(chainer.Chain):

        def __init__(self, input_size, hidden_size, output_size):
            super(Q_Network, self).__init__(
                fc1=L.Linear(input_size, hidden_size),
                fc2=L.Linear(hidden_size, hidden_size),
                fc3=L.Linear(hidden_size, hidden_size // 2),
                fc4=L.Linear(hidden_size, hidden_size // 2),
                state_value=L.Linear(hidden_size // 2, 1),
                advantage_value=L.Linear(hidden_size // 2, output_size)
            )
            self.input_size = input_size
            self.hidden_size = hidden_size
            self.output_size = output_size

        def __call__(self, x):
            h = F.relu(self.fc1(x))
            h = F.relu(self.fc2(h))
            hs = F.relu(self.fc3(h))
            ha = F.relu(self.fc4(h))
            state_value = self.state_value(hs)
            advantage_value = self.advantage_value(ha)
            advantage_mean = (F.sum(advantage_value, axis=1) / float(self.output_size)).reshape(-1, 1)
            q_value = F.concat([state_value for _ in range(self.output_size)], axis=1) + (
                    advantage_value - F.concat([advantage_mean for _ in range(self.output_size)], axis=1))
            return q_value

        def reset(self):
            self.cleargrads()


    Q = Q_Network(input_size=env.history_t + 1, hidden_size=100, output_size=3)
    Q_ast = copy.deepcopy(Q)
    optimizer = chainer.optimizers.Adam()
    optimizer.setup(Q)

    epoch_num = 50
    step_max = len(env.data) - 1
    memory_size = 200
    batch_size = 50
    epsilon = 1.0
    epsilon_decrease = 1e-3
    epsilon_min = 0.1
    start_reduce_epsilon = 200
    train_freq = 10
    update_q_freq = 20
    gamma = 0.97
    show_log_freq = 5

    memory = []
    total_step = 0
    total_rewards = []
    total_losses = []

    start = time.time()
    for epoch in range(epoch_num):

        pobs = env.reset()
        step = 0
        done = False
        total_reward = 0
        total_loss = 0

        while not done and step &lt; step_max:

            # select act
            pact = np.random.randint(3)
            if np.random.rand() &gt; epsilon:
                pact = Q(np.array(pobs, dtype=np.float32).reshape(1, -1))
                pact = np.argmax(pact.data)

            # act
            obs, reward, done = env.step(pact)

            # add memory
            memory.append((pobs, pact, reward, obs, done))
            if len(memory) &gt; memory_size:
                memory.pop(0)

            # train or update q
            if len(memory) == memory_size:
                if total_step % train_freq == 0:
                    shuffled_memory = np.random.permutation(memory)
                    memory_idx = range(len(shuffled_memory))
                    for i in memory_idx[::batch_size]:
                        batch = np.array(shuffled_memory[i:i + batch_size])
                        b_pobs = np.array(batch[:, 0].tolist(), dtype=np.float32).reshape(batch_size, -1)
                        b_pact = np.array(batch[:, 1].tolist(), dtype=np.int32)
                        b_reward = np.array(batch[:, 2].tolist(), dtype=np.int32)
                        b_obs = np.array(batch[:, 3].tolist(), dtype=np.float32).reshape(batch_size, -1)
                        b_done = np.array(batch[:, 4].tolist(), dtype=np.bool)

                        q = Q(b_pobs)

                        indices = np.argmax(q.data, axis=1)
                        maxqs = Q_ast(b_obs).data
                        target = copy.deepcopy(q.data)
                        for j in range(batch_size):
                        Q.reset()
                        loss = F.mean_squared_error(q, target)
                        total_loss += loss.data
                        loss.backward()
                        optimizer.update()

                if total_step % update_q_freq == 0:
                    Q_ast = copy.deepcopy(Q)

            # epsilon
            if epsilon &gt; epsilon_min and total_step &gt; start_reduce_epsilon:
                epsilon -= epsilon_decrease

            # next step
            total_reward += reward
            pobs = obs
            step += 1
            total_step += 1

        total_rewards.append(total_reward)
        total_losses.append(total_loss)

        if (epoch + 1) % show_log_freq == 0:
            log_reward = sum(total_rewards[((epoch + 1) - show_log_freq):]) / show_log_freq
            log_loss = sum(total_losses[((epoch + 1) - show_log_freq):]) / show_log_freq
            elapsed_time = time.time() - start
            print('\t'.join(map(str, [epoch + 1, epsilon, total_step, log_reward, log_loss, elapsed_time])))
            start = time.time()

    return Q, total_losses, total_rewards


Q, total_losses, total_rewards = train_dddqn(Environment1(train)) 
</code></pre>

<p>My question is how can I save and load this Model which has been train very well?I know Kreas has some function like: model.save and load_model.</p>

<p>So what's the specify code I need for this Chainer code?</p>
",7984318,,,,,2019-01-06 00:34:33,Chainer how to save and load DQN model,<machine-learning><keras><deep-learning><reinforcement-learning><chainer>,1,0,,,,CC BY-SA 4.0,
1476,55602633,1,55901691,,2019-04-09 23:23:23,,3,1241,"<p>I'm trying to calculate the gradient with tf.GradientTape. When I try to do it using as inputs the loss and Model.trainable_weights (tf.keras.Model) the result that returns me in an array of None. what am I doing wrong? The tensorflow version I use is 1.13.0.</p>

<p>The implemneted algorithm is a OnPolicy DQN(Not usual DQN) so that I don't use a target network(whihch is used as behavioural network in conventional DQN code). So, I wanted to differentiate the Error, which is defined as a minibatch MSE of Y(which is R + gamma * max_a Q(s', a')) and Q(s,a) in the code below.</p>

<pre class=""lang-py prettyprint-override""><code>import gym
import numpy as np
import tensorflow as tf
from collections import deque

# ==== import below from my repo ====
from common.wrappers import MyWrapper   # just a wrapper to set a reward at the terminal state -1
from common.params import Parameters    # params for training
from common.memory import ReplayBuffer  # Experience Replay Buffer

tf.enable_eager_execution()

class Model(tf.keras.Model):
    def __init__(self, num_action):
        super(Model, self).__init__()
        self.dense1 = tf.keras.layers.Dense(16, activation='relu')
        self.dense2 = tf.keras.layers.Dense(16, activation='relu')
        self.dense3 = tf.keras.layers.Dense(16, activation='relu')
        self.pred = tf.keras.layers.Dense(num_action, activation='softmax')

    def call(self, inputs):
        x = self.dense1(inputs)
        x = self.dense2(x)
        x = self.dense3(x)
        pred = self.pred(x)
        return pred


class DQN:
    """"""
    On policy DQN

    """"""

    def __init__(self, num_action):
        self.num_action = num_action
        self.model = Model(num_action)
        self.optimizer = tf.train.AdamOptimizer()

    def predict(self, state):
        return self.model(tf.convert_to_tensor(state[None, :], dtype=tf.float32)).numpy()[0]

    def update(self, state, action, target):
        # target: R + gamma * Q(s',a')
        # calculate Q(s,a)
        q_values = self.predict(state)
        actions_one_hot = tf.one_hot(action, self.num_action, 1.0, 0.0)
        action_probs = tf.reduce_sum(actions_one_hot * q_values, reduction_indices=-1)

        # Minibatch MSE =&gt; (1/batch_size) * (R + gamma * Q(s',a') - Q(s,a))^2
        loss = tf.reduce_mean(tf.squared_difference(target, action_probs))
        return loss


if __name__ == '__main__':
    reward_buffer = deque(maxlen=5)
    env = MyWrapper(gym.make(""CartPole-v0""))
    replay_buffer = ReplayBuffer(5000)
    params = Parameters(mode=""CartPole"")
    agent = DQN(env.action_space.n)

    for i in range(2000):
        state = env.reset()

        total_reward = 0
        for t in range(210):
            # env.render()
            action = np.argmax(agent.predict(state)) # behave greedily
            next_state, reward, done, info = env.step(action)
            replay_buffer.add(state, action, reward, next_state, done)

            total_reward += reward
            state = next_state

            if done:
                print(""Episode {0} finished after {1} timesteps"".format(i, t + 1))

                if i &gt; 10:
                    print(""Update"")
                    with tf.GradientTape() as tape:
                        states, actions, rewards, next_states, dones = replay_buffer.sample(params.batch_size)
                        next_Q = agent.predict(next_states)
                        Y = rewards + params.gamma * np.max(next_Q, axis=1) * np.logical_not(dones)
                        loss = agent.update(states, actions, Y)
                        print(loss)

                    grads = tape.gradient(loss, agent.model.trainable_weights)

                    # ==== THIS RETURNS ONLY NONE ====
                    print(grads)
                    agent.optimizer.apply_gradients(zip(grads, agent.model.trainable_weights))
                break

        # store the episode reward
        reward_buffer.append(total_reward)

        # check the stopping condition
        if np.mean(reward_buffer) &gt; 195:
            print(""GAME OVER!!"")
            break

    env.close()
import gym
import numpy as np
import tensorflow as tf
from collections import deque

# ==== import below from my repo ====
from common.wrappers import MyWrapper   # just a wrapper to set a reward at the terminal state -1
from common.params import Parameters    # params for training
from common.memory import ReplayBuffer  # Experience Replay Buffer

tf.enable_eager_execution()

class Model(tf.keras.Model):
    def __init__(self, num_action):
        super(Model, self).__init__()
        self.dense1 = tf.keras.layers.Dense(16, activation='relu')
        self.dense2 = tf.keras.layers.Dense(16, activation='relu')
        self.dense3 = tf.keras.layers.Dense(16, activation='relu')
        self.pred = tf.keras.layers.Dense(num_action, activation='softmax')

    def call(self, inputs):
        x = self.dense1(inputs)
        x = self.dense2(x)
        x = self.dense3(x)
        pred = self.pred(x)
        return pred


class DQN:
    """"""
    On policy DQN

    """"""

    def __init__(self, num_action):
        self.num_action = num_action
        self.model = Model(num_action)
        self.optimizer = tf.train.AdamOptimizer()

    def predict(self, state):
        return self.model(tf.convert_to_tensor(state[None, :], dtype=tf.float32)).numpy()[0]

    def update(self, state, action, target):
        # target: R + gamma * Q(s',a')
        # calculate Q(s,a)
        q_values = self.predict(state)
        actions_one_hot = tf.one_hot(action, self.num_action, 1.0, 0.0)
        action_probs = tf.reduce_sum(actions_one_hot * q_values, reduction_indices=-1)

        # Minibatch MSE =&gt; (1/batch_size) * (R + gamma * Q(s',a') - Q(s,a))^2
        loss = tf.reduce_mean(tf.squared_difference(target, action_probs))
        return loss


if __name__ == '__main__':
    reward_buffer = deque(maxlen=5)
    env = MyWrapper(gym.make(""CartPole-v0""))
    replay_buffer = ReplayBuffer(5000)
    params = Parameters(mode=""CartPole"")
    agent = DQN(env.action_space.n)

    for i in range(2000):
        state = env.reset()

        total_reward = 0
        for t in range(210):
            # env.render()
            action = np.argmax(agent.predict(state)) # behave greedily
            next_state, reward, done, info = env.step(action)
            replay_buffer.add(state, action, reward, next_state, done)

            total_reward += reward
            state = next_state

            if done:
                print(""Episode {0} finished after {1} timesteps"".format(i, t + 1))

                if i &gt; 10:
                    print(""Update"")
                    with tf.GradientTape() as tape:
                        states, actions, rewards, next_states, dones = replay_buffer.sample(params.batch_size)
                        next_Q = agent.predict(next_states)
                        Y = rewards + params.gamma * np.max(next_Q, axis=1) * np.logical_not(dones)
                        loss = agent.update(states, actions, Y)
                        print(loss)

                    grads = tape.gradient(loss, agent.model.trainable_weights)

                    # ==== THIS RETURNS ONLY NONE ====
                    print(grads)
                    agent.optimizer.apply_gradients(zip(grads, agent.model.trainable_weights))
                break

        # store the episode reward
        reward_buffer.append(total_reward)

        # check the stopping condition
        if np.mean(reward_buffer) &gt; 195:
            print(""GAME OVER!!"")
            break

    env.close()
import gym
import numpy as np
import tensorflow as tf
from collections import deque

# ==== import below from my repo ====
from common.wrappers import MyWrapper   # just a wrapper to set a reward at the terminal state -1
from common.params import Parameters    # params for training
from common.memory import ReplayBuffer  # Experience Replay Buffer

tf.enable_eager_execution()

class Model(tf.keras.Model):
    def __init__(self, num_action):
        super(Model, self).__init__()
        self.dense1 = tf.keras.layers.Dense(16, activation='relu')
        self.dense2 = tf.keras.layers.Dense(16, activation='relu')
        self.dense3 = tf.keras.layers.Dense(16, activation='relu')
        self.pred = tf.keras.layers.Dense(num_action, activation='softmax')

    def call(self, inputs):
        x = self.dense1(inputs)
        x = self.dense2(x)
        x = self.dense3(x)
        pred = self.pred(x)
        return pred


class DQN:
    """"""
    On policy DQN

    """"""

    def __init__(self, num_action):
        self.num_action = num_action
        self.model = Model(num_action)
        self.optimizer = tf.train.AdamOptimizer()

    def predict(self, state):
        return self.model(tf.convert_to_tensor(state[None, :], dtype=tf.float32)).numpy()[0]

    def update(self, state, action, target):
        # target: R + gamma * Q(s',a')
        # calculate Q(s,a)
        q_values = self.predict(state)
        actions_one_hot = tf.one_hot(action, self.num_action, 1.0, 0.0)
        action_probs = tf.reduce_sum(actions_one_hot * q_values, reduction_indices=-1)

        # Minibatch MSE =&gt; (1/batch_size) * (R + gamma * Q(s',a') - Q(s,a))^2
        loss = tf.reduce_mean(tf.squared_difference(target, action_probs))
        return loss


if __name__ == '__main__':
    reward_buffer = deque(maxlen=5)
    env = MyWrapper(gym.make(""CartPole-v0""))
    replay_buffer = ReplayBuffer(5000)
    params = Parameters(mode=""CartPole"")
    agent = DQN(env.action_space.n)

    for i in range(2000):
        state = env.reset()

        total_reward = 0
        for t in range(210):
            # env.render()
            action = np.argmax(agent.predict(state)) # behave greedily
            next_state, reward, done, info = env.step(action)
            replay_buffer.add(state, action, reward, next_state, done)

            total_reward += reward
            state = next_state

            if done:
                print(""Episode {0} finished after {1} timesteps"".format(i, t + 1))

                if i &gt; 10:
                    print(""Update"")
                    with tf.GradientTape() as tape:
                        states, actions, rewards, next_states, dones = replay_buffer.sample(params.batch_size)
                        next_Q = agent.predict(next_states)
                        Y = rewards + params.gamma * np.max(next_Q, axis=1) * np.logical_not(dones)
                        loss = agent.update(states, actions, Y)
                        print(loss)

                    grads = tape.gradient(loss, agent.model.trainable_weights)

                    # ==== THIS RETURNS ONLY NONE ====
                    print(grads)
                    agent.optimizer.apply_gradients(zip(grads, agent.model.trainable_weights))
                break

        # store the episode reward
        reward_buffer.append(total_reward)

        # check the stopping condition
        if np.mean(reward_buffer) &gt; 195:
            print(""GAME OVER!!"")
            break

    env.close()
import gym
import numpy as np
import tensorflow as tf
from collections import deque

# ==== import below from my repo ====
from common.wrappers import MyWrapper   # just a wrapper to set a reward at the terminal state -1
from common.params import Parameters    # params for training
from common.memory import ReplayBuffer  # Experience Replay Buffer

tf.enable_eager_execution()

class Model(tf.keras.Model):
    def __init__(self, num_action):
        super(Model, self).__init__()
        self.dense1 = tf.keras.layers.Dense(16, activation='relu')
        self.dense2 = tf.keras.layers.Dense(16, activation='relu')
        self.dense3 = tf.keras.layers.Dense(16, activation='relu')
        self.pred = tf.keras.layers.Dense(num_action, activation='softmax')

    def call(self, inputs):
        x = self.dense1(inputs)
        x = self.dense2(x)
        x = self.dense3(x)
        pred = self.pred(x)
        return pred


class DQN:
    """"""
    On policy DQN

    """"""

    def __init__(self, num_action):
        self.num_action = num_action
        self.model = Model(num_action)
        self.optimizer = tf.train.AdamOptimizer()

    def predict(self, state):
        return self.model(tf.convert_to_tensor(state[None, :], dtype=tf.float32)).numpy()[0]

    def update(self, state, action, target):
        # target: R + gamma * Q(s',a')
        # calculate Q(s,a)
        q_values = self.predict(state)
        actions_one_hot = tf.one_hot(action, self.num_action, 1.0, 0.0)
        action_probs = tf.reduce_sum(actions_one_hot * q_values, reduction_indices=-1)

        # Minibatch MSE =&gt; (1/batch_size) * (R + gamma * Q(s',a') - Q(s,a))^2
        loss = tf.reduce_mean(tf.squared_difference(target, action_probs))
        return loss


if __name__ == '__main__':
    reward_buffer = deque(maxlen=5)
    env = MyWrapper(gym.make(""CartPole-v0""))
    replay_buffer = ReplayBuffer(5000)
    params = Parameters(mode=""CartPole"")
    agent = DQN(env.action_space.n)

    for i in range(2000):
        state = env.reset()

        total_reward = 0
        for t in range(210):
            # env.render()
            action = np.argmax(agent.predict(state)) # behave greedily
            next_state, reward, done, info = env.step(action)
            replay_buffer.add(state, action, reward, next_state, done)

            total_reward += reward
            state = next_state

            if done:
                print(""Episode {0} finished after {1} timesteps"".format(i, t + 1))

                if i &gt; 10:
                    print(""Update"")
                    with tf.GradientTape() as tape:
                        states, actions, rewards, next_states, dones = replay_buffer.sample(params.batch_size)
                        next_Q = agent.predict(next_states)
                        Y = rewards + params.gamma * np.max(next_Q, axis=1) * np.logical_not(dones)
                        loss = agent.update(states, actions, Y)
                        print(loss)

                    grads = tape.gradient(loss, agent.model.trainable_weights)

                    # ==== THIS RETURNS ONLY NONE ====
                    print(grads)
                    agent.optimizer.apply_gradients(zip(grads, agent.model.trainable_weights))
                break

        # store the episode reward
        reward_buffer.append(total_reward)

        # check the stopping condition
        if np.mean(reward_buffer) &gt; 195:
            print(""GAME OVER!!"")
            break

    env.close()

</code></pre>
",9246727,,,,,2019-04-29 11:13:14,"Eager Execution, tf.GradientTape only returns None",<python-3.x><tensorflow><keras><gradient><reinforcement-learning>,1,0,,,,CC BY-SA 4.0,
1477,55754315,1,55873230,,2019-04-18 22:12:46,,1,49,"<p>For training I randomly generate a grid of shape N contaning values 0 and 1.
There are two actions defined [0,1] and I want to teach a policy using DQN to take action of 0 when the next number is 1 and take action 1 when next number in the array is 0.</p>

<p>I am using DQN, with Keras to create my network</p>

<pre><code>Example :
N=11
grid=[ 0,1,0,1,1,1,1,0,0,0,0]
Agent mark = 0.5
start point=0
current state =[ 0.5,1,0,1,1,1,1,0,0,0,0]
action=[0,1]
</code></pre>

<p>Consider we only move to the right of the array:
The next step should take a VALID action 0 resulting in the following state:</p>

<pre><code>Next state=[ 0,0.5,1,0,1,1,1,1,0,0,0]
</code></pre>

<p>This is enforced through experience replay.
It trains well and I reach a win rate of 100% (Calculated by solving the same maze consecutively for 10 times.
Now its time to evaluate it on a variation of this grid say:</p>

<pre><code>[0,0,0,0,1,0,1,1,0,1,0]
</code></pre>

<p>starting from</p>

<pre><code>[0.5,0,0,0,1,0,1,1,0,1,0] 
</code></pre>

<p>The network fails to predict the correct valid action which in this case is 1.</p>

<p>My network looks like this:</p>

<pre><code>Dense
Relu
Dense
Relu
Dense (number_of_actions)
</code></pre>
",5766463,,,,,2019-04-26 18:34:15,Network trains well on a grid of shape N but when evaluating on any variation fails,<python><tensorflow><keras><reinforcement-learning><q-learning>,1,0,,,,CC BY-SA 4.0,
1483,71894769,1,71894956,,2022-04-16 14:34:46,,1,963,"<p>I want to compile my DQN Agent but I get error:
<code>AttributeError: 'Adam' object has no attribute '_name'</code>,</p>
<pre><code>DQN = buildAgent(model, actions)
DQN.compile(Adam(lr=1e-3), metrics=['mae'])
</code></pre>
<p>I tried adding fake <code>_name</code> but it doesn't work, I'm following a tutorial and it works on tutor's machine, it's probably some new update change but how to fix this</p>
<p>Here is my full code:</p>
<pre><code>from keras.layers import Dense, Flatten
import gym
from keras.optimizer_v1 import Adam
from rl.agents.dqn import DQNAgent
from rl.policy import BoltzmannQPolicy
from rl.memory import SequentialMemory

env = gym.make('CartPole-v0')
states = env.observation_space.shape[0]
actions = env.action_space.n

episodes = 10

def buildModel(statez, actiones):
    model = Sequential()
    model.add(Flatten(input_shape=(1, statez)))
    model.add(Dense(24, activation='relu'))
    model.add(Dense(24, activation='relu'))
    model.add(Dense(actiones, activation='linear'))
    return model

model = buildModel(states, actions)

def buildAgent(modell, actionz):
    policy = BoltzmannQPolicy()
    memory = SequentialMemory(limit=50000, window_length=1)
    dqn = DQNAgent(model=modell, memory=memory, policy=policy, nb_actions=actionz, nb_steps_warmup=10, target_model_update=1e-2)
    return dqn

DQN = buildAgent(model, actions)
DQN.compile(Adam(lr=1e-3), metrics=['mae'])
DQN.fit(env, nb_steps=50000, visualize=False, verbose=1)
</code></pre>
",15501288,,1740577,,2022-04-16 14:59:45,2022-04-16 15:05:06,Keras: AttributeError: 'Adam' object has no attribute '_name',<python><tensorflow><keras><reinforcement-learning>,1,2,,,,CC BY-SA 4.0,
1488,71488360,1,71488454,,2022-03-15 20:03:52,,-1,101,"<p>I am looking for a solution to train an AI in NodeJS using reinforcement learning.
So far I could only find solutions in python...</p>
<p>I want an AI to give a buy/sell trigger based on price data and some technical indicators.</p>
",13861073,,,,,2022-03-15 20:13:54,reinforcement learning in NodeJS?,<node.js><keras><artificial-intelligence><reinforcement-learning>,1,0,,,,CC BY-SA 4.0,
1490,72332126,1,72336841,,2022-05-21 18:11:40,,0,135,"<p>Before I start, I am quite new to Keras and machine learning. I know the theory quite well but the syntax less so.</p>
<p>I am trying to create a reinforcement learning neural network using Keras. The problem to be solved is essentially the travelling salesman problem. The problem is, is that the network is fed in its location and the environment, which is a randomly created network of points such as [[0,5],[30,17],[19,83]..., and as the agent travels through this network, it changes as a point cannot be visited again. So if the agent goes from [0,0] to [0,5] then [30,17], the input would look like [0,5],[30,17],[19,83] to [30,17],[19,83] to [19,83]. There is a similar issue with the output, which is just the index of the possible locations to move to. This means that there could be any number of outputs.</p>
<p>The size of the input is initially 100, and the output could also be anywhere between 0 and 100. Methods like padding the inputs with a number would not work as the network would be fed a location impossible to get to, and there is a similar problem with padding the output with a number - the network can just stay in the same position whilst 'moving' ([0,0] to [0,0] etc). The agent also has limited time, so even with filling with random numbers it could just travel to locations which don't actually exist which doesn't solve the problem at hand.</p>
<p>How would I dynamically change the input and output sizes? Is it even possible, and if not, how should it be done?</p>
<p>edit: code because someone wanted it. Quite unintelligible but in essence a class containing the actions able to be done, the input in the form of self.state, and the enviroment in self.point_space. Reward is calculated as the distance travelled at each step and when complete, the distance compared to a random loop. The more important thing is if i can change the input and output sizes.</p>
<pre><code>class GraphEnv(Env):
  def __init__(self):
      self.point_space = createpoints()
      self.action_space = Discrete(len(self.point_space))
      self.observation_space = self.point_space.copy()
      self.state = [[0,0]]
     
      for i in self.observation_space:
        self.state.append(i)
      self.length = len(self.point_space)
      self.totallen = 0
      self.unchangedpoint_space = self.point_space.copy()
  def step(self, action):
    oldstate = self.state[0]
    self.state = []
    self.state.append(list(self.point_space[action-1]))
    try:
      del self.point_space[action-1]
    except:
      pass
    self.observation_space = self.point_space.copy()
    for i in self.observation_space:
        self.state.append(i)
    self.action_space = Discrete(len(self.point_space))
    #print(&quot;self.state = &quot;, self.state)

    reward = int(-math.sqrt((oldstate[0] - self.state[0][0])**2 + (oldstate[1] - self.state[0][1])**2))
    self.totallen += reward

    self.length -= 1
    if self.length &lt;= 0:
      #print(&quot;unchanged =&quot;, self.unchangedpoint_space)
      randomscore = scoreforrandom(self.unchangedpoint_space)
      reward = self.totallen - randomscore
      #print(&quot;totallen =&quot;,self.totallen)
      #print(&quot;randomscore =&quot;, randomscore)
      #print(&quot;reward&quot;, reward)
      done = True
    else:
      done = False


    info = {}

    return(self.state,reward,done,info)
  def render(self):
    
    pass
  def reset(self):
    self.state = [[0,0]]
    self.length = 100
    self.point_space = createpoints()
    self.observation_space = self.point_space.copy()
    self.state.append(self.observation_space)
    self.unchangedpoint_space = self.point_space.copy()
    #print(&quot;unchanged on init&quot;, self.unchangedpoint_space)
    self.action_space = Discrete(len(self.point_space))
    self.totallen = 0
    pass
</code></pre>
<p>The video i used as help: <a href=""https://www.youtube.com/watch?v=bD6V3rcr_54&amp;t=77s&amp;ab_channel=NicholasRenotte"" rel=""nofollow noreferrer"">https://www.youtube.com/watch?v=bD6V3rcr_54&amp;t=77s&amp;ab_channel=NicholasRenotte</a></p>
",15673855,,15673855,,2022-05-21 19:18:44,2022-07-05 16:34:32,Variable input and output size for Keras,<machine-learning><keras><deep-learning><neural-network><reinforcement-learning>,1,2,,,,CC BY-SA 4.0,
1493,71978756,1,71978964,,2022-04-23 10:15:28,,3,4102,"<p>I want to make an AI playing my custom environment, unfortunately, when I run my code, following error accrues:</p>
<pre><code>  File &quot;C:\Program Files\JetBrains\PyCharm Community Edition 2021.2\plugins\python-ce\helpers\pydev\_pydev_bundle\pydev_umd.py&quot;, line 198, in runfile
    pydev_imports.execfile(filename, global_vars, local_vars)  # execute the script
  File &quot;C:\Program Files\JetBrains\PyCharm Community Edition 2021.2\plugins\python-ce\helpers\pydev\_pydev_imps\_pydev_execfile.py&quot;, line 18, in execfile
    exec(compile(contents+&quot;\n&quot;, file, 'exec'), glob, loc)
  File &quot;D:/PycharmProjects/Custom Enviroment AI/Enviroment.py&quot;, line 88, in &lt;module&gt;
    DQN = buildAgent(model, actions)
  File &quot;D:/PycharmProjects/Custom Enviroment AI/Enviroment.py&quot;, line 82, in buildAgent
    dqn = DQNAgent(model, memory=memory, policy=policy, nb_actions=actions, nb_steps_warmup=10,
  File &quot;D:\PycharmProjects\Custom Enviroment AI\venv\lib\site-packages\rl\agents\dqn.py&quot;, line 108, in __init__
    if hasattr(model.output, '__len__') and len(model.output) &gt; 1:
  File &quot;D:\PycharmProjects\Custom Enviroment AI\venv\lib\site-packages\keras\engine\keras_tensor.py&quot;, line 221, in __len__
    raise TypeError('Keras symbolic inputs/outputs do not '
TypeError: Keras symbolic inputs/outputs do not implement `__len__`. You may be trying to pass Keras symbolic inputs/outputs to a TF API that does not register dispatching, preventing Keras from automatically converting the API call to a lambda layer in the Functional Model. This error will also get raised if you try asserting a symbolic input/output directly.
</code></pre>
<p>The error says that you souldn't use len() and you should use .shape istead, unfortunately this seems to be an error inside tensorflow
My full code is:</p>
<pre><code>from rl.memory import SequentialMemory
from rl.policy import BoltzmannQPolicy
from rl.agents.dqn import DQNAgent
from keras.layers import Dense
import tensorflow as tf
import numpy as np
import random
import pygame
import gym


class Env(gym.Env):
    def __init__(self):
        self.action_space = gym.spaces.Discrete(4)
        self.observation_space = gym.spaces.MultiDiscrete([39, 27])
        self.screen = pygame.display.set_mode((800, 600))
        self.PlayerX = 0
        self.PlayerY = 0
        self.FoodX = 0
        self.FoodY = 0
        self.state = [self.FoodX - self.PlayerX + 19, self.FoodY - self.PlayerY + 14]
        self.timeLimit = 1000

    def render(self, mode=&quot;human&quot;):
        self.screen.fill((0, 0, 0))
        pygame.draw.rect(self.screen, (255, 255, 255), pygame.Rect(self.PlayerX * 40, self.PlayerY * 40, 40, 40))
        pygame.draw.rect(self.screen, (255, 0, 0), pygame.Rect(self.FoodX * 40, self.FoodY * 40, 40, 40))
        pygame.display.update()

    def reset(self):
        self.FoodX = random.randint(1, 19)
        self.FoodY = random.randint(1, 14)
        self.PlayerX = 0
        self.PlayerY = 0
        self.timeLimit = 1000
        return self.state

    def step(self, action):
        self.timeLimit -= 1
        reward = -1

        if action == 0 and self.PlayerY &gt; 0:
            self.PlayerY -= 1
        if action == 1 and self.PlayerX &gt; 0:
            self.PlayerX -= 1
        if action == 2 and self.PlayerY &lt; 14:
            self.PlayerY += 1
        if action == 3 and self.PlayerX &lt; 19:
            self.PlayerX += 1

        if self.PlayerX == self.FoodX and self.PlayerY == self.FoodY:
            reward += 30
            self.FoodX = random.randint(1, 19)
            self.FoodY = random.randint(1, 14)

        if self.timeLimit &lt;= 0:
            done = True
        else:
            done = False

        self.state = [self.FoodX - self.PlayerX, self.FoodY - self.PlayerY]
        return self.state, reward, done


env = Env()

states = env.observation_space.shape
actions = env.action_space.n


def build_model(states, actions):
    model = tf.keras.Sequential()
    model.add(Dense(2, activation='relu', input_shape=states))
    model.add(Dense(4, activation='relu'))
    model.add(Dense(actions, activation='linear'))
    return model


def buildAgent(model, actions):
    policy = BoltzmannQPolicy()
    memory = SequentialMemory(limit=50000, window_length=1)
    dqn = DQNAgent(model, memory=memory, policy=policy, nb_actions=actions, nb_steps_warmup=10,
                   target_model_update=1e-2)
    return dqn


model = build_model(states, actions)
DQN = buildAgent(model, actions)
DQN.compile(tf.keras.optimizers.Adam(learning_rate=1e-3), metrics=['mae'])
DQN.fit(env, nb_steps=50000, visualize=False, verbose=1)
scores = DQN.test(env, nb_episodes=100, visualize=True)
print(np.mean(scores.history['episode_reward']))
pygame.quit()
model.save('model.h5')
</code></pre>
<p>I use Tensorflow: 2.8.0. This seems to be an error in Tensorflow's code but I have no idea what to do</p>
",15501288,,9657861,,2022-04-23 10:48:11,2022-11-14 12:18:35,Keras symbolic inputs/outputs do not implement `__len__` error,<python><python-3.x><tensorflow><keras><reinforcement-learning>,2,0,,,,CC BY-SA 4.0,
1494,72392440,1,72418339,,2022-05-26 13:14:37,,2,153,"<p>I'm fairly new to reinforcement learning and I've built an agent that feeds two inputs to its neural network (first input is a tuple with two numbers representing the agents current position | second input is an array of numbers ranging from 0 to 3 representing what type of requests the agent receives from the environment) and outputs which movement is the best (move forwards, backwards, sideways etc...)</p>
<p>Each episode has 300 steps, the for loop inside the train_pos_nn() takes +5s (each call to predict() takes about 20ms and each call to fit() takes about 7ms), which amounts to +25 minutes per episode, which is too much time. (about 17 days to finish 1000 episodes which is the required number of episodes to converge / it takes the same amount of time on Google Colab (<em>(Edit: even when using the GPU option and gpu cannot be setup to be used on my local machine)</em>)</p>
<p>Is there any way I can reduce the amount of time it takes the agent to train ?</p>
<pre><code>n_possible_movements = 9
MINIBATCH_SIZE = 32

class DQNAgent(object):
    def __init__(self):
        #self.gamma = 0.95 
        self.epsilon = 1.0
        self.epsilon_decay = 0.8
        self.epsilon_min = 0.1
        self.learning_rate = 10e-4 
        self.tau = 1e-3
                        
        # Main models
        self.model_uav_pos = self._build_pos_model()

        # Target networks
        self.target_model_uav_pos = self._build_pos_model()
        # Copy weights
        self.target_model_uav_pos.set_weights(self.model_uav_pos.get_weights())

        # An array with last n steps for training
        self.replay_memory_pos_nn = deque(maxlen=REPLAY_MEMORY_SIZE)
        
    def _build_pos_model(self): # compile the DNN
        # create the DNN model
        dnn = self.create_pos_dnn()
        
        opt = Adam(learning_rate=self.learning_rate) #, decay=self.epsilon_decay)
        dnn.compile(loss=&quot;categorical_crossentropy&quot;, optimizer=opt, metrics=['accuracy'])
        
        return dnn
    
    def create_pos_dnn(self): 
        # initialize the input shape (The shape of an array is the number of elements in each dimension)
        pos_input_shape = (2,)
        requests_input_shape = (len(env.ues),)
        # How many possible outputs we can have
        output_nodes = n_possible_movements
        
        # Initialize the inputs
        uav_current_position = Input(shape=pos_input_shape, name='pos')
        ues_requests = Input(shape=requests_input_shape, name='requests')
        
        # Put them in a list
        list_inputs = [uav_current_position, ues_requests]
        
        # Merge all input features into a single large vector
        x = layers.concatenate(list_inputs)
        
        # Add a 1st Hidden (Dense) Layer
        dense_layer_1 = Dense(512, activation=&quot;relu&quot;)(x)
        
        # Add a 2nd Hidden (Dense) Layer
        dense_layer_2 = Dense(512, activation=&quot;relu&quot;)(dense_layer_1)
        
        # Add a 3rd Hidden (Dense) Layer
        dense_layer_3 = Dense(256, activation=&quot;relu&quot;)(dense_layer_2)
        
        # Output layer
        output_layer = Dense(output_nodes, activation=&quot;softmax&quot;)(dense_layer_3)

        model = Model(inputs=list_inputs, outputs=output_layer)
                        
        # return the DNN
        return model
    
    def remember_pos_nn(self, state, action, reward, next_state, done):
        self.replay_memory_pos_nn.append((state, action, reward, next_state, done)) 
        
    def act_upon_choosing_a_new_position(self, state): # state is a tuple (uav_position, requests_array)
        if np.random.rand() &lt;= self.epsilon: # if acting randomly, take random action
            return random.randrange(n_possible_movements)
        pos =  np.array([state[0]])
        reqs =  np.array([state[1]])
        act_values = self.model_uav_pos.predict(x=[pos, reqs]) # if not acting randomly, predict reward value based on current state
        return np.argmax(act_values[0]) 
        
    def train_pos_nn(self):
        print(&quot;In Training..&quot;)

        # Start training only if certain number of samples is already saved
        if len(self.replay_memory_pos_nn) &lt; MIN_REPLAY_MEMORY_SIZE:
            print(&quot;Exiting Training: Replay Memory Not Full Enough...&quot;)
            return

        # Get a minibatch of random samples from memory replay table
        minibatch = random.sample(self.replay_memory_pos_nn, MINIBATCH_SIZE)

        start_time = time.time()
        # Enumerate our batches
        for index, (current_state, action, reward, new_current_state, done) in enumerate(minibatch):
            print('...Starting Training...')
            target = 0
            pos =  np.array([current_state[0]])
            reqs =  np.array([current_state[1]])
            pos_next = np.array([new_current_state[0]])
            reqs_next = np.array([new_current_state[1]])
    
            if not done:
                target = reward + DISCOUNT * np.amax(self.target_model_uav_pos.predict(x=[pos_next, reqs_next]))
            else:
                target = reward

            # Update Q value for given state
            target_f = self.model_uav_pos.predict(x=[pos, reqs])
            target_f[0][action] = target

            self.model_uav_pos.fit([pos, reqs], \
                                   target_f, \
                                   verbose=2, \
                                   shuffle=False, \
                                   callbacks=None, \
                                   epochs=1 \
                                  )  
        end_time = time.time()
        print(&quot;Time&quot;, end_time - start_time)
        # Update target network counter every episode
        self.target_train()
        
    def target_train(self):
        weights = self.model_uav_pos.get_weights()
        target_weights = self.target_model_uav_pos.get_weights()
        for i in range(len(target_weights)):
            target_weights[i] = weights[i] * self.tau + target_weights[i] * (1 - self.tau)
        self.target_model_uav_pos.set_weights(target_weights)
</code></pre>
<pre><code># Main 
SIZE = 100 # size of the grid the agent is in
for episode in tqdm(range(1, n_episodes + 1), ascii=True, unit='episodes'):  
    # Reset environment and get initial state
    current_state = env.reset(SIZE)

    # Reset flag and start iterating until episode ends
    done = False
    steps_n = 300

    for t in range(steps_n): 
        # Normalize the input (the current state)
        current_state_normalized = normalize_pos_state(current_state)
        
        # Get new position for the agent
        action_pos = agent_dqn.act_upon_choosing_a_new_position(current_state_normalized)
        
        new_state, reward, done, _ = env.step(action_pos)
        
        agent_dqn.remember_pos_nn(current_state_normalized, action_pos, reward, normalize_pos_state(new_state), done)

        current_state = new_state # not normalized
        
        agent_dqn.train_pos_nn()

    # Decay epsilon
    if episode % 50 == 0:
        if agent_dqn.epsilon &gt; agent_dqn.epsilon_min:
            agent_dqn.epsilon *= agent_dqn.epsilon_decay
            agent_dqn.epsilon = max(agent_dqn.epsilon, agent_dqn.epsilon_min)

</code></pre>
",8163377,,8163377,,2022-05-28 15:55:14,2022-05-30 12:39:27,Keras Agent Training Takes Too Much Time,<tensorflow><machine-learning><keras><deep-learning><reinforcement-learning>,2,0,,,,CC BY-SA 4.0,
1495,72642300,1,72647405,,2022-06-16 07:52:46,,1,1649,"<pre><code>from tensorflow.keras.layers import Dense, Activation
from tensorflow.keras.models import Sequential, load_model
from tensorflow.keras.optimizers import Adam

def build_dqn(lr, n_actions, input_dims, fc1_dims, fc2_dims):
    model = Sequential([
        Dense(fc1_dims, input_shape=(input_dims,)),
        Activation('relu'),
        Dense(fc2_dims),
        Activation('relu'),
        Dense(n_actions)])

    model.compile(optimizer=Adam(lr=lr), loss='mse')

    return model
</code></pre>
<p>I am trying to understand Double Deep Q-Learning. There is a pretty good lecture here: <a href=""https://github.com/philtabor/Youtube-Code-Repository/tree/master/ReinforcementLearning/DeepQLearning"" rel=""nofollow noreferrer"">https://github.com/philtabor/Youtube-Code-Repository/tree/master/ReinforcementLearning/DeepQLearning</a></p>
<p>But when I tried to run the code, I got following errors:</p>
<pre><code>Traceback (most recent call last):
  File &quot;/home/panda/PycharmProjects/ddqn/main.py&quot;, line 33, in &lt;module&gt;
    ddqn_agent.learn()
  File &quot;/home/panda/PycharmProjects/ddqn/ddqn_keras.py&quot;, line 118, in learn
    self.update_network_parameters()
  File &quot;/home/panda/PycharmProjects/ddqn/ddqn_keras.py&quot;, line 121, in update_network_parameters
    self.q_target.model.set_weights(self.q_eval.model.get_weights())
AttributeError: 'Sequential' object has no attribute 'model'
</code></pre>
<p>And I have no clue on how to fix this. I guess keras has been updated to not allow this?</p>
<p>The different lines are respectively:</p>
<p><strong>line 33:</strong></p>
<pre><code>ddqn_agent.learn()
</code></pre>
<p><strong>line 118 (in <code>def learn(self):</code>):</strong></p>
<pre><code>self.update_network_parameters()
</code></pre>
<p><strong>line 121 (in <code>def update_network_parameters(self):</code>):</strong></p>
<pre><code>self.q_target.model.set_weights(self.q_eval.model.get_weights())
</code></pre>
<p><strong>line 76:</strong></p>
<pre><code>self.q_target = build_dqn(alpha, n_actions, input_dims, 256, 256)
</code></pre>
<p><strong>EDIT:</strong> updated the problem based on suggestions in the comment section. The suggestion was that I put a <code>tensforflow.</code> in front of keras in the imports. I get the same error as before (as you can see). Here is how the imports look like now:</p>
<p><a href=""https://i.stack.imgur.com/WhvDx.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/WhvDx.png"" alt=""enter image description here"" /></a></p>
",,user17350567,1740577,,2022-06-16 14:20:29,2022-06-16 14:20:29,AttributeError: 'Sequential' object has no attribute 'model',<python><tensorflow><keras><reinforcement-learning><keras-layer>,1,8,,,,CC BY-SA 4.0,
1505,70851935,1,73051479,,2022-01-25 16:09:45,,0,185,"<p>I'm trying to have my RL model play a game, but I've encountered a peculiar problem.</p>
<p>I am kind of new to all this, so maybe it's stupid, but:</p>
<p>My environment and everything are set up nicely and when testing works like a charm. I can see the inputs the model makes and the data it receives.</p>
<p>The issue is that after model.learn() the script just freezes. Now I tried removing the model.train() line and in that case, it follows through and finishes, although of course untrained.</p>
<p>I don't see anyone talking about implementing the train() function, but when I look at it it seems empty and unimplemented. Is this normal? Do I have to build the train() function myself?</p>
<p>Thanks in advance, if you need any code let me know. But I think the problem lies in my understanding?</p>
",6634327,,14332866,,2022-01-27 10:53:31,2022-07-20 12:13:41,StableBaslines3 PPO model train() freezes?,<reinforcement-learning><stable-baselines>,1,4,,,,CC BY-SA 4.0,
1507,56998397,1,57019226,,2019-07-11 23:14:52,,0,633,"<p>I have tried to implement Proximal Policy Optimization with Intrinsic Curiosity Rewards for statefull LSTM neural network.</p>
<p>Losses in both PPO and ICM are diverging and I would like to find out if its bug in code or badly selected hyperparameters.</p>
<h2>Code (where some wrong implementation could be):</h2>
<ul>
<li>In ICM model I use first layer LSTM too to match input dimensions.</li>
<li>In ICM whole dataset is propagated at once, with zeros as initial hidden(resultin tensors are different, than they would be if I propagated only 1 state or batch and re-use hidden cells)</li>
<li>In PPO advantage and discount reward processing the dataset is propagated one by one and hidden cells are re-used (exact opposite than in ICM because here it uses same model for selecting actions and this approach is &quot;real-time-like&quot;)</li>
<li>In PPO training model is trained on batches with re-use of hidden cells</li>
</ul>
<p>I have used <a href=""https://github.com/adik993/ppo-pytorch"" rel=""nofollow noreferrer"">https://github.com/adik993/ppo-pytorch</a> as default code and reworked it to run on my environment and use LSTM</p>
<p>I may provide code samples later if specifically requested due to large amount of rows</p>
<h2>Hyperparameters:</h2>
<pre class=""lang-py prettyprint-override""><code>def __init_curiosity(self):
        curiosity_factory=ICM.factory(MlpICMModel.factory(), policy_weight=1,
                                      reward_scale=0.1, weight=0.2,
                                      intrinsic_reward_integration=0.01,
                                      reporter=self.reporter)
        self.curiosity = curiosity_factory.create(self.state_converter,
                                                  self.action_converter)
        self.curiosity.to(self.device, torch.float32)
        self.reward_normalizer = StandardNormalizer()
    
def __init_PPO_trainer(self):
        self.PPO_trainer = PPO(agent = self,
                               reward = GeneralizedRewardEstimation(gamma=0.99, lam=0.95),
                               advantage = GeneralizedAdvantageEstimation(gamma=0.99, lam=0.95),
                               learning_rate = 1e-3,
                               clip_range = 0.3,
                               v_clip_range = 0.3,
                               c_entropy = 1e-2,
                               c_value = 0.5,
                               n_mini_batches = 32,
                               n_optimization_epochs = 10,                               
                               clip_grad_norm = 0.5)
        self.PPO_trainer.to(self.device, torch.float32)
</code></pre>
<h2>Training graphs:</h2>
<p>(Notice large numbers on y axis)</p>
<p><img src=""https://i.imgur.com/MjlmduX.png"" alt=""''"" /></p>
<p><img src=""https://i.imgur.com/5PwLSWF.png"" alt=""''"" /></p>
<p><img src=""https://i.imgur.com/PWbAIZS.png"" alt=""''"" /></p>
<p><img src=""https://i.imgur.com/ftUijtJ.png"" alt=""''"" /></p>
<p><img src=""https://i.imgur.com/U44ZQ8H.png"" alt=""''"" /></p>
<p><img src=""https://i.imgur.com/A8bHnb7.png"" alt=""''"" /></p>
<hr />
<h2>UPDATE</h2>
<p>For now I have reworked LSTM processing to use batches and hidden memory on all places (for both main model and ICM), but the problem is still present. I have traced it to output from ICM's model, here the output diverges mainly in <code>action_hat</code> tensor.</p>
",5990202,,-1,,2020-06-20 09:12:55,2019-07-13 14:01:54,Diverging losses in PPO + ICM using LSTM,<python><lstm><recurrent-neural-network><reinforcement-learning>,1,3,0,,,CC BY-SA 4.0,
1515,65653439,1,67736781,,2021-01-10 12:47:18,,0,1350,"<p>I want to set &quot;actor_hiddens&quot; a.k.a the hidden layers of the policy network of PPO in Rllib, and be able to set their weights. Is this possible? If yes please tell me how?
I know how to do it for DDPG in Rllib, but the problem with PPO is that I can't find the policy network.
Thanks.</p>
",12154789,,,,,2021-05-28 09:59:44,Policy network of PPO in Rllib,<reinforcement-learning><rllib>,1,0,,,,CC BY-SA 4.0,
1522,55839974,1,55842813,,2019-04-24 23:47:19,,1,143,"<p>I am trying to model UNO card game as Partially Observable Markov Decision Processes(POMDPs) . I did little bit of research, and came to conclusion that, the states will be the number of cards, the actions will be either to play or pick the card from unseen card deck. I am facing difficulty in formulating the state transition and observation model. I think, that observation model will depend on past actions and observation(History), but for that I need to relax Markov Assumption. I want to know that relaxing the Markov Assumption is better choice or not? Additionally, how exactly should I form the state and observation model.Thanks in advance.</p>
",5921787,,,,,2019-04-25 06:22:19,How to model UNO as a POMDP,<artificial-intelligence><reinforcement-learning><markov-decision-process>,1,0,,,,CC BY-SA 4.0,
1529,40487721,1,40487948,,2016-11-08 13:06:24,,1,194,"<p>I'm coding some RL behaviors on a pacman bot and I messed things up with one of my lists in one of my functions <strong>arg_allmax</strong> or <strong>chooseAction</strong></p>

<p>Here is the code of my class:</p>

<pre><code>package rl;

import java.util.ArrayList;
import java.util.Hashtable;

public class Qlearn {
    private double epsilon = 0.1; // Epsilon parameter for the Epsilon Greedy Strategy 
    private double alpha = 0.2; // Alpha parameter: used to influence o the refresh of Q
    private double gamma = 0.9; // used to notice or not the feedback of the next action ; if =0 -&gt; no feed back

private int actions[];
private Hashtable&lt; Tuple&lt;Integer,Integer&gt;, Double&gt; q; // Q(s,a) : hashTable : &lt;state,action&gt; -&gt; value of q


public Qlearn(int[] actions) {
    this.actions = actions;
    q = new Hashtable&lt; Tuple&lt;Integer,Integer&gt;, Double&gt;();
}

public Qlearn(int[] actions, double epsilon, double alpha, double gamma) {
    this.actions = actions;
    this.epsilon = epsilon;
    this.alpha = alpha;
    this.gamma = gamma;
    q = new Hashtable&lt; Tuple&lt;Integer,Integer&gt;, Double&gt;();
}

public Double getQ(int id_state, int id_action) {
    // get the value of Q for the state of id_state and the action id_action ( return 0 if the value is not in the hashtable ) 
    Tuple&lt;Integer,Integer&gt; t = new Tuple&lt;Integer,Integer&gt; (id_state, id_action); // we creatte a new integer object Tubple with the value of id_state and id_action 
    Double v = q.get(t);
    if(v != null) return v;
    else return 0.0;
}

// get the argmax of a list
public int argmax(double[] list) {
    int arg=-1;
    double max= 0;
    for ( int i = 0; i&lt;list.length; i++){
        if ( list[i]&gt;max ){
            max = list[i];
            arg = i;
        }
    }
    return arg;
}

// get all the argmax if the argmax has several iterations
public ArrayList&lt;Integer&gt; arg_allmax(double[] list) {
    ArrayList&lt;Integer&gt; args = new ArrayList&lt;Integer&gt;();
    int a = argmax(list);
    for ( int i = 0; i&lt; list.length; i++){
        if (list[i] == list[a]){
            args.add(i);
        }
    }
    return args;
}

// get the max of the list
public double max(double[] list) {
    double max_ = -1e20;
    int a = argmax(list);
    max_ = list[a];
    return max_;
}


/*
 * Fonction that updates the hashtable
 *      for the action  id_action and the state  id_state
 *      if Q(s,a) had an old value, we allocate it the new value+ alpha(value - old_value)
 *      if Q(s,a) had not an old value : we allocate reward
 */
public void learnQ(int id_state, int id_action, double reward, double value) {
    Tuple&lt;Integer,Integer&gt; t = new Tuple&lt;Integer,Integer&gt;(id_state,id_action);
    Double oldv = q.get(t);

    if(oldv == null) {

        q.put(t, reward);
    } else {

        q.put(t, oldv+alpha*(value-oldv));
    }
}

/*
 * Here is the Epsilon Greedy strategy
 *      with proba epsilon :we choose a random action
 *      avec proba 1-eps : we choose the most favorable action in fonction of  Q(s,a)
 */
public int chooseAction(int id_state) {
    int action = -1;
    if(Math.random() &lt; epsilon) {

        int i = (int)(Math.random()*actions.length);
        action = actions[i];

    } else { 
        double[] tab = new double[actions.length];
        ArrayList&lt;Integer&gt; argmaxarray = new ArrayList&lt;Integer&gt;();
        for ( int i=0; i&gt;actions.length; i++){
            tab[i]=actions[i];
        }
        argmaxarray=arg_allmax(tab);
        int i=(int)(Math.random()*argmaxarray.size());
        action=argmaxarray.get(i);

    }

    return action;
}


/*
 * Learning after the occurence of a move
 *      1) get the most profitable potential action from  Q(s',a)
 *      2) call learnQ
 */
public void learn(int id_state1, int id_action1, double reward, int id_state2) {
    int futureAction = 0;
    futureAction = chooseAction(id_state2);
    double maxqnew = 0; // REMPLIR  
    maxqnew = getQ(futureAction, id_state2);


    learnQ(id_state1, id_action1, reward, reward + gamma*maxqnew);

}

// Affiche Q(s,a)
private void printQvalue(int id_state) {
    for(int action : actions) {
        Tuple&lt;Integer,Integer&gt; t = new Tuple&lt;Integer,Integer&gt;(id_state,action);
        Double v = q.get(t);
        System.out.print(v+"" "");
    }
    System.out.println();
}
</code></pre>

<p>Here is what eclipse tells me :</p>

<pre><code>Exception in thread ""AWT-EventQueue-0"" java.lang.ArrayIndexOutOfBoundsException: -1
    at rl.Qlearn.arg_allmax(Qlearn.java:54)
    at rl.Qlearn.chooseAction(Qlearn.java:108)
    at rl.Qlearn.learn(Qlearn.java:138)
</code></pre>

<p>I think it comes somewhere in the else of the chooseAction method using the all_argmax fonction but I cannot find the exact error!</p>

<p>Here are the two involved methods (so it's more readable for you):</p>

<p>all_argmax :</p>

<pre><code>public ArrayList&lt;Integer&gt; arg_allmax(double[] list) {
    ArrayList&lt;Integer&gt; args = new ArrayList&lt;Integer&gt;();
    int a = argmax(list);
    for ( int i = 0; i&lt; list.length; i++){
        if (list[i] == list[a]){
            args.add(i);
        }
    }
    return args;
}
</code></pre>

<p>chooseAction : </p>

<pre><code>public int chooseAction(int id_state) {
    int action = -1;
    if(Math.random() &lt; epsilon) {

        int i = (int)(Math.random()*actions.length);
        action = actions[i];

    } else { 
        double[] tab = new double[actions.length];
        ArrayList&lt;Integer&gt; argmaxarray = new ArrayList&lt;Integer&gt;();
        for ( int i=0; i&gt;actions.length; i++){
            tab[i]=actions[i];
        }
        argmaxarray=arg_allmax(tab);
        int i=(int)(Math.random()*argmaxarray.size());
        action=argmaxarray.get(i);

    }

    return action;
}
</code></pre>
",7131190,,472495,,2016-11-08 19:06:27,2016-11-08 19:06:27,ArrayIndexOutOfBoundsException:-1,<java><list><arraylist><reinforcement-learning>,1,5,,,,CC BY-SA 3.0,
1533,37638751,1,37639537,,2016-06-05 05:25:43,,2,131,"<p>I am working on a temporal difference learning example (<a href=""https://www.youtube.com/watch?v=XrxgdpduWOU"" rel=""nofollow noreferrer"">https://www.youtube.com/watch?v=XrxgdpduWOU</a>), and I'm having some trouble with the following equation in my python implementation as I seem to be double counting rewards and Q. </p>

<p>If I coded the grid below as a 2d array, my current location is (2, 2) and the goal is (2, 3), assuming max reward is 1. Let Q(t) be the average mean of my current location, then r(t+1) is 1 and I assume max Q(t+1) is also 1, which results in my Q(t) becoming close to 2 (assuming gamma of 1). Is this correct, or should I assume that Q(n), where n is the end point is 0?</p>

<p><a href=""https://i.stack.imgur.com/QPsCB.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/QPsCB.png"" alt=""enter image description here""></a></p>

<p><a href=""https://i.stack.imgur.com/Ehpje.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/Ehpje.png"" alt=""Grid""></a></p>

<p>Edited to include code - I modified the get_max_q function to return 0 if it is the end point and the values are all now below 1 (which I assume is more correct since reward is just 1) but not sure if this is the right approach (previously I set it to return 1 when it was the end point).</p>

<pre><code>#not sure if this is correct
def get_max_q(q, pos):
    #end point 
    #not sure if I should set this to 0 or 1
    if pos == (MAX_ROWS - 1, MAX_COLS - 1):
        return 0
    return max([q[pos, am] for am in available_moves(pos)])

def learn(q, old_pos, action, reward):
    new_pos = get_new_pos(old_pos, action)
    max_q_next_move = get_max_q(q, new_pos) 

    q[(old_pos, action)] = q[old_pos, action] +  alpha * (reward + max_q_next_move - q[old_pos, action]) -0.04

def move(q, curr_pos):
    moves = available_moves(curr_pos)
    if random.random() &lt; epsilon:
        action = random.choice(moves)
    else:
        index = np.argmax([q[m] for m in moves])
        action = moves[index]

    new_pos = get_new_pos(curr_pos, action)

    #end point
    if new_pos == (MAX_ROWS - 1, MAX_COLS - 1):
        reward = 1
    else:
        reward = 0

    learn(q, curr_pos, action, reward)
    return get_new_pos(curr_pos, action)

=======================
OUTPUT
Average value (after I set Q(end point) to 0)
defaultdict(float,
            {((0, 0), 'DOWN'): 0.5999999999999996,
             ((0, 0), 'RIGHT'): 0.5999999999999996,
              ...
             ((2, 2), 'UP'): 0.7599999999999998})

Average value (after I set Q(end point) to 1)
defaultdict(float,
        {((0, 0), 'DOWN'): 1.5999999999999996,
         ((0, 0), 'RIGHT'): 1.5999999999999996,
         ....
         ((2, 2), 'LEFT'): 1.7599999999999998,
         ((2, 2), 'RIGHT'): 1.92,
         ((2, 2), 'UP'): 1.7599999999999998})
</code></pre>
",1614695,,1614695,,2016-06-05 06:11:47,2016-06-05 07:35:07,Double counting in temporal difference learning,<python><machine-learning><reinforcement-learning><temporal-difference>,1,2,,,,CC BY-SA 3.0,
1535,533960,1,641231,,2009-02-10 20:09:45,,7,1365,"<p>I have been reading a lot about <a href=""http://en.wikipedia.org/wiki/Reinforcement_Learning"" rel=""nofollow noreferrer"">Reinforcement Learning</a> lately, and I have found <a href=""http://incompleteideas.net/sutton/book/ebook/the-book.html"" rel=""nofollow noreferrer"">""Reinforcement Learning: An Introduction""</a> to be an excellent guide. The author's helpfully provice <a href=""http://incompleteideas.net/sutton/book/code/code.html"" rel=""nofollow noreferrer"">source code</a> for a lot of their worked examples.</p>

<p>Before I begin the question I should point out that my practical knowledge of lisp is minimal. I know the basic concepts and how it works, but I have never really used lisp in a meaningful way, so it is likely I am just doing something incredibly n00b-ish. :)</p>

<p>Also, the author states on his page that he will not answer questions about his code, so I did not contact him, and figured Stack Overflow would be a much better choice.</p>

<p>I have been trying to run the code on a linux machine, using both GNU's CLISP and SBCL but have not been able to run it. I keep getting a whole list of errors using either interpreter. In particular, most of the code appears to use a lot of utilities contained in a file 'utilities.lisp' which contains the lines</p>

<pre><code>(defpackage :rss-utilities
  (:use :common-lisp :ccl)
  (:nicknames :ut))

(in-package :ut)
</code></pre>

<p>The :ccl seems to refer to some kind of Mac-based version of lisp, but I could not confirm this, it could just be some other package of code.</p>

<pre><code>&gt; * (load ""utilities.lisp"")
&gt;
&gt; debugger invoked on a
&gt; SB-KERNEL:SIMPLE-PACKAGE-ERROR in
&gt; thread #&lt;THREAD ""initial thread""
&gt; RUNNING {100266AC51}&gt;:   The name
&gt; ""CCL"" does not designate any package.
&gt; 
&gt; Type HELP for debugger help, or
&gt; (SB-EXT:QUIT) to exit from SBCL.
&gt; 
&gt; restarts (invokable by number or by
&gt; possibly-abbreviated name):   0:
&gt; [ABORT] Exit debugger, returning to
&gt; top level.
&gt; 
&gt; (SB-INT:%FIND-PACKAGE-OR-LOSE ""CCL"")
</code></pre>

<p>I tried removing this particular piece (changing the line to</p>

<pre><code>  (:use :common-lisp)
</code></pre>

<p>but that just created more errors.</p>

<pre><code>&gt; ; in: LAMBDA NIL ;     (+
&gt; RSS-UTILITIES::*MENUBAR-BOTTOM* ;     
&gt; (/ (- RSS-UTILITIES::MAX-V
&gt; RSS-UTILITIES::V-SIZE) 2)) ;  ; caught
&gt; WARNING: ;   undefined variable:
&gt; *MENUBAR-BOTTOM*
&gt; 
&gt; ;     (-
&gt; RSS-UTILITIES::*SCREEN-HEIGHT*
&gt; RSS-UTILITIES::*MENUBAR-BOTTOM*) ;  ;
&gt; caught WARNING: ;   undefined
&gt; variable: *SCREEN-HEIGHT*
&gt; 
&gt; ;     (IF RSS-UTILITIES::CONTAINER ;  
&gt; (RSS-UTILITIES::POINT-H ;         
&gt; (RSS-UTILITIES::VIEW-SIZE
&gt; RSS-UTILITIES::CONTAINER)) ;        
&gt; RSS-UTILITIES::*SCREEN-WIDTH*) ;  ;
&gt; caught WARNING: ;   undefined
&gt; variable: *SCREEN-WIDTH*
&gt; 
&gt; ;     (RSS-UTILITIES::POINT-H
&gt; (RSS-UTILITIES::VIEW-SIZE
&gt; RSS-UTILITIES::VIEW)) ;  ; caught
&gt; STYLE-WARNING: ;   undefined function:
&gt; POINT-H
&gt; 
&gt; ;     (RSS-UTILITIES::POINT-V
&gt; (RSS-UTILITIES::VIEW-SIZE
&gt; RSS-UTILITIES::VIEW)) ;  ; caught
&gt; STYLE-WARNING: ;   undefined function:
&gt; POINT-V
</code></pre>

<p>Anybody got any idea how I can run this code? Am I just totally ignorant of all things lisp?</p>

<p><strong>UPDATE [March 2009]:</strong> I installed Clozure, but was still not able to get the code to run.</p>

<p>At the CCL command prompt, the command</p>

<pre><code>(load ""utilities.lisp"")
</code></pre>

<p>results in the following error output:</p>

<pre><code>;Compiler warnings :
;   In CENTER-VIEW: Undeclared free variable *SCREEN-HEIGHT*
;   In CENTER-VIEW: Undeclared free variable *SCREEN-WIDTH*
;   In CENTER-VIEW: Undeclared free variable *MENUBAR-BOTTOM* (2 references)
&gt; Error: Undefined function RANDOM-STATE called with arguments (64497 9) .
&gt; While executing: CCL::READ-DISPATCH, in process listener(1).
&gt; Type :GO to continue, :POP to abort, :R for a list of available restarts.
&gt; If continued: Retry applying RANDOM-STATE to (64497 9).
&gt; Type :? for other options.
1 &gt;
</code></pre>

<p>Unfortuately, I'm still learning about lisp, so while I have a sense that something is not fully defined, I do not really understand how to read these error messages.</p>
",277,kaybenleroll,69545,kaybenleroll,2017-10-17 14:18:19,2017-10-17 14:18:19,"How Do I Run Sutton and Barton's ""Reinforcement Learning"" Lisp Code?",<lisp><artificial-intelligence><common-lisp><reinforcement-learning><mcl>,5,0,,,,CC BY-SA 3.0,
1536,38149338,1,38159744,,2016-07-01 15:43:56,,1,81,"<p>While use a neural net to obtain generalization in high state spaces, what are the input units?</p>

<p>For example if the state vector is 1 dimensional, say position on the real  axis..there would only be one input unit? ( provided seperate network for each action)</p>
",5082661,,,,,2016-07-02 11:41:40,Reinforcement learning : Neural Net,<neural-network><reinforcement-learning>,1,0,,,,CC BY-SA 3.0,
1539,56384233,1,56396305,,2019-05-30 19:05:53,,1,130,"<p>While watching the Reinforcement Learning course by David Silver on youtube (and the slide: <a href=""http://www0.cs.ucl.ac.uk/staff/d.silver/web/Teaching_files/MDP.pdf"" rel=""nofollow noreferrer"">Lecture 2 MDP</a>), I found the &quot;Reward&quot; and &quot;Value Function&quot; really confusing.</p>
<ol>
<li><p>I tried to understand the &quot;given rewards&quot; marked on the slide (P11), but I cannot figure out why it is the case. Like, the &quot;Class 1: R = -2&quot; but &quot;Pub: R = +1&quot;</p>
<p>why <strong>the negative reward</strong> for Class and the positive reward for Pub? why <strong>the different value</strong>?</p>
</li>
<li><p>How to calculate the <strong>reward with the Discount Factor</strong>? (P17 and P18)</p>
</li>
</ol>
<p>I think the lack of intuition for Reinforcement Learning is the main reason why I have encountered this kind of problem...</p>
<p>So, I'd really appreciate it if someone can give me a little hint.</p>
",9201617,,-1,,2020-06-20 09:12:55,2019-06-03 20:11:25,Confused about Rewards in David Silver Lecture 2,<reinforcement-learning>,2,0,,,,CC BY-SA 4.0,
1541,1542690,1,1558387,,2009-10-09 09:02:22,,1,1030,"<p>I have to do some work with Q Learning, about a guy that has to move furniture around a house (it's basically that). If the house is small enough, I can just have a matrix that represents actions/rewards, but as the house size grows bigger that will not be enough. So I have to use some kind of generalization function for it, instead. My teacher suggests I use not just one, but several ones, so I could compare them and so. What you guys recommend?</p>

<p>I heard that for this situation people are using Support Vector Machines, also Neural Networks. I'm not really inside the field so I can't tell. I had in the past some experience with Neural Networks, but SVM seem a lot harder subject to grasp. Are there any other methods that I should look for? I know there must be like a zillion of them, but I need something just to start.</p>

<p>Thanks</p>
",130758,,164901,,2010-05-22 23:30:53,2010-05-22 23:30:53,Generalization functions for Q-Learning,<language-agnostic><artificial-intelligence><reinforcement-learning>,1,1,,,,CC BY-SA 2.5,
1543,46260775,1,46265324,,2017-09-17 04:52:38,,46,39904,"<p>I've seen such words as:</p>

<blockquote>
  <p>A policy defines the learning agent's way of behaving at a given time. Roughly
  speaking, a policy is a mapping from perceived states of the environment to actions to be taken when in those states.</p>
</blockquote>

<p>But still didn't fully understand. What exactly is a policy in reinforcement learning?</p>
",2844907,,712995,,2019-09-19 17:41:25,2020-05-25 19:10:52,What is a policy in reinforcement learning?,<machine-learning><terminology><reinforcement-learning><markov-decision-process>,3,0,0,2017-09-19 17:27:32,,CC BY-SA 4.0,
1545,64349126,1,64402160,,2020-10-14 08:08:20,,0,91,"<p>I'm using <a href=""https://nervanasystems.github.io/coach/index.html"" rel=""nofollow noreferrer"">rl coach</a> through AWS Sagemaker, and I'm running in an issue that I struggle to understand.</p>
<p>I'm performing RL using AWS Sagemaker for the learning, and AWS Robomaker for the environment, like in <a href=""https://docs.aws.amazon.com/deepracer/latest/developerguide/deepracer-how-it-works-service-architecture.html"" rel=""nofollow noreferrer"">DeepRacer</a> which uses rl coach as well. In fact, the code only little differs with the <a href=""https://github.com/aws/amazon-sagemaker-examples/tree/d6a25381975332aa79df61144fc0c2421bd5168c/reinforcement_learning/rl_deepracer_robomaker_coach_gazebo"" rel=""nofollow noreferrer"">DeepRacer code</a> on the learning side. But the environment is completely different though.</p>
<p>What happens:</p>
<ul>
<li>The graph manager initialization succeeds</li>
<li>A first checkpoint is generated (and uploaded to S3)</li>
<li>The agent loads the first checkpoint</li>
<li>The agent performs N episodes with the first policy</li>
<li>The graph manager fetches the N episodes</li>
<li>The graph manager performs 1 training step and create a second checkpoint (uploaded to S3)</li>
<li><strong>The agent fails to restore the model with the second checkpoint.</strong></li>
</ul>
<p>The agent raises an exception with the message : <code>Failed to restore agent's checkpoint: 'main_level/agent/main/online/global_step' </code></p>
<p>The traceback points to a bug happening in <a href=""https://github.com/NervanaSystems/coach/blob/master/rl_coach/architectures/tensorflow_components/savers.py"" rel=""nofollow noreferrer"">this rl coach module</a>:</p>
<pre class=""lang-py prettyprint-override""><code>File &quot;/someverylongpath/rl_coach/architectures/tensorflow_components/savers.py&quot;, line 93, in &lt;dictcomp&gt;
    for ph, v in zip(self._variable_placeholders, self._variables)
KeyError: 'main_level/agent/main/online/global_step'
</code></pre>
<p>I use just like Deepracer a <a href=""https://github.com/aws/amazon-sagemaker-examples/blob/d6a25381975332aa79df61144fc0c2421bd5168c/reinforcement_learning/rl_deepracer_robomaker_coach_gazebo/src/rl_coach.patch"" rel=""nofollow noreferrer"">patch on rl coach</a>.
One notable thing in the patch is :</p>
<pre><code>-        self._variables = tf.global_variables()
+        self._variables = tf.trainable_variables()
</code></pre>
<p>But shouldn't it result in <code>'main_level/agent/main/online/global_step' </code>  not beeing in self._variables ?
The problem i think is that global_step is in self._variables, and it should not be there.</p>
<p>So, there's a few things I don't understand about this problem, and I'm not used to rl coach so any help would be valuable.</p>
<ol>
<li>Why does it fails only the second time ? (Does the graph manager change the computational graph ?)</li>
<li>How to avoid global_step to be in self._variables ?</li>
</ol>
<p>A few more info:</p>
<ul>
<li>I use <code>rl-coach-slim 1.0.0</code> and <code>tensorflow 1.11.0</code></li>
<li>Note that I use just like Deepracer a <a href=""https://github.com/aws/amazon-sagemaker-examples/blob/d6a25381975332aa79df61144fc0c2421bd5168c/reinforcement_learning/rl_deepracer_robomaker_coach_gazebo/src/rl_coach.patch"" rel=""nofollow noreferrer"">patch on rl coach</a>.</li>
</ul>
",10283827,,10283827,,2020-10-15 15:34:55,2020-10-17 11:54:30,Reinforcement Learning coach : Saver fails to restore agent's checkpoint,<python><neural-network><reinforcement-learning>,1,0,,,,CC BY-SA 4.0,
1555,11846387,1,11853400,,2012-08-07 12:47:44,,1,195,"<p>I am using Boltzman exploration in Q-learning where I have at least 10 actions in each state. I know that with only two actions, Boltzman exploration can be applied quite simply as follows:</p>

<ol>
<li>Calculate pr1 and pr2 for the two actions with the Boltzman exploration equation.</li>
<li>Generate a random number <em>r</em></li>
<li>Assuming pr1>pr2. If r&lt;=pr1, take action corresponding to probability pr1. If r>pr1, take action corresponding to pr2.  </li>
</ol>

<p>However, how can I do this with 10 actions? At each decision step, I update the probabilities of all the actions. This gives me a probability distribution of all the actions where the probability of best action is highest. How do I select action in this case using the Boltzman exploration?</p>
",846400,,,,,2012-08-08 09:51:36,Boltzman exploration with more than two actions in Q-learning,<machine-learning><reinforcement-learning><q-learning>,2,0,,,,CC BY-SA 3.0,
1556,43881897,1,43956736,,2017-05-10 00:43:44,,3,2977,"<p>I am trying to program a reinforcement learning algorithm using policy gradients, as inspired by <a href=""http://karpathy.github.io/2016/05/31/rl/"" rel=""nofollow noreferrer"">Karpathy's blog article</a>. Karpathy's example has only two actions UP or DOWN, so a single output neuron is sufficient (high activation=UP, low activation=DOWN). I want to extend this to multiple actions, so I believe I need a softmax activation function on the output layer. However, I am not certain about what the gradient for the output layer should be. </p>

<p>If I was using a cross-entropy loss function with the softmax activation in a supervised learning context, the gradient for neuron is simply:</p>

<pre><code>g[i] = a[i] - target[i]
</code></pre>

<p>where <code>target[i] = 1</code> for the desired action and <code>0</code> for all others.</p>

<p>To use this for reinforcement learning I would multiply <code>g[i]</code> by the discounted reward before back-propagating.</p>

<p>However, it seems that reinforcement learning uses negative log-likelihood as the loss instead of cross-entropy. <strong>How does that change the gradient?</strong></p>
",1759557,,3924118,,2019-02-19 11:25:21,2019-02-19 11:25:21,What is the policy gradient when multiple actions are possible?,<neural-network><reinforcement-learning>,1,0,0,,,CC BY-SA 4.0,
1557,43884495,1,43886524,,2017-05-10 05:51:49,,3,161,"<p>I am new to reinforcement learning. So it might seem a trivial question </p>

<p>Assuming 3 states {x,y,z} and 2 actions {a,b} </p>

<p>Why is the total number of policies/search space 2^3 = 8.</p>

<p>Consider these cases, </p>

<pre><code>x - y - z (actions a,b)
x - z - y (actions a,b)
y - x - z (actions a,b)
y - z - x (actions a,b)
z - x - y (actions a,b)
z - y - x (actions a,b)
</code></pre>

<p>This would only give me 6 policies? Then considering * 2 due to actions (b,a) it will give me 12 policies. </p>

<p>Am i missing something? </p>
",1939166,,,,,2017-05-10 08:39:41,Reinforcement learning Total number of policies given finite states and actions,<machine-learning><reinforcement-learning>,1,0,,,,CC BY-SA 3.0,
1562,56238007,1,56243204,,2019-05-21 12:10:34,,1,51,"<p>The DQN algorithm below</p>

<p><a href=""https://i.stack.imgur.com/Ozvl2.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/Ozvl2.png"" alt=""enter image description here""></a></p>

<p><a href=""https://medium.com/@jonathan_hui/rl-dqn-deep-q-network-e207751f7ae4"" rel=""nofollow noreferrer"">Source</a></p>

<p>We have phi_t, a_t, r_t and phi_{t+1} fields in D's records. Why don't we have a 'y' field in D's records, so we can store 'y' values once calculated? </p>

<p>I mean, the minibatches are chosen randomly from D without any restrictions, so one record may be chosen multiple times, especially when the number of D's records are not large enough. If that happen, y needs to be recalculated multiple times. Am I thinking it correctly?</p>
",3745149,,,,,2019-05-21 22:04:18,"In DQN, why y_i is calculated but not stored?",<reinforcement-learning>,1,0,,,,CC BY-SA 4.0,
1566,56016016,1,56018458,,2019-05-07 05:23:15,,2,88,"<p>In my understanding, reinforcement learning will get a reward from the action.</p>

<p>However, when playing a video game, there is no reward ( reward == 0 ) in most of the steps (ex: street fighter), eventually, we got a reward ( ex: player win, reward = 1 ), there are so many actions, how machine know which one is the key point to win this game ?</p>
",5518300,,,,,2019-05-07 08:27:34,How machine know which step can get max reward?,<machine-learning><reinforcement-learning>,1,1,,,,CC BY-SA 4.0,
1567,44157418,1,44172980,,2017-05-24 11:42:18,,1,2626,"<p>You have a policy, which is effectively a probability distribution of actions for all my states. A value function determines the best course of actions to achieve highest reward.</p>

<p>So I have a random policy. I get the value function. I update my policy with a new distribution according to the value function. I get a value function of this new updated policy and reevaluate once again.</p>

<p>From this definition I have trouble understanding how value iteration will then work and I think it's from a misunderstanding of what a value function is.</p>

<p>Is a value function not the best course of actions it is just a course of actions that will determine a reward? Does policy iteration simply look for a value function that provides a higher reward than it's current reward and then update immediately which gives a new distribution of actions for my states (a new policy) and then iteratively does this for every one of its states until convergence? </p>

<p>In that case is value iteration looking for the single best possible action at every state in the sequence (as opposed to one that is just better)? I am struggling here to understand why one <em>wouldn't</em> update the policy?</p>

<p>Are my understandings of policy and value function etc correct?</p>

<p>Thanks a lot! </p>

<p>I think my understanding of policy is certainly incorrect: If a policy is simply a distribution over all the possible actions for my states then I'm not entirely sure what ""updating"" it means. If it's simply updating the distribution then how exactly does value iteration even work if it's working with a ""worse"" distribution since isn't the policy initially random when initialized? I can't understand how these would converge and be equally good?</p>
",3853339,,,,,2017-05-25 05:33:19,Understanding policy and value functions reinforcement learning,<dynamic-programming><policy><reinforcement-learning>,2,1,0,,,CC BY-SA 3.0,
1569,44118627,1,44118719,,2017-05-22 17:16:16,,1,81,"<p>I'm trying to create a stochastic environment for a custom RL algorithm the purpose of this code is to take an ordered dictionary (example: OrderedDict([(0,1),(1,0), (2,0),(3,0)]) first number in tuples is indx second is probability) and return the new state at random weighted by the probability that the state occurred as defined in the ordered dictionary (in the example above there is a 100% chance that it enters state 0)</p>

<p>The problem I am having is that for some reason when indx is 0( for the above example input), probability is also 0. I expected probability to be 1. </p>

<p>in this same case pcloud[0] == 1 which is what I want. which means that there is something I am mistaken about in how I use enumerate but I don't know what it is.</p>

<pre><code>def collapse(pcloud): 
        randomnum  = Random.uniform(0,1)
        threshold = 0

        for indx , probability in enumerate(pcloud):
            threshold += probability
            if randomnum &lt;= threshold:                    
                return indx
        raise ValueError(""For some reason the probabilities can't be compared with the &lt;= operator."")
        #it should never get here.
        return
</code></pre>

<p>to run the code create an ordered dictionary.</p>

<pre><code>from collections import OrderedDict
import random as Random
#all probabilities should sum to 1
pcloud = OrderedDict()
pcloud[0] = 1
pcloud[1] = 0
pcloud[2] = 0
pcloud[3] = 0

#then run the function
print collapse(pcloud)
</code></pre>
",3777797,,3777797,,2018-09-12 00:32:50,2018-09-12 00:32:50,Choosing a random state weighted by probability,<python><enumerate><reinforcement-learning>,1,3,,,,CC BY-SA 4.0,
1571,6699222,1,6700522,,2011-07-14 20:00:24,,34,13063,"<p>I know SVMs are supposedly 'ANN killers' in that they automatically select representation complexity and find a global optimum (see <a href=""http://www.svms.org/anns.html"">here</a> for some SVM praising quotes).</p>

<p>But here is where I'm unclear -- do all of these claims of superiority hold for just the case of a 2 class decision problem or do they go further? (I assume they hold for non-linearly separable classes or else no-one would care) </p>

<p>So a sample of some of the cases I'd like to be cleared up:</p>

<ul>
<li>Are SVMs better than ANNs with many classes? </li>
<li>in an online setting?</li>
<li>What about in a semi-supervised case like reinforcement learning?</li>
<li>Is there a better unsupervised version of SVMs?</li>
</ul>

<p>I don't expect someone to answer all of these lil' subquestions, but rather to give some general bounds for when SVMs are better than the common ANN equivalents (e.g. FFBP, recurrent BP, Boltzmann machines, SOMs, etc.) in practice, and preferably, in theory as well.</p>
",821806,,3924118,,2019-06-16 12:58:01,2019-06-16 12:58:01,When should I use support vector machines as opposed to artificial neural networks?,<machine-learning><neural-network><svm><reinforcement-learning>,5,0,0,,,CC BY-SA 4.0,
1574,60566382,1,60569835,,2020-03-06 14:44:31,,1,39,"<p>Q-learning is a very simple to thing to implement and can be easily applied to explore and solve various environments or games. But as the complexity of the states increase and no. of possible actions increase, the practicality of Q-learning decreases.    </p>

<p>Supposing I have a game (let's take driving a car in GTA as an example) for which I am feeding states as pre-processed frames and asking it to take some action. But here, two problems arise:-  </p>

<ol>
<li>The no. of Q-values increase as there are a lot of unique states with corresponding ""high"" reward actions.</li>
<li>The states values will also comprise of a sizable array as these are all pixel value, thus they become very bulky.</li>
</ol>

<blockquote>
  <p>Thus, if we are faced with multiple Q-values and big 'state' values, then it would take some time for the agent to compare which state it is in and then take the action, by which we would have transitioned in a new state (Speed is a very important factor in this)</p>
</blockquote>

<p>So, how would we solve this scenario? I <em>think</em> we can use maybe Monte Carlo for this but it might also take time. So is there any other solution/algorithm to solve it? Or I can actually use Q-learning in this scenario? Or maybe I should just get a DDR5 RAM and call it a day? I am on DDR3 right now BTW ;)    </p>

<p>Any help or guidance?</p>
",8648710,,,,,2020-03-06 18:38:37,Maximum Q-values in practical scenario?,<tensorflow><reinforcement-learning><q-learning>,1,0,,,,CC BY-SA 4.0,
1575,24392268,1,28437820,,2014-06-24 16:55:35,,0,392,"<p>I have been trying to use NER feature of NLTK. I want to extract such entities from the articles. I know that it can not be perfect in doing so but I wonder if there is human intervention in between to manually tag NEs, will it improve?</p>

<p>If yes, is it possible with present model in NLTK to continually train the model. (Semi-Supervised Training)</p>
",3413239,,,,,2015-02-10 17:18:54,NLTK NER: Continuous Learning,<nlp><nltk><named-entity-recognition><reinforcement-learning>,1,3,,,,CC BY-SA 3.0,
1577,61055924,1,61134289,,2020-04-06 08:40:40,,5,210,"<p>I have created a word-level text generator using an LSTM model. But in my case, not every word is suitable to be selected. I want them to match additional conditions: </p>

<ol>
<li>Each word has a map: if a character is a vowel then it will write 1 if not, it will write 0 (for instance, <em>overflow</em> would be <code>10100010</code>). Then, the sentence generated needs to meet a given structure, for instance, <code>01001100</code> (<em>hi</em> <code>01</code> and <em>friend</em> <code>001100</code>). </li>
<li>The last vowel of the last word must be the one provided. Let's say is <em>e</em>. (<em>fri<strong>e</strong>nd</em> will do the job, then). </li>
</ol>

<p>Thus, to handle this scenario, I've created a pandas dataframe with the following structure: </p>

<pre><code>word    last_vowel  word_map
-----   ---------   ----------
hello   o           01001
stack   a           00100
jhon    o           0010
</code></pre>

<p>This is my current workflow:</p>

<ol>
<li>Given the sentence structure, I choose a random word from the dataframe which matches the pattern. For instance, if the sentence structure is <code>0100100100100</code>, we can choose the word <em>hello</em>, as its vowel structure is <code>01001</code>.</li>
<li>I subtract the selected word from the remaining structure: <code>0100100100100</code> will become <code>00100100</code> as we've removed the initial <code>01001</code> (<em>hello</em>).</li>
<li>I retrieve all the words from the dataframe which matches part of the remaining structure, in this case, <em>stack</em> <code>00100</code> and <em>jhon</em> <code>0010</code>.</li>
<li>I pass the current word sentence content (just <em>hello</em> by now) to the LSTM model, and it retrieves the weights of each word.</li>
<li>But I don't just want to select the best option, I want to select the best option contained in the selection of point 3. So I choose the word with the highest estimation within that list, in this case, <em>stack</em>.</li>
<li>Repeat from point 2 until the remaining sentence structure is empty.</li>
</ol>

<p>That works like a charm, but there is one remaining condition to handle: the last vowel of the sentence.</p>

<p>My way to deal with this issue is the following:</p>

<ol>
<li>Generating 1000 sentences forcing that the last vowel is the one specified.</li>
<li>Get the rmse of the weights returned by the LSTM model. The better the output, the higher the weights will be.</li>
<li>Selecting the sentence which retrieves the higher rank. </li>
</ol>

<p>Do you think is there a better approach? Maybe a GAN or reinforcement learning?</p>

<p>EDIT: I think another approach would be adding WFST. I've heard about <a href=""https://www.oreilly.com/content/how-to-get-superior-text-processing-in-python-with-pynini/"" rel=""nofollow noreferrer"">pynini library</a>, but I don't know how to apply it to my specific context.</p>
",1709738,,1709738,,2020-04-12 17:40:24,2020-04-12 17:40:24,How to restrict the sequence prediction in an LSTM model to match a specific pattern?,<machine-learning><lstm><reinforcement-learning><generative-adversarial-network><fst>,2,0,0,,,CC BY-SA 4.0,
1585,6726055,1,6727478,,2011-07-17 19:01:58,,2,1925,"<p>What's the appropriate way to update your R(s) function during <a href=""http://en.wikipedia.org/wiki/Q-learning"" rel=""nofollow"">Q-learning</a>? For example, say an agent visits state s1 five times, and receives rewards [0,0,1,1,0]. Should I calculate the mean reward, e.g. R(s1) = sum([0,0,1,1,0])/5? Or should I use a moving average that gives greater weight to the more recent reward values received for that state? Most of the descriptions of Q-learning I've read treat R(s) as some sort of constant, and never seem to cover how you might learn this value over time as experience is accumulated.</p>

<p>EDIT: I may be confusing the R(s) in Q-Learning with R(s,s') in a <a href=""http://en.wikipedia.org/wiki/Markov_decision_process"" rel=""nofollow"">Markov Decision Process</a>. The question remains similar. When learning an MDP, what's the best way to update R(s,s')?</p>
",247542,,247542,,2011-07-17 21:13:22,2016-02-26 14:11:42,How to Learn the Reward Function in a Markov Decision Process,<machine-learning><reinforcement-learning><q-learning>,2,1,0,,,CC BY-SA 3.0,
1586,61076973,1,61621498,,2020-04-07 09:37:00,,0,348,"<p>Created a simple game in unity where ball should hit the targets without hitting the walls. So, started training and the results was too bad. The ball is just collecting one of the 4 targets. But EndEpisode() happens when it collects the last target.</p>

<p><a href=""https://i.stack.imgur.com/iQT95.png"" rel=""nofollow noreferrer"">Screenshot of the scene and the balls path throughout the training of 1650,000 steps(if im not wrong, since I called it a generation for every 10,000 steps of training.)</a></p>

<p>The ball doesn't even tries to hit the second target. What is wrong with my code?</p>

<p>I've even tried with RayPerceptionSensor3D replacing the sphere with a cylinder so that it doesn't roll over and disturb the rayperceptionSensor3d. But it gives even much worse results.</p>

<pre><code>using System.Security.Cryptography;
using System.Data.SqlTypes;
using System.Security;
using System.Runtime.InteropServices;
using System.Net.Sockets;
using System.ComponentModel.Design.Serialization;
using System.Collections.Generic;
using UnityEngine;
using MLAgents;
using MLAgents.Sensors;
using TMPro;

public class MazeRoller : Agent
{

    Rigidbody rBody;
    Vector3 ballpos;
    void Start () {
        rBody = GetComponent&lt;Rigidbody&gt;();
        ballpos = rBody.transform.position;
    }


    public TextMeshPro text;
    public TextMeshPro miss;
    public TextMeshPro hit;
    int count=0,c=0,h=0,m=0;

    int boxescollect=0;

    public Transform Target;
    public Transform st1;
    public Transform st2;
    public Transform st3;

    public override void OnEpisodeBegin()
    {
        rBody.angularVelocity = Vector3.zero;
        rBody.velocity = Vector3.zero;
        rBody.transform.position = ballpos;
        boxescollect=0;

        st1.GetComponent&lt;Renderer&gt; ().enabled = true;
        st1.GetComponent&lt;Collider&gt; ().enabled = true;

        st2.GetComponent&lt;Renderer&gt; ().enabled = true;
        st2.GetComponent&lt;Collider&gt; ().enabled = true;

        st3.GetComponent&lt;Renderer&gt; ().enabled = true;
        st3.GetComponent&lt;Collider&gt; ().enabled = true;
    }


    void OnCollisionEnter(Collision collision)
    {
        if(collision.gameObject.name == ""Target"")
        {
            if(st1.GetComponent&lt;Renderer&gt; ().enabled==true || st2.GetComponent&lt;Renderer&gt; ().enabled==true || st3.GetComponent&lt;Renderer&gt; ().enabled==true)
            {
                SetReward(-3.0f+(float)(boxescollect));
            }

            SetReward(2.0f);

            h++;
            hit.SetText(h+"""");

            EndEpisode();
        }

        else if(collision.gameObject.name == ""Target1"")
        {
            boxescollect++;
            AddReward(0.2f);
            st1.GetComponent&lt;Renderer&gt; ().enabled = false;
            st1.GetComponent&lt;Collider&gt; ().enabled = false;
        }

        else if(collision.gameObject.name == ""Target2"")
        {
            boxescollect++;
            AddReward(0.4f);
            st2.GetComponent&lt;Renderer&gt; ().enabled = false;
            st2.GetComponent&lt;Collider&gt; ().enabled = false;
        }

        else if(collision.gameObject.name == ""Target3"")
        {
            boxescollect++;
            AddReward(0.6f);
            st3.GetComponent&lt;Renderer&gt; ().enabled = false;
            st3.GetComponent&lt;Collider&gt; ().enabled = false;

        }

        //collision.gameObject.name == ""wall1""||collision.gameObject.name == ""wall2""||collision.gameObject.name == ""wall3""||collision.gameObject.name == ""wall4""||collision.gameObject.name == ""wall5""||collision.gameObject.name == ""wall6""||collision.gameObject.name == ""wall7""

        else if(collision.gameObject.tag == ""wall"")
        {

            if(st1.GetComponent&lt;Renderer&gt; ().enabled==true || st2.GetComponent&lt;Renderer&gt; ().enabled==true || st3.GetComponent&lt;Renderer&gt; ().enabled==true)
            {
                AddReward(-3.0f+(float)(boxescollect));
            }

            SetReward(-1.0f);
            m++;
            miss.SetText(m+"""");
            EndEpisode();
        }


    }

    public override void CollectObservations(VectorSensor sensor)
    {
        // Target and Agent positions
        sensor.AddObservation(Target.position);
        sensor.AddObservation(this.transform.position);

        sensor.AddObservation(boxescollect);
        sensor.AddObservation(boxescollect-3);

        sensor.AddObservation(st1.position);
        sensor.AddObservation(st2.position);
        sensor.AddObservation(st3.position);


        float dist = Vector3.Distance(Target.position,this.transform.position);
        //Distance between Agent and target
        sensor.AddObservation(dist);

        float d1 = Vector3.Distance(st1.position,this.transform.position);
        //Distance between Agent and target
        sensor.AddObservation(d1);


        float d2 = Vector3.Distance(st2.position,this.transform.position);
        //Distance between Agent and target
        sensor.AddObservation(d2);


        float d3 = Vector3.Distance(st3.position,this.transform.position);
        //Distance between Agent and target
        sensor.AddObservation(d3);

        // Agent velocity
        sensor.AddObservation(rBody.velocity.x);
        sensor.AddObservation(rBody.velocity.z);
    }

    public float speed = 10;
    public override void OnActionReceived(float[] vectorAction)
    {
        Vector3 controlSignal = Vector3.zero;
        controlSignal.x = vectorAction[0];
        controlSignal.z = vectorAction[1];
        //speed = vectorAction[2];
        rBody.AddForce(controlSignal * speed);
        //speed=0;

        count++;

        if(count==10000)
        {

            count=0;
            h=0;
            m=0;
            c++;
            miss.SetText(m+"""");
            hit.SetText(h+"""");
            text.SetText(c+"""");
        }

    }

    public override float[] Heuristic()
    {
        var action = new float[2];
        action[0] = Input.GetAxis(""Horizontal"");
        action[1] = Input.GetAxis(""Vertical"");
        return action;
    }
}

</code></pre>

<p><a href=""https://i.stack.imgur.com/2VChi.png"" rel=""nofollow noreferrer"">weird Graph of the training - tensorboard</a>
This is what I get after the training in tensorboard.</p>
",8496551,,8496551,,2020-04-07 09:51:55,2020-05-09 01:39:44,Why is my AI model trains but doesn't evolve - ML Agents,<c#><unity3d><tensorboard><reinforcement-learning><ml-agent>,1,3,,,,CC BY-SA 4.0,
1588,61367054,1,61428048,,2020-04-22 14:02:22,,1,499,"<p>How can you prevent the agent from non-stop repeating the same action circle?</p>

<p>Of course, somehow with changes in the reward system. But are there general rules you could follow or try to include in your code to prevent such a problem?</p>

<hr>

<p>To be more precise, my actual problem is this one:</p>

<p>I'm trying to teach an ANN to learn Doodle Jump using Q-Learning. After only a few generations the agent keeps jumping on one and the same platform/stone over and over again, non-stop. It doesn't help to increase the length of the random-exploration-time. </p>

<p>My reward system is the following:</p>

<ul>
<li>+1 when the agent is living</li>
<li>+2 when the agent jumps on a platform</li>
<li>-1000 when it dies</li>
</ul>

<p>An idea would be to reward it negative or at least with 0 when the agent hits the same platform as it did before. But to do so, I'd have to pass a lot of new input-parameters to the ANN: x,y coordinates of the agent and x,y coordinates of the last visited platform.</p>

<p>Furthermore, the ANN then would also have to learn that a platform is 4 blocks thick, and so on. </p>

<p>Therefore, I'm sure that this idea I just mentioned wouldn't solve the problem, contrarily I believe that the ANN would in general simply not learn well anymore, because there are too many unuseful and complex-to-understand inputs.</p>
",12232720,,12232720,,2020-04-22 16:07:31,2020-04-25 15:27:56,"Agent repeats the same action circle non stop, Q learning",<python><tensorflow><reinforcement-learning><q-learning>,1,3,,,,CC BY-SA 4.0,
1595,9010576,1,9012260,,2012-01-25 21:28:41,,2,2343,"<p>I am wondering how to go about training a neural network without providing it with training values. My premise for this is that the neural network(s) will be used on a robot that can receive positive/negative feedback from sensors. IE, in order to train it to roam freely without bumping into things, a positive feedback occurs when no collision sensors or proximity sensors are triggered. A negative feedback occurs when the collision/proximity sensors ARE triggered. How can the neural network be trained using this method?</p>

<p>I am writing this in C++</p>
",797115,,,,,2012-01-26 00:01:17,Neural Network Learning Without Training Values,<machine-learning><reinforcement-learning><neural-network>,2,3,0,,,CC BY-SA 3.0,
1596,56091464,1,56100724,,2019-05-11 14:56:01,,0,97,"<p>I have a model whose states depend on multiple actions; I can take a single parameter as action, but what if the state transition depends on more than one action?</p>
",9805604,,1771479,,2019-05-15 09:48:27,2019-05-15 09:48:27,How can I take actions and states when my transition between states depends on multiple actions simultaneously?,<reinforcement-learning><q-learning>,1,1,,,,CC BY-SA 4.0,
1597,38009309,1,38621430,,2016-06-24 08:54:39,,6,583,"<p>I was attempting to create a neural network that utilizes reinforcement learning. I picked scikit-neuralnetwork as the library (because it's simple). It seems though, that fitting twice crashes Theano.</p>

<p>Here's the simplest code that causes the crash (Note, it doesn't matter what layers there are, nor does the learning rate or n_iter):</p>



<pre><code>import numpy as np
from sknn.mlp import Classifier, Layer

clf = Classifier(
    layers=[
        Layer(""Softmax"")
        ],
    learning_rate=0.001,
    n_iter=1)

clf.fit(np.array([[0.]]), np.array([[0.]])) # Initialize the network for learning

X = np.array([[-1.], [1.]])
Y = np.array([[1.], [0.]])

clf.fit(X, Y) # crash
</code></pre>

<p>And here's the error I got:</p>

<pre><code>ValueError: Input dimension mis-match. (input[0].shape[1] = 2, input[1].shape[1] = 1)
Apply node that caused the error: Elemwise{Mul}[(0, 1)](y, LogSoftmax.0)
Toposort index: 12
Inputs types: [TensorType(float64, matrix), TensorType(float64, matrix)]
Inputs shapes: [(1L, 2L), (1L, 1L)]
Inputs strides: [(16L, 8L), (8L, 8L)]
Inputs values: [array([[ 1.,  0.]]), array([[ 0.]])]
Outputs clients: [[Sum{axis=[1], acc_dtype=float64}(Elemwise{Mul}[(0, 1)].0)]]
</code></pre>

<p>Tested in Python 2.7.11</p>

<p>Does sknn not support fitting multiple times, or am I doing some idiotic mistake? If it doesn't, how are you supposed to implement reinforcement learning?</p>
",3731357,,,,,2016-07-27 19:17:59,sknn - input dimension mismatch on second fit,<python><scikit-learn><reinforcement-learning>,1,0,,,,CC BY-SA 3.0,
1598,56309064,1,56317871,,2019-05-25 21:59:54,,2,142,"<p>I am interested in studying reinforcement learning, in particular how to use RL for dynamic pricing. I tried to read some papers about it, and most of the time I see examples where authors try to simulate the environment in order to see the best options. </p>

<ol>
<li><a href=""https://arxiv.org/pdf/1803.09967.pdf"" rel=""nofollow noreferrer"">https://arxiv.org/pdf/1803.09967.pdf</a> RL for fair dynamic pricing </li>
<li><a href=""https://ieeexplore.ieee.org/document/1210269"" rel=""nofollow noreferrer"">https://ieeexplore.ieee.org/document/1210269</a> - Reinforcement learning applications in dynamic pricing of retail markets</li>
</ol>

<p>I am trying to understand, in this case every time we have some sort of uncertainty it is possible to simulate the environment to get the answer. And every time we have new input (environment and state is different) we run the program go get the results? Is it possible to deploy the RL model?</p>

<p>I would really appreciate any information/links related to RL in dynamic pricing and how it is possible to use/reuse the RL models.</p>
",9693537,,3453768,,2019-05-26 04:14:05,2019-05-26 22:34:42,How to use reinforcement learning models MDP Q-learning?,<model><reinforcement-learning>,1,7,0,,,CC BY-SA 4.0,
1599,19470745,1,19768210,,2013-10-19 20:23:15,,1,1757,"<p>Could anybody help to explain how to following value function been generated, the problem and solution are attached, I just don't know how the solution is generated. thank you!
<img src=""https://i.stack.imgur.com/PluqI.jpg"" alt=""problem""></p>

<p><img src=""https://i.stack.imgur.com/lxhOG.jpg"" alt=""solution""></p>

<p>STILL NEED HELP WITH THIS!!!</p>
",1551870,,1551870,,2013-10-21 05:37:35,2013-11-05 07:53:35,How to calculate the value function in reinforcement learning,<artificial-intelligence><reinforcement-learning>,1,2,,,,CC BY-SA 3.0,
1605,43611944,1,43624893,,2017-04-25 13:23:37,,0,109,"<p>Learner might be in training stage, where it update Q-table for bunch of epoch.</p>

<p>In this stage, Q-table would be updated with gamma(discount rate), learning rate(alpha), and action would be chosen by random action rate.</p>

<p>After some epoch, when reward is getting stable, let me call this ""training is done"". Then do I have to ignore these parameters(gamma, learning rate, etc) after that?</p>

<p>I mean, in training stage, I got an action from Q-table like this:</p>

<pre><code>if rand_float &lt; rar:
    action = rand.randint(0, num_actions - 1)
else:
    action = np.argmax(Q[s_prime_as_index])
</code></pre>

<p>But after training stage, Do I have to remove <code>rar</code>, which means I have to get an action from Q-table like this?</p>

<pre><code>action = np.argmax(self.Q[s_prime])
</code></pre>
",3595632,,,,,2017-04-26 08:36:59,Reinforce Learning: Do I have to ignore hyper parameter(?) after training done in Q-learning?,<reinforcement-learning><q-learning>,2,3,,,,CC BY-SA 3.0,
1607,37922621,1,37938427,,2016-06-20 12:28:55,,0,475,"<p>I'm trying to navigate an agent in a n*n gridworld domain by using Q-Learning + a feedforward neural network as a q-function approximator. Basically the agent should find the best/shortest way to reach a certain terminal goal position (+10 reward). Every step the agent takes it gets -1 reward. In the gridworld there are also some positions the agent should avoid (-10 reward, terminal states,too).</p>

<p>So far I implemented a Q-learning algorithm, that saves all Q-values in a Q-table and the agent performs well. 
In the next step, I want to replace the Q-table by a neural network, trained online after every step of the agent. I tried a feedforward NN with one hidden layer and four outputs, representing the Q-values for the possible actions in the gridworld (north,south,east, west). 
As input I used a nxn zero-matrix, that has a ""1"" at the current positions of the agent.</p>

<p>To reach my goal I tried to solve the problem from the ground up:</p>

<ol>
<li><p>Explore the gridworld with standard Q-Learning and use the Q-map as training data for the Network once Q-Learning is finished
--> worked fine</p></li>
<li><p>Use Q-Learning and provide the updates of the Q-map as trainingdata
for NN (batchSize = 1)
--> worked good</p></li>
<li><p>Replacy the Q-Map completely by the NN. (This is the point, when it gets interesting!)</p>

<p>-> FIRST MAP: 4 x 4
As described above, I have 16 ""discrete"" Inputs, 4 Output and it works fine with 8 neurons(relu) in the hidden layer (learning rate: 0.05). I used a greedy policy with an epsilon, that reduces from 1 to 0.1 within 60 episodes.
<a href=""http://i.stack.imgur.com/hMiGv.jpg"" rel=""nofollow"">The test scenario is shown here.</a> Performance is compared beetween standard qlearning with q-map and ""neural"" qlearning (in this case i used 8 neurons and differnt dropOut rates).</p></li>
</ol>

<p>To sum it up: Neural Q-learning works good for small grids, also the performance is okay and reliable.</p>

<p>-> Bigger MAP: 10 x 10</p>

<p>Now I tried to use the neural network for bigger maps.
At first I tried this simple <a href=""http://i.stack.imgur.com/8bm58.png"" rel=""nofollow"">case</a>.</p>

<p>In my case the neural net looks as following: 100 input; 4 Outputs; about 30 neurons(relu) in one hidden layer; again I used a decreasing exploring factor for greedy policy; over 200 episodes the learning rate decreases from 0.1 to 0.015 to increase stability.</p>

<p>At frist I had problems with convergence and interpolation between single positions caused by the discrete input vector. 
To solve this I added some neighbour positions to the vector with values depending on thier distance to the current position. This improved the learning a lot and the policy got better. Performance with 24 neurons is seen in the picture above.</p>

<p>Summary: the simple case is solved by the network, but only with a lot of parameter tuning (number of neurons, exploration factor, learning rate) and special input transformation.</p>

<p>Now here are my questions/problems I still haven't solved:</p>

<p>(1) My network is able to solve really simple cases and examples in a 10 x 10 map, but it fails as the problem gets a bit more complex. In cases where failing is very likely, the network has no change to find a correct policy.
I'm open minded for any idea that could improve performace in this cases.</p>

<p>(2) Is there a smarter way to transform the input vector for the network? I'm sure that adding the neighboring positons to the input vector on the one hand improve the interpolation of the q-values over the map, but on the other hand makes it harder to train special/important postions to the network. I already tried standard cartesian two-dimensional input (x/y) on an early stage, but failed.</p>

<p>(3) Is there another network type than feedforward network with backpropagation, that generally produces better results with q-function approximation? Have you seen projects, where a FF-nn performs well with bigger maps? </p>
",6270723,,6270723,,2016-06-20 12:44:46,2016-06-21 07:46:50,How can I improve the performance of a feedforward network as a q-value function approximator?,<neural-network><reinforcement-learning><q-learning><feed-forward>,1,0,,,,CC BY-SA 3.0,
1613,49401486,1,49402646,,2018-03-21 08:34:59,,2,45,"<p>First of all I found difficulties formulating my question, feedback is welcome.</p>

<p>I have to make a machine learning agent to play dots and boxes.</p>

<p>I'm just in the early stages but came up with the question: if I let my machine learning agent (with a specific implementation) play against a copy of itself to learn and improve it's gameplay, wouldn't it just make a strategy against that specific kind of gameplay?</p>

<p>Would it be more interesting if I let my agent play and learn against different forms of other agents in an arbitrary fashion?</p>
",6078387,,,,,2018-03-21 09:34:53,Machine learning: specific strategy learned because of playing against specific agent?,<machine-learning><artificial-intelligence><reinforcement-learning>,1,0,,,,CC BY-SA 3.0,
1614,66622216,1,68614196,,2021-03-14 07:50:12,,1,252,"<p>When creating a <strong>DQN</strong> agents with TFAGENTS it's possible to specify
a function to mask valid/invalid actions.</p>
<p>This is done by specifying the <a href=""https://www.tensorflow.org/agents/api_docs/python/tf_agents/agents/DqnAgent"" rel=""nofollow noreferrer"">observation_and_action_constraint_splitter</a> function.</p>
<p>Apparently it's not possible to do the same for a <strong><a href=""https://www.tensorflow.org/agents/api_docs/python/tf_agents/agents/ReinforceAgent"" rel=""nofollow noreferrer"">REINFORCE</a></strong> agent.</p>
<p>How can I mask valid/invalid actions when using <strong>REINFORCE</strong> agents?</p>
",3306091,,,,,2021-08-02 23:48:05,TFAGENTS: Valid/Invalid actions for REINFORCE agents,<tensorflow><reinforcement-learning>,1,0,,,,CC BY-SA 4.0,
1616,49956690,1,49956797,,2018-04-21 14:17:33,,0,50,"<p>The following transitions has been seen in a markov decision process. try to determine it</p>

<pre><code> R  A  Sâ€² S

 0  U  C  B
-1  L  E  C
 0  D  C  A
-1  R  E  C
 0  D  C  A
+1  R  D  C
 0  U  C  B
+1  R  D  C
</code></pre>

<p>I need to find the states, transitions, rewards and probability of transitions.
I've solved all but the probabilities and I don't know how to compute them
If anyone can help, I just need to know where to start</p>
",5091507,,,,,2018-04-21 14:30:59,determine MDP from seen transitions,<artificial-intelligence><policy><reinforcement-learning><markov-decision-process>,1,0,,,,CC BY-SA 3.0,
1618,67089715,1,67090360,,2021-04-14 10:10:13,,0,66,"<p>Slice a 3d numpy array using a 1d lookup between indices</p>
<pre><code>import numpy as np
a = np.arange(12).reshape(2, 3, 2)
b = np.array([2, 0])
</code></pre>
<p>b maps i to j where i and j are the first 2 indexes of a, so â€‹a[i,j,k]</p>
<p>Desired result after applying b to a is:</p>
<pre><code>[[4 5]
â€‹ [6 7]]
</code></pre>
<p>Naive solution:</p>
<pre><code>c = np.empty(shape=(2, 2), dtype=int)
for i in range(2):
   â€‹j = b[i]
   â€‹c[i, :] = a[i, j, :]
</code></pre>
<p>Question: Is there a way to do this using a numpy or scipy routine or routines or fancy indexing?</p>
<p>Application: Reinforcement Learning finite MDPs where b is a deterministic policy vector pi(a|s), a is the state transition probabilities p(s'|s,a) and c is the state transition matrix for that policy vector p(s'|s). The arrays will be large and this operation will be repeated a large number of times so needs to be scaleable and fast.</p>
<p>What I have tried:</p>
<ol>
<li>Compiling using numba but line profiler suggests my code is slower compared to a similarly sized numpy routine. Also numpy is more widely understood and used.</li>
<li>Maintaining pi(a|s) as a sparse matrix (all zero except one 1 per row) b_as_a_matrix and then using einsum but this involves storing and updating the matrix and creates more work (an extra loop over j and sum operation).</li>
</ol>
<pre><code>c = np.einsum('ij,ijk-&gt;ik', b_as_a_matrix, a)
</code></pre>
",12794013,,12794013,,2021-04-14 10:29:56,2021-04-14 10:55:34,Slice a 3d numpy array using a 1d lookup between indices,<python><numpy><scipy><reinforcement-learning><numba>,1,1,,,,CC BY-SA 4.0,
1621,49451366,1,49451735,,2018-03-23 13:59:54,,8,2597,"<p>I am trying to implement a Q Learning agent to learn an optimal policy for playing against a random agent in a game of Tic Tac Toe.</p>

<p>I have created a plan that I believe will work. There is just one part that I cannot get my head around. And this comes from the fact that there are two players within the environment. </p>

<p>Now, a Q Learning agent should act upon the current state, <code>s</code>, the action taken given some policy, <code>a</code>, the successive state given the action, <code>s'</code>, and any reward received from that successive state, <code>r</code>. </p>

<p>Lets put this into a tuple <code>(s, a, r, s')</code></p>

<p>Now usually an agent will act upon every state it finds itself encountered in given an action, and use the Q Learning equation to update the value of the previous state. </p>

<p>However, as Tic Tac Toe has two players, we can partition the set of states into two. One set of states can be those where it is the learning agents turn to act. The other set of states can be where it is the opponents turn to act. </p>

<p>So, do we need to partition the states into two? Or does the learning agent need to update every single state that is accessed within the game?</p>

<p>I feel as though it should probably be the latter, as this might affect updating Q Values for when the opponent wins the game. </p>

<p>Any help with this would be great, as there does not seem to be anything online that helps with my predicament. </p>
",9491737,,6735980,,2018-03-23 14:20:26,2022-10-12 15:01:42,Q Learning Applied To a Two Player Game,<python><tic-tac-toe><reinforcement-learning><q-learning>,2,0,,,,CC BY-SA 3.0,
1622,49961516,1,49965160,,2018-04-22 00:50:44,,0,679,"<p>In Reinforcement Learning, why should we select actions according to an Ïµ-greedy approach rather than always selecting the optimal action ?</p>
",9464270,,,,,2018-04-23 23:00:21,"Reinforcement Learning, Ïµ-greedy approach vs optimal action",<reinforcement-learning>,2,1,0,,,CC BY-SA 3.0,
1623,31494361,1,31724056,,2015-07-18 18:42:24,,2,1179,"<p>I'm studying the simple GridWorld (3x4, as described in Russell &amp; Norvig Ch. 21.2) problem; I've solved it using Q-Learning and a QTable, and now I'd like to use a function approximator instead of a matrix.</p>

<p>I'm using MATLAB and have tried both neural networks and decision trees, but not getting the expected results, i.e. a bad policy is found. I've read some papers about the topic, but most of them are theoretical and don't dwell much on actual implementation.</p>

<p>I've been using offline learning because it's simpler. My approach goes like this:</p>

<ol>
<li>Initialize a decision tree (or NN) with 16 input binary units - one for each position in the grid plus the 4 possible actions (up, down, left, right).</li>
<li>Make a lot of iterations, saving for each of them the qstate and the calculated qvalue in a training set.</li>
<li>Train the decision tree (or NN) using the training set.</li>
<li>Erase the training set and Repeat from step 2, using the just trained decision tree (or NN) to calculate qvalues.</li>
</ol>

<p>It seems as it is too simple to be true, and indeed I don't get the expected results. Here's some MATLAB code:</p>

<pre><code>retrain = 1;
if(retrain) 
    x = zeros(1, 16); %This is my training set
    y = 0;
    t = 0; %Iterations
end
tree = fitrtree(x, y);
x = zeros(1, 16);
y = 0;
for i=1:100
    %Get the initial game state as a 3x4 matrix
    gamestate = initialstate();
    end = 0;
    while (end == 0)
        t = t + 1; %Increase the iteration

        %Get the index of the best action to take
        index = chooseaction(gamestate, tree);

        %Make the action and get the new game state and reward
        [newgamestate, reward] = makeaction(gamestate, index);

        %Get the state-action vector for the current gamestate and chosen action
        sa_pair = statetopair(gamestate, index);

        %Check for end of game
        if(isfinalstate(gamestate))
            end = 1;
            %Get the final reward
            reward = finalreward(gamestate);
            %Add a sample to the training set
            x(size(x, 1)+1, :) = sa_pair;
            y(size(y,  1)+1, 1) = updateq(reward, gamestate, index, newgamestate, tree, t, end);
        else
            %Add a sample to the training set
            x(size(x, 1)+1, :) = sa_pair;
            y(size(y, 1)+1, 1) = updateq(reward, gamestate, index, newgamestate, tree, t, end);
        end

        %Update gamestate
        gamestate = newgamestate;
    end
end
</code></pre>

<p>It chooses a random action half the time. <em>updateq</em> function is:</p>

<pre><code>function [ q ] = updateq( reward, gamestate, index, newgamestate, tree, iteration, finalstate )

alfa = 1/iteration;
gamma = 0.99;

%Get the action with maximum qvalue in the new state s'
amax = chooseaction(newgamestate, tree);

%Get the corresponding state-action vectors
newsa_pair = statetopair(newgamestate, amax);    
sa_pair = statetopair(gamestate, index);

if(finalstate == 0)
    X = reward + gamma * predict(tree, newsa_pair);
else
    X = reward;
end

q = (1 - alfa) * predict(tree, sa_pair) + alfa * X;    

end
</code></pre>

<p>Any suggestion would be greatly appreciated!</p>
",3830367,,,,,2015-07-30 12:41:13,Solving GridWorld using Q-Learning and function approximation,<neural-network><decision-tree><reinforcement-learning><q-learning><function-approximation>,1,0,,,,CC BY-SA 3.0,
1624,50874363,1,50875141,,2018-06-15 11:09:17,,-1,94,"<p>I need to take screenshots of a website continuously and pipe these data into a python array as fast as possible. Desirable would be at least 30 fps. It would be nice to have one screenshot/frame per function call, because I have to inject some JavaScript to the website after each frame.
The website is running a webgl canvas and is expecting keyboard input.</p>

<p>I already tried to make it with selenium and headless Firefox, but this is way too slow. What do you think is the best way to go to get close to my requirements?</p>

<p>Thanks in advance.</p>
",3307152,,3307152,,2018-06-15 11:38:30,2018-06-15 11:58:14,High-Speed Website Screenshots with Python,<javascript><python><html><reinforcement-learning>,1,0,,2018-06-15 17:22:13,,CC BY-SA 4.0,
1625,50905635,1,51435734,,2018-06-18 08:43:33,,2,494,"<p>Currently I'm trying to implement the REINFORCE policy gradient method (with neural network) for a game.  Now obviously, there are certain actions that are invalid in certain states (can't fire the rocket launcher if you don't have one!).</p>

<p>I tried to mask the softmax outputs (action probability) so that is only samples from valid actions.  This works fine (or so it seems), however after several iterations of training, these actions are no longer being chosen (all outputs for these nodes turn into 0 for certain input combination).  Interestingly, certain action node (invalid action) seems to give 1 (100% probability) in these cases.</p>

<p>This is causing a huge problem since I will then have to resort to randomly choosing the action to perform, which obviously doesn't do well.  Is there any other ways to deal with the problem?</p>

<p>P.S. I'm updating the network by setting the ""label"" as the chosen action node having the value of discounted reward, while the remaining actions to be 0, then doing a categorical_crossentropy in Keras.</p>
",4625843,,,,,2019-03-18 11:22:49,Policy Gradient (REINFORCE) for invalid actions,<policy><reinforcement-learning>,1,0,,,,CC BY-SA 4.0,
1626,50910555,1,50912240,,2018-06-18 13:24:19,,-2,784,"<p>I'm working on a tensorflow project, in which I have a neural network in a reinforcement learning system, used to predict the Q values. I have  50 inputs and 10 outputs. Some of the inputs are in the range 30-70, and the rest are between 0-1, so I normalize only the first group, using this formula:</p>

<blockquote>
  <p>x_new = (x - x_min)/(x_max - x_min)</p>
</blockquote>

<p>Although I know the mathematical base of neural networks, I do not have experience applying them in real cases, so I do not really know if the hyperparameters I am using are correctly chosen. The ones I have currently are:</p>

<ul>
<li>2 hidden layers with 10 and 20 neurons each</li>
<li>Learning rate of 0.5</li>
<li>Batch size of 10 (I have tried with different values until 256 obtaining the same result)</li>
</ul>

<p>The problem I'm not able to solve is that the weights of this neural network only change in the first two or three iterations, and stay fixed afterwards. </p>

<p>What I had read in other posts is that the algorithm is finding a local optima, and that the normalization of the inputs is a good idea to solve it. However, after normalizing the inputs, I am still in the same state. So, my question is if anyone knows where the problem may be, and if there is any other technique (like normalization) that I should add to my pipeline.   </p>

<p>I haven't added any line of code in the question, because I think my problem is rather conceptual. However, in case more details were needed, I would insert it.</p>
",6845388,,295783,,2018-06-19 14:45:34,2018-06-19 14:45:34,Weights of neural network not changing,<tensorflow><machine-learning><neural-network><deep-learning><reinforcement-learning>,1,0,,2018-06-19 04:15:59,,CC BY-SA 4.0,
1627,67789148,1,67794693,,2021-06-01 12:47:17,,4,1403,"<p>I am implementing simple <a href=""https://www.cs.toronto.edu/%7Evmnih/docs/dqn.pdf"" rel=""nofollow noreferrer"">DQN</a> algorithm using <code>pytorch</code>, to solve the CartPole environment from <code>gym</code>. I have been debugging for a while now, and I cant figure out why the model is not learning.</p>
<p>Observations:</p>
<ul>
<li>using <code>SmoothL1Loss</code> performs worse than <code>MSEloss</code>, but loss increases for both</li>
<li>smaller <code>LR</code> in <code>Adam</code> does not work, I have tested using 0.0001, 0.00025, 0.0005 and default</li>
</ul>
<p>Notes:</p>
<ul>
<li>I have debugged various parts of the algorithm individually, and can say with good confidence that the issue is in the <code>learn</code> function. I am wondering if this bug is due to me misunderstanding <code>detach</code> in pytorch or some other framework mistake im making.</li>
<li>I am trying to stick as close to the original paper as possible (linked above)</li>
</ul>
<p>References:</p>
<ul>
<li><a href=""https://gist.github.com/Pocuston/13f1a7786648e1e2ff95bfad02a51521"" rel=""nofollow noreferrer"">example</a>: GitHub gist</li>
<li><a href=""https://pytorch.org/tutorials/intermediate/reinforcement_q_learning.html"" rel=""nofollow noreferrer"">example</a>: pytroch official</li>
</ul>
<pre class=""lang-py prettyprint-override""><code>import torch as T
import torch.nn as nn
import torch.nn.functional as F

import gym
import numpy as np


class ReplayBuffer:
    def __init__(self, mem_size, input_shape, output_shape):
        self.mem_counter = 0
        self.mem_size = mem_size
        self.input_shape = input_shape

        self.actions = np.zeros(mem_size)
        self.states = np.zeros((mem_size, *input_shape))
        self.states_ = np.zeros((mem_size, *input_shape))
        self.rewards = np.zeros(mem_size)
        self.terminals = np.zeros(mem_size)

    def sample(self, batch_size):
        indices = np.random.choice(self.mem_size, batch_size)
        return self.actions[indices], self.states[indices], \
            self.states_[indices], self.rewards[indices], \
            self.terminals[indices]

    def store(self, action, state, state_, reward, terminal):
        index = self.mem_counter % self.mem_size

        self.actions[index] = action
        self.states[index] = state
        self.states_[index] = state_
        self.rewards[index] = reward
        self.terminals[index] = terminal
        self.mem_counter += 1


class DeepQN(nn.Module):
    def __init__(self, input_shape, output_shape, hidden_layer_dims):
        super(DeepQN, self).__init__()

        self.input_shape = input_shape
        self.output_shape = output_shape

        layers = []
        layers.append(nn.Linear(*input_shape, hidden_layer_dims[0]))
        for index, dim in enumerate(hidden_layer_dims[1:]):
            layers.append(nn.Linear(hidden_layer_dims[index], dim))
        layers.append(nn.Linear(hidden_layer_dims[-1], *output_shape))

        self.layers = nn.ModuleList(layers)

        self.loss = nn.MSELoss()
        self.optimizer = T.optim.Adam(self.parameters())

    def forward(self, states):
        for layer in self.layers[:-1]:
            states = F.relu(layer(states))
        return self.layers[-1](states)

    def learn(self, predictions, targets):
        self.optimizer.zero_grad()
        loss = self.loss(input=predictions, target=targets)
        loss.backward()
        self.optimizer.step()

        return loss


class Agent:
    def __init__(self, epsilon, gamma, input_shape, output_shape):
        self.input_shape = input_shape
        self.output_shape = output_shape
        self.epsilon = epsilon
        self.gamma = gamma

        self.q_eval = DeepQN(input_shape, output_shape, [64])
        self.memory = ReplayBuffer(10000, input_shape, output_shape)

        self.batch_size = 32
        self.learn_step = 0

    def move(self, state):
        if np.random.random() &lt; self.epsilon:
            return np.random.choice(*self.output_shape)
        else:
            self.q_eval.eval()
            state = T.tensor([state]).float()
            action = self.q_eval(state).max(axis=1)[1]
            return action.item()

    def sample(self):
        actions, states, states_, rewards, terminals = \
            self.memory.sample(self.batch_size)

        actions = T.tensor(actions).long()
        states = T.tensor(states).float()
        states_ = T.tensor(states_).float()
        rewards = T.tensor(rewards).view(self.batch_size).float()
        terminals = T.tensor(terminals).view(self.batch_size).long()

        return actions, states, states_, rewards, terminals

    def learn(self, state, action, state_, reward, done):
        self.memory.store(action, state, state_, reward, done)

        if self.memory.mem_counter &lt; self.batch_size:
            return

        self.q_eval.train()
        self.learn_step += 1
        actions, states, states_, rewards, terminals = self.sample()
        indices = np.arange(self.batch_size)
        q_eval = self.q_eval(states)[indices, actions]
        q_next = self.q_eval(states_).detach()
        q_target = rewards + self.gamma * q_next.max(axis=1)[0] * (1 - terminals)

        loss = self.q_eval.learn(q_eval, q_target)
        self.epsilon *= 0.9 if self.epsilon &gt; 0.1 else 1.0

        return loss.item()


def learn(env, agent, episodes=500):
    print('Episode: Mean Reward: Last Loss: Mean Step')

    rewards = []
    losses = [0]
    steps = []
    num_episodes = episodes
    for episode in range(num_episodes):
        done = False
        state = env.reset()
        total_reward = 0
        n_steps = 0

        while not done:
            action = agent.move(state)
            state_, reward, done, _ = env.step(action)
            loss = agent.learn(state, action, state_, reward, done)

            state = state_
            total_reward += reward
            n_steps += 1

            if loss:
                losses.append(loss)

        rewards.append(total_reward)
        steps.append(n_steps)

        if episode % (episodes // 10) == 0 and episode != 0:
            print(f'{episode:5d} : {np.mean(rewards):5.2f} '
                  f': {np.mean(losses):5.2f}: {np.mean(steps):5.2f}')
            rewards = []
            losses = [0]
            steps = []

    print(f'{episode:5d} : {np.mean(rewards):5.2f} '
          f': {np.mean(losses):5.2f}: {np.mean(steps):5.2f}')
    return losses, rewards


if __name__ == '__main__':
    env = gym.make('CartPole-v1')
    agent = Agent(1.0, 1.0,
                  env.observation_space.shape,
                  [env.action_space.n])

    learn(env, agent, 500)
</code></pre>
",5194362,,,,,2021-06-02 17:39:18,DQN Pytorch Loss keeps increasing,<python><machine-learning><pytorch><reinforcement-learning><q-learning>,1,0,0,,,CC BY-SA 4.0,
1628,53864434,1,53871660,,2018-12-20 07:49:51,,0,905,"<p>I just read the paper of <a href=""https://arxiv.org/pdf/1312.5602.pdf"" rel=""nofollow noreferrer"">Mnih (2013)</a> and was really wondering about the aspect that he talks about using RMSprop with <strong>minibatches of size 32</strong> (page 6).</p>

<p>My understanding of these kinds of reinforcement learning algorithms is, that there is only 1 or at least very little amount of training samples per fit, and in every fit I update the network.
Whereas in supervised learning I have up to millions of samples and divide them in minibatches of e.g. 32 and update the network after every minibatch, which makes sense.</p>

<p>So my question is: If I put only one sample into the neural network at a time, how does minibatches make sense? Did I understand something wrong about that concept?</p>

<p>Thanks in advance!</p>
",7329305,,,,,2018-12-20 16:47:28,Mini-batches in RL,<reinforcement-learning>,2,0,,,,CC BY-SA 4.0,
1633,53937461,1,53937540,,2018-12-26 21:40:47,,0,58,"<p>I would like to introduce a new layer as activation function in tensorflow. However, There are errors that can not be solved.
This is code of new layer.</p>

<pre><code>def smooth_relu(tensor):
    e=0.15
    alpha=0.005

    def smooth(tensor):

            smoothtensor=tf.cond(tensor&lt;(e+alpha) ,lambda: (tensor-alpha)*(tensor-alpha),lambda:e*((tensor-alpha)-self.e*0.5),    tf.cond(
                        pred,
                        true_fn=None,
                        false_fn=None,
                        strict=False,
                        name=None,
                        fn1=None,
                        fn2=None
                        ))


            return (smoothtensor)



    newtensor=tf.cond(tensor&lt;0 ,lambda :0, lambda:smooth(tensor))
    # In addition to return the result, we return my_random for initializing on each
    # iteration and alpha to check the final value used.

    return (newtensor)
</code></pre>

<p>This is error.</p>

<pre><code>ValueError: Shape must be rank 0 but is rank 2 for 'cond/Switch' (op: 'Switch') with input shapes: [1,1], [1,1].
</code></pre>
",10806797,,10806797,,2018-12-27 16:53:11,2018-12-27 16:53:11,Introduced a new layer using tensorflow,<python><tensorflow><deep-learning><reinforcement-learning>,1,0,,,,CC BY-SA 4.0,
1634,53923388,1,53928052,,2018-12-25 14:52:59,,-2,174,"<p>It was weird when I face it in reinforcement learning. A loss is MSE. Everything should be perfect to be gradient descent and now it is a gradient ascent. I wanna know the magic. I did numpy neural network. Change in a derivative lead to gradient ascent. What particular change in a derivative lead to gradient ascent? Is it that simple that autograd sees that it is concave or convex?</p>
",10395342,,,,,2022-10-11 17:54:05,What particular change of formula in target changes neural network from gradient descent into gradient ascent?,<deep-learning><pytorch><reinforcement-learning>,1,0,0,,,CC BY-SA 4.0,
1641,48011874,1,48020259,,2017-12-28 17:36:22,,1,1022,"<p>I have implemented VI (Value Iteration), PI (Policy Iteration), and QLearning algorithms using python. After comparing results, I noticed something. VI and PI algorithms converge to same utilities and policies. <strong>With same parameters, QLearning algorithm converge to different utilities, but same policies with VI and PI algorithms. Is this something normal?</strong> I read a lot of papers and books about MDP and RL, but couldn't find anything which tells if utilities of VI-PI algorithms should converge to same utilities with QLearning or not.</p>

<p>Following information is about my grid world and results.</p>

<p><strong>MY GRID WORLD</strong></p>

<p><a href=""https://i.stack.imgur.com/RpXFT.png"" rel=""nofollow noreferrer"">grid_world.png</a></p>

<ul>
<li>States => <em>{s0, s1, ... , s10}</em></li>
<li>Actions => <em>{a0, a1, a2, a3}</em> where: <em>a0 = Up, a1 = Right, a2 = Down, a3 = Left</em> for all states</li>
<li>There are 4 terminal states, which have <em>+1, +1, -10, +10</em> rewards.</li>
<li>Initial state is <em>s6</em></li>
<li>Transition probability of an action is <strong><em>P</em></strong>, and <strong><em>(1 - p) / 2</em></strong> to go left or right side of that action. <em>(For example: If P = 0.8, when agent tries to go UP, with 80% chance agent will go UP, and with 10% chance agent will go RIGHT, and 10% LEFT.)</em></li>
</ul>

<p><strong>RESULTS</strong></p>

<ul>
<li>VI and PI algorithm results with <em>Reward = -0.02, Discount Factor = 0.8, Probability = 0.8</em></li>
<li>VI converges after 50 iterations, PI converges after 3 iteration</li>
</ul>

<p><a href=""https://i.stack.imgur.com/WNEtW.png"" rel=""nofollow noreferrer"">vi_pi_results.png</a></p>

<ul>
<li>QLearning algorithm results with <em>Reward = -0.02, Discount Factor = 0.8, Learning Rate = 0.1, Epsilon (For exploration) = 0.1</em></li>
<li>Resulting utilities on the image of QLearning results are the maximum Q(s, a) pairs of each state.</li>
</ul>

<p><a href=""https://i.stack.imgur.com/Cs1Ry.png"" rel=""nofollow noreferrer"">qLearning_1million_10million_iterations_results.png</a></p>

<p>In addition, I also noticed that, when QLearning does 1 million iterations, states which are equally far away from the +10 rewarded terminal have the same utilities. Agent seems it does not care if it is going to the reward from a path which is near to -10 terminal or not, while agent cares about it on VI and PI algorithms. <strong>Is this because, in QLearning, we don't know the transition probability of environment?</strong></p>
",4498616,,,,,2017-12-29 09:23:47,"MDP & Reinforcement Learning - Convergence Comparison of VI, PI and QLearning Algorithms",<python><machine-learning><reinforcement-learning><q-learning><mdp>,1,0,,,,CC BY-SA 3.0,
1642,52462073,1,52529309,,2018-09-23 00:52:00,,1,81,"<p>I am experimenting with reinforcement learning in python using Keras. Most of the tutorials available use OpenAI Gym library to create the environment, state, and action sets. </p>

<p>After practicing with many good examples written by others, I decided that I want to create my own reinforcement learning environment, state, and action sets.</p>

<p>This is what I think will be fun to teach the machine to do.</p>

<ol>
<li>An array of integers from 1 to 4. I will call these targets.</li>
</ol>

<blockquote>
  <p>targets = [[1, 2, 3, 4]]</p>
</blockquote>

<ol start=""2"">
<li>Additional numbers list (at random) from 1 to 4. I will call these bullets.</li>
</ol>

<blockquote>
  <p>bullets = [1, 2, 3, 4]</p>
</blockquote>

<ol start=""3"">
<li>When I shoot a bullet to a target, the target's number will be the sum of original target num + bullet num.</li>
<li>I want to shoot a bullet (one at a time) at one of the targets to make </li>
<li>For example, given targets [1 2 3 4] and bullet 1, I want the machine to predict the correct index to shoot at.</li>
<li><p>In this case, it should be index 3, because 4 + 1 = 5</p>

<blockquote>
  <p>curr_state = [[1, 2, 3, 4]] </p>
  
  <p>bullet     = 1 </p>
  
  <p>action     = 3 (&lt;-- index of the curr_state) </p>
  
  <p>next_state = [[1, 2, 3, 5]]</p>
</blockquote></li>
</ol>

<p>I have been picking my brain to think of the best way to construct this into a reinforcement design. I tried some, but the model result is not very good (meaning, it most likely fails to make number 5). </p>

<p>Mostly because the state is a 2D: (1) targets; (2) bullet at that time. The method I employed so far is to convert the state as the following:</p>

<blockquote>
  <p>State  =  5 - targets - bullet</p>
</blockquote>

<p>I was wondering if anyone can think of a better way to design this model?
Thanks in advance!</p>
",5214063,,,,,2021-02-03 00:20:45,reinforcement learning model design - how to add upto 5,<reinforcement-learning>,1,1,,,,CC BY-SA 4.0,
1643,1620335,1,1620415,,2009-10-25 08:17:03,,5,2798,"<p>I'm considering using a neural network to power my enemies in a space shooter game i'm building and i'm wondering; how do you train neural networks when there is no one definitive good set of outputs for the network?</p>
",117069,,2838606,,2016-01-04 15:51:09,2016-01-04 15:51:09,"How to use neural networks to solve ""soft"" solutions?",<neural-network><artificial-intelligence><reinforcement-learning>,4,7,0,,,CC BY-SA 2.5,
1650,68782353,1,68782489,,2021-08-14 10:15:17,,0,468,"<p>Most materials (e.g., David Silver's online course) I can find offer discussions about the relationship between supervised learning and reinforcement learning. However, it is actually a comparison between supervised learning and online reinforcement learning where the agent runs in the environment (or simulates interactions) to get feedback given limited knowledge about the underlying dynamics.</p>
<p>I am more curious about offline (batch) reinforcement learning where the dataset (collected learning experiences) is given <em>a priori</em>. What are the differences compared to supervised learning then? and what are the similarities they may share?</p>
",9201617,,9201617,,2021-08-30 01:24:15,2021-08-30 01:24:15,Supervised learning v.s. offline (batch) reinforcement learning,<reinforcement-learning><unsupervised-learning>,1,1,,,,CC BY-SA 4.0,
1652,54494160,1,54571154,,2019-02-02 14:50:52,,0,181,"<p>My problem is the following. I have a simple grid world:</p>

<p><a href=""https://i.imgur.com/2QyetBg.png"" rel=""nofollow noreferrer"">https://i.imgur.com/2QyetBg.png</a></p>

<p>The agent starts at the initial state labeled with START, and the goal is to reach the terminal state labeled with END. But, the agent has to avoid the barriers labeled with X and before reaching the END state it has to collect all items labeled with F. I implemented it by using Q-Learning and Sarsa as well, and the agent reaches the END state and avoid the barriers (X states). So this part works well.</p>

<p>My question is, how can I make agent to collect all the items (F states) before reaches END state? By using Q-Learning or Sarsa it avoids the barriers, reaches the END state but does not collect all the items. Usually one F state is visited and after the agent heading to the END state.</p>

<p>Thank you for your help!</p>
",6099702,,,,,2019-02-07 10:23:40,Teach robot to collect items in grid world before reach terminal state by using reinforcement learning,<machine-learning><reinforcement-learning><q-learning><gridworld><sarsa>,1,0,,,,CC BY-SA 4.0,
1659,68847879,1,68866855,,2021-08-19 12:26:11,,0,39,"<p>I am new to reinforcement learning and I know the basic theory behind it. However, I could not map the problem to the existing frameworks. The problem is as follows:</p>
<ol>
<li><p><strong>Given</strong> an environment with resources: X, Y, and Z</p>
</li>
<li><p><strong>Given</strong> a set of items I, each with (x, y, z, r), where x, y, and z are required resources for the item to serve, and r is the reward the agent receives if the item is served, (X, Y, Z) &gt;&gt; (x, y, z)</p>
</li>
<li><p>To select the items from the set to serve, I am using a <strong>cost function f = ax + by + cz</strong>, where a, b, and c are predefined constants.</p>
</li>
<li><p>The items are prioritized for selection based on the ratio r/f</p>
</li>
<li><p><strong>Objective</strong>: select items to serve so that the total reward (sum of r for all selected items) is maximum considering x, y,  and z for each item and resources X, Y, and Z</p>
</li>
<li><p><strong>Problem</strong>: how to tune the values of a, b, and c, so that total reward is maximized?</p>
</li>
</ol>
<p>Can you please suggest to me the following?</p>
<p>a) whether I can use reinforcement learning to tune the 'good' values of constants a, b, and c</p>
<p>b) If YES, how can I do that?</p>
<p>c) If NO, any suggestions for appropriate solution approaches?</p>
<p>Thank you.</p>
",15129421,,,,,2021-08-20 18:48:12,can we get 'good' values of predefined constants in a cost function using reinforcement learning?,<optimization><reinforcement-learning><reward>,1,0,,,,CC BY-SA 4.0,
1662,54519830,1,54556448,,2019-02-04 15:57:57,,0,424,"<p>My question follows my examination of the code in the PyTorch DQN tutorial, but then refers to Reinforcement Learning in general: what are the best practices for optimal exploration/exploitation in reinforcement learning?</p>

<p>In the DQN tutorial, the steps_done variable is a global variable, and the EPS_DECAY = 200. This means that: after 128 steps, the epsilon threshold = 0.500; after 889 steps, the epsilon threshold = 0.0600; and after 1500 steps, the epsilon threshold = 0.05047.</p>

<p>This might work for the CartPole problem featured in the tutorial â€“ where the early episodes might be very short and the task fairly simple â€“ but what about on more complex problems in which far more exploration is required? For example, if we had a problem with 40,000 episodes, each of which had 10,000 timesteps, how would we set up the epsilon greedy exploration policy? Is there some rule of thumb thatâ€™s used in RL work?</p>

<p>Thank you in advance for any help.</p>
",4665251,,,,,2019-02-06 14:55:11,Best practices for exploration/exploitation in Reinforcement Learning,<pytorch><reinforcement-learning>,1,0,,,,CC BY-SA 4.0,
1666,54678896,1,54684415,,2019-02-13 20:29:09,,22,20765,"<p>When trying to create a neural network and optimize it using Pytorch, I am getting</p>
<blockquote>
<p>ValueError: optimizer got an empty parameter list</p>
</blockquote>
<p>Here is the code.</p>
<pre><code>import torch.nn as nn
import torch.nn.functional as F
from os.path import dirname
from os import getcwd
from os.path import realpath
from sys import argv

class NetActor(nn.Module):
    def __init__(self, args, state_vector_size, action_vector_size, hidden_layer_size_list):
        super(NetActor, self).__init__()
        self.args = args

        self.state_vector_size = state_vector_size
        self.action_vector_size = action_vector_size
        self.layer_sizes = hidden_layer_size_list
        self.layer_sizes.append(action_vector_size)

        self.nn_layers = []
        self._create_net()

    def _create_net(self):
        prev_layer_size = self.state_vector_size
        for next_layer_size in self.layer_sizes:
            next_layer = nn.Linear(prev_layer_size, next_layer_size)
            prev_layer_size = next_layer_size
            self.nn_layers.append(next_layer)

    def forward(self, torch_state):
        activations = torch_state
        for i,layer in enumerate(self.nn_layers):
            if i != len(self.nn_layers)-1:
                activations = F.relu(layer(activations))
            else:
                activations = layer(activations)

        probs = F.softmax(activations, dim=-1)
        return probs
</code></pre>
<p>and then the call</p>
<pre><code>        self.actor_nn = NetActor(self.args, 4, 2, [128])
        self.actor_optimizer = optim.Adam(self.actor_nn.parameters(), lr=args.learning_rate)
</code></pre>
<p>gives the very informative error</p>
<blockquote>
<p>ValueError: optimizer got an empty parameter list</p>
</blockquote>
<p>I find it hard to understand what exactly in the network's definition makes the network have parameters.</p>
<p>I am following and expanding the example I found in <a href=""https://github.com/pytorch/examples/blob/master/reinforcement_learning/reinforce.py"" rel=""nofollow noreferrer"">Pytorch's tutorial code</a>.</p>
<p>I can't really tell the difference between my code and theirs that makes mine think it has no parameters to optimize.</p>
<p><strong>How to make my network have parameters like the linked example?</strong></p>
",913098,,913098,,2022-07-27 14:58:46,2022-07-27 14:58:46,Pytorch ValueError: optimizer got an empty parameter list,<python><machine-learning><pytorch><reinforcement-learning><backpropagation>,1,0,0,,,CC BY-SA 4.0,
1667,54759093,1,55504309,,2019-02-19 04:56:17,,1,337,"<p>I am trying to understand Karpathy's pong code in Python explained here:  <a href=""http://karpathy.github.io/2016/05/31/rl/"" rel=""nofollow noreferrer"">karpathy pong</a></p>

<pre><code># forward the policy network and sample an action from the returned probability
  #########action 2 is up and 3 is down
  aprob, h = policy_forward(x)
  print(""aprob\n {}\n h\n {}\n"".format(aprob, h))
  #2 is up, 3 is down
  action = 2 if np.random.uniform() &lt; aprob else 3 # roll the dice!
  print(""action\n {}\n"".format(action))
  # record various intermediates (needed later for backprop)
  xs.append(x) # observation, ie. the difference frame?
  #print(""xs {}"".format(xs))
  hs.append(h) # hidden state obtained from forward pass
  #print(""hs {}"".format(hs)) 
  #if action is up, y = 1, else 0
  y = 1 if action == 2 else 0 # a ""fake label""
  print(""y \n{}\n"".format(y))
  dlogps.append(y - aprob) # grad that encourages the action that was taken to be taken (see http://cs231n.github.io/neural-networks-2/#losses if confused)
  print(""dlogps\n {}\n"".format(dlogps))
  # step the environment and get new measurements
  observation, reward, done, info = env.step(action)
  print(""observation\n {}\n reward\n {}\n done\n {}\n "".format(observation, reward, done))
  reward_sum += reward
  print(""reward_sum\n {}\n"".format(reward_sum))
  drs.append(reward) # record reward (has to be done after we call step() to get reward for previous action)
  print(""drs\n {}\n"".format(drs))
  if done: # an episode finished
    episode_number += 1
</code></pre>

<p>In the above snippet, I don't quite understand why a fake label is necessary and what this means:<br>
<code>dlogps.append(y - aprob)# grad that encourages the action that was taken to be taken (see http://cs231n.github.io/neural-networks-2/#losses if confused)</code></p>

<p>Why is it fake label <code>y</code> minus <code>aprob</code>?  </p>

<p>My understanding is that the network outputs a ""log probability"" of moving up but then the explanation seems to indicate that the label should actually be the reward obtained for taking that action and then encourage all actions within an episode if it is a winning one.  Thus, I don't understand how a fake label of 1 or 0 helps.</p>

<p>Also in the forward pass function, there is no log operation so how is it a log probability?</p>

<pre><code>#forward pass, how is logp a logp without any log operation?????
def policy_forward(x):
  h = np.dot(model['W1'], x)
  h[h&lt;0] = 0 # ReLU nonlinearity
  logp = np.dot(model['W2'], h)
  p = sigmoid(logp)
  #print(""p\n {}\n and h\n {}\n"".format(p, h))
  return p, h # return probability of taking action 2 (up), and hidden state
</code></pre>

<p>Edit:</p>

<p>I used print statements to see what's happening under the hood and discovered that since <code>y=0</code> for action down, <code>(y - aprob)</code> will be negative for action down.  His formula to modulate gradient with advantage <code>epdlogp *= discounted_epr</code> still ends up indicating whether a move down was good, ie. a negative number or bad, ie. a positive number.<br>
  And for action up, the reverse is true when formula is applied.  ie. positive number for <code>epdlogp *= discounted_epr</code> means action was good, and negative means action was bad.<br>
  So this seems to be a fairly neat way to implement but I still don't understand how <code>aprob</code> returned from the forward pass is a log probability since output to console looks like this:</p>

<pre><code>aprob
 0.5

action
 3

aprob
 0.5010495775824385

action
 2

aprob
 0.5023498477623756

action
 2

aprob
 0.5051575154468827

action
 2
</code></pre>

<p>Those look like probabilities between 0 and 1.  So is using <code>y - aprob</code> as a ""log probability"" just a hack that comes with intuition developed over many months and years of practice?  If so, are these hacks discovered by trial and error? </p>

<p>Edit:  Thanks to the great explanation by Tommy, I knew where to look in my Udacity Deep Learning course videos for a refresher on log probabilities and cross entropy:  <a href=""https://www.youtube.com/watch?time_continue=94&amp;v=iREoPUrpXvE"" rel=""nofollow noreferrer"">https://www.youtube.com/watch?time_continue=94&amp;v=iREoPUrpXvE</a></p>

<p>Also, this cheatsheet helped:  <a href=""https://ml-cheatsheet.readthedocs.io/en/latest/loss_functions.html"" rel=""nofollow noreferrer"">https://ml-cheatsheet.readthedocs.io/en/latest/loss_functions.html</a></p>
",9481613,,9481613,,2019-08-25 04:33:13,2019-08-25 04:33:13,Karpathy Pong cross-entropy/log loss explanation for y - aprob,<python><gradient><reinforcement-learning>,1,0,,,,CC BY-SA 4.0,
1668,54759181,1,54765538,,2019-02-19 05:05:29,,0,6281,"<p>I can't wrap my head around question: <strong>how exactly negative rewards helps machine to avoid them?</strong><p></p>
Origin of the question came from <a href=""https://github.com/GoogleCloudPlatform/tensorflow-without-a-phd/blob/master/tensorflow-rl-pong/trainer/task.py"" rel=""nofollow noreferrer"">google's solution for game Pong</a>. By their logic, once game finished (agent won or lost point), environment returns reward (+1 or -1). Any intermediate states return 0 as reward. That means each win/loose will return either [0,0,0,...,0,1] either [0,0,0,...,0,-1] reward arrays. Then they discount and standardize rewards:<p></p></p>

<pre><code>#rwd - array with rewards (ex. [0,0,0,0,0,0,1]), args.gamma is 0.99
prwd = discount_rewards(rwd, args.gamma)
prwd -= np.mean(prwd)
prwd /= np.std(prwd)
</code></pre>

<p>discount_rewards suppose to be some kind of standard function, impl can be <a href=""https://github.com/GoogleCloudPlatform/tensorflow-without-a-phd/blob/master/tensorflow-rl-pong/trainer/helpers.py"" rel=""nofollow noreferrer"">found here</a>. Result for win (+1) could be something like this: <p></p></p>

<pre><code>[-1.487 , -0.999, -0.507, -0.010,  0.492, 0.999, 1.512]
</code></pre>

<p>For loose (-1):</p>

<pre><code>[1.487 , 0.999, 0.507, 0.010,  -0.492, -0.999, -1.512]
</code></pre>

<p>As result each move gets rewarded. Their loss function looks like this: </p>

<pre><code>loss = tf.reduce_sum(processed_rewards * cross_entropies + move_cost)
</code></pre>

<p>Please, help me answer next questions:</p>

<ol>
<li><strong>Cross entropy function can produce output from 0 -> inf. Right?</strong></li>
<li><strong>Tensorflow optimizer minimize loss by absolute value (doesn't care about sign, perfect loss is always 0). Right?</strong></li>
<li><strong>If statement 2 is correct, then loss 7.234 is equally bad as -7.234. Right?</strong></li>
<li><strong>If everything above is correct, than how negative reward tells machine that it's bad, and positive tells machine that it's good?</strong></li>
</ol>

<p>I also <a href=""https://stackoverflow.com/questions/49801638/normalizing-rewards-to-generate-returns-in-reinforcement-learning"">read this answer</a>, however I still didn't manage to get the idea <strong>exactly why negative worse than positive</strong>. It makes more sense to me to have something like:</p>

<pre><code>loss = tf.reduce_sum(tf.pow(cross_entropies, reward))
</code></pre>

<p>But that experiment didn't went well.</p>
",4179364,,,,,2019-02-19 12:43:34,Negative reward in reinforcement learning,<python><tensorflow><machine-learning><reinforcement-learning><pong>,2,0,0,,,CC BY-SA 4.0,
1670,54734556,1,54751920,,2019-02-17 15:08:22,,9,734,"<p>I want to implement the following algorithm, taken from <a href=""http://incompleteideas.net/book/bookdraft2017nov5.pdf"" rel=""nofollow noreferrer"">this book, section 13.6</a>:</p>

<p><a href=""https://i.stack.imgur.com/YxBlr.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/YxBlr.png"" alt=""enter image description here""></a></p>

<p>I don't understand how to implement the update rule in pytorch (the rule for w is quite similar to that of theta).</p>

<p>As far as I know, torch requires a loss for <code>loss.backwward()</code>.</p>

<p>This form does not seem to apply for the quoted algorithm.</p>

<p>I'm still certain there is a correct way of implementing such update rules in pytorch.</p>

<p>Would greatly appreciate a code snippet of how the w weights should be updated, given that V(s,w) is the output of the neural net, parameterized by w.</p>

<hr>

<p><strong>EDIT:</strong> Chris Holland suggested a way to implement, and I implemented it. It does not converge on Cartpole, and I wonder if I did something wrong.</p>

<p>The critic does converge on the solution to the function <code>gamma*f(n)=f(n)-1</code> which happens to be the sum of the series <code>gamma+gamma^2+...+gamma^inf</code>
meaning, gamma=1 diverges. gamma=0.99 converges on 100, gamma=0.5 converges on 2 and so on. Regardless of the actor or policy.</p>

<p>The code:</p>

<pre><code>def _update_grads_with_eligibility(self, is_critic, delta, discount, ep_t):
    gamma = self.args.gamma
    if is_critic:
        params = list(self.critic_nn.parameters())
        lamb = self.critic_lambda
        eligibilities = self.critic_eligibilities
    else:
        params = list(self.actor_nn.parameters())
        lamb = self.actor_lambda
        eligibilities = self.actor_eligibilities

    is_episode_just_started = (ep_t == 0)
    if is_episode_just_started:
        eligibilities.clear()
        for i, p in enumerate(params):
            if not p.requires_grad:
                continue
            eligibilities.append(torch.zeros_like(p.grad, requires_grad=False))

    # eligibility traces
    for i, p in enumerate(params):

        if not p.requires_grad:
            continue
        eligibilities[i][:] = (gamma * lamb * eligibilities[i]) + (discount * p.grad)
        p.grad[:] = delta.squeeze() * eligibilities[i]
</code></pre>

<p>and</p>

<pre><code>expected_reward_from_t = self.critic_nn(s_t)
probs_t = self.actor_nn(s_t)
expected_reward_from_t1 = torch.tensor([[0]], dtype=torch.float)
if s_t1 is not None:  # s_t is not a terminal state, s_t1 exists.
    expected_reward_from_t1 = self.critic_nn(s_t1)

delta = r_t + gamma * expected_reward_from_t1.data - expected_reward_from_t.data

negative_expected_reward_from_t = -expected_reward_from_t
self.critic_optimizer.zero_grad()
negative_expected_reward_from_t.backward()
self._update_grads_with_eligibility(is_critic=True,
                                    delta=delta,
                                    discount=discount,
                                    ep_t=ep_t)
self.critic_optimizer.step()
</code></pre>

<hr>

<p><strong>EDIT 2:</strong>
Chris Holland's solution works. The problem originated from a bug in my code that caused the line</p>

<pre><code>if s_t1 is not None:
    expected_reward_from_t1 = self.critic_nn(s_t1)
</code></pre>

<p>to always get called, thus <code>expected_reward_from_t1</code> was never zero, and thus no stopping condition was specified for the bellman equation recursion.</p>

<p>With no reward engineering, <code>gamma=1</code>, <code>lambda=0.6</code>, and a single hidden layer of size 128 for both actor and critic, this converged on a rather stable optimal policy within 500 episodes.</p>

<p>Even faster with <code>gamma=0.99</code>, as the graph shows (best discounted episode reward is about 86.6).</p>

<p><a href=""https://i.stack.imgur.com/qfKsv.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/qfKsv.png"" alt=""thanks""></a></p>

<p><strong>BIG thank you to @Chris Holland, who ""gave this a try""</strong></p>
",913098,,913098,,2019-02-21 15:04:19,2019-02-21 15:27:48,Pytorch: How to create an update rule that doesn't come from derivatives?,<python><machine-learning><pytorch><reinforcement-learning><backpropagation>,1,10,0,,,CC BY-SA 4.0,
1675,69134882,1,69138177,,2021-09-10 15:53:15,,3,413,"<p>I have been trying to implement Reinforcement Learning books exercise 2.5</p>
<p>I have written this piece of code according to this pseudo version</p>
<p><a href=""https://i.stack.imgur.com/sOsXd.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/sOsXd.png"" alt=""enter image description here"" /></a></p>
<pre><code>class k_arm:
    def __init__(self, iter, method=&quot;incrementally&quot;):

        # self.iter placeholder
        self.iter = iter
        self.k = 10
        self.eps = .1
        
        # here is Q(a) and N(a)
        self.qStar = np.zeros(self.k)
        self.n = np.zeros(self.k)
        
        # Method just for experimenting different functions
        self.method = method
        
    def pull(self):
        
        # selecting argmax(Q(A)) action with prob. (1 - eps)
        eps = np.random.uniform(0, 1, 1)
        if eps &lt; self.eps or self.qStar.argmax() == 0:
            a = np.random.randint(10)
        else: a = self.qStar.argmax()
        
        # R bandit(A)
        r = np.random.normal(0, 0.01, 1)
        
        # N(A) &lt;- N(A) + 1
        self.n[a] += 1
        
        # Q(A) &lt;- Q(A) i / (N(A)) * (R - Q(A))
        if self.method == &quot;incrementally&quot;:
            self.qStar[a] +=  (r - self.qStar[a]) / self.n[a] 
            return self.qStar[a]`

</code></pre>
<pre><code>iter = 1000
rewards = np.zeros(iter)
c = k_arm(iter, method=&quot;incrementally&quot;)

for i in range(iter):    
    k = c.pull()
    rewards[i] = k
</code></pre>
<p>And I get this as a result</p>
<p><a href=""https://i.stack.imgur.com/iBHJL.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/iBHJL.png"" alt=""enter image description here"" /></a></p>
<p>Where I am expecting this kind of results.</p>
<p><a href=""https://i.stack.imgur.com/afAqK.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/afAqK.png"" alt=""enter image description here"" /></a></p>
<p>I have been trying to understand where am I went missing here, but I couldn't.</p>
",12727917,,12727917,,2021-09-10 19:41:32,2021-09-22 12:18:49,K-Arms Bandit Epsilon-Greedy Policy,<python><machine-learning><reinforcement-learning>,1,7,,,,CC BY-SA 4.0,
1677,69181347,1,71834659,,2021-09-14 16:23:49,,4,1687,"<p>How can I add the rewards to tensorboard logging in Stable Baselines3 using a custom environment?</p>
<p>I have this learning code</p>
<pre><code>model = PPO(
    &quot;MlpPolicy&quot;, env,
    learning_rate=1e-4,
    policy_kwargs=policy_kwargs,
    verbose=1,
    tensorboard_log=&quot;./tensorboard/&quot;)
</code></pre>
",1106247,,,,,2022-10-15 10:42:57,Stable-Baselines3 log rewards,<python><logging><reinforcement-learning><tensorboard><stable-baselines>,3,0,,,,CC BY-SA 4.0,
1679,54849812,1,54850936,,2019-02-24 07:41:59,,2,4134,"<p>I am trying to apply reiforcement learning mechanism to classification tasks.
I know it is useless thing to do because deep learning can overperform rl in the tasks. Anyway in research purposes I am doing.</p>

<p>I reward agent if he's correct positive 1 or not negative -1
and computate loss FUNC with <code>predicted_action(predicted_class)</code> and reward.</p>

<p>But I get an error:</p>

<blockquote>
  <p>element 0 of tensors does not require grad and does not have a grad_fn</p>
</blockquote>

<pre><code> # creating model
class Net(nn.Module):
    def __init__(self):
        super(Net, self).__init__()

        self.pipe = nn.Sequential(nn.Linear(9, 120),
                                 nn.ReLU(),
                                 nn.Linear(120, 64),
                                 nn.ReLU(),
                                 nn.Linear(64,2),
                                 nn.Softmax()
                                 )

    def forward(self, x):
        return self.pipe(x)


def env_step(action, label, size):
    total_reward = []

    for i in range(size):
        reward = 0

        if action[i] == label[i]:
            total_reward.append(reward+1)
            continue
        else:
            total_reward.append(reward-1)
            continue

    return total_reward




if __name__=='__main__':
    epoch_size = 100
    net = Net()
    criterion = nn.MSELoss()
    optimizer = optim.Adam(params=net.parameters(), lr=0.01)

    total_loss = deque(maxlen = 50)

    for epoch in range(epoch_size):
        batch_index = 0
        for i in range(13):
            # batch sample
            batch_xs = torch.FloatTensor(train_state[batch_index: batch_index+50])   # make tensor
            batch_ys = torch.from_numpy(train_label[batch_index: batch_index+50]).type('torch.LongTensor')  # make tensor

            # action_prob; e.g classification prob
            actions_prob = net(batch_xs)                                
            #print(actions_prob)
            action = torch.argmax(actions_prob, dim=1).unsqueeze(1)    
            #print(action)
            reward = np.array(env_step(action, batch_ys, 50))  
            #print(reward)

            reward = torch.from_numpy(reward).unsqueeze(1).type('torch.FloatTensor')
            #print(reward)
            action = action.type('torch.FloatTensor')

            optimizer.zero_grad()
            loss = criterion(action, reward)    
            loss.backward()
            optimizer.step()


            batch_index += 50
</code></pre>
",10143396,,4621513,,2020-06-10 08:44:23,2020-06-10 08:44:23,element 0 of tensors does not require grad and does not have a grad_fn,<python><pytorch><classification><reinforcement-learning>,1,0,,,,CC BY-SA 4.0,
1681,69217296,1,69217918,,2021-09-17 03:07:47,,3,82,"<p>I need a way to represent a dictionary (or NumPy 2D array) in pictorial form, perhaps something as pictured below.</p>
<p><a href=""https://i.stack.imgur.com/WHYvf.png"" rel=""nofollow noreferrer"">Q-table</a></p>
<p>My dictionary currently looks like this
<code>Q: {(0,'U'): -0.1, (0,'R'): -0.254, (0,'D'): -0.9, (0,'L'): -0.23, ...}</code> where U, R, D, L corresponds to the direction, Up, Down, Right &amp; Left</p>
<p>For additional context, I wish to visualize the Q-table for the SARSA learning method. I am running this in a Jupyter notebook.
I am running SARSA over a total of 100k episodes &amp; would want to visualize the Q-table every 10k episode ran.</p>
<p>I suppose matplotlib might be able to do this? But I am not very familiar with this particular type of representation.</p>
<p>If anyone might know of better ways to represent the Q-table (as opposed to this particular pictorial format), I am open to suggestions. I can also represent the Q-table as a 2D numpy array instead of a dictionary if using a 2D array would be better.</p>
<p>Thanks in advance for any responses!</p>
",15781733,,15781733,,2021-09-17 03:21:17,2021-09-17 04:51:02,Representing python dictionary in pictorial form,<python><numpy><dictionary><matplotlib><reinforcement-learning>,1,0,,,,CC BY-SA 4.0,
1717,73754505,1,73836667,,2022-09-17 11:35:22,,2,43,"<p>I'm learning to use RLlib. I've been running it in my debugger on an example script, and it works, but for some reason I get an error message about the monitoring service failing. This is the traceback:</p>
<pre><code>File &quot;/home/ramrachum/.venvs/ray_env/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.py&quot;, line 600, in &lt;module&gt;
  monitor = Monitor(
File &quot;/home/ramrachum/.venvs/ray_env/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.py&quot;, line 205, in __init__
  logger.exception(
File &quot;/usr/lib/python3.10/logging/__init__.py&quot;, line 1512, in exception
  self.error(msg, *args, exc_info=exc_info, **kwargs)
File &quot;/usr/lib/python3.10/logging/__init__.py&quot;, line 70, in error
File &quot;/usr/lib/python3.10/logging/__init__.py&quot;, line 1911, in _LogErrorReplacement
  msg,
File &quot;/home/ramrachum/.venvs/ray_env/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.py&quot;, line 199, in __init__
  prometheus_client.start_http_server(
File &quot;/home/ramrachum/.venvs/ray_env/lib/python3.10/site-packages/prometheus_client/exposition.py&quot;, line 168, in start_wsgi_server
  TmpServer.address_family, addr = _get_best_family(addr, port)
File &quot;/home/ramrachum/.venvs/ray_env/lib/python3.10/site-packages/prometheus_client/exposition.py&quot;, line 157, in _get_best_family
  infos = socket.getaddrinfo(address, port)
File &quot;/usr/lib/python3.10/socket.py&quot;, line 955, in getaddrinfo
  for res in _socket.getaddrinfo(host, port, family, type, proto, flags):

socket.gaierror: [Errno -5] No address associated with hostname
</code></pre>
<p>I'm trying to understand why this bug is happening and how I can fix it. The hostname it's trying to use is <code>''</code>, which sounds like something that shouldn't work. Working my way up the traceback, I see that in <code>ray/autoscaler/_private/monitor.py</code> line 201, there's this logic:</p>
<pre><code>addr=&quot;127.0.0.1&quot; if head_node_ip == &quot;127.0.0.1&quot; else &quot;&quot;,
</code></pre>
<p>Since in my case, <code>head_node_ip</code> is equal to <code>'192.168.1.116'</code>, the <code>else</code> clause is used and an empty address is passed on <code>getaddrinfo</code>.</p>
<p>I'm not sure what the logic of this code is. Can <code>getaddrinfo</code> even work with an empty string? How does this service work for people normally? How do I make it not fail?</p>
",76701,,,,,2022-09-24 10:56:44,Ray monitoring fails when binding to empty address,<python><reinforcement-learning><ray><rllib>,1,1,,,,CC BY-SA 4.0,
1719,73798095,1,74085034,,2022-09-21 09:01:56,,1,43,"<p>everyone.</p>
<p>What is the difference between Multi-agent reinforcement learning and Multi-objective reinforcement learning?</p>
<p>And can you explain the pros and cons of the two methods?</p>
<p>I think both methods can serve some objectives...</p>
",18469670,,,,,2022-10-16 06:23:44,Multi-agent reinforcement learning versus multi-objective reinforcement learning,<reinforcement-learning><multi-agent-reinforcement-learning>,2,0,,,,CC BY-SA 4.0,
1724,73802653,1,73805224,,2022-09-21 14:37:25,,-1,58,"<p>I'm new to machine learning and reinforcement learning, and I'm attempting to create an AI agent that learns to play Snake. I am having trouble choosing / developing a neural network architecture that can work with the shape of my input / output vectors.</p>
<p>My input is a 3x10x10 tensor, basically 3 layers of a 10x10 grid the snake moves on
(I only use 0s and 1s throughout the tensor, mark the position of the snake's body parts
in the first layer, mark the apple's position on the second layer, and the snake's head position on the 3rd).</p>
<p>For my output, I'm looking for a vector of 4 values, corresponding to the 4 possible moves a player has available (change direction to up / down / left / right).</p>
<p>I would appreciate any recommendations on how to go about choosing an architecture in this case, as well as any thoughts regarding the way I chose to encode my game state into an input vector for the agent to train.</p>
",12956147,,4685471,,2022-09-23 13:04:17,2022-09-23 13:04:17,Choosing a neural network architecture for Snake AI Agent,<machine-learning><neural-network><reinforcement-learning>,1,3,,2022-09-23 13:03:47,,CC BY-SA 4.0,
1727,73921013,1,73921489,,2022-10-01 19:01:39,,0,66,"<p>I have code to calculate the off-policy importance sampling estimate commonly used in reinforcement learning. <strong>It is not important to know what that is</strong>, but for someone who does it might help them understand this question a little better. Basically, I have a 1D array of instances of a custom <code>Episode</code> class. An <code>Episode</code> has four attributes, all of which are arrays of floats. I have a function which loops over all episodes and for each one, it does a computation based only on the arrays in that episode. The result of that computation is a float, which I then store in a <code>result</code> array. Don't worry about what <code>model.get_prob_this_action()</code> does, you can consider it a black box that takes two floats as input and returns a float. The code for this function before optimizing with JAX is:</p>
<pre class=""lang-py prettyprint-override""><code>def IS_estimate(model, theta, episodes):
    &quot;&quot;&quot; Calculate the unweighted importance sampling estimate
    for each episode in episodes.
    Return as an array, one element per episode
    &quot;&quot;&quot;
    # episodes is an array of custom Python class instances
    
    gamma = 1.0
    result = np.zeros(len(episodes))
    for ii, ep in enumerate(episodes):
        obs = ep.observations # 1D array of floats
        actions = ep.actions # 1D array of floats
        rewards = ep.rewards # 1D array of floats
        action_probs = ep.action_probs # 1D array of floats

        pi_news = np.zeros(len(obs))
        for jj in range(len(obs)):
            pi_news[jj] = model.get_prob_this_action(obs[jj],actions[jj])

        pi_ratio_prod = np.prod(pi_news / action_probs)

        weighted_return = weighted_sum_gamma(rewards, gamma)
        result[ii] = pi_ratio_prod * weighted_return

    return np.array(result)
</code></pre>
<p>Unfortunately, I cannot just rewrite the function to work on a single episode and then use <code>jax.vmap</code> to vectorize over that function. The reason is that the argument I want to vectorize is a custom <code>Episode</code> object, which JAX won't support.</p>
<p>I <em>can</em> get rid of the inner loop to get <code>pi_news</code> using <code>vmap</code>, like:</p>
<pre class=""lang-py prettyprint-override""><code>def IS_estimate(model, theta, episodes):
    &quot;&quot;&quot; Calculate the unweighted importance sampling estimate
    for each episode in episodes.
    Return as an array, one element per episode
    &quot;&quot;&quot;
    # episodes is an array of custom Python class instances
    
    gamma = 1.0
    result = np.zeros(len(episodes))
    for ii, ep in enumerate(episodes):
        obs = ep.observations # 1D array of floats
        actions = ep.actions # 1D array of floats
        rewards = ep.rewards # 1D array of floats
        action_probs = ep.action_probs # 1D array of floats

        vmapped_get_prob_this_action = vmap(model.get_prob_this_action,in_axes=(0,0))
        pi_news = vmapped_get_prob_this_action(obs,actions)

        pi_ratio_prod = np.prod(pi_news / action_probs)

        weighted_return = weighted_sum_gamma(rewards, gamma)
        result[ii] = pi_ratio_prod * weighted_return

    return np.array(result)

</code></pre>
<p>and this does help some. But ideally, I'd like to vmap my outer loop as well. Does anyone know how I would do this?</p>
",2712595,,,,,2022-10-01 20:29:58,How to use JAX vmap to efficiently calculate importance sampling estimate,<python><reinforcement-learning><jax>,1,0,,,,CC BY-SA 4.0,
1739,74168655,1,74252933,,2022-10-23 04:29:25,,0,33,"<p>For migration from ray 0.x to ray 1.x, <code>ray.rllib.models.Model</code> must be replaced with <code>ray.rllib.models.ModelV2</code>.</p>
<p>Are there any migration guidelines describing how to do this properly?</p>
<p>Thank you.</p>
",1616037,,,,,2022-10-30 12:10:29,Migration tips for ray.rllib.models: DeprecationWarning: `Model` has been deprecated. Use `ModelV2` instead,<migration><reinforcement-learning><ray><rllib>,1,0,,,,CC BY-SA 4.0,
1741,74214034,1,74215692,,2022-10-26 20:44:57,,0,126,"<p>I am training a REINFORCE algorithm on the CartPole environment. Due to the simple nature of the environment, I expect it to learn quickly. However, that doesn't happen.</p>
<p>Here is the main portion of the algorithm -</p>
<pre><code>for i in range(episodes):
    print(&quot;i = &quot;, i)
    state = env.reset()
    done = False
    transitions = []

    tot_rewards = 0
    while not done:

        act_proba = model(torch.from_numpy(state))
        action = np.random.choice(np.array([0,1]), p = act_proba.data.numpy())
        next_state, reward, done, info = env.step(action)
        tot_rewards += 1
        transitions.append((state, action, tot_rewards))
        state = next_state


    if i%50==0:
        print(&quot;i = &quot;, i, &quot;,reward = &quot;, tot_rewards)
    score.append(tot_rewards)
    reward_batch = torch.Tensor([r for (s,a,r) in transitions])
    disc_rewards = discount_rewards(reward_batch)
    nrml_disc_rewards = normalize_rewards(disc_rewards)
    state_batch = torch.Tensor([s for (s,a,r) in transitions])
    action_batch = torch.Tensor([a for (s,a,r) in transitions])
    pred_batch = model(state_batch)
    prob_batch = pred_batch.gather(dim=1, index=action_batch.long().view(-1, 1)).squeeze()
    loss = -(torch.sum(torch.log(prob_batch)*nrml_disc_rewards))
    opt.zero_grad()
    loss.backward()
    opt.step()
</code></pre>
<p>Here is the entire algorithm -</p>
<pre><code>#I referred to this when writing the code - https://github.com/DeepReinforcementLearning/DeepReinforcementLearningInAction/blob/master/Chapter%204/Ch4_book.ipynb

import numpy as np
import gym
import torch
from torch import nn
env = gym.make('CartPole-v0')
learning_rate = 0.0001
episodes = 10000

def discount_rewards(reward, gamma = 0.99):
    return torch.pow(gamma, torch.arange(len(reward)))*reward
def normalize_rewards(disc_reward):
    return disc_reward/(disc_reward.max())

class NeuralNetwork(nn.Module):
    def __init__(self, state_size, action_size):
        super(NeuralNetwork, self).__init__()
        self.state_size = state_size
        self.action_size = action_size
        self.linear_relu_stack = nn.Sequential(
            nn.Linear(state_size, 300),
            nn.ReLU(),
            nn.Linear(300, 128),
            nn.ReLU(),
            nn.Linear(128, 128),
            nn.ReLU(),
            nn.Linear(128, action_size),
            nn.Softmax()
        )

    def forward(self,x):
        x = self.linear_relu_stack(x)
        return x

model = NeuralNetwork(env.observation_space.shape[0], env.action_space.n)
opt = torch.optim.Adam(params = model.parameters(), lr = learning_rate)
score = []
for i in range(episodes):
    print(&quot;i = &quot;, i)
    state = env.reset()
    done = False
    transitions = []

    tot_rewards = 0
    while not done:

        act_proba = model(torch.from_numpy(state))
        action = np.random.choice(np.array([0,1]), p = act_proba.data.numpy())
        next_state, reward, done, info = env.step(action)
        tot_rewards += 1
        transitions.append((state, action, tot_rewards))
        state = next_state


    if i%50==0:
        print(&quot;i = &quot;, i, &quot;,reward = &quot;, tot_rewards)
    score.append(tot_rewards)
    reward_batch = torch.Tensor([r for (s,a,r) in transitions])
    disc_rewards = discount_rewards(reward_batch)
    nrml_disc_rewards = normalize_rewards(disc_rewards)
    state_batch = torch.Tensor([s for (s,a,r) in transitions])
    action_batch = torch.Tensor([a for (s,a,r) in transitions])
    pred_batch = model(state_batch)
    prob_batch = pred_batch.gather(dim=1, index=action_batch.long().view(-1, 1)).squeeze()
    loss = -(torch.sum(torch.log(prob_batch)*nrml_disc_rewards))
    opt.zero_grad()
    loss.backward()
    opt.step()
</code></pre>
",11628437,,11628437,,2022-10-26 23:25:58,2022-10-27 01:19:53,Why is my REINFORCE algorithm not learning?,<reinforcement-learning>,1,0,,,,CC BY-SA 4.0,
1746,74103447,1,74110563,,2022-10-17 21:30:41,,2,59,"<p>I'm using RLlib to train a reinforcement learning policy (PPO algorithm). I want to see the weights in the neural network underlying the policy.</p>
<p>After digging through RLlib's <code>PPO</code> object, I found the TensorFlow <code>Graph</code> object. I thought that I would find the weights of the neural network there. But I can't find them. I see that this graph has ~1,000 nodes but I can't for the life of me find where TensorFlow is hiding the actual weights for the neural network. I looked through the nodes. I was told to keep an eye out for <code>tf.Variable</code> objects, but I couldn't find any. The closest thing I could find are nodes of type <code>ReadVariableOp</code>, but I couldn't find a <code>tf.Variable</code> in them. I did find a <code>tf.Tensor</code> in there, but I'm not sure whether it holds actual numbers, and if so how to get them.</p>
<p>Where do I find the weights of my neural network?</p>
",76701,,,,,2022-10-22 12:05:50,Get the neural network weights out of a Tensorflow `Graph`,<python><tensorflow><neural-network><reinforcement-learning><rllib>,1,0,,,,CC BY-SA 4.0,
1752,74138999,1,74140926,,2022-10-20 11:19:17,,0,25,"<p>I am new to the field of Q-learning (QL) and I am trying to implement a small task using QL in MATLAB. The task is : Say there is one transmitter, one receiver and between them there are 10 relays. The main part is that I want to choose one of the relay using QL that will carry the signal from transmitter to receiver successfully.</p>
<p>So, as per QL theory, we need to define state, action, reward. Hence I had chosen them as:
State : [P1,...,P10] where P1 is power from 1st relay to receiver. Like wise P10 is power from 10th relay to receiver.</p>
<p>action : [1,...,10] where action is nothing but choosing that relay which has highest power at that time.</p>
<p>My query is I am not getting how should I choose reward in this case ?</p>
<p>Any help in this regard will be highly appreciated.</p>
",19038205,,,,,2022-10-20 13:43:21,confusion in selecting reward in q-learning,<algorithm><reinforcement-learning><q-learning>,1,0,,,,CC BY-SA 4.0,
1767,74444263,1,74444506,,2022-11-15 10:42:50,,2,21,"<p>I'm trying to tune the hyperparameters using the Stable-Baseline-3 Library for the network architecture.</p>
<p>My configuration file is:</p>
<pre><code>program: main.py
method: bayes
name: sweep
metric:
  goal: minimize
  name: train/loss
parameters:
  batch_size:
    values: [16, 32, 64, 128, 256, 512, 1024]
  epochs:
    values: [20, 50, 100, 200, 250, 300]
  lr:
    max: 0.1
    min: 0.000001
</code></pre>
<p>But if I try to add to the parameters:</p>
<pre><code>  policy_kwargs:
    net_arch:
      pi:
        values: [[ 128, 128 ],[ 256, 256 ],[ 512, 512 ]]
      vf:
        values: [[ 128, 128 ],[ 256, 256 ],[ 512, 512 ]]
</code></pre>
<p>I got the following error:</p>
<pre><code>wandb.errors.CommError: Invalid sweep config: invalid hyperparameter configuration: policy_kwargs
</code></pre>
<p>Is it possible to use wandb sweep with Stable-Baseline-3 for the network architecture?</p>
",794402,,,,,2022-11-15 11:01:42,Hyperparameter Tuning with Wandb Sweep for custom parameters,<deep-learning><pytorch><reinforcement-learning><stable-baselines><wandb>,1,0,,,,CC BY-SA 4.0,
1768,74464029,1,74464151,,2022-11-16 16:29:59,,0,22,"<p>I am creating a basic gridworld RL problem and I need to calculate the return for some given episode. I currently have the array of rewards, and I would like to element-wise multiply this with a list of the form:</p>
<pre><code>[gamma**0, gamma**1, gamma**2, ....]
</code></pre>
<p>In order to get:</p>
<pre><code>[r_0*gamma**0, r_1*gamma**1, r_2*gamma**2, ....]
</code></pre>
<p>and then use np.sum() to get the entire return.</p>
<p>How can I complete that first step? I tried using Logspace, but it isn't quite what I want (or I'm doing it wrong).</p>
",15048596,,,,,2022-11-16 16:39:22,Numpy - How to get an array of the pattern gamma^t for some 0-t?,<python><arrays><numpy><reinforcement-learning>,1,0,,,,CC BY-SA 4.0,
1776,56270347,1,56300549,,2019-05-23 07:43:43,,1,94,"<p>The DQN algorithm below</p>

<p><a href=""https://i.stack.imgur.com/Ozvl2.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/Ozvl2.png"" alt=""enter image description here""></a></p>

<p><a href=""https://medium.com/@jonathan_hui/rl-dqn-deep-q-network-e207751f7ae4"" rel=""nofollow noreferrer"">Source</a></p>

<p>At the gradient descent line, there's something I don't quite understand. </p>

<p>For example, if I have 8 actions, then the output Q is a vector of 8 components, right? </p>

<p>But for each record in D, the return y_i is only a scalar with respect to a given action. How can I perform gradient descent on (y_i - Q)^2 ? I think it's not guaranteed that within a minibatch I have all actions' returns for a state.</p>
",3745149,,,,,2019-05-24 23:40:16,"In DQN, hwo to perform gradient descent when each record in experience buffer corresponds to only one action?",<reinforcement-learning>,1,0,,,,CC BY-SA 4.0,
1777,10722064,1,10722300,,2012-05-23 14:27:54,,67,29367,"<p>I know the basics of feedforward neural networks, and how to train them using the backpropagation algorithm, but I'm looking for an algorithm than I can use for training an ANN online with reinforcement learning.</p>

<p>For example, the <a href=""http://www.google.com/search?q=cart%20pole%20swing%20up"" rel=""noreferrer"">cart pole swing up</a> problem is one I'd like to solve with an ANN. In that case, I don't know what should be done to control the pendulum, I only know how close I am to the ideal position. I need to have the ANN learn based on reward and punishment. Thus, supervised learning isn't an option.</p>

<p>Another situation is something like the <a href=""http://en.wikipedia.org/wiki/Snake_%28video_game%29"" rel=""noreferrer"">snake game</a>, where feedback is delayed, and limited to goals and anti-goals, rather than reward.</p>

<p>I can think of some algorithms for the first situation, like hill-climbing or genetic algorithms, but I'm guessing they would both be slow. They might also be applicable in the second scenario, but incredibly slow, and not conducive to online learning.</p>

<p>My question is simple: <strong>Is there a simple algorithm for training an artificial neural network with reinforcement learning?</strong> I'm mainly interested in real-time reward situations, but if an algorithm for goal-based situations is available, even better.</p>
",785745,,,,,2017-11-08 09:33:34,Training a Neural Network with Reinforcement learning,<algorithm><language-agnostic><machine-learning><neural-network><reinforcement-learning>,2,1,0,,,CC BY-SA 3.0,
1780,48312854,1,48554288,,2018-01-18 02:19:26,,0,197,"<p>I use a full-connected network to get the whole words distribution from the last state of an encoder.</p>

<p>For example, there are 5 words in the vocabulary.</p>

<pre><code>P = [0.1, 0.1, 0.2, 0.2, 0,4]
</code></pre>

<p>And the ground truth is a words' set for this train data.</p>

<p>I sample 3 words from the 5 words and if the target set contains the 3 words , then I want the probability of the 3 words in <code>P</code> increase, for this state. </p>

<p>If one of the 3 word is not in the target set, then I want the probability of the word in <code>P</code> decrease, for this state.</p>

<p>So I wrote these code:</p>

<pre><code>reward = [0,0,0]
</code></pre>

<p>Suppose the first 3 words are sampled from <code>P</code>, and only the first 2 of the 3 words are in the target set. And the third word is not in the target set. Then</p>

<pre><code>reward = [1,1,-1]
</code></pre>

<p>Then I compute the negative sum and dot product of <code>reward</code> and sampled 3 <code>P2=[0.1, 0.1, 0.2]</code> as the loss</p>

<pre><code>loss = -sum(reward * P2.log())
</code></pre>

<p>But I fail to get the result: The top probability words can be selected from the vocabulary for every state.</p>
",2455061,,3782161,,2018-01-18 10:46:35,2018-02-01 02:05:47,How to define the loss function or how to optimize if the target is a set?,<nlp><deep-learning><reinforcement-learning>,1,1,,,,CC BY-SA 3.0,
1781,13343336,1,13463887,,2012-11-12 12:00:19,,3,392,"<p>I am working on the power management of a system. The objectives that I am looking to minimize are power consumption and average latency. I have a single objective function having the linearly weighted sum of both the objectives:</p>

<pre><code>C=w.P_avg+(1-w).L_avg,      where w belongs to (0,1)
</code></pre>

<p>I am using Q-learning to find a pareto-optimal trade-off curve by varying the weight w and setting different preference to power consumption and average latency. I do obtain a pareto-optimal curve. My objective, now, is to provide a constraint (e.g., average latency L_avg) and thus tuning/finding the value of w to meet the given criteria. Mine is an online algorithm, so the tuning of w should take place in an online fashion.</p>

<p>Could I be provided any hint or suggestions in this regard?</p>
",846400,,,,,2019-11-07 05:49:51,Multi-Criteria Optimization with Reinforcement Learning,<machine-learning><power-management><reinforcement-learning>,1,0,0,,,CC BY-SA 3.0,
1787,51036420,1,51045893,,2018-06-26 06:43:28,,2,174,"<p>In recent reinforcement learning researches about Atari games, agents performance is evaluated by <em>human start</em>.</p>

<ul>
<li><a href=""https://arxiv.org/abs/1507.04296"" rel=""nofollow noreferrer"" title=""[1507.04296] Massively Parallel Methods for Deep Reinforcement Learning"">[1507.04296] Massively Parallel Methods for Deep Reinforcement Learning</a></li>
<li><a href=""https://arxiv.org/abs/1509.06461"" rel=""nofollow noreferrer"" title=""[1509.06461] Deep Reinforcement Learning with Double Q-learning"">[1509.06461] Deep Reinforcement Learning with Double Q-learning</a></li>
<li><a href=""https://arxiv.org/abs/1511.05952"" rel=""nofollow noreferrer"" title=""[1511.05952] Prioritized Experience Replay"">[1511.05952] Prioritized Experience Replay</a></li>
</ul>

<p>In the <em>human start</em> evaluation, learned agents begin episodes of randomly sampled point from a human professional's game-play.</p>

<p>My question is:<br>
Where can I get this human professional's game-play trace data?<br>
For fare comparison, the trace data should be same among each research but I could not find the data.</p>
",2500650,,2500650,,2018-06-26 07:03:00,2018-06-26 14:55:47,human trace data for evaluation of reinforcement learning agent playing Atari?,<reinforcement-learning><q-learning>,1,0,,,,CC BY-SA 4.0,
1789,50037674,1,50096884,,2018-04-26 07:55:13,,9,15228,"<p>I am trying to run a lunar_lander on reinforcement
learning, but when I run it, it occurs an error.
Plus my computer is osx system.</p>

<p>Here is the code of lunar lander:</p>

<pre><code>import numpy as np
import gym
import csv

from keras.models import Sequential
from keras.layers import Dense, Activation, Flatten
from keras.optimizers import Adam

from rl.agents.dqn import DQNAgent
from rl.policy import BoltzmannQPolicy, EpsGreedyQPolicy
from rl.memory import SequentialMemory

import io
import sys
import csv

# Path environment changed to make things work properly
# export DYLD_FALLBACK_LIBRARY_PATH=$DYLD_FALLBACK_LIBRARY_PATH:/usr/lib


# Get the environment and extract the number of actions.
ENV_NAME = 'LunarLander-v2'
env = gym.make(ENV_NAME)
np.random.seed(123)
env.seed(123)
nb_actions = env.action_space.n

# Next, we build a very simple model.
model = Sequential()
model.add(Flatten(input_shape=(1,) + env.observation_space.shape))
model.add(Dense(16))
model.add(Activation('relu'))
model.add(Dense(16))
model.add(Activation('relu'))
model.add(Dense(16))
model.add(Activation('relu'))
model.add(Dense(nb_actions))
model.add(Activation('linear'))
#print(model.summary())

# Finally, we configure and compile our agent. You can use every built-in Keras optimizer and
# even the metrics!
memory = SequentialMemory(limit=300000, window_length=1)
policy = EpsGreedyQPolicy()
dqn = DQNAgent(model=model, nb_actions=nb_actions, memory=memory, nb_steps_warmup=10,
               target_model_update=1e-2, policy=policy)
dqn.compile(Adam(lr=1e-3), metrics=['mae'])

# After training is done, we save the final weights.
dqn.load_weights('dqn_{}_weights.h5f'.format(ENV_NAME))

# Redirect stdout to capture test results
old_stdout = sys.stdout
sys.stdout = mystdout = io.StringIO()

# Evaluate our algorithm for a few episodes.
dqn.test(env, nb_episodes=200, visualize=False)

# Reset stdout
sys.stdout = old_stdout

results_text = mystdout.getvalue()

# Print results text
print(""results"")
print(results_text)

# Extact a rewards list from the results
total_rewards = list()
for idx, line in enumerate(results_text.split('\n')):
    if idx &gt; 0 and len(line) &gt; 1:
        reward = float(line.split(':')[2].split(',')[0].strip())
        total_rewards.append(reward)

# Print rewards and average
print(""total rewards"", total_rewards)
print(""average total reward"", np.mean(total_rewards))

# Write total rewards to file
f = open(""lunarlander_rl_rewards.csv"",'w')
wr = csv.writer(f)
for r in total_rewards:
     wr.writerow([r,])
f.close()
</code></pre>

<p>Here is the error:</p>

<pre><code>Traceback (most recent call last):
  File ""/s/user/Document/Semester2/Advanced Machine Learning/Lab/Lab6/lunar_lander_ml_states_player.py"", line 23, in &lt;module&gt;
    env = gym.make(ENV_NAME)
  File ""/s/user/anaconda/envs/untitled/lib/python3.6/site-packages/gym/envs/registration.py"", line 167, in make
    return registry.make(id)
  File ""/s/user/anaconda/envs/untitled/lib/python3.6/site-packages/gym/envs/registration.py"", line 119, in make
    env = spec.make()
  File ""/s/user/anaconda/envs/untitled/lib/python3.6/site-packages/gym/envs/registration.py"", line 85, in make
    cls = load(self._entry_point)
  File ""/s/user/anaconda/envs/untitled/lib/python3.6/site-packages/gym/envs/registration.py"", line 14, in load
    result = entry_point.load(False)
  File ""/s/user/anaconda/envs/untitled/lib/python3.6/site-packages/pkg_resources/__init__.py"", line 2405, in load
    return self.resolve()
  File ""/s/user/anaconda/envs/untitled/lib/python3.6/site-packages/pkg_resources/__init__.py"", line 2411, in resolve
    module = __import__(self.module_name, fromlist=['__name__'], level=0)
  File ""/s/user/anaconda/envs/untitled/lib/python3.6/site-packages/gym/envs/box2d/__init__.py"", line 1, in &lt;module&gt;
    from gym.envs.box2d.lunar_lander import LunarLander
  File ""/s/user/anaconda/envs/untitled/lib/python3.6/site-packages/gym/envs/box2d/lunar_lander.py"", line 4, in &lt;module&gt;
    import Box2D
  File ""/s/user/anaconda/envs/untitled/lib/python3.6/site-packages/Box2D/__init__.py"", line 20, in &lt;module&gt;
    from .Box2D import *
  File ""/s/user/anaconda/envs/untitled/lib/python3.6/site-packages/Box2D/Box2D.py"", line 435, in &lt;module&gt;
    _Box2D.RAND_LIMIT_swigconstant(_Box2D)
AttributeError: module '_Box2D' has no attribute 'RAND_LIMIT_swigconstant'
</code></pre>

<p>I tried to reinstall Box2d by following the guide of <a href=""https://github.com/pybox2d/pybox2d/blob/master/INSTALL.md"" rel=""noreferrer"">https://github.com/pybox2d/pybox2d/blob/master/INSTALL.md</a>
but it still doesn't work, could anyone help me ? </p>
",9591860,,9591860,,2019-01-23 13:58:05,2020-01-27 22:19:33,AttributeError: module '_Box2D' has no attribute 'RAND_LIMIT_swigconstant',<python><machine-learning><box2d><reinforcement-learning>,2,0,0,,,CC BY-SA 4.0,
1791,18720247,1,18727213,,2013-09-10 13:26:19,,1,453,"<p>I'm attempting to pose a problem as a reinforcement learning problem. My difficulty is that the state which an agent is in changes randomly. They must simply choose an action within the state they are in. I want to learn appropriate actions for all states based on the reward they receive for performing actions.</p>

<p>Question:</p>

<p>Is this a specific type of RL problem?
If there is no successor state, so how would one calculate the value of a state?</p>
",1550094,,254477,,2013-09-10 13:46:02,2013-09-11 05:58:35,Reinforcement Learning without Successor State,<reinforcement-learning><mdp>,2,0,0,,,CC BY-SA 3.0,
1792,32846262,1,33363323,,2015-09-29 14:13:09,,11,11274,"<p>The difference between Q-learning and SARSA is that Q-learning compares the current state and the best possible next state, whereas SARSA compares the current state against the actual next state.</p>

<p>If a greedy selection policy is used, that is, the action with the highest action value is selected 100% of the time, are SARSA and Q-learning then identical?</p>
",781608,,3924118,,2018-08-21 11:12:36,2019-04-09 22:26:56,Are Q-learning and SARSA with greedy selection equivalent?,<reinforcement-learning><q-learning><sarsa>,3,2,0,,,CC BY-SA 4.0,
1794,22537246,1,22554601,,2014-03-20 15:11:24,,2,2358,"<p>I'm trying to do a simple Q learning algorithm, but for whatever reason it doesn't converge. The agent should basically get from one point on the 5x5 grid to the goal one. When I run it it seems to have found the most optimal way however it doesn't converge and I can't figure out why. Any help would be appreciated. I feel like there is one little mistake somewhere so that's why I'm looking for a fresh set of eyes.</p>

<p>Code:</p>

<pre><code>function Q=ReinforcementLearning

clc;

format short

format compact

% three input: R, alpha and gamma

% immediate reward matrix; 

% row and column = states; -Inf = no door between room

R=[-inf,-inf,    0,    0, -inf;

   -inf,-inf,    0,    0, -inf;

      0,   0, -inf, -inf,  100;

      0,   0, -inf, -inf, -inf;

   -inf,-inf,    0, -inf, 100];



gamma=0.8;            % learning parameter
alpha=0.5;
oldQ = 0;


Q=zeros(size(R));      % initialize Q as zero

q1=ones(size(R))*inf;  % initialize previous Q as big number

count=0;               % counter


for episode=0:50000   

   % random initial state

   y=randperm(size(R,1));

   state=y(1);




   % select any action from this state

   x=find(R(state,:)&gt;=0);        % find possible action of this state

   if size(x,1)&gt;0,

      x1=RandomPermutation(x);   % randomize the possible action

      x1=x1(1);                  % select an action 

   end


   MaxQ=max(Q,[],2);

   %Q(state,x1) = R(state,x1) + (gamma * MaxQ(x1)); %old function that works perfectly (converges)

   Q(state,x1)=  oldQ + alpha * (R(state,x1)+ (gamma * MaxQ(x1)) - oldQ); % new one that I need to implement

   oldQ = Q(state,x1);



   state=x1; %#ok&lt;NASGU&gt;


   %Q = round(Q);


   % break if convergence: small deviation on q for 1000 consecutive

   if sum(sum(abs(q1-Q)))&lt;5 &amp; sum(sum(Q &gt; 0))

      if count&gt;1000,

         episode        % report last episode

         break          % for

      else

         count=count+1; % set counter if deviation of q is small

      end

   else

      q1=Q;

      count=0; % reset counter when deviation of q from previous q is large

   end

end


%normalize q

g=max(max(Q));

episode

if g&gt;0, 

   Q=100*Q/g;
   roundQ = round(Q);
   roundQ

end  
</code></pre>
",3442593,,,,,2014-04-20 21:29:29,Q Learning Algorithm Issue,<algorithm><matlab><reinforcement-learning><q-learning><temporal-difference>,1,2,0,,,CC BY-SA 3.0,
1795,40910757,1,40928030,,2016-12-01 12:31:08,,0,1668,"<p>Good morning,
In Q-learning, the agents take actions until reaching their goal. The algorithm is executed many times until obtaining convergence. For example, the goal is to obtain a maximum throughput until the end of the time simulation. The simulation time is divided into n equal periods T and the reward varies over time. So, the agents update their states n times at the begenning of each period. In this case, n is considered as the number of steps or iterations? In addition, the update of the Q-value is done after executing the selected action or before the execution (using the reward function which is an approximation of the real reward)?
I would be grateful if you can answer to my questions. </p>
",5588998,,5588998,,2016-12-01 17:57:29,2021-11-15 07:37:42,iterations and reward in q-learning,<reinforcement-learning><q-learning>,1,0,,,,CC BY-SA 3.0,
1796,29904270,1,30011176,,2015-04-27 19:25:21,,17,3524,"<p>I'm looking at this SARSA-Lambda implementation (Ie: SARSA with eligibility traces) and there's a detail which I still don't get.</p>

<p><img src=""https://i.stack.imgur.com/WBHbX.png"" alt=""enter image description here""></p>

<p>(Image from <a href=""http://webdocs.cs.ualberta.ca/~sutton/book/ebook/node77.html"">http://webdocs.cs.ualberta.ca/~sutton/book/ebook/node77.html</a>)</p>

<p>So I understand that all Q(s,a) are updated rather than only the one the agent has chosen for the given time-step. I also understand the E matrix is not reset at the start of each episode.</p>

<p>Let's assume for a minute that panel 3 of Figure 7.12 was the end-state of episode 1.</p>

<p>At the start of episode 2, the agent moves north instead of east, and let's assume this gives it a reward of -500. Wouldn't this affect also all states that were visited in the previous episode?</p>

<p>If the idea is to reward those states which have been visited in the current episode, then why isn't the matrix containing all e(s,a) values reset at the beginning of each episode? It just seems like with this implementation states that have been visited in the previous episode are 'punished' or 'rewarded' for actions done by the agent in this new episode.</p>
",1876047,,1560599,,2015-05-03 07:11:27,2015-05-03 07:15:42,Eligibility trace reinitialization between episodes in SARSA-Lambda implementation,<machine-learning><reinforcement-learning><sarsa>,1,1,0,,,CC BY-SA 3.0,
1802,29978406,1,30017803,,2015-04-30 21:48:39,,1,1338,"<p>I have successfully implemented a SARSA algorithm (both one-step and using eligibility traces) using table lookup. In essence, I have a q-value matrix where each row corresponds to a state and each column to an action.</p>

<p>Something like:</p>

<pre><code>[Q(s1,a1), Q(s1,a2), Q(s1,a3), Q(s1,a4)]
[Q(s2,a1), (Q(s2,a2), Q(s2a3), Q(s2, a2]
.
.
.
[Q(sn,a1), Q(sn,a2), Q(sn,a3), Q(sn,a4)]
</code></pre>

<p>At each time-step, a row from the matrix is picked and, depending on policy, an action is picked and updated according to SARSA rules.</p>

<p>I am now trying to implement it as a neural-network using gradient descent.</p>

<p>My first hypothesis was to create a two-layer network, the input layer having as many input neurons as there are states, and the output layer having as many output neurons as there are actions. Each input would be fully connected to each output. (So, in fact, it would look as the matrix above)</p>

<p>My input vector would be a 1xn row vector, where n is the number of input neurons. All values in the input vector would be 0, except for the index corresponding to the current state which would be 1. Ie:</p>

<pre><code>[0 0 0 1 0 0]
</code></pre>

<p>Would be an input vector for an agent in state 4.</p>

<p>So, the process would be something like:</p>

<pre><code>[0 0 0 1 0 0] X [ 4 7 9 3]
                [ 5 3 2 9]
                [ 3 5 6 9]
                [ 9 3 2 6]
                [ 2 5 7 8]
                [ 8 2 3 5]
</code></pre>

<p>Where I have created a random, sample weight-matrix.</p>

<p>The result would be:</p>

<pre><code>[9 3 2 6]
</code></pre>

<p>Meaning that if a greedy policy was picked action 1 should be picked and the connection between the fourth input neuron and the first output neuron should become stronger by:</p>

<pre><code>dw = dw_old + learning_rate*(reward + discount*network_output - dw_old)
</code></pre>

<p>(Equation taken from SARSA algorithm)</p>

<p>HOWEVER - this implementation doesn't convince me. According to what I read, the network weights should be used to calculate the Q-value of a state-action pair, but I'm not sure they should <em>represent</em> such values. (Especially because I've usually seen weight values only being included between 0 and 1.)</p>

<p>Any advice?</p>
",1876047,,1560599,,2015-05-03 19:29:18,2015-05-03 19:29:18,Implementing SARSA using Gradient Discent,<machine-learning><reinforcement-learning><sarsa>,1,0,,,,CC BY-SA 3.0,
1804,13981880,1,15362341,,2012-12-20 22:58:26,,1,1540,"<p>I am currently trying to understand how TD-Gammon works and have two questions:</p>

<p>1) I found an <a href=""http://www.stanford.edu/group/pdplab/pdphandbook/handbookch10.html"" rel=""nofollow"">article</a> which explains the weight update. It consists of three part. The last part is an differentiation of V(s) with respect to w. In the text it is called a ""running sum"". How do I calculate that value? (I'm only interested in the weight changes from the output to the hidden layer, not in further weight changes)</p>

<p>2) After having read this procedure of updating the weights, there has one question arised: Why don't we just create a target value for a state using reinforcement learning and give that value to our neural network, so that it learns to return that value for the current state? Why is there an extra updating rule directly manipulating the weights?</p>
",1406177,,,,,2013-03-12 13:16:13,Weight update - Reinforcement Learning + Neural Networks,<neural-network><reinforcement-learning>,1,0,0,,,CC BY-SA 3.0,
1807,48878053,1,48880275,,2018-02-20 04:50:20,,3,2816,"<p>Just for context, I'm trying to implement a gradient descent algorithm with Tensorflow.</p>

<p>I have a matrix <code>X</code></p>

<pre><code>[ x1 x2 x3 x4 ]
[ x5 x6 x7 x8 ]
</code></pre>

<p>which I multiply by some feature vector <code>Y</code> to get <code>Z</code></p>

<pre><code>      [ y1 ]
Z = X [ y2 ]  = [ z1 ]
      [ y3 ]    [ z2 ]
      [ y4 ]
</code></pre>

<p>I then put Z through a softmax function, and take the log. I'll refer to the output matrix as W.</p>

<p>All this is implemented as follows (little bit of boilerplate added so it's runnable)</p>

<pre><code>sess = tf.Session()
num_features = 4
num_actions = 2

policy_matrix = tf.get_variable(""params"", (num_actions, num_features))
state_ph = tf.placeholder(""float"", (num_features, 1))
action_linear = tf.matmul(params, state_ph)
action_probs = tf.nn.softmax(action_linear, axis=0)
action_problogs = tf.log(action_probs)
</code></pre>

<p>W (corresponding to <code>action_problogs</code>) looks like </p>

<pre><code>[ w1 ]
[ w2 ]
</code></pre>

<p>I'd like to find the gradient of <code>w1</code> with respect to the matrix <code>X</code>- that is, I'd like to calculate</p>

<pre><code>          [ d/dx1 w1 ]
d/dX w1 =      .
               .
          [ d/dx8 w1 ]
</code></pre>

<p>(preferably still looking like a matrix so I can add it to <code>X</code>, but I'm really not concerned about that)</p>

<p>I was hoping that <code>tf.gradients</code> would do the trick. I calculated the ""gradient"" like so</p>

<pre><code>problog_gradient = tf.gradients(action_problogs, policy_matrix)
</code></pre>

<p>However, when I inspect <code>problog_gradient</code>, here's what I get</p>

<pre><code>[&lt;tf.Tensor 'foo_4/gradients/foo_4/MatMul_grad/MatMul:0' shape=(2, 4) dtype=float32&gt;]
</code></pre>

<p>Note that this has exactly the same shape as <code>X</code>, but that it really shouldn't. I was hoping to get a list of two gradients, each with respect to 8 elements. I suspect that I'm instead getting two gradients, but each with respect to four elements.</p>

<p>I'm very new to tensorflow, so I'd appreciate and explanation of what's going on and how I might achieve the behavior I desire.</p>
",1543167,,1543167,,2018-02-20 05:11:46,2019-04-05 17:46:28,Tensorflow gradient with respect to matrix,<python><matrix><tensorflow><gradient-descent><reinforcement-learning>,2,0,0,,,CC BY-SA 3.0,
1809,6085691,1,6085987,,2011-05-22 02:36:34,,5,3014,"<p>I am having trouble understanding the SARSA algorithm:
<a href=""http://en.wikipedia.org/wiki/SARSA"" rel=""nofollow"">http://en.wikipedia.org/wiki/SARSA</a></p>

<p>In particular, when updating the Q value what is gamma? and what values are used for s(t+1) and a(t+1)?</p>

<p>Can someone explain this algorithm to me?</p>

<p>Thanks.</p>
",543117,,49329,,2011-05-22 13:13:46,2011-05-22 13:13:46,SARSA algorithm,<artificial-intelligence><reinforcement-learning>,1,1,0,,,CC BY-SA 3.0,
1810,60720154,1,60720195,,2020-03-17 10:10:05,,1,641,"<p>I'm solving an MIT lab on Reinforcement Learning and am stuck on the reward function.
The particular code block is this:
<a href=""https://colab.research.google.com/github/aamini/introtodeeplearning/blob/master/lab3/solutions/RL_Solution.ipynb#scrollTo=5_Q2OFYtQ32X&amp;line=19&amp;uniqifier=1"" rel=""nofollow noreferrer"">https://colab.research.google.com/github/aamini/introtodeeplearning/blob/master/lab3/solutions/RL_Solution.ipynb#scrollTo=5_Q2OFYtQ32X&amp;line=19&amp;uniqifier=1</a></p>

<p>A simpler version of the relevant code is:</p>

<pre><code>import numpy as np

rewards=[0.,0,0,0,0,1]
discounted_rewards = np.zeros_like(rewards)
R = 0
for t in reversed(range(0, len(rewards))):
    # update the total discounted reward
    R = R * .95 + rewards[t]
    discounted_rewards[t] = R
discounted_rewards
</code></pre>

<p>Which gives output as:</p>

<pre><code>array([0.77378094, 0.81450625, 0.857375, 0.9025, 0.95 ,1.])
</code></pre>

<p>The provided explanation is that we want to encourage having rewards sooner rather than later. How does using <code>reversed</code> in the for loop help with that ?</p>
",11501160,,,,,2020-03-18 09:18:29,Why discounted reward function is reversed?,<python><python-3.x><tensorflow><tensorflow2.0><reinforcement-learning>,1,0,,,,CC BY-SA 4.0,
1817,61556374,1,61599551,,2020-05-02 08:19:54,,1,326,"<p>OpenAI has a great Taxonomy of Reinforcement Learning Algorithms. I was wondering where the following papers would go on this tree?</p>

<ol>
<li><a href=""https://arxiv.org/abs/2002.06038"" rel=""nofollow noreferrer"">Never Give Up</a></li>
<li><a href=""https://arxiv.org/abs/2003.13350"" rel=""nofollow noreferrer"">Agent57</a></li>
<li><a href=""https://arxiv.org/abs/1911.08265"" rel=""nofollow noreferrer"">MuZero</a></li>
<li><a href=""https://openreview.net/pdf?id=r1lyTjAqYX"" rel=""nofollow noreferrer"">R2D2</a></li>
</ol>

<p><a href=""https://i.stack.imgur.com/7rKjW.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/7rKjW.png"" alt=""A Taxonomy of RL Algorithms""></a></p>
",8883170,,,,,2020-05-04 18:55:10,"Where do NGU, R2D2, MuZero and Agent57 fit on the Taxonomy of Reinforcement Learning?",<reinforcement-learning>,1,0,,,,CC BY-SA 4.0,
1824,37803003,1,37824823,,2016-06-14 04:27:33,,1,997,"<p>I'm watching the Berkely CS 294 course about Deep Reinforcement Learning. However, I meet some troubles on the assignment. I tried to implement the equation below. I think it is quite simple but I failed to obtain the expected result as showed in the comments. There must be something that I misunderstood. Details are shown in the code below.  Can anyone help?</p>

<p><a href=""http://quicklatex.com/cache3/4b/ql_a4e0ff64c86ce8e3e60f94cfb9fc4b4b_l3.png"" rel=""nofollow noreferrer"">state value function http://quicklatex.com/cache3/4b/ql_a4e0ff64c86ce8e3e60f94cfb9fc4b4b_l3.png</a></p>

<p>Here is my code:</p>

<pre><code>def compute_vpi(pi, P, R, gamma):
    """"""
    :param pi: a deterministic policy (1D array: S -&gt; A)
    :param P: the transition probabilities (3D array: S*A*S -&gt; R)
    :param R: the reward function (3D array: S*A*S -&gt; R)
    :param gamma: the discount factor (scalar)
    :return: vpi, the state-value function for the policy pi
    """"""
    nS = P.shape[0]
    # YOUR CODE HERE
    ############## Here is what I wrote ######################
    vpi = np.zeros([nS,])
    for i in range(nS):
        for j in range(nS):
            vpi[i] += P[i, pi[i], j] * (R[i, pi[i], j] + gamma*vpi[j])
    ##########################################################
    # raise NotImplementedError()
    assert vpi.shape == (nS,)
    return vpi


pi0 = np.zeros(nS,dtype='i')
compute_vpi(pi0, P_rand, R_rand, gamma)

# Expected output:
# array([ 5.206217  ,  5.15900351,  5.01725926,  4.76913715,  5.03154609,
#         5.06171323,  4.97964471,  5.28555573,  5.13320501,  5.08988046])
</code></pre>

<p>What I got:</p>

<pre><code>array([ 0.61825794,  0.67755819,  0.60497582,  0.30181986,  0.67560153,
    0.88691815,  0.73629922,  1.09325453,  1.15480849,  1.21112992])
</code></pre>

<p>Some Init code:</p>

<pre><code>nr.seed(0) # seed random number generator
nS = 10
nA = 2
# nS: number of states
# nA: number of actions
R_rand = nr.rand(nS, nA, nS) # reward function
# R[i,j,k] := R(s=i, a=j, s'=k), 
# i.e., the dimensions are (current state, action, next state)
P_rand = nr.rand(nS, nA, nS) 
# P[i,j,k] := P(s'=k | s=i, a=j)
# i.e., dimensions are (current state, action, next state)

P_rand /= P_rand.sum(axis=2,keepdims=True) # normalize conditional probabilities
gamma = 0.90
</code></pre>
",4968861,,15168,,2016-06-15 02:11:11,2016-06-15 02:11:11,How to implement the state value function?,<python><reinforcement-learning>,1,5,0,,,CC BY-SA 3.0,
1827,8091877,1,8093536,,2011-11-11 09:21:06,,2,412,"<p>I've set out to build an AI-engine that learns to play Tetris, i.e. an engine that can improve it's performance, perhaps by adjusting its heuristics, and so forth. Let's say that I've got the GUI out of the way--where would I begin in building the engine? Which resources would I, as a beginner, use to understand the Machine Learning concepts involved in this?</p>

<p>In particular, i am looking for explanatory material based on <em>code</em> and practical examples, rather than mathematical treatments of the subject.</p>
",848423,,66549,,2011-11-11 21:38:24,2011-11-11 21:38:24,What is the preferred machine learning technique for building a real-time game player simulator?,<machine-learning><reinforcement-learning><tetris>,1,0,0,2011-11-12 02:04:04,,CC BY-SA 3.0,
1828,55963251,1,55965588,,2019-05-03 04:23:20,,1,711,"<p>I am trying to train Echo State Network for text generation with stochastic optimization along the lines of Reinforcement learning, where the optimization depends on the reward signal.</p>

<p>I have observed that during evaluation, when I sample from the probability distribution, the bleu score is bigger than when I argmax from the distribution. The difference is almost more than 0.10 points (BLEU Score is generally between the range 0 and 1 ).
I am not sure why does that happen.
Help needed.</p>
",1588610,,,,,2019-05-03 08:59:43,argmax from probability distribution better policy than random sampling from softmax?,<machine-learning><neural-network><deep-learning><nlp><reinforcement-learning>,1,0,,,,CC BY-SA 4.0,
1829,56103110,1,56109540,,2019-05-12 20:16:25,,0,137,"<p>Really don't know where to start seeking for the right algorithm.</p>

<p>I'm building a web application that collects <a href=""https://schema.org/"" rel=""nofollow noreferrer"">schema.org</a> data from different webshops as Amazon, Shopify, etc. It collects data every 6h and shows the current and lowest price. It is used for monitoring products and buying at the lowest price.</p>

<p>My goal is to <strong>recognize products from different shops as the same product.</strong> Every shop has its own title for the same product. </p>

<p>Example: </p>

<pre><code>Google Pixel 2 64GB Clearly White (Unlocked) Smartphone 
Google Pixel 2 GSM/CDMA Google Unlocked (Clearly White, 64GB, US warranty) 
</code></pre>

<p>Problems:</p>

<ol>
<li>don't have a lot of data (only products chosen by the user)</li>
<li>needs to support every new product that app doesn't have data history</li>
</ol>
",11034876,,,,,2019-05-13 09:33:36,String matching algorithm for product recognition,<machine-learning><neural-network><dataset><reinforcement-learning>,1,0,,,,CC BY-SA 4.0,
1830,56104472,1,56113154,,2019-05-13 00:10:36,,2,7742,"<p>I try to set ""export OPENBLAS_NUM_THREADS=1"" as <a href=""https://ray.readthedocs.io/en/latest/example-rl-pong.html?highlight=openblas"" rel=""nofollow noreferrer"">this document</a> suggests. But I found a strange phenomenon that setting this significantly impairs the performance of my RL algorithms(I've done some tests for TD3 and SAC, all results consistently indicate that ""export OPENBLAS_NUM_THREADS=1"" impairs the performance). Why would this cause such a big problem?</p>

<p>BTW, the algorithms are implemented using Tensorflow1.13, data are fed into the neural network through <a href=""https://www.tensorflow.org/api_docs/python/tf/data/Dataset"" rel=""nofollow noreferrer"">tf.data.Dataset</a>. all tests are done on <a href=""https://gym.openai.com/envs/BipedalWalker-v2/"" rel=""nofollow noreferrer"">BipedalWalker-v2</a> environment from OpenAI's Gym.</p>
",7850499,,7850499,,2019-05-13 12:59:23,2019-05-23 10:21:30,"Why would setting ""export OPENBLAS_NUM_THREADS=1"" impair the performance?",<python><multithreading><tensorflow><reinforcement-learning><openblas>,1,0,,,,CC BY-SA 4.0,
1831,56234489,1,56663947,,2019-05-21 08:46:55,,2,1020,"<p>I implemented a 3x3 OX game by q-learning ( it works perfectly in AI v.s AI and AI v.s Human), but I can't go one step further to 4x4 OX game since it will eat up all my PC memory and crash.</p>

<p>Here is my current problem:
<a href=""https://stackoverflow.com/questions/56231392/access-violation-in-huge-array"">Access violation in huge array?</a></p>

<p>In my understanding, a 3x3 OX game has a total 3(space, white, black) ^ 9 = 19683 possible states. ( same pattern different angle still count )</p>

<p>For a 4x4 OX game, the total state will be 3 ^ 16 = 43,046,721</p>

<p>For a regular go game, 15x15 board, the total state will be 3 ^ 225 ~ 2.5 x 10^107 </p>

<p>Q1. I want to know my calculation is correct or not. ( for 4x4 OX game, I need a 3^16 array ? )</p>

<p>Q2. Since I need to calculate each Q value ( for each state, each action), I need such a large number of array, is it expected? any way to avoid it?</p>
",5518300,,5518300,,2019-05-21 14:01:42,2019-06-19 09:06:01,The huge amount of states in q-learning calculation,<c++><machine-learning><reinforcement-learning>,3,8,0,,,CC BY-SA 4.0,
1832,56535491,1,56538600,,2019-06-11 02:00:08,,1,50,"<p>I know the definition: - 
An optimal policy (pi)* satisfies (pi)* >= (pi) for all (pi)
An optimal policy is guaranteed to exist, but may not be unique.
What do these two lines mean?</p>
",11547744,,,,,2019-06-11 07:29:13,What is Optimality in Reinforcement Learning?,<machine-learning><deep-learning><reinforcement-learning>,1,1,0,,,CC BY-SA 4.0,
1835,56270744,1,56288879,,2019-05-23 08:10:01,,2,1239,"<p>I'm using Q learning and i want to know if i can use the tf.losses.mean_squared_error loss calculation function if i have a reward function which can give negative rewards.</p>

<p>Because if i have for exemple as output of my network the following Q values : (0.1, 0.2, 1), and i calculate that my real Q values should be (0.1, -5, 1), if i use the mean_squared_error function the loss for the second Q value will go positive am i wrong ? Because of the square operation so the gradient descend will not be based on correct loss ?</p>
",10168321,,,,,2019-05-24 14:25:38,tf.losses.mean_squared_error with negative target,<tensorflow><neural-network><reinforcement-learning><loss-function><q-learning>,2,0,,,,CC BY-SA 4.0,
1838,56541683,1,56887299,,2019-06-11 10:28:03,,1,304,"<p>I have been trying to implement Actor Critic with a convolutional neural network. There are two different images as the state for the reinforcement learning agent. The output(actions for Actor) of the CNN is the (almost)same for different inputs after (random)initialisation. Due to this even after training the agent never learns anything useful. </p>

<p>State definitions(2 inputs):</p>

<p>Input 1: [1,1,90,128] image with maximum value for the pixel being 45.</p>

<p>Input 2: [1,1,45,80] image with maximum value for the pixel being 45.</p>

<p>Expected output by the actor: [x,y]: a 2-dimensional vector according to the state. Here x is expected in the range [0,160] and y is expected in the range [0,112] </p>

<p>Different types of modifications tried with the input:</p>

<p>1: Feed both the images as it is.</p>

<p>2: Feed both images with normalisation as <code>(img/45)</code> so that pixel values are from [0,1]</p>

<p>3:Feed both images with normalisation as <code>2*((img/45)-0.5)</code> so that pixel values are from [-1,1]</p>

<p>4: Feed both images with normalisation as <code>(img-mean)/std</code></p>

<p>Result: The output of the CNN remains almost same.</p>

<p>The code for the definition of actor has been given below.</p>

<pre><code>import numpy as np
import pandas as pd
from tqdm import tqdm
import time
import cv2
import matplotlib.pyplot as plt
import torch
import torch.nn as nn
from torch.autograd import Variable
import torch.nn.functional as F


device = torch.device(""cuda"" if torch.cuda.is_available() else ""cpu"")

class Actor(nn.Module):
    def __init__(self, action_dim, max_action):
        super(Actor,self).__init__()
        # state image [1,1,90,128]
        self.conv11 = nn.Conv2d(1,16,5)
        self.conv11_bn = nn.BatchNorm2d(16)
        self.conv12 = nn.Conv2d(16,16,5)
        self.conv12_bn = nn.BatchNorm2d(16)
        self.fc11 = nn.Linear(19*29*16,500)
        # dim image [1,1,45,80]
        self.conv21 = nn.Conv2d(1,16,5) 
        self.conv21_bn = nn.BatchNorm2d(16)
        self.conv22 = nn.Conv2d(16,16,5)
        self.conv2_bn = nn.BatchNorm2d(16)
        self.fc21 = nn.Linear(8*17*16,250)
        # common pool
        self.pool  = nn.MaxPool2d(2,2)
        # after concatenation
        self.fc2 = nn.Linear(750,100)
        self.fc3 = nn.Linear(100,10)
        self.fc4 = nn.Linear(10,action_dim)
        self.max_action = max_action

    def forward(self,x,y):
        # state image
        x = self.conv11_bn(self.pool(F.relu(self.conv11(x))))
        x = self.conv11_bn(self.pool(F.relu(self.conv12(x))))
        x = x.view(-1,19*29*16)
        x = F.relu(self.fc11(x))
        # state dim
        y = self.conv11_bn(self.pool(F.relu(self.conv21(y))))
        y = self.conv11_bn(self.pool(F.relu(self.conv22(y))))
        y = y.view(-1,8*17*16)
        y = F.relu(self.fc21(y))
        # concatenate
        z = torch.cat((x,y),dim=1)
        z = F.relu(self.fc2(z))
        z = F.relu(self.fc3(z))
        z = self.max_action*torch.tanh(self.fc4(z))
        return z

# to read different sample states for testing
obs = []
for i in range(200):
    obs.append(np.load('eval_episodes/obs_'+str(i)+'.npy',allow_pickle=True))

obs = np.array(obs)

def tensor_from_numpy(state):
    # to add dimensions to tensor to make it [batch_size,channels,height,width] 
    state_img = state
    state_img = torch.from_numpy(state_img).float()
    state_img = state_img[np.newaxis, :]
    state_img = state_img[np.newaxis, :].to(device)
    return state_img


actor = Actor(2,torch.FloatTensor([160,112]))
for i in range(20):
    a = tensor_from_numpy(obs[i][0])
    b = tensor_from_numpy(obs[i][2])    
    print(actor(a,b))
</code></pre>

<p>Output of the above code:</p>

<pre><code>tensor([[28.8616,  3.0934]], grad_fn=&lt;MulBackward0&gt;)
tensor([[27.4125,  3.2864]], grad_fn=&lt;MulBackward0&gt;)
tensor([[28.2210,  2.6859]], grad_fn=&lt;MulBackward0&gt;)
tensor([[27.6312,  3.9528]], grad_fn=&lt;MulBackward0&gt;)
tensor([[25.9290,  4.2942]], grad_fn=&lt;MulBackward0&gt;)
tensor([[26.9652,  4.5730]], grad_fn=&lt;MulBackward0&gt;)
tensor([[27.1342,  2.9612]], grad_fn=&lt;MulBackward0&gt;)
tensor([[27.6494,  4.2218]], grad_fn=&lt;MulBackward0&gt;)
tensor([[27.3122,  1.9945]], grad_fn=&lt;MulBackward0&gt;)
tensor([[29.6915,  1.9938]], grad_fn=&lt;MulBackward0&gt;)
tensor([[28.2001,  2.5967]], grad_fn=&lt;MulBackward0&gt;)
tensor([[26.8502,  4.4917]], grad_fn=&lt;MulBackward0&gt;)
tensor([[28.6489,  3.2022]], grad_fn=&lt;MulBackward0&gt;)
tensor([[28.1455,  2.7610]], grad_fn=&lt;MulBackward0&gt;)
tensor([[27.2369,  3.4243]], grad_fn=&lt;MulBackward0&gt;)
tensor([[25.9513,  5.3057]], grad_fn=&lt;MulBackward0&gt;)
tensor([[28.1400,  3.3242]], grad_fn=&lt;MulBackward0&gt;)
tensor([[28.2049,  2.6622]], grad_fn=&lt;MulBackward0&gt;)
tensor([[26.7446,  2.5966]], grad_fn=&lt;MulBackward0&gt;)
tensor([[25.3867,  5.0346]], grad_fn=&lt;MulBackward0&gt;)
</code></pre>

<p>The states(<code>.npy</code>) files can be found <a href=""https://drive.google.com/file/d/1Grd57e0hmJzQgLDowhvW6xLaozZygrUW/view?usp=sharing"" rel=""nofollow noreferrer"">here</a>
For different states the actions should vary from [0-160,0-112] but here the outputs just vary a little bit. </p>

<p>Note: the input images are initially sparse(lots of zeros in the image)</p>

<p>Is there a problem with the state pixel values or the network definition?</p>

<p>EDIT: I think the problem has got to do something with the normalisation or the sparsity of the inputs, because I also tried the same network with tensorflow and am facing the same problem there.</p>
",8421999,,8421999,,2019-06-13 04:45:50,2019-07-04 11:48:14,Output of a CNN doesn't change much with the input,<python><deep-learning><conv-neural-network><pytorch><reinforcement-learning>,1,2,,,,CC BY-SA 4.0,
1843,1730824,1,1731170,,2009-11-13 17:43:52,,1,1749,"<p>I am currently using Q-Learning to try to teach a bot how to move in a room filled with walls/obstacles. It must start in any place in the room and get to the goal state(this might be, to the tile that has a door, for example).
Currently when it wants to move to another tile, it will go to that tile, but I was thinking that in the future I might add a random chance of going to another tile, instead of that. It can only move up, down, left and right. Reaching the goal state yields +100 and the rest of the actions will yield 0.</p>

<p>I am using the algorithm found <a href=""http://people.revoledu.com/kardi/tutorial/ReinforcementLearning/Q-Learning-Algorithm.htm"" rel=""nofollow noreferrer"">here</a>, which can be seen in the image bellow.</p>

<p><img src=""https://i.stack.imgur.com/sAQwk.png"" alt=""alt text"">
<img src=""https://i.stack.imgur.com/P2XK6.png"" alt=""alt text""></p>

<p>Now, regarding this, I have some questions:</p>

<ol>
<li>When using Q-Learning, a bit like
Neural Networks, I must make
distinction between a learning phase
and a using phase? I mean, it seems
that what they shown on the first
picture is a learning one and in the
second picture a using one.</li>
<li>I read somewhere that it'd take an
infinite number of steps to reach to
the optimum Q values table. Is that
true? I'd say that isn't true, but I
must be missing something here.</li>
<li><p>I've heard also about TD(Temporal
Differences), which seems to be
represented by the following
expression:</p>

<pre><code>Q(a, s) = Q(a, s) * alpha * [R(a, s) + gamma * Max { Q(a', s' } - Q(a, s)]
</code></pre>

<p>which for alpha = 1, just seems the
one shown first in the picture. What
difference does that gamma make,
here?</p></li>
<li>I have run in some complications if
I try a very big room(300x200
pixels, for example). As it
essentially runs randomly, if the
room is very big then it will take a
lot of time to go randomly from the
first state to the goal state. What
methods can I use to speed it up? I
thought maybe having a table filled
with trues and falses, regarding
whatever I have in that episode
already been in that state or not.
If yes, I'd discard it, if no, I'd
go there. If I had already been in
all those states, then I'd go to a
random one. This way, it'd be just
like what am I doing now, knowing
that I'd repeat states a less often
that I currently do.</li>
<li>I'd like to try something else than
my lookup table for Q-Values, so I
was thinking in using Neural
Networks with back-propagation for
this. I will probably try having a
Neural Network for each action (up,
down, left, right), as it seems it's
what yields best results. Are there
any other methods (besides SVM, that
seem way too hard to implement
myself) that I could use and
implement that'd give me good
Q-Values function approximation?</li>
<li>Do you think Genetic Algorithms
would yield good results in this
situation, using the Q-Values matrix
as the basis for it? How could I
test my fitness function? It gives me the impression that GA are generally used for things way more random/complex. If we watch carefully we will notice that the Q-Values follow a clear trend - having the higher Q values near the goal and lower ones the farther away you are from them. Going to try to reach that conclusion by GA probably would take way too long?</li>
</ol>
",130758,,4099598,,2015-06-21 01:03:05,2020-05-22 08:14:37,Improving Q-Learning,<language-agnostic><artificial-intelligence><genetic-algorithm><reinforcement-learning>,2,2,0,,,CC BY-SA 3.0,
1845,57198765,1,57216349,,2019-07-25 09:33:48,,1,50,"<p>I'm struggling to figure out how I want to do this so I hope someone here may offer some guidance.</p>

<p>Scenario - I have a 10 character string, lets call it the DNA, made up of the following characters:</p>

<p>F<br>
-<br>
+<br>
[<br>
]<br>
X  </p>

<p>for example <code>DNA = ['F', 'F', '+', '+', '-', '[', 'X', '-', ']', '-']</code>  </p>

<p>Now these DNA strings get converted to physical representations from whence I can get a <em>fitness</em> or <em>reward</em> value. So an RL flowchart for this scenario would look like this:  </p>

<p>P.S. The maximum fitness is not known or specified.</p>

<p>Step 1: Get random DNA string</p>

<p>Step 2: Compute fitness</p>

<p>Step 3: Get another random DNA string</p>

<p>Step 4: Compute fitness </p>

<p>Step 5: Compute gradient and <em>see</em> which way is up</p>

<p>Step 6: Train ML algorithm to generate better and better DNA strings until fitness no longer increases</p>

<p>For clarity sake the best DNA string, i.e. the one who will return the highest fitness, for my purposes now is:<br>
<code>['F', 'X', 'X', 'X', 'X', 'F', 'X', 'X', 'X', 'X']</code></p>

<p>How can I train a ML algorithm to learn this and output this DNA string?</p>

<p>I'm trying to wrap my brain around Policy Gradient methods but what will my input to the ML algorithm be? There are no states like in the OpenAI Gym examples.</p>

<p><strong>EDIT:</strong>
Final goal - Algorithm that learns to generate higher fitness value DNA strings. This has to happen without any human supervision i.e. NOT supervised learning but reinforcement learning. </p>

<p>Akin to a GA that will evolve better and better DNA strings</p>
",7167076,,7167076,,2019-07-26 07:04:21,2019-07-26 08:45:55,Difficult reinforcement learning query,<reinforcement-learning><policy-gradient-descent>,1,4,,,,CC BY-SA 4.0,
1847,38988582,1,47333847,,2016-08-17 05:16:12,,11,12537,"<p>Is the classic Q-learning algorithm, using lookup table (instead of function approximation), equivalent to dynamic programming?</p>
",4702833,,3924118,,2018-11-22 16:01:37,2018-11-22 16:01:37,Q-learning vs dynamic programming,<machine-learning><dynamic-programming><reinforcement-learning><q-learning>,3,0,0,,,CC BY-SA 4.0,
1848,57603707,1,57605907,,2019-08-22 07:02:00,,2,1357,"<p><a href=""https://github.com/keon/deep-q-learning/blob/master/dqn.py#L52"" rel=""nofollow noreferrer"">https://github.com/keon/deep-q-learning/blob/master/dqn.py#L52</a></p>

<pre><code>def replay(self, batch_size):
    minibatch = random.sample(self.memory, batch_size)
    for state, action, reward, next_state, done in minibatch:
        target = reward
        if not done:
            target = (reward + self.gamma *
                      np.amax(self.model.predict(next_state)[0]))
        target_f = self.model.predict(state)
        target_f[0][action] = target
        self.model.fit(state, target_f, epochs=1, verbose=0)
</code></pre>

<p>This code seems can't get help from GPU since it trains data once per action.</p>

<pre><code>self.model.fit(state, target_f, epochs=1, verbose=0)
</code></pre>

<p>How to change this code to train in parallel, then get help from GPU?</p>
",11805922,,4685471,,2019-08-22 12:01:29,2019-08-22 12:01:29,How to make this RL code get GPU support?,<python><tensorflow><gpu><reinforcement-learning>,1,2,0,,,CC BY-SA 4.0,
1851,57106676,1,58752611,,2019-07-19 06:24:29,,0,544,"<p>I've been trying to implement DQN with a target network and I'm getting some really weird results. </p>

<p>When I try to train my DQN from scratch on Cartpole, it doesn't seem to learn and loss increases in an exponential fashion. </p>

<p>However, if I load in a pretrained model trained without the use of a target network, the model works very well, outperforming the continued training of the tradition DQN. </p>

<p>Can someone take a look at my code and tell me what the problem is?</p>

<pre class=""lang-py prettyprint-override""><code>import torch
import torch.nn as nn
import torch.optim as optim
import torch.nn.functional as F
import random
import math
import numpy as np
import gym
import matplotlib.pyplot as plt



class DQN(nn.Module):
    def __init__(self, input_dim, output_dim):
        super(DQN, self).__init__()
        self.linear1 = nn.Linear(input_dim, 16)
        self.linear2 = nn.Linear(16, 32)
        self.linear3 = nn.Linear(32, 32)
        self.linear4 = nn.Linear(32, output_dim)


    def forward(self, x):
        x = F.relu(self.linear1(x))
        x = F.relu(self.linear2(x))
        x = F.relu(self.linear3(x))
        return self.linear4(x)


final_epsilon = 0.05
initial_epsilon = 1
epsilon_decay = 5000
global steps_done
steps_done = 0


def select_action(state):
    global steps_done
    sample = random.random()
    eps_threshold = final_epsilon + (initial_epsilon - final_epsilon) * \
                    math.exp(-1. * steps_done / epsilon_decay)
    if sample &gt; eps_threshold:
        with torch.no_grad():
            state = torch.Tensor(state)
            steps_done += 1
            q_calc = model(state)
            node_activated = int(torch.argmax(q_calc))
            return node_activated
    else:
        node_activated = random.randint(0,1)
        steps_done += 1
        return node_activated


class ReplayMemory(object): # Stores [state, reward, action, next_state, done]

    def __init__(self, capacity):
        self.capacity = capacity
        self.memory = [[],[],[],[],[]]

    def push(self, data):
        """"""Saves a transition.""""""
        for idx, point in enumerate(data):
            #print(""Col {} appended {}"".format(idx, point))
            self.memory[idx].append(point)

    def sample(self, batch_size):
        rows = random.sample(range(0, len(self.memory[0])), batch_size)
        experiences = [[],[],[],[],[]]
        for row in rows:
            for col in range(5):
                experiences[col].append(self.memory[col][row])
        return experiences

    def __len__(self):
        return len(self.memory[0])


input_dim, output_dim = 4, 2
model = DQN(input_dim, output_dim)
target_net = DQN(input_dim, output_dim)
target_net.load_state_dict(model.state_dict())
target_net.eval()
tau = 1
discount = 0.99

learning_rate = 1e-4
optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)

memory = ReplayMemory(65536)
BATCH_SIZE = 128


def optimize_model():
    if len(memory) &lt; BATCH_SIZE:
        return 0
    experiences = memory.sample(BATCH_SIZE)
    state_batch = torch.Tensor(experiences[0])
    action_batch = torch.LongTensor(experiences[1]).unsqueeze(1)
    reward_batch = torch.Tensor(experiences[2])
    next_state_batch = torch.Tensor(experiences[3])
    done_batch = experiences[4]

    pred_q = model(state_batch).gather(1, action_batch)

    next_state_q_vals = torch.zeros(BATCH_SIZE)

    for idx, next_state in enumerate(next_state_batch):
        if done_batch[idx] == True:
            next_state_q_vals[idx] = -1
        else:
            # .max in pytorch returns (values, idx), we only want vals
            next_state_q_vals[idx] = ((target_net(next_state_batch[idx]).max(0)[0]).detach())

    better_pred = (reward_batch + next_state_q_vals).unsqueeze(1)

    loss = F.smooth_l1_loss(pred_q, better_pred)
    optimizer.zero_grad()
    loss.backward()
    for param in model.parameters():
        param.grad.data.clamp_(-1, 1)
    optimizer.step()
    return loss


env = gym.make('CartPole-v0')
for i_episode in range(300):
    model.train()
    target_net.eval()
    observation = env.reset()
    episode_loss = 0
    if i_episode % tau == 0:
        target_net.load_state_dict(model.state_dict())
    for t in range(200):
        #env.render()
        state = observation
        action = select_action(observation)
        observation, reward, done, _ = env.step(action)

        if done:
            next_state = [0,0,0,0]
        else:
            next_state = observation

        memory.push([state, action, reward, next_state, done])
        optimize_model()
        if done:
            print(""Episode {} finished after {} timesteps"".format(i_episode, t+1))
            break
env.close()

</code></pre>
",8327565,,,,,2019-11-07 16:00:43,Weird results when playing with DQN with targets,<reinforcement-learning><q-learning>,1,0,0,,,CC BY-SA 4.0,
1853,57724414,1,57762933,,2019-08-30 09:49:30,,2,1535,"<p>I have a mismatch in shapes between inputs and the model of my reinforcement learning project.</p>

<p>I have been closely following the AWS examples, specifically the cartpole example. However I have built my own custom environment. What I am struggling to understand is how to change my environment so that it is able to work with the prebuilt Ray RLEstimator.</p>

<p>Here is the code for the environment:</p>

<pre class=""lang-py prettyprint-override""><code>from enum import Enum
import math

import gym
from gym import error, spaces, utils, wrappers
from gym.utils import seeding
from gym.envs.registration import register
from gym.spaces import Discrete, Box


import numpy as np

# from float_space import FloatSpace


def sigmoid_price_fun(x, maxcust, gamma):
    return maxcust / (1 + math.exp(gamma * max(0, x)))


class Actions(Enum):
    DECREASE_PRICE = 0
    INCREASE_PRICE = 1
    HOLD = 2


PRICE_ADJUSTMENT = {
    Actions.DECREASE_PRICE: -0.25,
    Actions.INCREASE_PRICE: 0.25,
    Actions.HOLD: 0
}


class ArrivalSim(gym.Env):
    """""" Simple environment for price optimising RL learner. """"""


    def __init__(self, price):
        """"""
        Parameters
        ----------
        price : float
            The initial price to use.
        """"""
        super().__init__()
        self.price = price
        self.revenue = 0
        self.action_space = Discrete(3)  # [0, 1, 2]  #increase or decrease
        self.observation_space = Box(np.array(0.0),np.array(1000))
#         self.observation_space = FloatSpace(price)

    def step(self, action):
        """""" Enacts the specified action in the environment.

        Returns the new price, reward, whether we're finished and an empty dict for compatibility with Gym's
        interface. """"""

        self._take_action(Actions(action))
        next_state = self.price
#         next_state = self.observation_space.sample()
        reward = self._get_reward()
        done = False

        if next_state &lt; 0 or reward == 0:
            done = True

        print(next_state, reward, done, {})

        return np.array(next_state), reward, done, {}

    def reset(self):
        """""" Resets the environment, selecting a random initial price. Returns the price. """"""

#         self.observation_space.value = np.random.rand()
#         return self.observation_space.sample()
        self.price = np.random.rand()
        return self.price

    def _take_action(self, action):
#         self.observation_space.value += PRICE_ADJUSTMENT[action]
        self.price += PRICE_ADJUSTMENT[action]

    def _get_reward(self,price):
#         price = self.observation_space.value
#         return max(np.random.poisson(sigmoid_price_fun(price, 50, 0.5)) * price, 0)
        self.revenue = max(np.random.poisson(sigmoid_price_fun(self.price, 50, 0.5)) * self.price, 0)
        return max(np.random.poisson(sigmoid_price_fun(self.price, 50, 0.5)) * self.price, 0)


#     def render(self, mode='human'):
#         super().render(mode)

def testEnv():
    register(
        id='ArrivalSim-v0',
        entry_point='env:ArrivalSim',
        kwargs= {'price' : 40}
    )
    env = gym.make('ArrivalSim-v0')

    env.reset()
    for _ in range(20):
        test = env.action_space.sample()
        print(test)
        print(env.observation_space)
        env.step(test)  # take a random action
    env.close()



if __name__ =='__main__':

    testEnv()

</code></pre>

<p>Here is the training script</p>

<pre class=""lang-py prettyprint-override""><code>import json
import os

import gym
import ray
from ray.tune import run_experiments
from ray.tune.registry import register_env
from gym.envs.registration import register

from sagemaker_rl.ray_launcher import SageMakerRayLauncher


def create_environment(env_config):
    import gym
#     from gym.spaces import Space
    from gym.envs.registration import register

    # This import must happen inside the method so that worker processes import this code
    register(
        id='ArrivalSim-v0',
        entry_point='env:ArrivalSim',
        kwargs= {'price' : 40}
    )
    return gym.make('ArrivalSim-v0')



class MyLauncher(SageMakerRayLauncher):

    def register_env_creator(self):
        register_env(""ArrivalSim-v0"", create_environment)

    def get_experiment_config(self):
        return {
          ""training"": {
            ""env"": ""ArrivalSim-v0"",
            ""run"": ""PPO"",
            ""stop"": {
              ""episode_reward_mean"": 5000,
            },
            ""config"": {
              ""gamma"": 0.995,
              ""kl_coeff"": 1.0,
              ""num_sgd_iter"": 10,
              ""lr"": 0.0001,
              ""sgd_minibatch_size"": 32768,
              ""train_batch_size"": 320000,
              ""monitor"": False,  # Record videos.
              ""model"": {
                ""free_log_std"": False
              },
              ""use_gae"": False,
              ""num_workers"": (self.num_cpus-1),
              ""num_gpus"": self.num_gpus,
              ""batch_mode"": ""complete_episodes""

            }
          }
        }

if __name__ == ""__main__"":
    MyLauncher().train_main()
</code></pre>

<p>Here is the code I run in Jupyter:</p>

<pre><code>metric_definitions = RLEstimator.default_metric_definitions(RLToolkit.RAY)
environment = env = {
    'SAGEMAKER_REQUIREMENTS': 'requirements.txt', # path relative to `source_dir` below.
}

estimator = RLEstimator(entry_point=""train.py"",
                        source_dir='.',
                        toolkit=RLToolkit.RAY,
                        toolkit_version='0.6.5',
                        framework=RLFramework.TENSORFLOW,
                        dependencies=[""sagemaker_rl""],
#                         image_name='price-response-ray-cpu',
                        role=role,
#                         train_instance_type=""ml.c5.2xlarge"",
                        train_instance_type='local',
                        train_instance_count=1,
#                         output_path=s3_output_path,
#                         base_job_name=job_name_prefix,
                        metric_definitions=metric_definitions
#                         hyperparameters={
                          # Attention scientists!  You can override any Ray algorithm parameter here:
                          #""rl.training.config.horizon"": 5000,
                          #""rl.training.config.num_sgd_iter"": 10,
                        #}
                    )

estimator.fit(wait=True)
job_name = estimator.latest_training_job.job_name
print(""Training job: %s"" % job_name)
</code></pre>

<p>The error message I have been receiving has been the following:</p>

<pre><code>algo-1-dxwxx_1  | == Status ==
algo-1-dxwxx_1  | Using FIFO scheduling algorithm.
algo-1-dxwxx_1  | Resources requested: 0/3 CPUs, 0/0 GPUs
algo-1-dxwxx_1  | Memory usage on this node: 1.1/4.1 GB
algo-1-dxwxx_1  | 
algo-1-dxwxx_1  | == Status ==
algo-1-dxwxx_1  | Using FIFO scheduling algorithm.
algo-1-dxwxx_1  | Resources requested: 2/3 CPUs, 0/0 GPUs
algo-1-dxwxx_1  | Memory usage on this node: 1.4/4.1 GB
algo-1-dxwxx_1  | Result logdir: /opt/ml/output/intermediate/training
algo-1-dxwxx_1  | Number of trials: 1 ({'RUNNING': 1})
algo-1-dxwxx_1  | RUNNING trials:
algo-1-dxwxx_1  |  - PPO_ArrivalSim-v0_0:   RUNNING
algo-1-dxwxx_1  | 
algo-1-dxwxx_1  | (pid=72) 2019-08-30 09:35:13,030  WARNING ppo.py:172 -- FYI: By default, the value function will not share layers with the policy model ('vf_share_layers': False).
algo-1-dxwxx_1  | 2019-08-30 09:35:13,063   ERROR trial_runner.py:460 -- Error processing event.
algo-1-dxwxx_1  | Traceback (most recent call last):
algo-1-dxwxx_1  |   File ""/usr/local/lib/python3.6/dist-packages/ray/tune/trial_runner.py"", line 409, in _process_trial
algo-1-dxwxx_1  |     result = self.trial_executor.fetch_result(trial)
algo-1-dxwxx_1  |   File ""/usr/local/lib/python3.6/dist-packages/ray/tune/ray_trial_executor.py"", line 314, in fetch_result
algo-1-dxwxx_1  |     result = ray.get(trial_future[0])
algo-1-dxwxx_1  |   File ""/usr/local/lib/python3.6/dist-packages/ray/worker.py"", line 2316, in get
algo-1-dxwxx_1  |     raise value
algo-1-dxwxx_1  | ray.exceptions.RayTaskError: ray_worker (pid=72, host=b9b15d495b68)
algo-1-dxwxx_1  |   File ""/usr/local/lib/python3.6/dist-packages/ray/rllib/models/model.py"", line 83, in __init__
algo-1-dxwxx_1  |     restored, num_outputs, options)
algo-1-dxwxx_1  |   File ""/usr/local/lib/python3.6/dist-packages/ray/rllib/models/model.py"", line 135, in _build_layers_v2
algo-1-dxwxx_1  |     raise NotImplementedError
algo-1-dxwxx_1  | NotImplementedError
algo-1-dxwxx_1  | 
algo-1-dxwxx_1  | During handling of the above exception, another exception occurred:
algo-1-dxwxx_1  | 
algo-1-dxwxx_1  | ray_worker (pid=72, host=b9b15d495b68)
algo-1-dxwxx_1  |   File ""/usr/local/lib/python3.6/dist-packages/ray/rllib/agents/agent.py"", line 276, in __init__
algo-1-dxwxx_1  |     Trainable.__init__(self, config, logger_creator)
algo-1-dxwxx_1  |   File ""/usr/local/lib/python3.6/dist-packages/ray/tune/trainable.py"", line 88, in __init__
algo-1-dxwxx_1  |     self._setup(copy.deepcopy(self.config))
algo-1-dxwxx_1  |   File ""/usr/local/lib/python3.6/dist-packages/ray/rllib/agents/agent.py"", line 373, in _setup
algo-1-dxwxx_1  |     self._init()
algo-1-dxwxx_1  |   File ""/usr/local/lib/python3.6/dist-packages/ray/rllib/agents/ppo/ppo.py"", line 77, in _init
algo-1-dxwxx_1  |     self.env_creator, self._policy_graph)
algo-1-dxwxx_1  |   File ""/usr/local/lib/python3.6/dist-packages/ray/rllib/agents/agent.py"", line 506, in make_local_evaluator
algo-1-dxwxx_1  |     extra_config or {}))
algo-1-dxwxx_1  |   File ""/usr/local/lib/python3.6/dist-packages/ray/rllib/agents/agent.py"", line 714, in _make_evaluator
algo-1-dxwxx_1  |     async_remote_worker_envs=config[""async_remote_worker_envs""])
algo-1-dxwxx_1  |   File ""/usr/local/lib/python3.6/dist-packages/ray/rllib/evaluation/policy_evaluator.py"", line 288, in __init__
algo-1-dxwxx_1  |     self._build_policy_map(policy_dict, policy_config)
algo-1-dxwxx_1  |   File ""/usr/local/lib/python3.6/dist-packages/ray/rllib/evaluation/policy_evaluator.py"", line 661, in _build_policy_map
algo-1-dxwxx_1  |     policy_map[name] = cls(obs_space, act_space, merged_conf)
algo-1-dxwxx_1  |   File ""/usr/local/lib/python3.6/dist-packages/ray/rllib/agents/ppo/ppo_policy_graph.py"", line 176, in __init__
algo-1-dxwxx_1  |     seq_lens=existing_seq_lens)
algo-1-dxwxx_1  |   File ""/usr/local/lib/python3.6/dist-packages/ray/rllib/models/catalog.py"", line 215, in get_model
algo-1-dxwxx_1  |     seq_lens)
algo-1-dxwxx_1  |   File ""/usr/local/lib/python3.6/dist-packages/ray/rllib/models/catalog.py"", line 255, in _get_model
algo-1-dxwxx_1  |     num_outputs, options)
algo-1-dxwxx_1  |   File ""/usr/local/lib/python3.6/dist-packages/ray/rllib/models/model.py"", line 86, in __init__
algo-1-dxwxx_1  |     input_dict[""obs""], num_outputs, options)
algo-1-dxwxx_1  |   File ""/usr/local/lib/python3.6/dist-packages/ray/rllib/models/fcnet.py"", line 37, in _build_layers
algo-1-dxwxx_1  |     scope=label)
algo-1-dxwxx_1  |   File ""/usr/local/lib/python3.6/dist-packages/tensorflow/contrib/framework/python/ops/arg_scope.py"", line 182, in func_with_args
algo-1-dxwxx_1  |     return func(*args, **current_args)
algo-1-dxwxx_1  |   File ""/usr/local/lib/python3.6/dist-packages/tensorflow/contrib/layers/python/layers/layers.py"", line 1854, in fully_connected
algo-1-dxwxx_1  |     outputs = layer.apply(inputs)
algo-1-dxwxx_1  |   File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/base_layer.py"", line 817, in apply
algo-1-dxwxx_1  |     return self.__call__(inputs, *args, **kwargs)
algo-1-dxwxx_1  |   File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/layers/base.py"", line 374, in __call__
algo-1-dxwxx_1  |     outputs = super(Layer, self).__call__(inputs, *args, **kwargs)
algo-1-dxwxx_1  |   File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/base_layer.py"", line 730, in __call__
algo-1-dxwxx_1  |     self._assert_input_compatibility(inputs)
algo-1-dxwxx_1  |   File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/base_layer.py"", line 1493, in _assert_input_compatibility
algo-1-dxwxx_1  |     str(x.shape.as_list()))
algo-1-dxwxx_1  | ValueError: Input 0 of layer default/fc1 is incompatible with the layer: : expected min_ndim=2, found ndim=1. Full shape received: [None]
algo-1-dxwxx_1  | 
algo-1-dxwxx_1  | 2019-08-30 09:35:13,064   INFO ray_trial_executor.py:178 -- Destroying actor for trial PPO_ArrivalSim-v0_0. If your trainable is slow to initialize, consider setting reuse_actors=True to reduce actor creation overheads.
algo-1-dxwxx_1  | 2019-08-30 09:35:13,076   INFO trial_runner.py:497 -- Attempting to recover trial state from last checkpoint.
algo-1-dxwxx_1  | (pid=72) 2019-08-30 09:35:13,041  INFO policy_evaluator.py:278 -- Creating policy evaluation worker 0 on CPU (please ignore any CUDA init errors)
</code></pre>

<p>I am not sure how to change the input the environment gives to the model or the models setup itself. It seems the documentations are quite obscure. I have a hunch that problem lies with the observation and action spaces</p>

<p>Here is the reference to the original aws project example:
<a href=""https://github.com/awslabs/amazon-sagemaker-examples/tree/master/reinforcement_learning/rl_roboschool_ray"" rel=""nofollow noreferrer"">https://github.com/awslabs/amazon-sagemaker-examples/tree/master/reinforcement_learning/rl_roboschool_ray</a></p>
",11889909,,874188,,2019-08-30 10:28:13,2019-09-18 20:19:34,How to make the inputs and model have the same shape (RLlib Ray Sagemaker reinforcement learning),<python><reinforcement-learning><amazon-sagemaker><ray>,1,0,0,,,CC BY-SA 4.0,
1855,740389,1,742889,,2009-04-11 16:32:19,,25,6344,"<p>For an ai-class project I need to implement a reinforcement learning algorithm which beats a simple game of tetris. The game is written in Java and we have the source code. I know the basics of reinforcement learning theory but was wondering if anyone in the SO community had hands on experience with this type of thing.</p>

<ol>
<li>What would your recommended readings be for an implementation of reinforced learning in a tetris game?</li>
<li>Are there any good open source projects that accomplish similar things that would be worth checking out?</li>
</ol>

<p>Edit: The more specific the better, but general resources about the subject are welcomed.</p>

<p><strong>Follow up:</strong> </p>

<p>Thought it would be nice if I posted a followup.</p>

<p>Here's the solution (code and writeup) I ended up with for any future students :).</p>

<p><strong><a href=""http://dl.getdropbox.com/u/30163/AI.PAPER.DESIMONE.GOCHEV.doc"" rel=""noreferrer"">Paper</a></strong> / <strong><a href=""http://dl.getdropbox.com/u/30163/tetris_done.tar.gz"" rel=""noreferrer"">Code</a></strong></p>
",67445,,1695962,,2015-04-07 16:09:44,2019-11-27 16:50:02,Good implementations of reinforcement learning?,<language-agnostic><artificial-intelligence><machine-learning><reinforcement-learning>,9,0,0,,,CC BY-SA 3.0,
1856,56296626,1,56298077,,2019-05-24 16:45:06,,1,381,"<p>I'm trying to learn policy gradient methods for reinforcement learning but I stuck at the score function part.</p>

<p>While searching for maximum or minimum points in a function, we take the derivative and set it to zero, then look for the points that holds this equation.</p>

<p>In policy gradient methods, we do it by taking the gradient of the expectation of trajectories and we get:</p>

<p><a href=""https://cdn-images-1.medium.com/max/2400/1*4Y7BwUu2JBRIJ8bxXkzDjg.png"" rel=""nofollow noreferrer"">Objective function image</a></p>

<p>Here I could not get how this gradient of log policy shifts the distribution (through its parameters Î¸) to increase the scores of its samples <b>mathematically</b>? Don't we look for something that make this objective function's gradient zero as I explained above?</p>
",5481706,,,,,2019-05-24 20:40:59,How does score function help in policy gradient?,<reinforcement-learning><policy-gradient-descent>,1,0,,,,CC BY-SA 4.0,
1857,56168830,1,56225354,,2019-05-16 12:41:31,,3,263,"<p>I'm using neural network and tensorflow to for reinforcement learning on various stuff with Q learning method, and I want to know what is the solution to reduce the outputs possibilities when a specific action corresponding to a specific output isn't realisable in the environment at a specific state.</p>

<p>For example, my network is learning to play a game in which 4 actions are performed. But there is a specific state in which action 1 isn't performable in the environment but my neural network Q values indicate me that action 1 is the best thing to do. What do I have to do in this situation?</p>

<p>(Is just chosing a random valid action the best way to counter this problem ?)</p>
",10168321,,8893595,,2019-05-16 13:36:15,2019-05-20 17:04:50,How to reduce a neural network output when a certain action isn't performable,<tensorflow><neural-network><output><reinforcement-learning>,1,3,,,,CC BY-SA 4.0,
1862,56456148,1,56457923,,2019-06-05 07:42:54,,0,121,"<p><strong>Is it possible to consider the output of one neural network as two or more sets of outputs ?</strong></p>

<p>I explain myself a bit more (in a q learning context):</p>

<blockquote>
  <p>Imagine i have two agents in the same environement and each agents
  have a different  amount of performable actions. Both of the agents
  will have the same input vector containing environnemental variables
  to chose their actions.</p>
</blockquote>

<p>The question is :</p>

<p><strong>Can I use a unique neural network to control both agents ?</strong> </p>

<p>One exemple:</p>

<blockquote>
  <p>Agent 1 have 3 performable actions and Agent 2 have only 2 performable
  actions. An important thing is that the agent will have to work
  cooperatively to maximize the reward. Can i use 1 neural network with
  5 outputs to chose the best action to do for both agents ? like the
  first 3 outputs of the network will be the Q values for the first
  agent and the 2 others will be the Q values for agent 2. My reward
  function will always be based on the global results, each agents will
  not have specific reward.</p>
</blockquote>

<p>Is it possible ? Because i didn't find anything talking about that.
If you need more precisions just ask.</p>

<p>I also know that a possible solution should be to make a network with 3 * 2 outputs and each output would be a couple of actions (1 action for each agent), but i really want to know if someone already did someone like i explained before or just if someone know that can't work and why.</p>
",10168321,,,,,2019-06-05 09:37:29,"Is it possible to train a neural network with ""splited"" output",<tensorflow><neural-network><reinforcement-learning><q-learning>,1,0,,,,CC BY-SA 4.0,
1863,56506937,1,56508859,,2019-06-08 13:38:10,,0,1340,"<p>I am trying to understand how to use <a href=""https://pymdptoolbox.readthedocs.io/en/latest/api/example.html"" rel=""nofollow noreferrer"">mdptoolbox</a> and had a few questions.</p>

<p>What does <code>20</code> mean in the following statement?</p>

<pre><code>P, R = mdptoolbox.example.forest(10, 20, is_sparse=False)
</code></pre>

<p>I understand that <code>10</code> here denotes the number of possible states. What does <code>20</code> mean here? Does it represent the total number of actions per state? I want to restrict the MDP to exactly 2 actions per state. How could I do this?</p>

<p>The shape of <code>P</code> returned above is <code>(2, 10, 10)</code>. What does <code>2</code> represent here? No matter what values I use for total states and actions, it is always <code>2</code>.</p>
",11277952,,4772009,,2019-06-08 19:06:47,2019-06-08 19:06:47,Understanding the argument values for mdptoolbox forest example,<python><numpy><reinforcement-learning><mdptoolbox>,1,2,,,,CC BY-SA 4.0,
1864,27293219,1,27293543,,2014-12-04 11:44:21,,3,839,"<p>I am trying to implement the Q-Learning. The general algorithm from <a href=""http://en.wikipedia.org/wiki/Q-learning#Algorithm"" rel=""nofollow noreferrer"">here</a> is as below</p>

<p><img src=""https://i.stack.imgur.com/e3hgc.png"" alt=""enter image description here""></p>

<p>In the statement</p>

<p><img src=""https://i.stack.imgur.com/LEYon.png"" alt=""enter image description here""></p>

<p>I just don't get it that should i implement the above statement of the original pseudo-code <strong>recursively</strong> for all next states which current state/action can lead us to and max it every time </p>

<p>OR just <strong>choose the maximum value of the next state with current action</strong> from the Action-State Q-Value table?</p>

<p>Thanks in advance.</p>
",1198291,,,,,2014-12-04 11:58:50,Is Q-Learning Algorithm's implementation recursive?,<algorithm><recursion><reinforcement-learning><q-learning>,1,0,0,,,CC BY-SA 3.0,
1866,56624332,1,56634362,,2019-06-17 03:18:24,,1,766,"<p>Is <strong>Q function</strong> synonymous to <strong>action-value function</strong>? I see it used interchangeably and also together ( ex. ""Q action-value function"" ).</p>
",11066596,,,,,2019-06-17 15:23:56,Q function vs action-value function,<reinforcement-learning>,1,0,,,,CC BY-SA 4.0,
1868,56344773,1,70138503,,2019-05-28 14:45:39,,0,50,"<p>I am tentatively trying to train a deep reinforcement learning model the maze escaping task, and each time it takes one image as the input (e.g., a different ""maze"").</p>

<p>Suppose I have about 10K different maze images, and the ideal case is that after training N mazes, my model would do a good job to quickly solve the puzzle in the rest 10K - N images.   </p>

<p>I am writing to inquire some good idea/empirical evidences on how to select a good N for the training task.</p>

<p>And in general, how should I estimate and enhance the ability of ""transfer learning"" of my reinforcement model? Make it more generalized? </p>

<p>Any advice or suggestions would be appreciate it very much. Thanks.</p>
",1488533,,,,,2021-11-27 20:36:01,Train a reinforcement learning model with a large amount of images,<deep-learning><reinforcement-learning><transfer-learning>,1,2,,,,CC BY-SA 4.0,
1870,63635815,1,63724151,,2020-08-28 14:32:07,,1,614,"<p>I am using Vowpal Wabbit's contextual bandit to rank various action given a context.</p>
<pre><code>Train Data:
&quot;1:10:0.1 | 123&quot;
&quot;2:9:0.1 | 123&quot;
&quot;3:8:0.1 | 123&quot;
&quot;4:7:0.1 | 123&quot;
&quot;5:6:0.1 | 123&quot;
&quot;6:5:0.1 | 123&quot;
&quot;7:4:0.1 | 123&quot;

Test Data:
&quot; | 123&quot;
</code></pre>
<p>Now, the expected ranking of action should be (from least loss to most loss):</p>
<pre><code>7 6 5 4 3 2 1
</code></pre>
<p>Using <code>--cb</code> just returns the most optimal action:</p>
<pre><code>7
</code></pre>
<p>And using <code>--cb_explore</code> returns a pdf of the actions to be explored but it doesn't seem to help in ranking.</p>
<pre><code>[0.0071428571827709675, 0.0071428571827709675, 0.0071428571827709675, 0.0071428571827709675, 0.0071428571827709675, 0.0071428571827709675, 0.9571428298950195]
</code></pre>
<p>Is there any other way of using vw's contextual bandit for ranking?</p>
",6603039,,,,,2022-01-04 18:38:41,How to learn to rank using Vowpal Wabbit's contextual bandit?,<machine-learning><reinforcement-learning><vowpalwabbit><recommendation-engine>,2,0,0,,,CC BY-SA 4.0,
1871,56838708,1,56842785,,2019-07-01 15:53:08,,0,115,"<p>I'm coding a simple q-learning example and to update q-values you need a maxQ'.</p>

<p>I'm not sure if maxQ' is referring to the sum of all possible rewards or the highest possible reward:</p>

<p><a href=""https://i.stack.imgur.com/LlcgL.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/LlcgL.png"" alt=""enter image description here""></a></p>
",,user11105005,13302,,2019-07-02 07:39:14,2019-07-02 07:39:14,Is MaxQ' sum of all possible rewards or highest possible reward?,<reinforcement-learning><q-learning>,1,1,,,,CC BY-SA 4.0,
1872,45937774,1,46001664,,2017-08-29 11:17:44,,-3,509,"<p>I am trying to create a Deep Q-Network (DQN) similar to <a href=""https://github.com/deepmind/dqn"" rel=""nofollow noreferrer"">Deepmind DQN3.0</a> using Tensorflow, but I am having some difficulties. I think that the cause is TensorFlow's auto-differential approach.</p>

<p>Please see this pic. This is the architecture of DQN3.0.</p>

<p><a href=""https://i.stack.imgur.com/1oFAD.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/1oFAD.png"" alt=""DQN architecture""></a></p>

<p>In supervised learning, in order to approximate the output of the network to the label, calculate the difference by the loss function, back propagate it, and update the parameters with the optimizer.</p>

<p>In DQN, the state that AI experienced in the past accumulated in memory is input to two neural networks of TergetNetwork and Network again, and the difference between the two networks is reflected in the Network.</p>

<p>The output of each network is not the probability that the total will be 1 but the expected value.
TergetNetwork's output will include the discount rate(gamma) and the reward earned at that time.</p>

<p>And, looking at the implementation of DQN 3.0 (lua + torch), it compares the outputs of the current network to the action selected at that time, and backward propagates the difference directly by the backward method.</p>

<pre><code>function nql:getQUpdate(args)
    local s, a, r, s2, term, delta
    local q, q2, q2_max

    s = args.s
    a = args.a
    r = args.r
    s2 = args.s2
    term = args.term

    -- The order of calls to forward is a bit odd in order
    -- to avoid unnecessary calls (we only need 2).

    -- delta = r + (1-terminal) * gamma * max_a Q(s2, a) - Q(s, a)
    term = term:clone():float():mul(-1):add(1)

    local target_q_net
    if self.target_q then
        target_q_net = self.target_network
    else
        target_q_net = self.network
    end

    -- Compute max_a Q(s_2, a).
    q2_max = target_q_net:forward(s2):float():max(2)

    -- Compute q2 = (1-terminal) * gamma * max_a Q(s2, a)
    q2 = q2_max:clone():mul(self.discount):cmul(term)

    delta = r:clone():float()

    if self.rescale_r then
        delta:div(self.r_max)
    end
    delta:add(q2)

    -- q = Q(s,a)
    local q_all = self.network:forward(s):float()
    q = torch.FloatTensor(q_all:size(1))
    for i=1,q_all:size(1) do
        q[i] = q_all[i][a[i]]
    end
    delta:add(-1, q)

    if self.clip_delta then
        delta[delta:ge(self.clip_delta)] = self.clip_delta
        delta[delta:le(-self.clip_delta)] = -self.clip_delta
    end

    local targets = torch.zeros(self.minibatch_size, self.n_actions):float()
    for i=1,math.min(self.minibatch_size,a:size(1)) do
        targets[i][a[i]] = delta[i]
    end

    if self.gpu &gt;= 0 then targets = targets:cuda() end

    return targets, delta, q2_max
end


function nql:qLearnMinibatch()
    -- Perform a minibatch Q-learning update:
    -- w += alpha * (r + gamma max Q(s2,a2) - Q(s,a)) * dQ(s,a)/dw
    assert(self.transitions:size() &gt; self.minibatch_size)

    local s, a, r, s2, term = self.transitions:sample(self.minibatch_size)

    local targets, delta, q2_max = self:getQUpdate{s=s, a=a, r=r, s2=s2,
    term=term, update_qmax=true}

    -- zero gradients of parameters
    self.dw:zero()

    -- get new gradient
    self.network:backward(s, targets)
</code></pre>

<p>For this reason, if you do not mind the speed of the compute block in the above figure, you can calculate it by using Numpy etc. on the CPU instead of using Tensorflow, and I can exclude it from auto-differentiation I am thinking.</p>

<p>In DQN3.0, backpropagation is only computed only from output layer of <em>Network</em> (in blue). However, with my model in Tensorflow it starts from the final op <code>mul</code>.</p>

<p>I want to start backpropagation from that same output layer as in DQN3.0 using Tensorflow.</p>

<p>I understand that i can get <code>grads_and_vars</code> using compute_gradients() optimizer method, and run a manual differential process created from scratch. But, I think that implementing such a differential of convolution layer is very difficult to me.</p>

<p>Can I exclude compute block ops at auto-differential using Tensorflow functions or something? Or are there other methods to solve this?</p>
",8531777,,8531777,,2017-08-30 07:25:18,2017-09-01 13:55:04,[Deep Q-Network]How to exclude ops at auto-differential of Tensorflow,<tensorflow><neural-network><deep-learning><artificial-intelligence><reinforcement-learning>,2,4,,,,CC BY-SA 3.0,
1874,63602222,1,64514982,,2020-08-26 16:50:33,,9,1020,"<p>I have made a small script in Python to solve various Gym environments with policy gradients.</p>
<pre class=""lang-python prettyprint-override""><code>import gym, os
import numpy as np
#create environment
env = gym.make('Cartpole-v0')
env.reset()
s_size = len(env.reset())
a_size = 2

#import my neural network code
os.chdir(r'C:\---\---\---\Python Code')
import RLPolicy
policy = RLPolicy.NeuralNetwork([s_size,a_size],learning_rate=0.000001,['softmax']) #a 3layer network might be ([s_size, 5, a_size],learning_rate=1,['tanh','softmax'])
#it supports the sigmoid activation function also
print(policy.weights)

DISCOUNT = 0.95 #parameter for discounting future rewards

#first step
action = policy.feedforward(env.reset)
state,reward,done,info = env.step(action)

for t in range(3000):
    done = False
    states = [] #lists for recording episode
    probs2 = []
    rewards = []
    while not done:
        #env.render() #to visualize learning

        probs = policy.feedforward(state)[-1] #calculate probabilities of actions
        action = np.random.choice(a_size,p=probs) #choose action from probs

        #record and update state
        probs2.append(probs) 
        states.append(state)
        state,reward,done,info = env.step(action)
        rewards.append(reward) #should reward be before updating state?

    #calculate gradients
    gradients_w = []
    gradients_b = []
    for i in range(len((rewards))):
        totalReward = sum([rewards[t]*DISCOUNT**t for t in range(len(rewards[i:]))]) #discounted reward
        ## !! this is the line that I need help with
        gradient = policy.backpropagation(states[i],totalReward*(probs2[i])) #what should be backpropagated through the network
        ## !!

        ##record gradients
        gradients_w.append(gradient[0])
        gradients_b.append(gradient[1])
    #combine gradients and update the weights and biases
    gradients_w = np.array(gradients_w,object)
    gradients_b = np.array(gradients_b,object)
    policy.weights += policy.learning_rate * np.flip(np.sum(gradients_w,0),0) #np.flip because the gradients are calculated backwards
    policy.biases += policy.learning_rate * np.flip(np.sum(gradients_b,0),0)
    #reset and record
    env.reset()
    if t%100==0:
        print('t'+str(t),'r',sum(rewards))
</code></pre>
<p>What should be passed backwards to calculate the gradients? I am using gradient ascent but I could switch it to descent. Some people have defined the reward function as <em>totalReward*log(probabilities)</em>. Would that make the score derivative <em>totalReward*(1/probs)</em> or <em>log(probs)</em> or something else?  Do you use a cost function like cross entropy?
I have tried<br />
<code>totalReward*np.log(probs)</code><br />
<code>totalReward*(1/probs)</code><br />
<code>totalReward*(probs**2)</code><br />
<code>totalReward*probs</code></p>
<pre class=""lang-python prettyprint-override""><code>probs = np.zeros(a_size)  
probs[action] = 1  
totalRewards*probs
</code></pre>
<p>and a couple others.
The last one is the only one that was able to solve any of them and it only worked on Cartpole. I have tested the various loss or score functions for thousands of episodes with gradient ascent and descent on Cartpole, Pendulum, and MountainCar. Sometimes it will improve a small amount but it will never solve it. What am I doing wrong?</p>
<p>And here is the RLPolicy code. It is not well written or pseudo coded but I don't think it is the problem because I checked it with gradient checking several times. But it would be helpful even if I could narrow it down to a problem with the neural network or somewhere else in my code.</p>
<pre class=""lang-python prettyprint-override""><code>#Neural Network
import numpy as np
import random, math, time, os
from matplotlib import pyplot as plt

def activation(x,function):
    if function=='sigmoid':
        return(1/(1+math.e**(-x))) #Sigmoid
    if function=='relu':
        x[x&lt;0]=0
        return(x)
    if function=='tanh':
        return(np.tanh(x.astype(float))) #tanh
    if function=='softmax':
        z = np.exp(np.array((x-max(x)),float))
        y = np.sum(z)
    return(z/y)
def activationDerivative(x,function):
    if function=='sigmoid':
        return(x*(1-x))
    if function=='relu':
        x[x&lt;0]==0
        x[x&gt;0]==1
        return(x)
    if function=='tanh':
        return(1-x**2)
    if function=='softmax':
        s = x.reshape(-1,1)
        return(np.diagflat(s) - np.dot(s, s.T))

class NeuralNetwork():
    
    def __init__ (self,layers,learning_rate,momentum,regularization,activations):
        self.learning_rate = learning_rate   
        if (isinstance(layers[1],list)):
            h = layers[1][:]
            del layers[1]
            for i in h:
                layers.insert(-1,i)
        self.layers = layers
        self.weights = [2*np.random.rand(self.layers[i]*self.layers[i+1])-1 for i in range(len(self.layers)-1)]
        self.biases = [2*np.random.rand(self.layers[i+1])-1 for i in range(len(self.layers)-1)]    
        self.weights = np.array(self.weights,object)
        self.biases = np.array(self.biases,object)
        self.activations = activations
    def feedforward(self, input_array):
        layer = input_array
        neuron_outputs = [layer]
        for i in range(len(self.layers)-1):
            layer = np.tile(layer,self.layers[i+1])
            layer = np.reshape(layer,[self.layers[i+1],self.layers[i]])
            weights = np.reshape(self.weights[i],[self.layers[i+1],self.layers[i]])
            layer = weights*layer
            layer = np.sum(layer,1)#,self.layers[i+1]-1)
            layer = layer+self.biases[i]
            layer = activation(layer,self.activations[i])
            neuron_outputs.append(np.array(layer,float))
        return(neuron_outputs)
    def neuronErrors(self,l,neurons,layerError,n_os):
        if (l==len(self.layers)-2):
            return(layerError)
        totalErr = [] #total error
        for e in range(len(layerError)): #-layers
            e = e*self.layers[l+2]
            a_ws = self.weights[l+1][e:e+self.layers[l+1]]
            e = int(e/self.layers[l+2])
            err = layerError[e]*a_ws #error
            totalErr.append(err)
        return(sum(totalErr))
    def backpropagation(self,state,loss):
        weights_gradient = [np.zeros(self.layers[i]*self.layers[i+1]) for i in range(len(self.layers)-1)]
        biases_gradient = [np.zeros(self.layers[i+1]) for i in range(len(self.layers)-1)]  
        neuron_outputs = self.feedforward(state)
        grad = self.individualBackpropagation(loss, neuron_outputs)
        return(grad)

    def individualBackpropagation(self, difference, neuron_outputs): #number of output
        lr = self.learning_rate
        n_os = neuron_outputs[:]
        w_o = self.weights[:]
        b_o = self.biases[:]
        w_n = self.weights[:]
        b_n = self.biases[:]
        gradient_w = []
        gradient_b = []
        error = difference[:] #error for neurons
        for l in range(len(self.layers)-2,-1,-1):
            p_n = np.tile(n_os[l],self.layers[l+1]) #previous neuron
            neurons = np.arange(self.layers[l+1])
            error = (self.neuronErrors(l,neurons,error,n_os))
            if not self.activations[l]=='softmax':
                error = error*activationDerivative(neuron_outputs[l+1],self.activations[l])
            else:
                error = error @ activationDerivative(neuron_outputs[l+1],self.activations[l]) #because softmax derivative returns different dimensions
            w_grad = np.repeat(error,self.layers[l]) #weights gradient
            b_grad = np.ravel(error) #biases gradient
            w_grad = w_grad*p_n
            b_grad = b_grad
            gradient_w.append(w_grad)
            gradient_b.append(b_grad)
        return(gradient_w,gradient_b)

</code></pre>
<p>Thanks for any answers, this is my first question here.</p>
",14170431,,14170431,,2020-09-16 22:22:34,2020-10-24 15:29:54,What Loss Or Reward Is Backpropagated In Policy Gradients For Reinforcement Learning?,<python><reinforcement-learning><backpropagation><policy-gradient-descent>,3,2,0,,,CC BY-SA 4.0,
1875,56697930,1,56700571,,2019-06-21 06:24:01,,1,436,"<p>I am working on learning q-tables and ran through a simple version which only used a 1-dimensional array to move forward and backward. now I am trying 4 direction movement and got stuck on controlling the person.</p>

<p>I got the random movement down now and it will eventually find the goal. but I want it to learn how to get to the goal instead of randomly stumbling on it. So I would appreciate any advice on adding a qlearning into this code. Thank you.</p>

<p>Here is my full code as it stupid simple right now. </p>

<pre><code>import numpy as np
import random
import math

world = np.zeros((5,5))
print(world)
# Make sure that it can never be 0 i.e the start point
goal_x = random.randint(1,4)
goal_y = random.randint(1,4)
goal = (goal_x, goal_y)
print(goal)
world[goal] = 1
print(world)

LEFT = 0
RIGHT = 1
UP = 2
DOWN = 3
map_range_min = 0
map_range_max = 5

class Agent:
    def __init__(self, current_position, my_goal, world):
        self.current_position = current_position
        self.last_postion = current_position
        self.visited_positions = []
        self.goal = my_goal
        self.last_reward = 0
        self.totalReward = 0
        self.q_table = world


    # Update the totoal reward by the reward        
    def updateReward(self, extra_reward):
        # This will either increase or decrese the total reward for the episode
        x = (self.goal[0] - self.current_position[0]) **2
        y = (self.goal[1] - self.current_position[1]) **2
        dist = math.sqrt(x + y)
        complet_reward = dist + extra_reward
        self.totalReward += complet_reward 

    def validate_move(self):
        valid_move_set = []
        # Check for x ranges
        if map_range_min &lt; self.current_position[0] &lt; map_range_max:
            valid_move_set.append(LEFT)
            valid_move_set.append(RIGHT)
        elif map_range_min == self.current_position[0]:
            valid_move_set.append(RIGHT)
        else:
            valid_move_set.append(LEFT)
        # Check for Y ranges
        if map_range_min &lt; self.current_position[1] &lt; map_range_max:
            valid_move_set.append(UP)
            valid_move_set.append(DOWN)
        elif map_range_min == self.current_position[1]:
            valid_move_set.append(DOWN)
        else:
            valid_move_set.append(UP)
        return valid_move_set

    # Make the agent move
    def move_right(self):
        self.last_postion = self.current_position
        x = self.current_position[0]
        x += 1
        y = self.current_position[1]
        return (x, y)
    def move_left(self):
        self.last_postion = self.current_position
        x = self.current_position[0]
        x -= 1
        y = self.current_position[1]
        return (x, y)
    def move_down(self):
        self.last_postion = self.current_position
        x = self.current_position[0]
        y = self.current_position[1]
        y += 1
        return (x, y)
    def move_up(self):
        self.last_postion = self.current_position
        x = self.current_position[0]
        y = self.current_position[1]
        y -= 1
        return (x, y)

    def move_agent(self):
        move_set = self.validate_move()
        randChoice = random.randint(0, len(move_set)-1)
        move = move_set[randChoice]
        if move == UP:
            return self.move_up()
        elif move == DOWN:
            return self.move_down()
        elif move == RIGHT:
            return self.move_right()
        else:
            return self.move_left()

    # Update the rewards
    # Return True to kill the episode
    def checkPosition(self):
        if self.current_position == self.goal:
            print(""Found Goal"")
            self.updateReward(10)
            return False
        else:
            #Chose new direction
            self.current_position = self.move_agent()
            self.visited_positions.append(self.current_position)
            # Currently get nothing for not reaching the goal
            self.updateReward(0)
            return True


gus = Agent((0, 0) , goal)
play = gus.checkPosition()
while play:
    play = gus.checkPosition()

print(gus.totalReward)
</code></pre>
",1009508,,,,,2022-09-15 08:09:25,How can I change this to use a q table for reinforcement learning,<python><artificial-intelligence><reinforcement-learning><q-learning>,1,9,,,,CC BY-SA 4.0,
1876,56700948,1,56766128,,2019-06-21 09:49:04,,14,5656,"<p>I'm reading through the <a href=""https://arxiv.org/pdf/1707.06347.pdf"" rel=""noreferrer"">original PPO paper</a> and trying to match this up to the input parameters of the <a href=""https://stable-baselines.readthedocs.io/en/master/modules/ppo2.html"" rel=""noreferrer"">stable-baselines PPO2</a> model.</p>

<p>One thing I do not understand is the <code>total_timesteps</code> parameter in the <a href=""https://stable-baselines.readthedocs.io/en/master/modules/ppo2.html#stable_baselines.ppo2.PPO2.learn"" rel=""noreferrer""><code>learn</code></a> method.</p>

<p>The paper mentions</p>

<blockquote>
  <p>One style of policy gradient implementation...  runs the policy for T timesteps (where T is much less than the
  episode length)</p>
</blockquote>

<p>While the stable-baselines documentation describes the <code>total_timesteps</code> parameter as</p>

<blockquote>
  <p>(int) The total number of samples to train on</p>
</blockquote>

<p>Therefore I would think that <code>T</code> in the paper and <code>total_timesteps</code> in the documentation are the same parameter.</p>

<p>What I do not understand is the following:</p>

<ul>
<li><p>Does <code>total_timesteps</code> always need to be less than or equal to the total number of available ""frames"" (samples) in an environment (say if I had a finite number of frames like 1,000,000). If so, why?</p></li>
<li><p>By setting <code>total_timesteps</code> to a number less than the number of available frames, what portion of the training data does the agent see? For example, if <code>total_timesteps=1000</code>, does the agent only ever see the first 1000 frames?</p></li>
<li><p>Is an episode defined as the total number of available frames, or is it defined as when the agent first ""looses"" / ""dies""? If the latter, then how can you know in advance when the agent will die to be able set <code>total_timesteps</code> to a lesser value?</p></li>
</ul>

<p>I'm still learning the terminology behind RL, so I hope I've been able to explain my question clearly above. Any help / tips would be very much welcomed.</p>
",4139143,,,,,2019-06-26 05:59:34,Understanding the total_timesteps parameter in stable-baselines' models,<python><reinforcement-learning>,1,0,0,,,CC BY-SA 4.0,
1877,56763327,1,56765670,,2019-06-25 23:37:48,,1,2768,"<p>I'm doing a project at the moment which would require tensorflowjs to create a neural network that learns from reinforcement learning algorithms. Is this possible to do?</p>
",7840769,,,,,2019-06-26 09:11:50,Can I use reinforcement learning in tensorflowjs?,<javascript><machine-learning><neural-network><reinforcement-learning><tensorflowjs>,1,0,0,,,CC BY-SA 4.0,
1884,64013919,1,64016647,,2020-09-22 16:27:11,,1,555,"<p><em>Referring to the RL book by Sutton and Barto, 2nd ed., Ch-3, pg-60.</em></p>
<p>Here is the 5x5 grid world and the value of each state:
<a href=""https://i.stack.imgur.com/1Q2Qz.png"" rel=""nofollow noreferrer"">gridoworld with state values</a></p>
<p>Using the Bellman Backup equation, the value of each state can be calculated:</p>
<p>Here is the calculation for the middle (3,3) cell:</p>
<p><a href=""https://i.stack.imgur.com/Xg5bG.png"" rel=""nofollow noreferrer"">calculation of state value</a></p>
<p>Using the values from the upper, lower, left and right cells,
along with a random policy with <code>pi = 1/4</code>
and all the transition probabilities <code>p(s',r|s,a) = 1</code>,
the calculation holds.</p>
<p><strong>But what about the corner cells?</strong></p>
<p>Say, 3.3 at the top left. How to calculate that?</p>
<p>Using the lower (1.5) and right (8.8) values only doesn't work. Also, it must be considered that when the agent performs the upper and left actions, it remains on the grid but receives a reward of -1.</p>
<p>Can you please help me calculate the corner cell values? Reading the github implementations isn't helping either.</p>
",5549401,,,,,2020-09-22 19:39:25,Gridworld from Sutton's RL book: how to calculate value function for corner cells?,<reinforcement-learning><markov-decision-process>,1,0,0,,,CC BY-SA 4.0,
1885,59510603,1,59518059,,2019-12-28 11:48:01,,2,5781,"<p>I am initializing a Convolutional DQN with the following code:</p>

<pre><code>class ConvDQN(nn.Module):

    def __init__(self, input_dim, output_dim):
        super(ConvDQN, self).__init__()
        self.input_dim = input_dim
        self.output_dim = output_dim
        self.conv = nn.Sequential(
            nn.Conv2d(self.input_dim, 32, kernel_size=8, stride=4),
            nn.ReLU(),
            nn.Conv2d(32, 64, kernel_size=4, stride=2),
            nn.ReLU(),
            nn.Conv2d(64, 64, kernel_size=3, stride=1),
            nn.ReLU()
        )

        self.fc_input_dim = self.feature_size()

        self.fc = nn.Sequential(
            nn.Linear(self.fc_input_dim, 128),
            nn.ReLU(),
            nn.Linear(128, 256),
            nn.ReLU(),
            nn.Linear(256, self.output_dim)
        )

    def forward(self, state):
        features = self.conv(state)
        features = features.view(features.size(0), -1)
        qvals = self.fc(features)
        return qvals

    def feature_size(self):
        return self.conv(autograd.Variable(torch.zeros(1, *self.input_dim))).view(1, -1).size(1)

</code></pre>

<p>And it gives me the error:</p>

<pre><code>  File ""dqn.py"", line 86, in __init__
    self.fc_input_dim = self.feature_size()
  File ""dqn.py"", line 105, in feature_size
    return self.conv(autograd.Variable(torch.zeros(32, *self.input_dim))).view(1, -1).size(1)
  File ""C:\Users\ariji\AppData\Local\Programs\Python\Python37\lib\site-packages\torch\nn\modules\module.py"", line 489, in __call__
    result = self.forward(*input, **kwargs)
  File ""C:\Users\ariji\AppData\Local\Programs\Python\Python37\lib\site-packages\torch\nn\modules\container.py"", line 92, in forward
    input = module(input)
  File ""C:\Users\ariji\AppData\Local\Programs\Python\Python37\lib\site-packages\torch\nn\modules\module.py"", line 489, in __call__
    result = self.forward(*input, **kwargs)
  File ""C:\Users\ariji\AppData\Local\Programs\Python\Python37\lib\site-packages\torch\nn\modules\conv.py"", line 320, in forward
    self.padding, self.dilation, self.groups)
RuntimeError: Expected 4-dimensional input for 4-dimensional weight [32, 4, 8, 8], but got 2-dimensional input of size [1, 4] instead
</code></pre>

<p>So I get the fact that the input that I am passing to the convolutional network is of incorrect dimensions. What I do not understand is how I am supposed to add the required dimensions to my input? Or should I change something in my convolutional network?</p>
",11137158,,,,,2019-12-29 09:01:56,"RuntimeError: Expected 4-dimensional input for 4-dimensional weight [32, 4, 8, 8], but got 2-dimensional input of size [1, 4] instead",<python><conv-neural-network><pytorch><reinforcement-learning><torch>,1,1,0,,,CC BY-SA 4.0,
1886,22935418,1,22935684,,2014-04-08 11:20:29,,1,121,"<p>I am trying to solve the 4 box puzzle problem. My logic according to my experience is every number is  swap with space and then reaches to correct position.</p>

<p>i have this:</p>

<pre><code>1 3
2
</code></pre>

<p>my goal state:</p>

<pre><code>1 2
3
</code></pre>

<p>what am using the approach: (Key:R-> right, D->down; up->up; L->left)</p>

<pre><code>                    1 3
                    2
                 R/         D|  up\   \L
                 /           |     \   \
               1 3          1      (not possible boundary exists)    
                 2          2 3

         R/  D|  up\   \L
         /    |     \   \ 
                3       1 3
              1 2       2
         ...............
        .................
</code></pre>

<p>Here is what i am doing but i am getting every time in some tree same state but not correct goal state. If i am using wrong tree approach or algorithm please tell me. Because i am implementing this sort of tree after studying through internet.</p>
",3508182,,4706171,,2017-03-24 20:20:57,2017-03-24 22:49:28,solving 4 puzzle with tree,<data-structures><machine-learning><artificial-intelligence><reinforcement-learning><q-learning>,2,1,,,,CC BY-SA 3.0,
1888,5010247,1,5011476,,2011-02-15 22:26:30,,2,903,"<p>I've started toying with reinforcement learning (using the Sutton book). <strong>I fail to fully understand is the paradox between having to reduce the markov state space while on the other hand not making assumptions about what's important and what's not.</strong></p>

<p><strong>background</strong></p>

<p>Eg. the checkers example, Sutton says that one should not assign rewards to certain actions in the game, such as defeating an opponents piece. He claims this may optimize the AI for taking pieces not win the game. Thus, rewards should only be given to the result you want to achieve (eg win the game).</p>

<p><strong>Question 1</strong></p>

<p>Assume a (Texas hold'em) Poker AI with a markov state only of the players hand and the cards on the table. This has around 52*51*50*49*48*47*46/1*2*3*4*5*6*7 states. Now assume we want the AI to take players money pool + their bets into consideration. This will make the Markov state space approach ""infinite number of combinations"" if we assume 8 players each having between $1-200.000.</p>

<p><strong>Question 2</strong></p>

<p>One state-reducing-strategy could be to divide players cash into either <em>poor</em>, <em>medium</em> or <em>rich</em>. This seriously reduces our state space, however, how do I know that a) 3 groups is sufficient? b) what are the discriminating limits for each group?</p>

<p>cheers, </p>
",454017,,,,,2020-05-27 22:36:12,Reducing the number of markov-states in reinforcement learning,<artificial-intelligence><reinforcement-learning>,3,0,0,,,CC BY-SA 2.5,
1889,28021734,1,28022044,,2015-01-19 09:47:25,,9,3612,"<p>I could not understand how to update Q values for tic tac toe game. I read all about that but I could not imagine how to do this. I read that Q value is updated end of the game, but I haven't understand that if there is Q value for each action ?</p>
",4462426,,,,,2016-04-10 11:32:51,Q Learning Algorithm for Tic Tac Toe,<machine-learning><artificial-intelligence><tic-tac-toe><reinforcement-learning><q-learning>,2,0,0,,,CC BY-SA 3.0,
1890,41505285,1,41510369,,2017-01-06 12:01:48,,2,790,"<p>I am working through chapter 2, section 7, of Sutton &amp; Barto's <em>Reinforcement Learning: An Introduction</em>, which deals with gradient methods in the multi-armed bandit problem.  (I realize that the 2nd edition is a draft and it seems that the sections move around a little bit, but my file has section 2.7 titled ""Gradient Bandits"".)  I have managed to use the methods in sections 2.3-2.5 without issue, but I am consistently getting results using the gradient methods that are perplexing.  I will walk through my code and show an example.</p>

<p>Just initializing everything here:</p>

<pre><code>import random
import math
import numpy as np, numpy.random

# number of arms (k) and step-size (alpha)
k = 10
alpha = 0.1

# initialize preference function (H), and reward distribution (R)
H = {i: 0 for i in range(k)}
R = {i: [random.uniform(-100,100), 1] for i in range(k)}
</code></pre>

<p>I'm using stationary reward distributions, and I am using dictionaries to represent those distributions.  I am assuming each reward is described by a Gaussian, so I am mapping actions to rewards with the following function:</p>

<pre><code>def getReward(action, rewardDistribution):
  return random.gauss(rewardDistribution[action][0], rewardDistribution[action][1])
</code></pre>

<p>The so-called ""preference function"" <code>H</code>, which is used to determine the action probabilities, is also given by a dictionary.  I am spreading out the choices across a very wide range, as each reward is described by a Gaussian distribution with standard deviation 1 located somewhere between -100 and 100.  I am doing because my intuition is telling me that it will make it harder for the algorithm to settle on a sub-optimal choice, but I am finding that the opposite is occurring.</p>

<p>This code selects my actions at each iteration:</p>

<pre><code>def selectAction(policy):
  return np.random.choice(list(policy.keys()), p=list(policy.values()))
</code></pre>

<p>And next is the code that runs the iterations of the algorithm.  Note that <code>pi</code> is the policy and is initialized to give probability <code>1/k</code> to each action.</p>

<pre><code>avgReward = 0
for i in range(100000):
  pi = {i: math.exp(H[i])/sum([math.exp(H[j]) for j in range(k)]) for i in range(k)}
  A = selectAction(pi)
  R_A = getReward(A, R)
  avgReward += (R_A - avgReward)/(i + 1)
  H = {i: H[i] + alpha*(R_A - avgReward)*((i == A) - pi[i]) for i in range(k)}
</code></pre>

<p>Notice I'm running 100,000 iterations, which to me seems like it should be overkill.  This is my first attempt at this problem, so my intuition may be off, but I tried to set this up to make it easy for the algorithm to find the optimal choice.  So what I expect is the process with converge on the action with the distribution having the highest expected value and will continue hitting it as the iterations go by.  But, when I print out results relative to each possible action by the bandit, this is what I see:</p>

<pre><code>for i in range(k):
  print(""Expected reward: "" + str(R[i][0]) + "" | Selection probability: "" + str(pi[i]) + "" | Preference: "" + str(H[i]))

Expected reward: -50.62506110888989 | Selection probability: 3.617077909489526e-13 | Preference: -7.82992533515
Expected reward: 11.866419726345484 | Selection probability: 1.2337498052271344e-10 | Preference: -1.99777839484
Expected reward: 75.41139657867947 | Selection probability: 1.5933517476231946e-09 | Preference: 0.560588358966
Expected reward: -72.44467653824414 | Selection probability: 3.4267025247257986e-13 | Preference: -7.88399339198
Expected reward: -43.466561447399 | Selection probability: 1.5933517476231946e-09 | Preference: 0.560588358966
Expected reward: -75.99171566420297 | Selection probability: 1.5933517476231946e-09 | Preference: 0.560588358966
Expected reward: -82.11920932060593 | Selection probability: 3.120658098513757e-13 | Preference: -7.97754791911
Expected reward: 95.00643386364632 | Selection probability: 1.5933517476231946e-09 | Preference: 0.560588358966
Expected reward: 31.384022070017835 | Selection probability: 1.2605442916195123e-08 | Preference: 2.62887724114
Expected reward: 49.83925652065625 | Selection probability: 0.9999999808967586 | Preference: 20.8180143641
</code></pre>

<p>The last action has an expected reward of <strong>49.8</strong>, and the bandit selects it virtually every time.  This is the 3rd-best of the 10 options, but it ignores an option that has an expected reward of <strong>75.4</strong> and another one that has an expected reward of <strong>95.0</strong>.</p>

<p>So, my question: why is this bandit missing the optimal choice?  This is just an example, this is happening on a pretty consistent basis when I run the program.  Is my intuition off with respect to what I should expect the bandit to do, or have I coded this algorithm up incorrectly?</p>
",5795920,,,,,2017-01-06 16:50:49,Counterintuitive results on multi-armed bandit exercise,<python><machine-learning><gradient-descent><reinforcement-learning>,1,0,0,,,CC BY-SA 3.0,
1891,65551065,1,65830102,,2021-01-03 14:42:51,,3,449,"<p>I am trying to reproduce with R an algorithm described in Sutton and Barto (2018), but I was not able to produce a matrix with arrows as the one described by the authors on page 65:</p>
<p><a href=""https://i.stack.imgur.com/RNNJV.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/RNNJV.png"" alt=""enter image description here"" /></a></p>
<p>I tried to use the package &quot;fields&quot; for this purpose, but without much success.</p>
<p>In Python the solution proposed by Shangtong Zhang and Kenta Shimada
relies on using the arrows symbols:
ACTIONS_FIGS=[ 'â†', 'â†‘', 'â†’', 'â†“']
but this does not work nicely with R...</p>
<p>EDIT: I coded the initial actions and the action updates numerically as follows:</p>
<pre><code>library(data.table)
action_random = data.table(cell=c(1:25))
action_random$action_up = action_random$action_right = action_random$action_down =
action_random$action_left = rep(1,25)
action_random$proba = rep(1/4,25)
action_random
</code></pre>
<p>I was also able to adapt the code posted <a href=""https://stackoverflow.com/questions/27030055/draw-3x3-square-grid-in-r"">here</a>, to draw a simple grid with simple arrows:</p>
<pre><code>arrows = matrix(c(&quot;\U2190&quot;,&quot;\U2191&quot;,&quot;\U2192&quot;,&quot;\U2193&quot;),nrow=2,ncol=2)
grid_arrows = expand.grid(x=1:ncol(arrows),y=1:nrow(arrows))
grid_arrows$val = arrows[as.matrix(grid_arrows[c('y','x')])]

library(ggplot2)

ggplot(grid_arrows, aes(x=x, y=y, label=val)) + 
  geom_tile(fill='transparent', colour = 'black') + 
  geom_text(size = 14) + 
  scale_y_reverse() +
  theme_classic() + 
  theme(axis.text  = element_blank(),
        panel.grid = element_blank(),
        axis.line  = element_blank(),
        axis.ticks = element_blank(),
        axis.title = element_blank())
</code></pre>
<p>However:<br />
(i) There is no unicode available for the nice 2 are 4-directional arrows reported in Table $\pi_\ast$ above<br />
(ii) ... and so I was not trying to code the bijection between the numerical values in the Table &quot;action_random&quot; and a nice Table with arrows in it...</p>
<p>Any hint helping to resolve issues (i) and (ii) are welcome.</p>
",4614651,,4614651,,2021-01-13 10:48:03,2021-01-21 15:01:46,R: matrix with directional arrows,<r><symbols><reinforcement-learning>,2,2,0,,,CC BY-SA 4.0,
1894,23172250,1,23246092,,2014-04-19 16:03:01,,3,2882,"<p>I have to solve this problem with Q-learning.
Well, actually I have to evaluated a Q-learning based policy on it.</p>

<p>I am a tourist manager.</p>

<p>I have <strong>n</strong> hotels, each can contain a different number of persons.</p>

<p>for each person I put in a hotel I get a reward, based on which room I have chosen.</p>

<p>If I want I can also murder the person, so it goes in no hotel but it gives me a different reward.
 (OK,that's a joke...but it's to say that I can have a self transition. so the number of people in my rooms doesn't change after that action).</p>

<ul>
<li><p>my state is a vector containing the number of persons in each hotel.</p></li>
<li><p>my action is a vector of zeroes and ones which tells me where do I<br>
put the new person.</p></li>
<li>my reward matrix is formed by the rewards I get for each transition<br>
between states (even the self transition one).</li>
</ul>

<p>now,since I can get an unlimited number of people (i.e. I can fill it but I can go on killing them) how can I build the Q matrix? without the Q matrix I can't get a policy and so I can't evaluate it...</p>

<p>What do I see wrongly? should I choose a random state as final? Do I have missed the point at all?</p>
",1834153,,1926629,,2014-04-20 21:57:15,2017-05-09 14:21:24,is Q-learning without a final state even possible?,<machine-learning><reinforcement-learning><q-learning>,4,4,0,,,CC BY-SA 3.0,
1895,49615100,1,49615786,,2018-04-02 16:38:14,,2,357,"<p>I am trying to run my own version of baselines code source of reinforcement learning on github: (<a href=""https://github.com/openai/baselines/tree/master/baselines/ppo2"" rel=""nofollow noreferrer"">https://github.com/openai/baselines/tree/master/baselines/ppo2</a>).</p>

<p>Whatever I do, I keep having the same display which looks like this : </p>

<p><a href=""https://i.stack.imgur.com/BlyuA.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/BlyuA.png"" alt=""enter image description here""></a></p>

<p>Where can I edit it ?  I know I should edit the ""learn"" method but I don't know how </p>
",8974267,,8089674,,2018-04-02 17:04:03,2018-04-02 17:27:43,How can I change baselines code output/replay (PPO) on github?,<python><artificial-intelligence><reinforcement-learning>,1,0,,,,CC BY-SA 3.0,
1896,59792100,1,59804130,,2020-01-17 17:20:10,,-1,164,"<p>The objective of game is for rabbit to find the single cabbage in a very large field in min time and eat it. The action space is discrete (up/down/right/left), the state space is continuous (a very large field). Episode ends after 100 jumps, when rabbit finds the cabbage, or gets out of the field (whichever happens first). One of state variables is distance to the cabbage. Since the field is very large I am using reward shaping (small positive/negative reward for getting closer to/farther from the cabbage). In order to get to the cabbage in min time the reward should be 0 for each jump and 1 when cabbage is found. However, shaping the reward breaks the scheme. Is there an elegant way to employ reward shaping in this game and how? Thanks for any advice.</p>
",10718104,,,,,2020-01-18 19:37:48,How to handle rewards for variable length episodes with reward at terminal state,<reinforcement-learning>,1,1,,,,CC BY-SA 4.0,
1899,5215856,1,6866152,,2011-03-07 04:34:08,,16,2453,"<p>All the <a href=""http://en.wikipedia.org/wiki/Reinforcement_learning"">reinforcement learning</a> algorithms I've read about are usually applied to a single agent that has a fixed number of actions. Are there any reinforcement learning algorithms for making a decision while taking into account a variable number of actions? For example, how would you apply a RL algorithm in a computer game where a player controls N soldiers, and each soldier has a random number of actions based its condition? You can't formulate fixed number of actions for a global decision maker (i.e. ""the general"") because the available actions are continually changing as soldiers are created and killed. And you can't formulate a fixed number of actions at the soldier level, since the soldier's actions are conditional based on its immediate environment. If a soldier sees no opponents, then it might only be able to walk, whereas if it sees 10 opponents, then it has 10 new possible actions, attacking 1 of the 10 opponents.</p>
",247542,,1000551,,2017-10-24 12:28:39,2020-05-07 07:00:33,Reinforcement Learning With Variable Actions,<machine-learning><reinforcement-learning><planning>,3,2,0,,,CC BY-SA 2.5,
1900,41791932,1,41792802,,2017-01-22 14:13:41,,-2,142,"<p>I get the gist of how to do it but I canÂ´t seem to understand how I save the (State,Value) pairs with Tetris having so many different states that using a hash map should not work natively because of memory. Maybe you can map different states to one single or is there another trick? Or do I just have the wrong idea about it? </p>
",5166930,,,,,2017-01-22 15:32:48,Reinforced Learning for Tetris,<machine-learning><reinforcement-learning>,1,0,,,,CC BY-SA 3.0,
1905,60021599,1,60021962,,2020-02-01 22:26:11,,3,286,"<p>Optimal value of state-action by bellman optimal equation(63 page of sutton 2018) is</p>

<p><img src=""https://latex.codecogs.com/gif.latex?%24%24Q%28s%2Ca%29%20%3D%20%5Csum_%7Bs%27%2C%20r%7Dp%28s%27%2C%20r%7Cs%2Ca%29%28r%20&plus;%20%5Cgamma%20%5Cmax_a%20Q%28s%27%2Ca%27%29%24%24"" alt=""Math expression""></p>

<p>and Q-learning is</p>

<p><img src=""https://wikimedia.org/api/rest_v1/media/math/render/svg/678cb558a9d59c33ef4810c9618baf34a9577686"" alt=""Q-learning""></p>

<p>I have known that Q-learning is model-free. so It doesn't need a probability of transition for next state.</p>

<p>However, p(s'r|s,a) of bellman equation is probability of transition for next state s' with reward r when s, a are given. so I think to get a Q(s,a), it needs probability of transition.</p>

<p>Q of bellman equation and Q of q-learning is different?</p>

<p>If it is same, how q-learning can work as model-free?</p>

<p>Is there any way to get a Q(s,a) regardless of probability of transition for q-learning?</p>

<p>Or Am i confusing something?</p>
",9255374,,9255374,,2020-02-03 04:52:23,2020-02-03 04:52:23,Relationship between bellman optimal equation and Q-learning,<machine-learning><artificial-intelligence><reinforcement-learning><q-learning>,1,0,,,,CC BY-SA 4.0,
1907,59769111,1,60033725,,2020-01-16 11:52:24,,0,334,"<p>When updating the policy in PPO, does the clipping part stop the weights at the exact value for which r(Î¸) is exactly 1Â±epsilon or does it allow to exceed that value and then prevent it from going any further by setting the gradient to zero? Which one is what is actually going on?</p>
",4807022,,,,,2020-02-03 05:46:59,Does PPO's gradient clipping really prevent r(Î¸) from exceeding 1Â±epsilon?,<algorithm><machine-learning><artificial-intelligence><reinforcement-learning>,1,0,0,,,CC BY-SA 4.0,
1908,49691932,1,49692864,,2018-04-06 11:36:24,,0,182,"<p>I'm trying to <strong>implement a Deep Q Network that plays Doom</strong> (vizdoom)</p>

<p>However I'm stuck (since yesterday) with <strong>the problem of one hot encoding and its consequences</strong>: in fact, I have 3 possible actions that are encoded like that</p>

<p><code>[[True, False, False], [False, True, False], [False, False, True]]</code> size = [Batch_size, 3]</p>

<p>When I one_hot encode this action array I obtain an array of this size [BatchSize, 3, 3]</p>

<p>As consequence when I want to calculate my Q-value estimation:</p>

<p><code>Q = tf.reduce_sum(tf.multiply(self.output, self.actions_one_hot), axis=1)</code></p>

<p>The <code>tf.multiply(self.output, self.actions_one_hot)</code> produces an error:</p>

<p><code>InvalidArgumentError: Incompatible shapes: [10,3] vs. [10,3,3]
     [[Node: DQNetwork/Mul = Mul[T=DT_FLOAT, _device=""/job:localhost/replica:0/task:0/device:CPU:0""](DQNetwork/dense/BiasAdd, DQNetwork/one_hot)]]</code></p>

<p><strong>I understand that these 2 have incompatible shapes to be multiplied</strong> but I don't understand what I must do to make them compatible.</p>

<p>To be more clear <a href=""https://github.com/simoninithomas/Test_doom_dqn"" rel=""nofollow noreferrer"">this is the notebook with each part explained</a>:</p>

<p>I'm sure that I made a really stupid mistake but I don't see it.</p>

<p>Thanks for your help!</p>
",4871661,,4871661,,2018-04-06 12:12:00,2018-04-06 12:35:33,Incompatible shapes when output * actions_one_hot,<python><tensorflow><deep-learning><reinforcement-learning>,1,0,,,,CC BY-SA 3.0,
1910,31208655,1,31212105,,2015-07-03 14:00:56,,7,864,"<p>A few of my lab mates have been playing around cross-entropy reinforcement learning. From everything I can gather from them and quick internet searches, the cross-entropy method seems nearly identical to genetic algorithms. Can someone explain to me what the real difference between these two techniques is if one actually exists?</p>
",3513267,,,,,2021-04-18 12:57:45,Whats the difference between Cross-Entropy and Genetic Algorithms?,<machine-learning><genetic-algorithm><reinforcement-learning>,2,0,0,,,CC BY-SA 3.0,
1920,42224533,1,42259329,,2017-02-14 11:05:45,,0,473,"<p>I'm having trouble finding a good reward function for the pendulum problem, the function I'm using: -x ** 2 + - 0.25 * (xdot ** 2)
which is the quadratic error from the top. with x representing the current location of the pendulum and xdot the angular velocity.</p>

<p>its takes a lot of time with this function and sometimes doesn't work.
any one have some other suggestions?
I've been looking in google but didn't find anything i could use</p>
",7562555,,,,,2017-02-15 20:25:16,"Reinforcement learning, pendulum python",<python><reinforcement-learning>,1,2,,,,CC BY-SA 3.0,
1922,8801136,1,8920156,,2012-01-10 09:41:20,,3,1680,"<p>I want to implement a reinforcement learning connect four agent.
I am unsure how to do so and how it should look. I am familiar with the theoretical aspects of reinforcement learning but don't know how they should be implemented.</p>

<p>How should it be done?
Should I use TD(lambda) or Q-learning, and how do MinMax trees come in to this?
How does my Q and V functions work (Quality of action and Value of state). How do I score those things? What is my base policy which I improve, and what is my model?
Another thing is how should I save the states or statesXactions (depending on the learning algorithm). Should I use neural networks or not? And if yes, how?</p>

<p>I am using JAVA.</p>

<p>Thanks. </p>
",618517,,,,,2012-01-19 01:57:16,Want to implement a reinforcement learning connect four agent,<java><reinforcement-learning>,1,3,,,,CC BY-SA 3.0,
1924,8776230,1,8776257,,2012-01-08 08:13:05,,0,1385,"<p>What is the best algorithm for reinforcement learning for a four in a row game.
I want to build a four in a row game that will use one of the RL algorithms to play:
Q-Learning, MinMax etc.</p>

<p>What is the best one to use considering I am using Java.</p>
",618517,,,,,2017-08-29 21:41:07,Best algorithm for reinforcement learning for a four in a row game,<java><reinforcement-learning>,2,0,,,,CC BY-SA 3.0,
1926,8804768,1,8947501,,2012-01-10 14:23:32,,3,1505,"<p>How to use MinMax trees with Q-Learning?</p>

<p>I want to implement a Q-Learning connect four agent and heard that adding MinMax trees into it helps.</p>
",618517,,49329,,2012-01-19 02:15:43,2020-01-11 11:09:50,How to use MinMax trees with Q-Learning?,<artificial-intelligence><reinforcement-learning><game-ai>,3,0,,,,CC BY-SA 3.0,
1927,8828637,1,8829620,,2012-01-12 00:24:18,,3,856,"<p>I'm planning on running a machine learning algorithm that learns node values and edge weights. The algorithm is very similar to the value iteration algorithm <a href=""http://inst.eecs.berkeley.edu/~cs188/fa10/slides/FA10%20cs188%20lecture%2010%20--%20MDPs%20II%20%286PP%29.pdf"" rel=""nofollow"">here</a>. Each node represents a location and each edge is a path to a new location. Each node and edge maintains a value that represents how desirable it is. These values are updated every iteration based on the values of the previous iteration.</p>

<p>I plan on using neo4j as the backend. There will be around 600,000 Nodes and 100,000,000 edges for now, but more may be added later (The graph won't fit in memory). What is the best way to retain the values from the previous iteration? 2 ways that come to mind are: </p>

<ol>
<li>clone the current database and use 1 copy as the ""current iteration"" copy and 1 copy as the ""previous iteration"" copy.</li>
<li>At the end of every iteration move all values of each node and edge to a ""previous iteration"" property.</li>
</ol>

<p>How do people usually do this? Is there a better way?</p>
",766953,,766953,,2012-01-12 19:36:49,2012-01-12 22:41:02,Reinforcement learning with neo4j: make 2 copies of the graph vs store 2 copies of all values on 1 graph,<machine-learning><neo4j><reinforcement-learning>,2,0,0,,,CC BY-SA 3.0,
1928,61898668,1,61901561,,2020-05-19 18:57:18,,15,4742,"<p><a href=""https://pytorch.org/tutorials/beginner/blitz/cifar10_tutorial.html#sphx-glr-beginner-blitz-cifar10-tutorial-py"" rel=""noreferrer"">Here</a> they mention the need to include <code>optim.zero_grad()</code> when training to zero the parameter gradients. My question is: Could I do as well <code>net.zero_grad()</code> and would that have the same effect? Or is it necessary to do  <code>optim.zero_grad()</code>. Moreover, what happens if I do both? If I do none, then the gradients get accumulated, but what does that exactly mean? do they get added? In other words, what's the difference between doing <code>optim.zero_grad()</code> and <code>net.zero_grad()</code>. I am asking because <a href=""https://github.com/nicehiro/ddpg/blob/master/ddpg_pytorch/agent.py"" rel=""noreferrer"">here, line 115</a> they use <code>net.zero_grad()</code> and it is the first time I see that, that is an implementation of a reinforcement learning algorithm, where one has to be especially careful with the gradients because there are multiple networks and gradients, so I suppose there is a reason for them to do <code>net.zero_grad()</code> as opposed to <code>optim.zero_grad()</code>. </p>
",12627448,,,,,2020-05-19 22:05:37,net.zero_grad() vs optim.zero_grad() pytorch,<pytorch><reinforcement-learning>,1,0,0,,,CC BY-SA 4.0,
1937,43848376,1,43883405,,2017-05-08 13:02:10,,0,599,"<p>I have been introduced to RL by Sutton's book. In order to further this knowledge I would like to explore how the agent learns from raw pixels and try to implement an example using H2O. I want to use the Java API.</p>

<ol>
<li>Is Sparkling water the distribution I should use ?</li>
<li>How do I stream raw pixels to h2o ? How can a ping-pong game, for example, be used to make the h2o RL agent learn  ? What Deep Learning h2o API is used ?</li>
</ol>

<p>I would appreciate if the answers pertain to h2o as I refer to other literature to learn about RL.</p>

<p>Update : <a href=""http://h2o2016.wpengine.com/wp-content/themes/h2o2016/images/resources/DeepLearningBooklet.pdf"" rel=""nofollow noreferrer"">http://h2o2016.wpengine.com/wp-content/themes/h2o2016/images/resources/DeepLearningBooklet.pdf</a></p>

<p>But still I need to figure out how to use Java to stream image pixels from a game to help the h2o RL agent learn. Examples use R and Python mostly.</p>
",1234909,,1234909,,2017-05-09 08:15:37,2017-05-10 04:06:37,Reinforcement Learning - Learning from raw pixels,<h2o><reinforcement-learning>,1,0,0,,,CC BY-SA 3.0,
1938,63139124,1,63139220,,2020-07-28 16:55:06,,1,420,"<p>I have a reinforcement learning Actor Critic model with lstm.
During initial training it is giving same action value for all the states.</p>
<p>Can someone expert in AI/RL please help to let me know if this is normal behavior during training?
Also can you please help to let me know what should be the ideal size of lstm and linear layers if I have a state_dimension = 50 and action_dimension = 3.</p>
<p>Thanks in advance</p>
",2783767,,,,,2020-07-29 12:35:04,Reinforcement learning actor predicting same actions during initial training,<tensorflow><pytorch><artificial-intelligence><actor><reinforcement-learning>,1,0,,,,CC BY-SA 4.0,
1939,45377404,1,45391630,,2017-07-28 15:35:52,,6,1689,"<p>While trying to implement the <code>Episodic Semi-gradient Sarsa with</code> a Neural Network as the approximator I wondered how I choose the optimal action based on the currently learned weights of the network. If the action space is discrete I can just calculate the estimated value of the different actions in the current state and choose the one which gives the maximimum. But this seems to be not the best way of solving the problem. Furthermore, it does not work if the action space can be continous (like the acceleration of a self-driving car for example).</p>

<p>So, basicly I am wondering how to solve the 10th line <code>Choose A' as a function of q(S', , w)</code> in this pseudo-code of Sutton:
<a href=""https://i.stack.imgur.com/nQotE.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/nQotE.png"" alt=""enter image description here""></a></p>

<p>How are these problems typically solved? Can one recommend a good example of this algorithm using <code>Keras</code>?</p>

<p>Edit: Do I need to modify the pseudo-code when using a network as the approximator? So, that I simply minimize the <code>MSE</code> of the prediction of the network and the reward <code>R</code> for example?</p>
",3157209,,3157209,,2017-07-28 15:52:02,2019-10-24 06:35:57,Episodic Semi-gradient Sarsa with Neural Network,<neural-network><reinforcement-learning><sarsa>,1,0,0,,,CC BY-SA 3.0,
1940,62674505,1,62676198,,2020-07-01 10:11:17,,1,2047,"<p>I am new to TensorFlow. I made the following Neural Network in TensorFlow 1.x</p>
<pre><code>import tensorflow as tf
import numpy as np

import tflearn

class ActorNetwork(object):
    &quot;&quot;&quot;
    Input to the network is the state, output is the action
    under a deterministic policy.
    The output layer activation is a tanh to keep the action
    between -action_bound and action_bound
    &quot;&quot;&quot;

    def __init__(self, sess, state_dim, action_dim, action_bound, learning_rate, tau, batch_size):
        self.sess = sess
        self.s_dim = state_dim
        self.a_dim = action_dim
        self.action_bound = action_bound
        self.learning_rate = learning_rate
        self.tau = tau
        self.batch_size = batch_size

        # Actor Network
        self.inputs, self.out, self.scaled_out = self.create_actor_network()

        self.network_params = tf.trainable_variables()

        # Target Network
        self.target_inputs, self.target_out, self.target_scaled_out = self.create_actor_network()

        self.target_network_params = tf.trainable_variables()[
            len(self.network_params):]

        # Op for periodically updating target network with online network
        # weights
        self.update_target_network_params = \
            [self.target_network_params[i].assign(tf.multiply(self.network_params[i], self.tau) +
                                                  tf.multiply(self.target_network_params[i], 1. - self.tau))
                for i in range(len(self.target_network_params))]

        # This gradient will be provided by the critic network
        self.action_gradient = tf.placeholder(tf.float32, [None, self.a_dim])

        # Combine the gradients here
        self.unnormalized_actor_gradients = tf.gradients(
            self.scaled_out, self.network_params, -self.action_gradient)
        self.actor_gradients = list(map(lambda x: tf.math.divide(x, self.batch_size), self.unnormalized_actor_gradients))

        # Optimization Op
        self.optimize = tf.train.AdamOptimizer(self.learning_rate).\
            apply_gradients(zip(self.actor_gradients, self.network_params))

        self.num_trainable_vars = len(
            self.network_params) + len(self.target_network_params)

    def create_actor_network(self):
        inputs = tflearn.input_data(shape=[None, self.s_dim])
        net = tflearn.fully_connected(inputs, 400)
        net = tflearn.layers.normalization.batch_normalization(net)
        net = tflearn.activations.relu(net)
        net = tflearn.fully_connected(net, 300)
        net = tflearn.layers.normalization.batch_normalization(net)
        net = tflearn.activations.relu(net)
        # Final layer weights are init to Uniform[-3e-3, 3e-3]
        w_init = tflearn.initializations.uniform(minval=-0.003, maxval=0.003)
        out = tflearn.fully_connected(
            net, self.a_dim, activation='tanh', weights_init=w_init)
        # Scale output to -action_bound to action_bound
        scaled_out = tf.multiply(out, self.action_bound)
        return inputs, out, scaled_out

    def train(self, inputs, a_gradient):
        self.sess.run(self.optimize, feed_dict={
            self.inputs: inputs,
            self.action_gradient: a_gradient
        })

    def predict(self, inputs):
        return self.sess.run(self.scaled_out, feed_dict={
            self.inputs: inputs
        })

    def predict_target(self, inputs):
        return self.sess.run(self.target_scaled_out, feed_dict={
            self.target_inputs: inputs
        })

    def update_target_network(self):
        self.sess.run(self.update_target_network_params)

    def get_num_trainable_vars(self):
        return self.num_trainable_vars
</code></pre>
<p>When I call it once it does not give any error, however in the second time it gives an error. For example</p>
<pre><code>with tf.Session() as sess:
    actor1 = ActorNetwork(sess, 1, 2, 1, 0.01, 0.003, 200)
    actor2 = ActorNetwork(sess, 1, 2, 1, 0.01, 0.003, 200)
</code></pre>
<p>I get the following error for only actor2:</p>
<p>TypeError: unsupported operand type(s) for /: 'NoneType' and 'int'</p>
<p>It has something to do with None value in the lambda function. But, why does it not provide an error for the first time?</p>
<p>Edit: Stack trace:</p>
<pre><code>---------------------------------------------------------------------------
TypeError                                 Traceback (most recent call last)
&lt;ipython-input-2-2323bc1d5028&gt; in &lt;module&gt;()
      1 with tf.Session() as sess:
      2     actor1 = ActorNetwork(sess, 1, 2, 1, 0.01, 0.003, 200)
----&gt; 3     actor2 = ActorNetwork(sess, 1, 2, 1, 0.01, 0.003, 200)

3 frames
&lt;ipython-input-1-895268594a81&gt; in __init__(self, sess, state_dim, action_dim, action_bound, learning_rate, tau, batch_size)
     48         self.unnormalized_actor_gradients = tf.gradients(
     49             self.scaled_out, self.network_params, -self.action_gradient)
---&gt; 50         self.actor_gradients = list(map(lambda x: tf.math.divide(x, self.batch_size), self.unnormalized_actor_gradients))
     51 
     52         # Optimization Op

&lt;ipython-input-1-895268594a81&gt; in &lt;lambda&gt;(x)
     48         self.unnormalized_actor_gradients = tf.gradients(
     49             self.scaled_out, self.network_params, -self.action_gradient)
---&gt; 50         self.actor_gradients = list(map(lambda x: tf.math.divide(x, self.batch_size), self.unnormalized_actor_gradients))
     51 
     52         # Optimization Op

/tensorflow-1.15.2/python3.6/tensorflow_core/python/util/dispatch.py in wrapper(*args, **kwargs)
    178     &quot;&quot;&quot;Call target, and fall back on dispatchers if there is a TypeError.&quot;&quot;&quot;
    179     try:
--&gt; 180       return target(*args, **kwargs)
    181     except (TypeError, ValueError):
    182       # Note: convert_to_eager_tensor currently raises a ValueError, not a

/tensorflow-1.15.2/python3.6/tensorflow_core/python/ops/math_ops.py in divide(x, y, name)
    323     return DivideDelegateWithName(x, name) / y
    324   else:
--&gt; 325     return x / y
    326 
    327 

TypeError: unsupported operand type(s) for /: 'NoneType' and 'int'

TypeError: unsupported operand type(s) for /: 'NoneType' and 'int'
</code></pre>
<p>EDIT-2: From the suggestion, I wrote in TF 2.x. This actually eliminated the error. But are these two networks the same?</p>
<pre><code>class ActorNetwork(object):
  def __init__(self, state_dim, action_dim, action_bound, learning_rate, tau, batch_size):
    self.state_dim = state_dim
    self.action_dim = action_dim
    self.action_bound = action_bound
    self.learning_rate = learning_rate
    self.tau  = tau
    self.batch_size = batch_size
    self.optimizer = tf.keras.optimizers.Adam(self.learning_rate)

    #actor network
    self.inputs, self.out, self.scaled_out = self.create_actor_network()
    self.actor_model = keras.Model(inputs=self.inputs, outputs=self.scaled_out, name='actor_network')
    self.network_params = self.actor_model.trainable_variables

    #target actor network
    self.target_inputs, self.target_out, self.target_scaled_out = self.create_actor_network()
    self.target_actor_model = keras.Model(inputs=self.target_inputs, outputs=self.target_scaled_out, name='target_actor_network')
    self.target_network_params = self.target_actor_model.trainable_variables


  def create_actor_network(self):
    inputs = Input(shape = (self.state_dim,), batch_size = None, name = &quot;actor_input_state&quot;)

    net = layers.Dense(400, name = 'actor_dense_1a')(inputs)
    net = layers.BatchNormalization()(net)
    net = layers.Activation(activation=tf.nn.relu)(net)

    net = layers.Dense(300, name = 'actor_dense_1b')(net)
    net = layers.BatchNormalization()(net)
    net = layers.Activation(activation=tf.nn.relu)(net)

    # net = layers.Dense(20, name = 'actor_dense_1c')(net)
    # net = layers.BatchNormalization()(net)
    # net = layers.Activation(activation=tf.nn.relu)(net)

    # net = layers.Dense(10, name = 'actor_dense_1d')(net)
    # net = layers.BatchNormalization()(net)
    # net = layers.Activation(activation=tf.nn.relu)(net)
    
    w_init = tf.random_uniform_initializer(minval=-0.003, maxval=0.003, seed=None)
    out = layers.Dense(self.action_dim, activation='tanh', name = 'actor_dense_2', kernel_initializer = w_init)(net)
    scaled_out = tf.multiply(out, self.action_bound, name = &quot;actions_scaling&quot;)
    return inputs, out, scaled_out
  
  def update_target_network(self):
    self.update_target_network_params = [self.target_network_params[i].assign(tf.multiply(self.network_params[i], self.tau) + tf.multiply(self.target_network_params[i], 1-self.tau)) for i in range(len(self.target_network_params))]
  
  def train(self, inputs, a_gradient):
    with tf.GradientTape() as self.tape:
      self.prediction = self.actor_model(inputs)
    self.unnormalized_actor_gradients = self.tape.gradient(self.prediction, self.network_params, output_gradients = -a_gradient)
    self.actor_gradients = list(map(lambda x: tf.math.divide(x, self.batch_size), self.unnormalized_actor_gradients))
    self.optimizer.apply_gradients(zip(self.actor_gradients, self.network_params))
    
  def predict(self, inputs):
    return self.actor_model(inputs)

  def predict_target(self, inputs):
    return self.target_actor_model(inputs)
</code></pre>
",3701747,,3701747,,2020-07-02 04:39:13,2020-07-02 04:39:13,TensorFlow 1.x: TypeError: unsupported operand type(s) for /: 'NoneType' and 'int',<python><tensorflow><deep-learning><reinforcement-learning><tflearn>,1,2,,,,CC BY-SA 4.0,
1947,45390903,1,46268748,,2017-07-29 15:24:27,,2,793,"<p>I am trying to implement the Episodic Semi-gradient Sarsa for Estimating q described in Sutton's book to solve the <code>Mountain Car Task</code>. To approximate <code>q</code> I want to use a <code>neural network</code>. Therefore, I came up with <a href=""https://gist.github.com/FlashTek/0dfddf46c4d50c4e068f1ecbad1d03b5"" rel=""nofollow noreferrer"">this</a> code. But sadly my agent is not really learning to solve the task. In some episodes the solution is found very fast (100-200 steps), but sometimes the agent needs more than 30k steps. I think, that I made some elementary mistake in my implementation, but I am not able to find it myself. Can someone help me, and point out the error/mistake in my implementation?</p>
",3157209,,,,,2018-02-15 00:22:12,Sarsa with neural network to solve the Mountain Car Task,<reinforcement-learning><sarsa>,1,0,,,,CC BY-SA 3.0,
1949,45382763,1,45468784,,2017-07-28 21:36:22,,0,456,"<p>I have set up a Q-learning problem in R, and would like some help with the theoretical correctness of my approach in framing the problem.</p>

<p><strong>Problem structure</strong>
For this problem, the environment consists of 10 possible states. When in each state, the agent has 11 potential actions which it can choose from (these actions are the same regardless of the state which the agent is in). Depending on the particular state which the agent is in and the subsequent action which the agent then takes, there is a unique distribution for transition to a next state i.e. the transition probabilities to any next state are dependant on (only) the previous state as well as the action then taken.</p>

<p>Each episode has 9 iterations i.e. the agent can take 9 actions and make 9 transitions before a new episode begins. In each episode, the agent will begin in state 1. </p>

<p>In each episode, after each of the agent's 9 actions, the agent will get a reward which is dependant on the agent's (immediately) previous state and their (immediately) previous action as well as the state which they landed on i.e. the agent's reward structure is dependant on a state-action-state triplet (of which there will be 9 in an episode).</p>

<p>The transition probability matrix of the agent is static, and so is the reward matrix.</p>

<p>I have set up two learning algorithms. In the first, the q-matrix update happens after each action in each episode. In the second, the q-matrix is updated after each episode. The algorithm uses an epsilon greedy learning formula.</p>

<p>The big problem is that in my Q-learning, my agent is not learning. It gets less and less of a reward over time. I have looked into other potential problems such as simple calculation errors, or bugs in code, but I think that the problem lies with the conceptual structure of my q-learning problem.</p>

<p><strong>Questions</strong></p>

<ul>
<li>I have set up my Q-matrix as being a 10 row by 11 column matrix i.e. all the 10 states are the rows and the 11 actions are the columns. Would this be the best way to do so? This means that an agent is learning a policy which says that ""whenever you are in state x, do action y""</li>
<li>Given this unique structure of my problem, would the standard Q-update still apply? i.e. Q[cs,act]&lt;&lt;-Q[cs,act]+alpha*(Reward+gamma*max(Q[ns,])-Q[cs,act]) 
Where cs is current state; act is action chosen; Reward is the reward given your current state, your action chosen and the next state which you will transition to; ns is the next state which you will transition to given your last state and last action (note that you transitioned to this state stochastically).</li>
<li>Is there an open AI gym in R? Are there Q-learning packages for problems of this structure?</li>
</ul>

<p>Thanks and cheers</p>
",5211911,,157726,,2018-12-23 23:00:38,2018-12-23 23:00:38,Q-learning with a state-action-state reward structure and a Q-matrix with states as rows and actions as columns,<algorithm><machine-learning><artificial-intelligence><reinforcement-learning><q-learning>,1,3,,,,CC BY-SA 3.0,
1950,63203574,1,63203783,,2020-08-01 09:12:10,,0,773,"<p>I am having nn Actor Critic TD3 model with LSTM in my AI.
For every training, I am creating batches of sequential data and training my AI.</p>
<p>Can someone expert please help to let know if I require epochs as well for this AI.And in general how many epochs can I run with this code, because I am creating many batches on one training step is it feasible to have epochs as well.</p>
<p>Below is training step code</p>
<pre><code>def train(
        self,
        replay_buffer,
        iterations,
        batch_size=50,
        discount=0.99,
        tau=0.005,
        policy_noise=0.2,
        noise_clip=0.5,
        policy_freq=2,
        ):
        
        b_state = torch.Tensor([])
        b_next_state = torch.Tensor([])
        b_done = torch.Tensor([])
        b_reward = torch.Tensor([])
        b_action = torch.Tensor([])

        for it in range(iterations):

            # print ('it: ', it, ' iterations: ', iterations)

      # Step 4: We sample a batch of transitions (s, sâ€™, a, r) from the memory

            (batch_states, batch_next_states, batch_actions,
             batch_rewards, batch_dones) = \
                replay_buffer.sample(batch_size)

            batch_states = batch_states.astype(float)
            batch_next_states = batch_next_states.astype(float)
            batch_actions = batch_actions.astype(float)
            batch_rewards = batch_rewards.astype(float)
            batch_dones = batch_dones.astype(float)

            state = torch.from_numpy(batch_states)
            next_state = torch.from_numpy(batch_next_states)
            action = torch.from_numpy(batch_actions)
            reward = torch.from_numpy(batch_rewards)
            done = torch.from_numpy(batch_dones)

            b_size = 1
            seq_len = state.shape[0]
            batch = b_size
            input_size = state_dim

            state = torch.reshape(state, ( 1,seq_len, state_dim))
            next_state = torch.reshape(next_state, ( 1,seq_len, state_dim))
            done = torch.reshape(done, ( 1,seq_len, 1))
            reward = torch.reshape(reward, ( 1, seq_len, 1))
            action = torch.reshape(action, ( 1, seq_len, action_dim))
            
            b_state = torch.cat((b_state, state),dim=0)
            b_next_state = torch.cat((b_next_state, next_state),dim=0)
            b_done = torch.cat((b_done, done),dim=0)
            b_reward = torch.cat((b_reward, reward),dim=0)
            b_action = torch.cat((b_action, action),dim=0)
            
            # state = torch.reshape(state, (seq_len, 1, state_dim))
            # next_state = torch.reshape(next_state, (seq_len, 1,
            #         state_dim))
            # done = torch.reshape(done, (seq_len, 1, 1))
            # reward = torch.reshape(reward, (seq_len, 1, 1))
            # action = torch.reshape(action, (seq_len, 1, action_dim))
            
            # b_state = torch.cat((b_state, state),dim=1)
            # b_next_state = torch.cat((b_next_state, next_state),dim=1)
            # b_done = torch.cat((b_done, done),dim=1)
            # b_reward = torch.cat((b_reward, reward),dim=1)
            # b_action = torch.cat((b_action, action),dim=1)
                                                      
        print(&quot;dim state:&quot;,b_state.shape)

      # for h and c shape (num_layers * num_directions, batch, hidden_size)

        ha0 = torch.zeros(lstm_layers, b_state.shape[0], state_dim)
        ca0 = torch.zeros(lstm_layers, b_state.shape[0], state_dim)
        hc0 = torch.zeros(lstm_layers, b_state.shape[0], state_dim + action_dim)
        cc0 = torch.zeros(lstm_layers, b_state.shape[0], state_dim + action_dim)
        # Step 5: From the next state sâ€™, the Actor target plays the next action aâ€™
          
        b_next_action = self.actor_target(b_next_state, (ha0, ca0))
        b_next_action = b_next_action[0]
  
        # Step 6: We add Gaussian noise to this next action aâ€™ and we clamp it in a range of values supported by the environment
          
        noise = torch.Tensor(b_next_action).data.normal_(0,
                policy_noise)
        noise = noise.clamp(-noise_clip, noise_clip)
        b_next_action = (b_next_action + noise).clamp(-self.max_action,
                self.max_action)
  
        # Step 7: The two Critic targets take each the couple (sâ€™, aâ€™) as input and return two Q-values Qt1(sâ€™,aâ€™) and Qt2(sâ€™,aâ€™) as outputs
        
        result = self.critic_target(b_next_state, b_next_action, (hc0,cc0))
        target_Q1 = result[0]
        target_Q2 = result[1]
  
        # Step 8: We keep the minimum of these two Q-values: min(Qt1, Qt2)
          
        target_Q = torch.min(target_Q1, target_Q2).double()
          
        # Step 9: We get the final target of the two Critic models, which is: Qt = r + Î³ * min(Qt1, Qt2), where Î³ is the discount factor
          
        target_Q = b_reward + (1 - b_done) * discount * target_Q
          
        # Step 10: The two Critic models take each the couple (s, a) as input and return two Q-values Q1(s,a) and Q2(s,a) as outputs
          
        b_action_reshape = torch.reshape(b_action, b_next_action.shape)
        result = self.critic(b_state, b_action_reshape, (hc0, cc0))
        current_Q1 = result[0]
        current_Q2 = result[1]
          
        # Step 11: We compute the loss coming from the two Critic models: Critic Loss = MSE_Loss(Q1(s,a), Qt) + MSE_Loss(Q2(s,a), Qt)
          
        critic_loss = F.mse_loss(current_Q1, target_Q) \
            + F.mse_loss(current_Q2, target_Q)
          
        # Step 12: We backpropagate this Critic loss and update the parameters of the two Critic models with a SGD optimizer
          
        self.critic_optimizer.zero_grad()
        critic_loss.backward()
        self.critic_optimizer.step()
          
        # Step 13: Once every two iterations, we update our Actor model by performing gradient ascent on the output of the first Critic model
        
        out = self.actor(b_state, (ha0, ca0))
        out = out[0]
        (actor_loss, hx, cx) = self.critic.Q1(b_state, out, (hc0,cc0))
        actor_loss = -1 * actor_loss.mean()
        self.actor_optimizer.zero_grad()
        actor_loss.backward()
        self.actor_optimizer.step()
  
        # Step 14: Still once every two iterations, we update the weights of the Actor target by polyak averaging
          
        for (param, target_param) in zip(self.actor.parameters(),
                self.actor_target.parameters()):
            target_param.data.copy_(tau * param.data + (1 - tau)
                    * target_param.data)
  
        # Step 15: Still once every two iterations, we update the weights of the Critic target by polyak averaging
          
        for (param, target_param) in zip(self.critic.parameters(),
                self.critic_target.parameters()):
            target_param.data.copy_(tau * param.data + (1 - tau)
                    * target_param.data)
</code></pre>
",2783767,,,,,2020-08-01 09:38:36,how many epochs required for model with lstm training,<python><pytorch><artificial-intelligence><reinforcement-learning>,1,0,,,,CC BY-SA 4.0,
1952,10793034,1,10816320,,2012-05-29 04:22:40,,2,894,"<p>I am currently working on famous <a href=""http://library.rl-community.org/wiki/Mountain_Car_%28Java%29"" rel=""nofollow"">Mountain Car problem</a> from reinforcement learning. This problem is of continuous nature, meaning I have two variables: one position - ranging from -1.2 to 0.5 and velocity - ranging from -0.07 to 0.07. And I have 3 possible actions - reverse acceleration, forward acceleration and neutral, actions result in changing position in appropriate direction. Because of how acceleration is calculated my position variable is continuous, meaning that I can't use a lookup table, so I tried to divide position-velocity axis in rectangular sectors, dividing position into buckets of width 0.05 and velocity into buckets of 0.005 length, assigning each sector an index, I did it like this:</p>

<pre><code>public int discretiseObservation(Observation observation) {
    double position = observation.getDouble(0) ;
    double velocity = observation.getDouble(1);

    boolean positionNegativeFlag = position &lt; 0;
    boolean velocityNegativeFlag = velocity &lt; 0;

    double absolutePosition = Math.abs(position);
    double absoluteVelocity = Math.abs(velocity);

    double discretePosition = Math.floor(absolutePosition / 0.05);
    double discreteVelocity = Math.floor(absoluteVelocity / 0.005);

    if(velocityNegativeFlag) {
        discreteVelocity += 14;
    }

    if(positionNegativeFlag) {
        discretePosition += 10;
    }

    return (int)discretePosition * 28 + (int)discreteVelocity;
}
</code></pre>

<p>But this scheme results in some sectors having the same index number. Do you have any idea how can I discretize this two continuous variables? </p>

<p><strong>Upd:</strong> Sorry forgot to mention that when position or velocity exceeds maximum or minimum value I set it back to maximum or minimum value</p>
",1125515,,682105,,2013-08-27 08:23:26,2013-08-27 08:23:26,Discretization dilemma,<java><math><reinforcement-learning><calc>,2,2,,,,CC BY-SA 3.0,
1953,45768126,1,45787264,,2017-08-19 05:47:19,,1,94,"<p>The standard supervised classification setup: we have a bunch of samples, each with the correct label out of <code>N</code> labels. We build a NN with N outputs, transform those to probabilities with softmax, and the loss is the mean <code>cross-entropy</code> between each NN output and the corresponding true label, represented as a <code>1-hot</code> vector with <code>1</code> in the true label and <code>0</code> elsewhere. We then optimize this loss by following its gradient. The classification error is used just to measure our model quality. </p>

<p>HOWEVER, I know that when doing <code>policy gradient</code> we can use the <a href=""http://adv-ml-2017.wdfiles.com/local--files/course-schedule/rl_class1.pdf"" rel=""nofollow noreferrer"">likelihood ratio trick</a>, and we no longer need to use <code>cross-entropy</code>! our loss simply <code>tf.gather</code> the NN output corresponding to the correct label. E.g. <a href=""https://gist.github.com/shanest/535acf4c62ee2a71da498281c2dfc4f4"" rel=""nofollow noreferrer"">this solution of OpenAI gym CartPole</a>. </p>

<p>WHY can't we use the same trick when doing supervised learning? I was thinking that the reason we used <code>cross-entropy</code> is because it is differentiable, but apparently <code>tf.gather</code> is <a href=""https://stackoverflow.com/questions/45701722/tensorflow-how-come-gather-nd-is-differentiable"">differentiable as well</a>. </p>

<p>I mean - IF we measure ourselves on classification error, and we CAN optimize for classification error as it's differentiable, isn't it BETTER to also optimize for classification error instead of this weird <code>cross-entropy</code> proxy? </p>
",574187,,,,,2017-08-20 22:16:53,"When we do supervised classification with NN, why do we train for cross-entropy and not for classification error?",<tensorflow><neural-network><gradient-descent><reinforcement-learning>,2,0,,,,CC BY-SA 3.0,
1955,38228574,1,38232331,,2016-07-06 16:03:54,,0,94,"<p>In a grid-world if i start taking actions following initial policy as a discrete distribution among available actions. let say i have at each state four actions (north, south, east, west), now i decide that in each state 50% of the time i will choose action ""north"". 30 % time i will choose action ""south"". 10 % time action ""east"" and rest 10% action ""west"". what effect it will have on optimal policy. if i had chosen uniform random distribution among actions. i guess that exploring an action more frequently will let q value for that state and action pair will converge fast and it will be more authentic. but no way if i explore an action more its q-value will be more. please tell me if i am correct or not.</p>
",2160015,,,,,2016-07-06 19:34:12,can reinforcement learning agent learn a discrete distribution,<reinforcement-learning>,1,0,,,,CC BY-SA 3.0,
1959,56520232,1,56663425,,2019-06-10 03:38:59,,-2,93,"<p>We know q-learning need tons of calculations:</p>

<p><a href=""https://stackoverflow.com/questions/56234489/the-huge-amount-of-states-in-q-learning-calculation"">The huge amount of states in q-learning calculation</a></p>

<p>For a gaming AI, it needs much more q-values than OX game, GO game.</p>

<p>How this is to be done to calculate these large amounts of q-values?</p>

<p>Thanks.</p>
",5518300,,5518300,,2019-06-19 08:48:36,2019-06-19 08:48:36,How deepmind reduce the calculation for Q values for Atari games?,<sql><c><tensorflow><machine-learning><reinforcement-learning>,1,3,,,,CC BY-SA 4.0,
1963,56926882,1,56930366,,2019-07-07 23:28:55,,-1,675,"<p>I am new to reinforcement learning as well as pytorch. I am learning it from Udemy. However, the code I have is the same as it is shown but I am having an error. I guess it is a pytorch error but can't debug it. If anyone helps it is really appreciated. </p>

<pre><code>import gym
import time
import torch
import matplotlib.pyplot as plt
from gym.envs.registration import register
register(id='FrozenLakeNotSlippery-v0',entry_point='gym.envs.toy_text:FrozenLakeEnv',kwargs={'map_name': '4x4', 'is_slippery':False})
env = gym.make('FrozenLakeNotSlippery-v0')
number_of_states = env.observation_space.n
number_of_actions = env.action_space.n
Q = torch.zeros([number_of_states,number_of_actions])
num_episodes = 1000
steps_total = []
gamma = 1
for i in range(num_episodes):
    state = env.reset()
    step = 0
    while True:
        step += 1
        #action = env.action_space.sample()
        random_values = Q[state]+torch.rand(1,number_of_actions)/1000
        action = torch.max(random_values,1)[1][0]
        new_state, reward, done, info = env.step(action)
        Q[state, action] = reward + gamma * torch.max(Q[new_state])
        state = new_state
        #time.sleep(0.4)
        #env.render()
        if done:
            steps_total.append(step)
            print (""Episode Finished after %i steps"" %step)
            break

print (""Average Num Steps: %2f"" %(sum(steps_total)/num_episodes))
plt.plot(steps_total)
plt.show()
</code></pre>

<p>The error I am having is the below one </p>

<pre><code>KeyError                                  Traceback (most recent call last)
&lt;ipython-input-11-a6aa419c3767&gt; in &lt;module&gt;
  8         random_values = Q[state]+torch.rand(1,number_of_actions)/1000
  9         action = torch.max(random_values,1)[1][0]
---&gt; 10         new_state, reward, done, info = env.step(action)
 11         Q[state, action] = reward + gamma * torch.max(Q[new_state])
 12         state = new_state

c:\users\souradip\appdata\local\programs\python\python36\lib\site-packages\gym\envs\toy_text\discrete.py in step(self, a)
 53 
 54     def step(self, a):
---&gt; 55         transitions = self.P[self.s][a]
 56         i = categorical_sample([t[0] for t in transitions], self.np_random)
 57         p, s, r, d= transitions[i]

KeyError: tensor(3)
</code></pre>
",10134464,,,,,2019-07-08 08:17:20,Reinforcement Learning with Pytorch. [Error: KeyError ],<python-3.x><pytorch><reinforcement-learning>,1,2,,,,CC BY-SA 4.0,
1966,57010404,1,57044636,,2019-07-12 15:57:40,,0,273,"<p>I have a model which detects an object and makes a bounding box over it. The problem is that those bounding boxes are not accurate and need to be a little more tight on the object rather than some body parts exceeding the box or some boxes bigger than the object size. I want to apply reinforcement learning to make bounding boxes more accurate as I have the information of perfect bounding boxes which is the target and the input images which have the inaccurate bounding boxes or the inaccurate coordinates. I found a paper online on the exact same topic but I cant find the code for the it builds an environment with defined states, actions and awards. As I am very new to reinforcement learning I can not make the environment from scratch. </p>

<p>Here is the paper <a href=""https://melaniemitchell.me/ResearchGroupContent/MastersTheses/AndrewClelandThesis.pdf"" rel=""nofollow noreferrer"">https://melaniemitchell.me/ResearchGroupContent/MastersTheses/AndrewClelandThesis.pdf</a></p>

<p>Is this whole approach of using and changing grid size measures as states and actions doable? If yes then can someone please link me to a code preferably on github which builds quite similar environment? If not then can someone give any suggestion of building the environment or what other approach I could use?</p>
",7879081,,7879081,,2019-07-12 16:04:13,2019-07-15 17:33:15,Bounding Box Refinement using Reinforcement Learning,<python><opencv><reinforcement-learning><bounding-box><q-learning>,1,0,,,,CC BY-SA 4.0,
1968,20135529,1,20137282,,2013-11-22 01:53:13,,6,2255,"<p>I want to use this q-learning (reinforcement learning) code. It seems like the code is correct, but I am getting errors and I don't know why:</p>

<pre><code>function  q=ReinforcementLearning 

clc;
format short;
format compact;
int state=0;


R= [-inf,-inf,-inf,-inf,   0,-inf;
-inf,-inf,-inf,   0,-inf, 100;
-inf,-inf,-inf,   0,-inf,-inf;
-inf,   0,   0,-inf,   0,-inf;
   0,-inf,-inf,   0,-inf, 100;
-inf,   0,-inf,-inf,   0, 100];

gamma=0.8;

q=zero(size(R));
q1=one(size(R))*inf;
count=0;

for episode = 0:20000;
y=randperm(size(R,1));
state=y(1);

x=find(R(state,:)&gt;=0);
if size(x,1)&gt;0,
  x1=RandomPermutation(x);
  x1=x1(1);
end

qMax=max(q,[],2);
q(state,x1)=R(state,x1)+ gamma* qMax(x1);
  int  state=x1;


if sum(sum(abs(q1-q)))&lt;0.0001 &amp;&amp; sum(sum(q&gt;0))
  if count &gt; 1000;

      break
  else
      count=count+1;
  end
else
  q1=q;
  count=0;
end

end
</code></pre>

<hr>

<p>But I am getting the following warning and error:</p>

<pre><code> enter code here
Warning: The method char/int will be removed in a
future relase. Use sym/int instead. For example
int(sym('x^2')). 
&gt; In char.int at 10
  In ReinforcementLearning at 6 
Error using mupadmex
Error in MuPAD command: Invalid integrand. [int]
Error in sym/int (line 107)
   rSym =
   mupadmex('symobj::intindef',f.s,x.s,options);
Error in char/int (line 12)
y = int(sym(f),varargin{:});
Error in ReinforcementLearning (line 6)
int state=0;
</code></pre>

<hr>

<p>This code can be found in the following link:
<a href=""http://people.revoledu.com/kardi/tutorial/ReinforcementLearning/Q-Learning-Matlab.htm"" rel=""nofollow"">http://people.revoledu.com/kardi/tutorial/ReinforcementLearning/Q-Learning-Matlab.htm</a></p>
",2994193,,2907532,,2019-12-05 04:28:35,2019-12-05 04:28:35,Reinforcement Learning,<matlab><machine-learning><reinforcement-learning>,1,1,0,,,CC BY-SA 3.0,
1969,1844178,1,1844231,,2009-12-04 00:54:49,,6,6181,"<p>Let's assume we're in a room where our agent can move along the xx and yy axis. At each point he can move up, down, right and left. So our state space can be defined by (x, y) and our actions at each point are given by (up, down, right, left). Let's assume that wherever our agent does an action that will make him hit a wall we will give him a negative reward of -1, and put him back in the state he was before. If he finds in the center of the room a puppet he wins +10 reward.</p>

<p>When we update our QValue for a given state/action pair, we are seeing what actions can be done in the new state and computing what is the maximum QValue that is possible to get there, so we can update our Q(s, a) value for our current state/action. What this means is that if we have a goal state in the point (10, 10), all states around it will have a QValue a bit smaller and smaller as they get farther. Now, in relationship to the walls, it seems to me the same is not true.</p>

<p>When the agent hits a wall(let's assume he's in the position (0, 0) and did the action UP), he will receive for that state/action a reward of -1, thus getting a QValue of -1.</p>

<p>Now, if later I am in the state (0, 1), and assuming all the other actions of state (0,0 0) are zero, when calculating the QValue of (0, 1) for the action LEFT, it will compute it the following way:</p>

<pre><code>Q([0,1], LEFT) = 0 + gamma * (max { 0, 0, 0, -1 } ) = 0 + 0 = 0
</code></pre>

<p>This is, having hit the wall doesn't propagate to nearby states, contrary to what happens when you have positive reward states.</p>

<p>In my optic this seems odd. At first I thought finding state/action pairs giving negative rewards would be learningwise as good as positive rewards, but from the example I have shown above, that statement doesn't seem to hold true. There seems to be a bias in the algorithm for taking far more into consideration positive rewards than negative ones. </p>

<p>Is this the expected behavior of QLearning? Shouldn't bad rewards be just as important as positive ones? What are ""work-arounds"" for this?</p>
",130758,,,,,2017-12-15 04:04:16,Negative rewards in QLearning,<artificial-intelligence><reinforcement-learning>,3,1,0,,,CC BY-SA 2.5,
1977,15693545,1,15711329,,2013-03-28 22:50:47,,1,846,"<p>I am programming a Feed Forward Neural Network which I want to use in combination with Reinforcement Learning. I have one hidden layer with tanh as activation function and a linear output layer.</p>

<p>I have three inputs which are normalized to [0,1]. There also are three output nodes, which gives the reward received from the environment. The rewards are always negative. At the beginning, when the chosen actions lead to bad decisions, it can be like -50000, with good decisions it can be -5.</p>

<p>I am struggling with the implementation of the back propagation. Since the rewards are so big, the error values are huge, which creates huge weights. After a few training rounds, the weights to the hidden layer are so big, my nodes in the hidden layer are only creating the values -1 or 1.</p>

<p>This is my code:</p>

<pre><code>public void trainState(double[] observation, double[] hiddenEnergy, double oldVal, int chosenAction, double target, double lambda)
{
    double deltaK = (target - oldVal) * lambda;
    double deltaJ;

    for (int j = 0; j &lt; _hiddenSize; j++)
    {
        deltaJ = (1- hiddenEnergy[j] * hiddenEnergy[j]) * deltaK * _toOutputWeights[j][chosenAction];

        for (int i = 0; i &lt; _inputSize; i++)
        {
            _toHiddenWeights[i][j] += deltaJ * observation[i];
        }
    }

    for (int i = 0; i &lt; _hiddenSize; i++)
    {
        _toOutputWeights[i][chosenAction] += deltaK * hiddenEnergy[i];
    }
}
</code></pre>
",1406177,,41782,,2013-03-29 20:59:23,2013-03-29 20:59:23,Training Neural Networks with big linear output,<neural-network><backpropagation><reinforcement-learning>,1,0,0,,,CC BY-SA 3.0,
1978,50098096,1,50102489,,2018-04-30 09:57:26,,1,54,"<p>I am new to Machine Learning, and I am trying to solve MountainCar-v0 using Q-learning. I can solve the problem now, but I am still confused.</p>

<p>According to the <a href=""https://github.com/openai/gym/wiki/MountainCar-v0"" rel=""nofollow noreferrer"">MountainCar-v0's Wiki</a>, the reward remains -1 for every step, even if the car has reached the destination. How does the invariant reward help the agent learn? If every step gives the same reward, how can the agent tell if it is a good move or a bad move?</p>

<p>Thanks in advance!</p>
",5685664,,,,,2018-04-30 14:22:22,How the invariant reward helps training?,<machine-learning><neural-network><artificial-intelligence><reinforcement-learning><q-learning>,1,0,,,,CC BY-SA 3.0,
1982,2929023,1,2936120,,2010-05-28 12:45:08,,1,315,"<p><strong>The update rule TD(0) Q-Learning:</strong></p>

<p><code>Q(t-1) = (1-alpha) * Q(t-1) + (alpha) * (Reward(t-1) + gamma* Max( Q(t) ) )</code>
<br/>Then take either the current best action (to optimize) or a random action (to explorer) </p>

<p>Where MaxNextQ is the maximum Q that can be got in the next state...</p>

<p><br/>
<strong>But in TD(1) I think update rule will be:</strong></p>

<p><code>Q(t-2) = (1-alpha) * Q(t-2) + (alpha) * (Reward(t-2) + gamma * Reward(t-1) + gamma * gamma * Max( Q(t) ) )</code></p>

<p><strong>My question:</strong><br/>
The term <code>gamma * Reward(t-1)</code> means that I will always take my best action at <code>t-1</code> .. which I think will prevent exploring.. <br/>
Can someone give me a hint?</p>

<p>Thanks</p>
",279691,,712995,,2019-10-20 07:55:16,2019-10-20 07:55:16,Update Rule in Temporal difference,<machine-learning><artificial-intelligence><reinforcement-learning><markov-models><temporal-difference>,1,1,0,,,CC BY-SA 2.5,
1983,21137556,1,21371808,,2014-01-15 12:32:23,,6,1901,"<p>I am thinking to implement a learning strategy for different types of agents in my model. To be honest, I still do not know what kind of questions should I ask first or where to start. </p>

<p>I have two types of agents which I want them to learn by experience, they have a pool of actions which each has different reward based on specific situations that might happen. 
I am new to reinforcement Learning methods, therefore any suggestions on what kind of questions should I ask myself is welcomed :) </p>

<p>Here is how I am going forward to formulate my problem:</p>

<ol>
<li>Agents have a lifetime and they keep track of a few things that matter for them and these indicators are different for different agents, for example, one agent wants to increase A another wants B more than A. </li>
<li>States are points in an agent's lifetime which they
Have more than one option (I do not have a clear definition for
States as they might happen a few times or not happen at all because
Agents move around and they might never face a situation)  </li>
<li>The reward is the an increase or decrease in an indicator that agents can get from an action in a specific State, and agent do not know what would be the gain if he chose another action.</li>
<li>The gain is not constant, the states are not well defined and there is no formal transition of one state into another,</li>
<li>For example agent can decide to share with one of the co-located agent (Action 1) or with all of the agents at the same location(Action 2) If certain conditions hold true Action A will be more rewarding for that agent, while in other conditions Action 2 will have higher reward; my problem is I did not see any example with unknown rewards since sharing in this scenario also depends on the other agent's characteristics (which affects the conditions of reward system) and in different states it will be different. </li>
</ol>

<p>In my model there is <strong>no relationship between the action and the following state</strong>,and that makes me wonder if its ok to think about RL in this situation at all. </p>

<p>What I am looking to optimize here is the ability for my agents to reason about current situation in a better way and not only respond to their need which is triggered by their internal states. They have a few personalities which can define their long term goal and can affect their decision making in   different situations, but I want them to remember what action in a situation helped them to increase their preferred long term goal.</p>
",2419462,,,,,2018-02-07 17:32:50,Implementing reinforcement learning in NetLogo (Learning in multi-agent models),<netlogo><reinforcement-learning><agent-based-modeling><q-learning>,1,0,0,,,CC BY-SA 3.0,
1984,57934683,1,57948296,,2019-09-14 10:36:48,,1,365,"<p><strong>I am trying to convert the underlying tensorflow model from <a href=""https://github.com/hill-a/stable-baselines"" rel=""nofollow noreferrer"">stable-baselines</a> to <a href=""https://github.com/tensorflow/tfjs"" rel=""nofollow noreferrer"">tensorflowjs</a> to be able to use the model on the browser. But I could not make the conversion work</strong></p>

<p>I followed <a href=""https://github.com/hill-a/stable-baselines/issues/329"" rel=""nofollow noreferrer"">this github issue</a> to create the necessary tensorflow files using the code:</p>

<pre class=""lang-py prettyprint-override""><code>def generate_checkpoint_from_model(model, checkpoint_name):  
        tf.saved_model.simple_save(model.sess, checkpoint_name, inputs={""obs"": model.act_model.obs_ph}, outputs={""action"": model.action_ph})
</code></pre>

<p>Then I try to transform the model using <a href=""https://www.tensorflow.org/js/tutorials/conversion/import_saved_model"" rel=""nofollow noreferrer"">tensorflowjs_converter</a></p>

<pre class=""lang-sh prettyprint-override""><code>tensorflowjs_converter --input_format=tf_saved_model test/ web_test
</code></pre>

<p>However, it gives me the following error:</p>

<pre><code>Unable to lift tensor &lt;tf.Tensor 'loss/action_ph:0' shape=(?,) dtype=int32&gt; because it depends transitively on placeholder &lt;tf.Operation 'loss/action_ph' type=Placeholder&gt; via at least one path, e.g.: loss/action_ph (Placeholder)
</code></pre>

<p>I created the following <a href=""https://colab.research.google.com/drive/17g9sPh9I1NDFeHlp3T1VG_tKV6HUFL61"" rel=""nofollow noreferrer"">colab notebook</a> with the error so you can try it.</p>

<p><strong>Does anyone knows how to make this conversion work?</strong></p>

<p>Thank you for the help</p>
",4288795,,,,,2019-09-15 21:26:44,convert stable-baselines tensorflow model to tensorflowjs,<python><tensorflow><reinforcement-learning><tensorflow.js><tensorflowjs-converter>,1,2,,,,CC BY-SA 4.0,
1987,57939611,1,57941991,,2019-09-14 22:06:31,,1,79,"<p>Suppose I have two arrays representing a probabilistic graph:</p>

<pre><code>      2
     / \
1 -&gt;     4 -&gt; 5 -&gt; 6 -&gt; 7
     \ /
      3  
</code></pre>

<p>Where the probability of going to state 2 is <code>0.81</code> and the probability of going to state 3 is <code>(1-0.81) = 0.19</code>. My arrays represent the estimated values of the states as well as the rewards. (<strong>Note:</strong> Each index of the array represents its respective state)</p>

<pre><code>V = [0, 3, 8, 2, 1, 2, 0]
R = [0, 0, 0, 4, 1, 1, 1]
</code></pre>

<p>The context doesn't matter so much, it's just to give an idea of where I'm coming from. I need to write a k-step look ahead function where I sum the discounted value of rewards and add it to the estimated value of the kth-state. </p>

<p>I have been able to do this so far by creating separate functions for each step look ahead. <strong>My goal of asking this question is to figure out how to refactor this code so that I don't repeat myself and use idiomatic Julia.</strong></p>

<p>Here is an example of what I am talking about:</p>

<pre><code>function Eâ‚(R::Array{Float64,1}, V::Array{Float64, 1}, P::Float64)
    V[1] + 0.81*(R[1] + V[2]) + 0.19*(R[2] + V[3])
end

function Eâ‚‚(R::Array{Float64,1}, V::Array{Float64, 1}, P::Float64)
    V[1] + 0.81*(R[1] + R[3]) + 0.19*(R[2] + R[4]) + V[4]
end

function Eâ‚ƒ(R::Array{Float64,1}, V::Array{Float64, 1}, P::Float64)
    V[1] + 0.81*(R[1] + R[3]) + 0.19*(R[2] + R[4]) + R[5] + V[5]
end

.
.
.
</code></pre>

<p>So on and so forth. It seems that if I was to ignore <code>Eâ‚()</code> this would be exceptionally easy to refactor. But because I have to discount the value estimate at two different states, I'm having trouble thinking of a way to generalize this for k-steps. </p>

<p>I think obviously I could write a single function that took an integer as a value and then use a bunch of if-statements but that doesn't seem in the spirit of Julia. Any ideas on how I could refactor this? A closure of some sort? A different data type to store R and V? </p>
",6582402,,6582402,,2019-10-07 16:42:40,2019-10-07 16:42:40,Julia way to write k-step look ahead function?,<arrays><julia><reinforcement-learning><probability-theory>,1,1,,,,CC BY-SA 4.0,
1989,57231285,1,57231918,,2019-07-27 10:34:33,,2,1044,"<p>Both definition seems to state they are mapping from states to actions then what is the difference or am i wrong ?</p>
",11714043,,,,,2019-07-27 11:58:28,What is the difference between model and policy w.r.t reinforcement learning,<model><reinforcement-learning><policy><mdp>,1,1,,,,CC BY-SA 4.0,
1991,1836731,1,1836820,,2009-12-02 23:53:13,,1,522,"<p>Let's imagine we have an (x,y) plane where a robot can move. Now we define the middle of our world as the goal state, which means that we are going to give a reward of 100 to our robot once it reaches that state.</p>

<p>Now, let's say that there are 4 states(which I will call A,B,C,D) that can lead to the goal state.</p>

<p>The first time we are in A and go to the goal state, we will update our QValues table as following:</p>

<pre><code>Q(state = A, action = going to goal state) = 100 + 0
</code></pre>

<p>One of 2 things can happen. I can end the episode here, and start a different one where the robot has to find again the goal state, or I can continue exploring the world even after I found the goal state. If I try to do this, I see a problem though. If I am in the goal state and go back to state A, it's Qvalue will be the following:</p>

<pre><code>Q(state = goalState, action = going to A) = 0 + gamma * 100
</code></pre>

<p>Now, if I try to go again to the goal state from A:</p>

<pre><code>Q(state = A, action = going to goal state) = 100 + gamma * (gamma * 100)
</code></pre>

<p>Which means that if I keep doing this, as 0 &lt;= gamma &lt;= 0, both qValues are going to rise forever.</p>

<p>Is this the expected behavior of QLearning? Am I doing something wrong? If this is the expected behavior, can't this lead to problems? I know that probabilistically, all the 4 states(A,B,C and D), will grow at the same rate, but even so it kinda bugs me having them growing forever.</p>

<p>The ideia of allowing the agent to continue exploring even after finding the goal has to do with that the nearer he is from the goal state, the more likely it is to being in states that can be updated at the moment.</p>
",130758,,164901,,2010-05-22 23:35:26,2010-05-22 23:35:26,QLearning and never-ending episodes,<artificial-intelligence><reinforcement-learning>,1,2,,,,CC BY-SA 2.5,
1993,57644787,1,57653218,,2019-08-25 09:22:59,,1,601,"<p>In the Q-learning algorithm, there is a reward function that rewards the action taken on the current state. My question is can I have a non-deterministic reward function that is affected by the time when an action on a state is performed.</p>

<p>For example, suppose the reward for an action taken on a state at time 1PM is r(s,a). After several iterations (suppose now at 3PM), the system touches the same state and performs the same action as it did at 1PM. Should the reward given at 3PM must be the same as the one given at 1PM? Or the reward function can be designed by taking time into consideration (i.e., the reward given on the same state and the same action but at different time can be different).</p>

<p>Above is the question I want to ask, and one more thing I want to say is I don't want to treat time as a characteristic of a state. It is because in this case none of the state can be the same (time is always increasing).</p>
",7996523,,,,,2019-08-26 07:07:51,Can I design a non-deterministic reward function in Q-learning?,<reinforcement-learning><q-learning>,1,0,,2019-08-28 10:07:15,,CC BY-SA 4.0,
1994,2723999,1,2789635,,2010-04-27 18:41:50,,14,8293,"<ul>
<li>I intend to use Reinforcement learning in my project but I do not know much how to implement it..</li>
<li>So I am looking for a library with different RL algorithms that I can use in my C# project..</li>
</ul>

<p>Thanks</p>

<p>Please Note:
I found NeuronDotNet library for neural networks, I am now looking for RL library..</p>

<p>EDIT: Or a Dot NET library</p>
",279691,,49329,,2011-06-08 16:41:12,2013-08-01 14:23:18,Reinforcement learning in C#,<c#><machine-learning><neural-network><reinforcement-learning>,3,2,0,2014-02-24 20:04:04,,CC BY-SA 2.5,
1996,57419989,1,59067923,,2019-08-08 20:16:41,,14,611,"<p>I am attempting to implement the algorithm from the <a href=""https://cling.csd.uwo.ca/cs346a/extra/tdgammon.pdf"" rel=""noreferrer"">TD-Gammon article</a> by Gerald Tesauro. The core of the learning algorithm is described in the following paragraph:</p>

<blockquote>
  <p><a href=""https://i.stack.imgur.com/ZaXPB.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/ZaXPB.png"" alt=""enter image description here""></a></p>
</blockquote>

<p>I have decided to have a single hidden layer (if that was enough to play world-class backgammon in the early 1990's, then it's enough for me). I am pretty certain that everything except the <code>train()</code> function is correct (they are easier to test), but I have no idea whether I have implemented this final algorithm correctly.</p>

<pre class=""lang-py prettyprint-override""><code>import numpy as np

class TD_network:
    """"""
    Neural network with a single hidden layer and a Temporal Displacement training algorithm
    taken from G. Tesauro's 1995 TD-Gammon article.
    """"""
    def __init__(self, num_input, num_hidden, num_output, hnorm, dhnorm, onorm, donorm):
        self.w21 = 2*np.random.rand(num_hidden, num_input) - 1
        self.w32 = 2*np.random.rand(num_output, num_hidden) - 1
        self.b2 = 2*np.random.rand(num_hidden) - 1
        self.b3 = 2*np.random.rand(num_output) - 1
        self.hnorm = hnorm
        self.dhnorm = dhnorm
        self.onorm = onorm
        self.donorm = donorm

    def value(self, input):
        """"""Evaluates the NN output""""""
        assert(input.shape == self.w21[1,:].shape)
        h = self.w21.dot(input) + self.b2
        hn = self.hnorm(h)
        o = self.w32.dot(hn) + self.b3
        return(self.onorm(o))

    def gradient(self, input):
        """"""
        Calculates the gradient of the NN at the given input. Outputs a list of dictionaries
        where each dict corresponds to the gradient of an output node, and each element in
        a given dict gives the gradient for a subset of the weights. 
        """""" 
        assert(input.shape == self.w21[1,:].shape)
        J = []
        h = self.w21.dot(input) + self.b2
        hn = self.hnorm(h)
        o = self.w32.dot(hn) + self.b3

        for i in range(len(self.b3)):
            db3 = np.zeros(self.b3.shape)
            db3[i] = self.donorm(o[i])

            dw32 = np.zeros(self.w32.shape)
            dw32[i, :] = self.donorm(o[i])*hn

            db2 = np.multiply(self.dhnorm(h), self.w32[i,:])*self.donorm(o[i])
            dw21 = np.transpose(np.outer(input, db2))

            J.append(dict(db3 = db3, dw32 = dw32, db2 = db2, dw21 = dw21))
        return(J)

    def train(self, input_states, end_result, a = 0.1, l = 0.7):
        """"""
        Trains the network using a single series of input states representing a game from beginning
        to end, and a final (supervised / desired) output for the end state
        """"""
        outputs = [self(input_state) for input_state in input_states]
        outputs.append(end_result)
        for t in range(len(input_states)):
            delta = dict(
                db3 = np.zeros(self.b3.shape),
                dw32 = np.zeros(self.w32.shape),
                db2 = np.zeros(self.b2.shape),
                dw21 = np.zeros(self.w21.shape))
            grad = self.gradient(input_states[t])
            for i in range(len(self.b3)):
                for key in delta.keys():
                    td_sum = sum([l**(t-k)*grad[i][key] for k in range(t + 1)])
                    delta[key] += a*(outputs[t + 1][i] - outputs[t][i])*td_sum
            self.w21 += delta[""dw21""]
            self.w32 += delta[""dw32""]
            self.b2 += delta[""db2""]
            self.b3 += delta[""db3""]

</code></pre>

<p>The way I use this is I play through a whole game (or rather, the neural net plays against itself), and then I send the states of that game, from start to finish, into <code>train()</code>, along with the final result. It then takes this game log, and applies the above formula to alter weights using the first game state, then the first and second game states, and so on until the final time, when it uses the entire list of game states.  Then I repeat that many times and hope that the network learns.</p>

<p>To be clear, I am not after feedback on my code writing. This was never meant to be more than a quick and dirty implementation to see that I have all the nuts and bolts in the right spots.</p>

<p>However, I have no idea whether it is correct, as I have thus far been unable to make it capable of playing tic-tac-toe at any reasonable level. There could be many reasons for that. Maybe I'm not giving it enough hidden nodes (I have used 10 to 12). Maybe it needs more games to train (I have used 200 000). Maybe it would do better with different normalisation functions (I've tried sigmoid and ReLU, leaky and non-leaky, in different variations). Maybe the learning parameters are not tuned right. Maybe tic-tac-toe and its deterministic gameplay means it ""locks in"" on certain paths in the game tree. Or maybe the training implementation is just wrong. Which is why I'm here.</p>

<p>Have I misunderstood Tesauro's algorithm?</p>
",2651045,,712995,,2019-10-20 07:54:08,2019-11-27 10:21:34,Implementing the TD-Gammon algorithm,<python><artificial-intelligence><reinforcement-learning><temporal-difference>,1,3,0,,,CC BY-SA 4.0,
1998,2750608,1,2758125,,2010-05-01 16:04:39,,3,529,"<ul>
<li>I am trying to use Multi-Layer NN  to implement probability function in Partially Observable Markov Process..</li>
<li>I thought inputs to the NN would be: current state, selected action, result state;
The output is a probability in [0,1] (prob. that performing selected action on current state will lead to result state)</li>
<li>In training, I fed the inputs stated before, into the NN, and I taught it the output=1.0 for each case that already occurred.</li>
</ul>

<p><b>The problem :</b><br/>
For nearly all test case the output probability is near 0.95..  no output was under 0.9 !
Even for nearly impossible results, it gave that high prob.</p>

<p>PS:I think this is because I taught it happened cases only, but not un-happened ones..
But I can not at each step in the episode teach it the output=0.0 for every un-happened action!</p>

<p>Any suggestions how to over come this problem? Or may be another way to use NN or to implement prob function?</p>

<p>Thanks</p>
",279691,,,,,2010-05-03 12:54:48,Reinforcement learning And POMDP,<machine-learning><probability><neural-network><reinforcement-learning><markov-models>,2,0,0,,,CC BY-SA 2.5,
2000,1783389,1,1784316,,2009-11-23 14:19:34,,11,5683,"<p>I do know that feedforward multi-layer neural networks with backprop are used with Reinforcement Learning as to help it generalize the actions our agent does. This is, if we have a big state space, we can do some actions, and they will help generalize over the whole state space.</p>

<p>What do recurrent neural networks do, instead? To what tasks are they used for, in general?</p>
",130758,,,,,2009-11-23 22:06:31,What are the uses of recurrent neural networks when using them with Reinforcement Learning?,<language-agnostic><artificial-intelligence><neural-network><reinforcement-learning>,2,0,0,,,CC BY-SA 2.5,
2002,2749498,1,2916730,,2010-05-01 09:56:06,,1,925,"<ul>
<li>I am working on a project with RL &amp; NN</li>
<li>I need to determine the action vector structure which will be fed to a neural network..</li>
</ul>

<p>I have 3 different actions (A &amp; B &amp; Nothing) each with different powers (e.g A100 A50 B100 B50)
I wonder what is the  best way to feed these actions to a NN in order to yield best results?</p>

<p>1- feed A/B to input 1, while action power 100/50/Nothing to input 2</p>

<p>2- feed A100/A50/Nothing to input 1, while B100/B50/Nothing to input 2</p>

<p>3- feed A100/A50 to input 1, while B100/B50 to input 2, while Nothing flag to input 3</p>

<p>4- Also to feed 100 &amp; 50 or normalize them to 2 &amp; 1 ?</p>

<p>I need reasons why to choose one method
Any suggestions are recommended</p>

<p>Thanks</p>
",279691,,,,,2010-05-26 21:13:34,Reinforcement learning with neural networks,<machine-learning><neural-network><reinforcement-learning><markov>,1,1,,,,CC BY-SA 2.5,
2004,58004237,1,58014773,,2019-09-19 05:28:18,,2,1930,"<p>While I was implementing agents for various problems...I have seen that my actor loss is reducing as expected. But my critic loss kept increases even though the policy learned is very. This happens for DDPG , PPO etc.</p>

<p>Any thoughts why my critic loss is increasing. </p>

<p>I tried playing with hyper parameters, it actually makes my policy worse.</p>
",10222414,,,,,2019-09-19 15:50:20,Critic Loss for RL Agent,<artificial-intelligence><reinforcement-learning>,1,0,0,,,CC BY-SA 4.0,
2006,39909340,1,39910230,,2016-10-07 04:22:49,,1,568,"<p>I am writing a simple grid world q-learning program using R. This is my grid world</p>

<p><a href=""https://i.stack.imgur.com/I4RQD.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/I4RQD.png"" alt=""enter image description here""></a></p>

<p>This simple grid world has 6 states in which state 1 and state 6 are starting and ending state. I avoided adding a fire pit, wall, wind so to keep my grid world as simple as possible. For reward matrix I have starting state value-0.1 and ending state a +1 and rest of the state 0. A -0.1 reward for starting state is to discourage the agent from coming back to the start position.</p>

<pre><code>#Reward and action-value matrix 
Row=state(1:6)
Column=actions(1:4)[Left,Right,Down,Up in that order]
</code></pre>

<p>I wrote my program in R and its working but with a problem in finding next state when current state is greater than 4th row. The Q matrix doesn't update after 4th row.</p>

<pre><code>#q-learning example
#https://en.wikipedia.org/wiki/Q-learning

# 2x3 grid world
# S for starting grid G for goal/terminal grid
# actions left right down up
#  4 5 6 state
#########
# [0,0,G]
# [S,0,0]
#########
#  1 2 3 state

#setting seed
set.seed(2016)
#number of iterations
N=10
#discount factor
gamma=0.9

#learning rate
alpha=0.1

#target state
tgt.state=6

#reward matrix starting grid has -0.1 and ending grid has 1
R=matrix( c( NA,  0,  NA,  0,
            -0.1, 0,  NA,  0,
             0,  NA,  NA,  1,
             NA,  0,-0.1, NA,
             0,   1,   0, NA, 
             0,   NA,  0, NA
            ),
          nrow=6,ncol=4,byrow = TRUE) 

#initializing Q matrix with zeros
Q=matrix( rep( 0, len=dim(R)[1]*dim(R)[2]), nrow = dim(R)[1],ncol=dim(R)[2])


for (i in 1:N) {
  ## for each episode, choose an initial state at random
  cs &lt;- 1
  ## iterate until we get to the tgt.state
  while (1) {
    ## choose next state from possible actions at current state
    ## Note: if only one possible action, then choose it;
    ## otherwise, choose one at random
    next.states &lt;- which(R[cs,] &gt; -1)
    if (length(next.states)==1)
      ns &lt;- next.states
    else
      ns &lt;- sample(next.states,1)
    ## this is the update
    Q[cs,ns] &lt;- Q[cs,ns] + alpha*(R[cs,ns] + gamma*max(Q[ns, which(R[ns,] &gt; -1)]) - Q[cs,ns])
    ## break out of while loop if target state is reached
    ## otherwise, set next.state as current.state and repeat      
    if (ns == tgt.state) break
    cs &lt;- ns
    Sys.sleep(0.5)
    print(Q)
  }
}
</code></pre>

<p>Currently when my algorithm starts the agent always start from the state-1. In the first state(first row of R) there are two actions either Right(R(1,2)) or Up(R(1,4)). If randomly selected an action say <strong>Up (R(1,4))</strong> then the agent move to next state as the action Q(4,action). </p>

<p>But now consider state-4(forth row or R) it has two action Right-R(4,2) and Down-R(4,3) this cause problem for my algorithm and if randomly select an action say, Right. Logically it should move to 5th state but my above code 
uses the action 2 as the next state. so instead of going to 5th state it goes to 2nd state.</p>

<p>In the end my algorithm will work perfectly if the dimension of state and action matrices are same(m x m) but in my problem my state and action matrices are different (m x n). I tried to find a solution to this problem but failed to find an logical approach to find next state for $max(Q(s',a'))$ currently I am stuck?</p>
",996366,,996366,,2016-10-07 05:55:03,2016-10-07 07:31:19,"Programmaticaly find next state for max(Q(s',a')) in q-learning using R",<r><reinforcement-learning><q-learning>,1,0,,,,CC BY-SA 3.0,
2011,58547686,1,58548047,,2019-10-24 19:03:19,,-1,2912,"<p>I have a challenge that my teacher gave to beat an army of his soldiers on a 18x24 grid, with random obstacles placed on the board. The game is turn based and I have an army of 50 soldiers, each of which needs to either move or attack on their turn.</p>

<p>My problem is I only have access to creating a class of soldiers to fight in this environment. Currently I have a method that evaluates the board position by looking at how many soldiers there are left from each team and does yourTeam - enemyTeam to get the current score, and I have a method that will produce the legal moves for the soldier. </p>

<p>I want to know how I would create a reinforcement learning agent in Java with what I have access to. If you know any ways to do this or any resources that may help that would be great. Thank you for the help!</p>
",12270562,,,,,2019-10-24 19:31:49,How do I make a reinforcement learning agent in Java?,<java><reinforcement-learning><multi-agent>,1,0,0,,,CC BY-SA 4.0,
2017,28082922,1,28086400,,2015-01-22 06:57:42,,0,303,"<p>To model my problem, I'll use a dating site as an example (although this is not the actual case). My problem is I have a set of keywords that a user can input that they like. Say ""Tall, dark hair, blue eyes"", etc. and I want to map them to other users that fit that criteria. More than that however, I need to be able to learn from data I get back to make better predictions that are not-so-exact matches. </p>

<p>For example, if other users that are looking for people with 'dark hair' like users with 'black hair', or have a height of 6'4 but don't mention they are tall. I want to be able to make an association between those similar keywords and be able to also suggest those as well so it best returns what the user wants, even if it wasn't exactly what they asked for. </p>

<p>My question is what algorithm/approach is best suited for this? I've been looking into areas like:</p>

<ul>
<li>decision trees, but those seem to break down when no keywords match. </li>
<li>naive bayes, which seem a bit more tolerant to missing connections, but require some prior knowledge about the connections, and since keywords can be anything, this seems like a road bloack</li>
<li>ANN, but these don't seem to do well with text input</li>
<li>KNN, but I'm not sure how to handle the possibly infinite user classifications?</li>
<li>Some sort of A* map search, where each time a user1 likes a user2, I make a map connection between user1's likes and user2's traits, if that connection already exists, I just shorten it, then find the closest N users. I'm just not sure how scalable this is.</li>
</ul>

<p>Any input is appreciated,
Thanks!</p>
",3122060,,,,,2015-01-22 10:24:52,Keyword association learning algorithm,<algorithm><machine-learning><data-mining><prediction><reinforcement-learning>,1,1,,,,CC BY-SA 3.0,
2018,40722683,1,40755627,,2016-11-21 14:37:13,,4,731,"<p>I'm trying to implement linear gradient-descent Sarsa based on <a href=""http://people.inf.elte.hu/lorincz/Files/RL_2006/SuttonBook.pdf"" rel=""nofollow noreferrer"">Sutton &amp; Barto's Book</a>, see the algorithm in the picture below.</p>

<p>However, I struggle to understand something in the algorithm:</p>

<ul>
<li>Is the dimension of w and z independent of how many different actions can be taken? It seems in the book they have dimension equal to the number of features, which I would say is independent on how many actions.</li>
<li>Is there a w and a z for each action? Also, I cannot see in the book that this should be the case.</li>
<li>If I am right in the two bullets above, then I cannot see how the index list F_a will depend on the actions, and therefore I cannot see how the action-value function q_a can depend on the actions (see the lines marked with yellow below in the algorithm) But the action-value must depend on the actions. So there is something I am not getting...</li>
</ul>

<p>I hope anyone can help clarifying this for me :)</p>

<p><a href=""https://i.stack.imgur.com/KBOZD.jpg"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/KBOZD.jpg"" alt=""Sarsa algo""></a></p>
",4572217,,,,,2016-11-23 03:41:52,"Understanding linear, gradient-descent Sarsa (based on Sutton & Barto)",<reinforcement-learning><sarsa>,1,0,,,,CC BY-SA 3.0,
2019,2846038,1,2870542,,2010-05-17 00:12:44,,3,1067,"<p>My toy project to learn &amp; apply Reinforcement Learning is:<br/>
- An agent tries to reach a <strong>goal</strong> state ""safely"" &amp; ""quickly""....<br/>
- But there are projectiles and <strong>rockets</strong> that are launched upon the agent in the way.<br/>
- The agent can determine rockets position -with some <strong>noise</strong>- only if they are ""near""<br/>
- The agent then must learn to <strong>avoid crashing</strong> into these rockets..<br/>
- The agent has -rechargable with time- <strong>fuel</strong> which is consumed in agent motion<br/>
- <strong>Continuous Actions</strong>: Accelerating forward - Turning with angle</p>

<p><br/>
I need some hints and names of RL algorithms that suit that case..<br/>
- I think it is POMDP , but can I model it as MDP and just ignore noise?<br/>
- In case POMDP, What is the recommended way for evaluating probability?<br/>
- Which is better to use in this case: Value functions or Policy Iterations?<br/>
- Can I use NN to model environment dynamics instead of using explicit equations?<br/>
- If yes, Is there a specific type/model of NN to be recommended?<br/>
- I think Actions must be discretized, right?<br/></p>

<p>I know it will take time and effort to learn such a topic, but I am eager to..<br/>
You may answer some of the questions if you can not answer all...<br/>
Thanks</p>
",279691,,279691,,2010-06-02 22:27:06,2013-08-01 14:30:43,Reinforcement learning toy project,<machine-learning><neural-network><reinforcement-learning>,2,1,0,,,CC BY-SA 2.5,
2023,64441708,1,64657666,,2020-10-20 08:53:18,,2,1026,"<p>I am using Stable Baselines 3 to train an agent to play <a href=""https://www.kaggle.com/joerakhimov/connectx-11"" rel=""nofollow noreferrer"">Connect 4</a> game. I am trying to take the case into account when an agent starts a game as a second player.</p>
<pre><code>self.env = self.ks_env.train([opponent, None]) 
</code></pre>
<p>When I am trying to run the code, I am getting the following error:</p>
<pre><code>invalid multinomial distribution (encountering probability entry &lt; 0)
/opt/conda/lib/python3.7/site-packages/torch/distributions/categorical.py in sample(self, sample_shape)
samples_2d = torch.multinomial(probs_2d, sample_shape.numel(), True).T
</code></pre>
<p>However, there is no problem when an agent is first player:</p>
<pre><code>self.env = self.ks_env.train([None, opponent])
</code></pre>
<p>I think problem is related to the Pytorch library. My question is how can I fix this issue?</p>
",2255924,,2255924,,2020-10-28 20:11:44,2020-11-03 06:25:29,Pytorch - RuntimeError: invalid multinomial distribution (encountering probability entry < 0),<pytorch><reinforcement-learning><stable-baselines>,1,0,0,,,CC BY-SA 4.0,
2024,28457688,1,28458869,,2015-02-11 15:09:58,,12,4201,"<p>Does any one know any example code of an algorithm <a href=""http://en.wikipedia.org/wiki/Ronald_J._Williams"" rel=""noreferrer"">Ronald J. Williams</a> proposed in<br>
<a href=""http://incompleteideas.net/sutton/williams-92.pdf"" rel=""noreferrer"">A class of gradient-estimating algorithms for reinforcement learning in neural networks</a></p>
",1367788,,1387612,,2015-02-11 15:55:11,2015-02-11 16:02:48,Any example code of REINFORCE algorithm proposed by Williams?,<reinforcement-learning>,1,0,,,,CC BY-SA 3.0,
2029,11327923,1,11329005,,2012-07-04 11:11:32,,3,1359,"<p>I want to ask if it is senseful using a standard backpropagation neural network with TD-learning method in a board game?</p>

<p>My method looks like:</p>

<ol>
<li>Play 1 game. Net is playing as both players with greedy policy and random moves sometimes.</li>
<li><p>For each stored game position (starting from terminal-1 and move to starting position) calculate estimated position value and desired position value, e.g.</p>

<pre><code>boards_values[i]['desired_value'] = boards_values[i]['estimated_value'] + 0.4 * ( boards_values[i+1]['estimated_value'] - boards_values[i]['estimated_value'] )
</code></pre></li>
<li><p>Create training patterns for net from entire game end train each with small learning rate for 1 epoch with standard back propagation algorithm.</p>

<pre><code>NN.train([pattern], iterations=1, N=0.001, M=0.000001)
</code></pre></li>
</ol>

<p>I tried some combinations of the above (learning not from one example but 30-40 patterns, decreasing/increasing learning speed and so on) in my tic tac toe game and never trained ideal player (it should never lose vs random). One of best example when NN agent play against random player is:</p>

<p>(play as first: win, tie, lose), (play as second: win, tie, lose), (sum: win, tie, lose)<br>
(191, 34, 275), (159, 102, 239), (350, 136, 514) - fresh net<br>
(427, 21, 52), (312, 16, 172), (739, 37, 224) - after +50k games</p>

<p>Input is 18 neurons in format:<br>
for each board cell set (1,0) for x, (0,0) for empty cell and (0,1) for o. Output is one unit win/lose probability estimation in range -1, 1.</p>

<p>Tic tac toe is only testing sandbox, when I finish it with success I will move to a more complex card game ('Lost Cities').</p>
",280747,,280747,,2012-07-04 12:07:11,2012-07-04 12:30:51,Use of classical back propagation neural network with TD-learning in board game,<artificial-intelligence><neural-network><reinforcement-learning>,1,0,0,,,CC BY-SA 3.0,
2030,39316972,1,39321007,,2016-09-04 12:34:43,,1,320,"<p>It has been proved that the Q-Learning algorithm converges to the Qs of the optimal policy which are unique. So is it correct to conclude that the Q-Learning algorithm cannot become overtrained?</p>
",6328652,,,,,2016-09-04 20:03:57,Can Q-Learning algorithm become overtrained?,<machine-learning><reinforcement-learning><q-learning>,1,0,,,,CC BY-SA 3.0,
2037,57544726,1,57544974,,2019-08-18 12:49:26,,2,185,"<p>I have started learning reinforcement learning and referring the book by Sutton. I was trying to understand the non-stationary environment which was quoted in the book as: </p>

<blockquote>
  <p>suppose the bandit task were nonstationary, that is, that the true
  values of the actions changed over time. In this case exploration is
  needed even in the deterministic case to make sure one of the
  nongreedy actions has not changed to become better than the greedy on</p>
</blockquote>

<p>This tells me that the true expected rewards value given an action changes with time. But does this mean with every time step ? I could clearly understand that how we track the rewards in such case i.e. by weighting the recent ones more than previous ones for every time step. However, does this also mean or give indication that target or true values change with every time step? I am trying to simulate the 10arm bandit problem with the same fig given below where we compare Upper Confidence-Bound Action Selection and Epsilon-greedy methods with sample average method for estimating action values in stationary environment. <a href=""https://i.stack.imgur.com/NolMF.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/NolMF.png"" alt=""Fig 2.3 of book with stationary environment""></a></p>

<p>if I have to simulate the same with non_stationary environment then how can I do that ? Below is my code : </p>

<pre><code>class NArmedBandit:

    #10-armed bandit testbed with sample averages 
    def __init__(self,k=10,step_size = 0.1,eps = 0,UCB_c = None, sample_avg_flag = False,
                 init_estimates = 0.0,mu = 0, std_dev = 1):
        self.k = k
        self.step_size = step_size
        self.eps = eps
        self.init_estimates = init_estimates
        self.mu = mu
        self.std_dev = std_dev
        self.actions = np.zeros(k)
        self.true_reward = 0.0
        self.UCB_c = UCB_c
        self.sample_avg_flag = sample_avg_flag
        self.re_init()


    def re_init(self):

        #true values of rewards for each action
        self.actions = np.random.normal(self.mu,self.std_dev,self.k) 

        # estimation for each action
        self.Q_t = np.zeros(self.k) + self.init_estimates

        # num of chosen times for each action
        self.N_t = np.zeros(self.k)

        #best action chosen
        self.optim_action = np.argmax(self.actions)

        self.time_step = 0


    def act(self):
        val = np.random.rand()
        if val &lt; self.eps:
            action = np.random.choice(np.arange(self.k))
            #print('action 1:',action)
        elif self.UCB_c is not None:
            #1e-5 is added so as to avoid division by zero
            ucb_estimates = self.Q_t + self.UCB_c * np.sqrt(np.log(self.time_step + 1) / (self.N_t + 1e-5))
            A_t = np.max(ucb_estimates)
            action = np.random.choice(np.where(ucb_estimates == A_t)[0])
        else:
            A_t = np.max(self.Q_t)
            action = np.random.choice(np.where(self.Q_t == A_t)[0])
            #print('action 2:',action)
        return action



    def step(self,action):

        # generating the reward under N(real reward, 1)
        reward = np.random.randn() + self.actions[action]
        self.time_step += 1
        self.N_t[action] += 1


        # estimation with sample averages
        if self.sample_avg_flag == True:
            self.Q_t[action] += (reward - self.Q_t[action]) / self.N_t[action]
        else:
            # non-staationary with constant step size 
            self.Q_t[action] += self.step_size * (reward - self.Q_t[action])

        return reward


    def play(self,tasks,num_time_steps):
        rewards = np.zeros((tasks, num_time_steps))
        optim_action_counts = np.zeros(rewards.shape)
        for task in trange(tasks):
            self.re_init()
            for t in range(num_time_steps):
                action = self.act()
                reward = self.step(action)
                rewards[task, t] = reward
                if action == self.optim_action:
                    optim_action_counts[task, t] = 1
        avg_optim_action_counts = optim_action_counts.mean(axis=0)
        avg_rewards = rewards.mean(axis=0)
        return avg_optim_action_counts, avg_rewards
</code></pre>

<p>Should I change the <code>actions array</code> (which are assumed as true estimates) defined in <code>re_init()</code> function by calling <code>re_init()</code> function after every time step in <code>play()</code> which is like changing true expected rewards for every action at each time step. I have already incorporated the code for calculating the rewards in case of non-stationary environment in <code>act()</code> and <code>step()</code> functions which are using constant step size <code>alpha = 0.1</code>. The only thing that I don't know is how do set up or simulate the non-stationary environment here and if it is correctly understood by me.</p>
",1493850,,562769,,2020-09-10 11:56:03,2020-09-10 11:56:03,What does non-stationarity mean and how to implement it in reinforcement learning as 10 arm bandit problem?,<python><reinforcement-learning>,1,0,,,,CC BY-SA 4.0,
2043,58033188,1,58034676,,2019-09-20 17:59:07,,6,169,"<p>I am currently reading <a href=""https://web.stanford.edu/class/psych209/Readings/SuttonBartoIPRLBook2ndEd.pdf"" rel=""nofollow noreferrer"">Reinforcement Learning: An Introduction</a> (RL:AI) and try to reproduce the first example with an n-armed bandit and simple reward averaging.</p>

<p><strong>Averaging</strong></p>

<pre><code>new_estimate = current_estimate + 1.0 / step * (reward - current_estimate)
</code></pre>

<p>In order reproduce the graph from the PDF, I am generating 2000 bandit-plays and let different agents play 2000 bandits for 1000 steps (as described in the PDF) and then average the reward as well as the percentage of optimal actions.</p>

<p><strong>In the PDF</strong>, the result looks like this:</p>

<p><a href=""https://i.stack.imgur.com/LeW4y.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/LeW4y.png"" alt=""enter image description here""></a></p>

<p>However, I am not able to reproduce this. If I am using simple averaging, all the agents with exploration (<code>epsilon &gt; 0</code>) actually play worse than an agent without exploration. This is weird because the possibility of exploration should allow agents to leave the local optimum more often and reach out to better actions.</p>

<p>As you can see below, this is not the case for my implementation. Also note that I have added agents which use weighted-averaging. These work but even in that case, raising <code>epsilon</code> results in a degradation of the agents performance. </p>

<p><a href=""https://i.stack.imgur.com/Dl83z.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/Dl83z.png"" alt=""enter image description here""></a></p>

<p>Any ideas what's wrong in my code?</p>

<h3>The code (MVP)</h3>

<pre><code>from abc import ABC
from typing import List

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

from multiprocessing.pool import Pool


class Strategy(ABC):

    def update_estimates(self, step: int, estimates: np.ndarray, action: int, reward: float):
        raise NotImplementedError()


class Averaging(Strategy):

    def __str__(self):
        return 'avg'

    def update_estimates(self, step: int, estimates: np.ndarray, action: int, reward: float):
        current = estimates[action]
        return current + 1.0 / step * (reward - current)


class WeightedAveraging(Strategy):

    def __init__(self, alpha):
        self.alpha = alpha

    def __str__(self):
        return 'weighted-avg_alpha=%.2f' % self.alpha

    def update_estimates(self, step: int, estimates: List[float], action: int, reward: float):
        current = estimates[action]
        return current + self.alpha * (reward - current)


class Agent:

    def __init__(self, nb_actions, epsilon, strategy: Strategy):
        self.nb_actions = nb_actions
        self.epsilon = epsilon
        self.estimates = np.zeros(self.nb_actions)
        self.strategy = strategy

    def __str__(self):
        return ','.join(['eps=%.2f' % self.epsilon, str(self.strategy)])

    def get_action(self):
        best_known = np.argmax(self.estimates)
        if np.random.rand() &lt; self.epsilon and len(self.estimates) &gt; 1:
            explore = best_known
            while explore == best_known:
                explore = np.random.randint(0, len(self.estimates))
            return explore
        return best_known

    def update_estimates(self, step, action, reward):
        self.estimates[action] = self.strategy.update_estimates(step, self.estimates, action, reward)

    def reset(self):
        self.estimates = np.zeros(self.nb_actions)


def play_bandit(agent, nb_arms, nb_steps):

    agent.reset()

    bandit_rewards = np.random.normal(0, 1, nb_arms)

    rewards = list()
    optimal_actions = list()

    for step in range(1, nb_steps + 1):

        action = agent.get_action()
        reward = bandit_rewards[action] + np.random.normal(0, 1)
        agent.update_estimates(step, action, reward)

        rewards.append(reward)
        optimal_actions.append(np.argmax(bandit_rewards) == action)

    return pd.DataFrame(dict(
        optimal_actions=optimal_actions,
        rewards=rewards
    ))


def main():
    nb_tasks = 2000
    nb_steps = 1000
    nb_arms = 10

    fig, (ax_rewards, ax_optimal) = plt.subplots(2, 1, sharex='col', figsize=(8, 9))

    pool = Pool()

    agents = [
        Agent(nb_actions=nb_arms, epsilon=0.00, strategy=Averaging()),
        Agent(nb_actions=nb_arms, epsilon=0.01, strategy=Averaging()),
        Agent(nb_actions=nb_arms, epsilon=0.10, strategy=Averaging()),
        Agent(nb_actions=nb_arms, epsilon=0.00, strategy=WeightedAveraging(0.5)),
        Agent(nb_actions=nb_arms, epsilon=0.01, strategy=WeightedAveraging(0.5)),
        Agent(nb_actions=nb_arms, epsilon=0.10, strategy=WeightedAveraging(0.5)),
    ]

    for agent in agents:

        print('Agent: %s' % str(agent))

        args = [(agent, nb_arms, nb_steps) for _ in range(nb_tasks)]
        results = pool.starmap(play_bandit, args)

        df_result = sum(results) / nb_tasks
        df_result.rewards.plot(ax=ax_rewards, label=str(agent))
        df_result.optimal_actions.plot(ax=ax_optimal)

    ax_rewards.set_title('Rewards')
    ax_rewards.set_ylabel('Average reward')
    ax_rewards.legend()
    ax_optimal.set_title('Optimal action')
    ax_optimal.set_ylabel('% optimal action')
    ax_optimal.set_xlabel('steps')
    plt.xlim([0, nb_steps])
    plt.show()


if __name__ == '__main__':
    main()
</code></pre>
",826983,,826983,,2019-09-20 21:33:01,2019-09-20 21:33:01,Using simple averaging for reinforcment learning,<python><reinforcement-learning>,1,4,,,,CC BY-SA 4.0,
2045,11980051,1,21991709,,2012-08-16 02:47:26,,2,1038,"<p>I was trying to implement in PyBrain something similar to a Maze problem. However, it's more similar to a room with an emergency exit, where you leave an agent in one of the rooms to find the exit.
To convert this to a computer method a bi-directional graph could be used with the weights showing the path between the rooms.</p>

<p>I tried to implement a new environment, but I'm kind of lost on what should be what.
For example, based on the abstract environment class I have thought about this:</p>

<pre><code>#!/usr/bin/python2.7

class RoomEnv(Environment):
    # number of action values acceptable by the environment
    # Two events: go forward and go back through the door (but, how we know what room is connect to another?)
    indim = 2
    # Maybe a matrix where 0 is no connection and 1 is a connection(?)
    #            A,B,C,D,E,F
    #indim = array([[0,0,0,0,0,0],  # A
                    [0,0,0,0,0,1],  # B
                    [0,0,0,0,0,0],  # C
                    [0,0,0,0,0,0],  # D
                    [0,0,0,0,0,1],  # E
                    [0,0,0,0,0,1],  # F
                  ])

    # the number of sensors is the number of the rooms
    outdim = 6

    def getSensors(self):
        # Initial state:
        # Could be any room, maybe something random(?)

    def performAction(self, action):
        # We should look at all the states possible to learn what are the best option to go to the outside state.
        # Maybe a for loop that goes through all the paths and use some weight to know where is the best option?

        print ""Action performed: "", action

    def reset(self):
        #Most environments will implement this optional method that allows for reinitialization. 
</code></pre>

<p>Sincerely,</p>
",1353689,,,,,2014-02-24 15:18:43,PyBrain Reinforcement Learning - Maze and Graph,<python-2.7><reinforcement-learning><pybrain>,1,0,0,,,CC BY-SA 3.0,
2046,28937803,1,28955191,,2015-03-09 08:32:38,,38,26700,"<p>How is Q-learning different from value iteration in reinforcement learning? </p>

<p>I know Q-learning is model-free and training samples are transitions <code>(s, a, s', r)</code>. But since we know the transitions and the reward for every transition in Q-learning, is it not the same as model-based learning where we know the reward for a state and action pair, and the transitions for every action from a state (be it stochastic or deterministic)? I do not understand the difference.</p>
",4609181,,3924118,,2019-05-13 14:21:14,2019-11-07 15:45:10,What is the difference between Q-learning and Value Iteration?,<machine-learning><artificial-intelligence><reinforcement-learning><q-learning>,3,1,0,,,CC BY-SA 4.0,
2048,47133913,1,47134486,,2017-11-06 09:44:18,,2,109,"<p>We assign +1 reward for reaching goal and -1 for reaching an unwanted state.</p>

<p>Is it necessary to give something like +0.01 reward for taking an action which reaches near to the goal and -0.01 reward for taking an action which does not ?</p>

<p>What will the significant changes with the reward policy mentioned above ? </p>
",6911973,,6911973,,2017-11-07 17:54:07,2017-11-07 17:54:07,What is importance of reward policy in Reinforcement learninig?,<artificial-intelligence><reinforcement-learning><q-learning>,1,0,,,,CC BY-SA 3.0,
2052,40121969,1,40123542,,2016-10-19 04:10:56,,0,967,"<p>In Q-learning, from its current state, the agent takes action at every discrete time step and after an action is performed, an agent receives an immediate reward to access the success or failure of performed action. 
Let's say that we want to control a vehicle speed using Q-learning where the actions are target speeds and agent's goal is to reach stop-line (which is 1km away from the starting point) as quickly as possible. </p>

<p>1) So in this example, does agent need to take action at every discrete time step (1sec) or agent can get an action at every 100m instead of every discrete time step. Is that a must to take action at every discrete time step?</p>

<p>2) what is mean by delayed reward in Q-learning?
is that updating reward once agent reaches to the target instead of updating reward after taking each action at every time step?
Thanks in advance :)</p>
",4702833,,,,,2016-10-19 06:13:37,Q-learning Updating Frequency,<machine-learning><dynamic-programming><reinforcement-learning><q-learning>,1,0,,,,CC BY-SA 3.0,
2053,58194497,1,58194591,,2019-10-02 02:31:43,,0,39,"<p>I am training a model using Tensorflow in Python 3, and have set up my own separate early stopping function. My model keeps the cost value fairly low for most of the training run, but then like normal, it reaches a certain point where not only does it no longer improve/minimize the cost function, but it gets exponentially worse and accelerates up. I have attached the values of my costs below.</p>

<p>I'm wondering if someone has an idea (pseudo-code, brainstorm, or link that I haven't found yet) of a way to improve my early stopping function to catch when this acceleration happens, and enforce the early stop. I don't necessarily want to have just a static number (like > 1.000) in case it hits that number but isn't done searching below. Maybe have some sort of an acceleration monitoring? A moving average? As you will see from the values and image, the acceleration is generally quite extreme at the end and will happen eventually without fail every training run. I'd like to be able to catch it as soon as possible, but still ensure that the move is drastic enough to enforce the stop. Thanks!</p>

<p><a href=""https://i.stack.imgur.com/Wsjry.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/Wsjry.png"" alt=""Image of Cost Acceleration""></a></p>

<pre><code>epoch:  1   cost:   0.032336
epoch:  2   cost:   0.015083
epoch:  3   cost:   0.003783
epoch:  4   cost:   0.011579
epoch:  5   cost:   0.00436
epoch:  6   cost:   0.003667
epoch:  7   cost:   0.000973
epoch:  8   cost:   0.002916
epoch:  9   cost:   0.016516
epoch:  10  cost:   0.00094
epoch:  11  cost:   0.000656
epoch:  12  cost:   0.001112
epoch:  13  cost:   0.000761
epoch:  14  cost:   0.002976
epoch:  15  cost:   0.004531
epoch:  16  cost:   0.00247
epoch:  17  cost:   0.005809
epoch:  18  cost:   0.011614
epoch:  19  cost:   0.004681
epoch:  20  cost:   0.002704
epoch:  21  cost:   0.001122
epoch:  22  cost:   0.109581
epoch:  23  cost:   0.001352
epoch:  24  cost:   0.000767
epoch:  25  cost:   0.009472
epoch:  26  cost:   0.003918
epoch:  27  cost:   0.007462
epoch:  28  cost:   0.002033
epoch:  29  cost:   0.004985
epoch:  30  cost:   0.006285
epoch:  31  cost:   0.004838
epoch:  32  cost:   0.008076
epoch:  33  cost:   0.008414
epoch:  34  cost:   0.008761
epoch:  35  cost:   0.002719
epoch:  36  cost:   0.002752
epoch:  37  cost:   0.00355
epoch:  38  cost:   0.012253
epoch:  39  cost:   0.052947
epoch:  40  cost:   0.005952
epoch:  41  cost:   0.012556
epoch:  42  cost:   0.018322
epoch:  43  cost:   0.042715
epoch:  44  cost:   0.045315
epoch:  45  cost:   0.051732
epoch:  46  cost:   0.072919
epoch:  47  cost:   0.013907
epoch:  48  cost:   0.088789
epoch:  49  cost:   0.045083
epoch:  50  cost:   0.038073
epoch:  51  cost:   0.033848
epoch:  52  cost:   0.022773
epoch:  53  cost:   0.198873
epoch:  54  cost:   0.020925
epoch:  55  cost:   0.02264
epoch:  56  cost:   0.039353
epoch:  57  cost:   0.055266
epoch:  58  cost:   0.057254
epoch:  59  cost:   0.048848
epoch:  60  cost:   0.072187
epoch:  61  cost:   0.066818
epoch:  62  cost:   0.111698
epoch:  63  cost:   0.121994
epoch:  64  cost:   0.216178
epoch:  65  cost:   0.4132
epoch:  66  cost:   0.243138
epoch:  67  cost:   0.628117
epoch:  68  cost:   0.349325
epoch:  69  cost:   0.413678
epoch:  70  cost:   0.376448
epoch:  71  cost:   0.931199
epoch:  72  cost:   5.495036
epoch:  73  cost:   2.914621
epoch:  74  cost:   7.160439
epoch:  75  cost:   13.324359
epoch:  76  cost:   22.426832
epoch:  77  cost:   116.921036
epoch:  78  cost:   285.824371
</code></pre>
",10018602,,,,,2019-10-02 02:44:48,Custom Early Stop Function - Stop When Cost Value Starts Accelerating Upward After Convergence?,<python><python-3.x><tensorflow><reinforcement-learning>,1,0,,,,CC BY-SA 4.0,
2054,40006763,1,40016555,,2016-10-12 19:23:48,,3,1123,"<p>I'm trying to implement Sarsa algorithm for solving a Frozen Lake environment from OpenAI gym. I've started soon to work with this but I think I understand it. </p>

<p>I also understand how Sarsa algorithm works, there're many sites where to find a pseudocode, and I get it. I've implemented this algorithm in my problem following all the steps, but when I check the final Q function after all the episodes I notice that all values tend to zero and I don't know why.</p>

<p>Here is my code, I hope someone can tell me why that happens.</p>

<pre><code>import gym
import random
import numpy as np

env = gym.make('FrozenLake-v0')

#Initialize the Q matrix 16(rows)x4(columns)
Q = np.zeros([env.observation_space.n, env.action_space.n])

for i in range(env.observation_space.n):
    if (i != 5) and (i != 7) and (i != 11) and (i != 12) and (i != 15):
        for j in range(env.action_space.n):
            Q[i,j] = np.random.rand()

#Epsilon-Greedy policy, given a state the agent chooses the action that it believes has the best long-term effect with probability 1-eps, otherwise, it chooses an action uniformly at random. Epsilon may change its value.

bestreward = 0
epsilon = 0.1
discount = 0.99
learning_rate = 0.1
num_episodes = 50000
a = [0,0,0,0,0,0,0,0,0,0]

for i_episode in range(num_episodes):

    # Observe current state s
    observation = env.reset()
    currentState = observation

    # Select action a using a policy based on Q
    if np.random.rand() &lt;= epsilon: #pick randomly
        currentAction = random.randint(0,env.action_space.n-1)
    else: #pick greedily            
        currentAction = np.argmax(Q[currentState, :])

    totalreward = 0
    while True:
        env.render()

        # Carry out an action a 
        observation, reward, done, info = env.step(currentAction)
        if done is True:
            break;

        # Observe reward r and state s'
        totalreward += reward
        nextState = observation

        # Select action a' using a policy based on Q
        if np.random.rand() &lt;= epsilon: #pick randomly
            nextAction = random.randint(0,env.action_space.n-1)
        else: #pick greedily            
            nextAction = np.argmax(Q[nextState, :])

        # update Q with Q-learning 
        Q[currentState, currentAction] += learning_rate * (reward + discount * Q[nextState, nextAction] - Q[currentState, currentAction])

        currentState = nextState
        currentAction = nextAction

        print ""Episode: %d reward %d best %d epsilon %f"" % (i_episode, totalreward, bestreward, epsilon)
        if totalreward &gt; bestreward:
            bestreward = totalreward
        if i_episode &gt; num_episodes/2:
            epsilon = epsilon * 0.9999
        if i_episode &gt;= num_episodes-10:
            a.insert(0, totalreward)
            a.pop()
        print a

        for i in range(env.observation_space.n):
            print ""-----""
            for j in range(env.action_space.n):
                print Q[i,j]
</code></pre>
",6820417,,,,,2016-10-13 08:59:38,"Sarsa algorithm, why Q-values tend to zero?",<python><reinforcement-learning><sarsa>,1,0,0,,,CC BY-SA 3.0,
2056,58203832,1,58204012,,2019-10-02 14:40:21,,1,307,"<p>I'm creating a custom <strong><em>Reinforcement Learning environment</em></strong>. So far, the environment is simply a 3 x 3 grid. I would like to create a custom environment which is why I do not work with OpenAI Gym. The ultimate objective is that a DQN- agent finds a suitable path to maximize the possible rewards and reaches a destination on the grid (Let's say, e.g., goal is to get to field with coordinates [2|2]).</p>

<p>I have created a sample class for the environment (class Env).
The ""architecture"" of the grid is described in the function <strong>build_canvas(self)</strong>. As visible, I used <em>tkinter.canvas</em> for defining the grid system. Unfortunately, when I try to instantiate an object of type <em>Env</em>, the grid is not displayed. </p>

<pre><code>    class Env(tk.Tk):

        def __init__(self):
            super(Env, self).__init__()
            print(""This is the standard constructor of our 
                   environment class."")
            self.build_canvas()

        def build_canvas(self):
            canvas = tk.Canvas(self, bg='green', height=HEIGHT, 
            width=WIDTH)

           ## Create grid with 3x3 (3 rows with 3 columns)
            for c in range(0, WIDTH, 60):
                 x1, y1, x2, y2 = c, 0, c, HEIGHT
                 canvas.create_line(x1, y1, x2, y2)
            for r in range(0, HEIGHT, 60):
                 x1, y1, x2, y2 = 0, r, HEIGHT, r
                 canvas.create_line(x1, y1, x2, y2)

            canvas.pack()
            return canvas


        def render(self):
            print(""This renders the environment to the screen."")

        def reset(self):
            print(""This resets the environment."")

       def step(self, action):
            print(""This takes an action and the environment."")


   if __name__ == ""__main__"":
           env = Env()
</code></pre>

<p>It just prints out the strings to the console, however, the grid is not loaded at all. Does anyone have suggestions?</p>
",11836870,,7432,,2019-10-02 15:11:12,2019-10-02 15:11:12,How to display tkinter-canvas for Reinforcement Learning environment,<python><tkinter><reinforcement-learning><tkinter-canvas>,1,0,0,,,CC BY-SA 4.0,
2057,40115282,1,40118060,,2016-10-18 18:17:12,,0,844,"<p>My problem is as follows:</p>

<p>The agent should at each state, adjust the water flow and a fan speed for a power plant boiler to receive a feedback of a double state: current temperature, amount of emissions.</p>

<p>If my agent has a tuple of actions and a tuple of states, does that mean i should split my q-learning problem into 2 where 1 agent would have a Q and R matrix for the water/temperature environment and the other agent for the fan speed/amount of emissions environment? Or is there a way to represent an R and Q matrix for the agent described originally?</p>
",3340234,,,,,2016-10-18 21:05:39,Q-learning with 2D actions and 2D states,<machine-learning><tensorflow><artificial-intelligence><reinforcement-learning>,1,1,,,,CC BY-SA 3.0,
2059,40137240,1,40138043,,2016-10-19 16:47:20,,2,1180,"<p>I am about to write a chess engine based on reinforcement learning.
I'd like to train an evaluation function and figure out what are the weights of the board's most important features.</p>

<p>I'm not an expert of machine learning, I'm trying to learn from books and tutorials. In each tutorial, the reward is quite straightforward, often 1, 0, maybe -1, but there's no such obvious reward in chess (regardless the check-mate positions).
For instance, assume I have a situation on the board. I make 10 (random) moves and at that point I should calculate the reward, the difference (or error) between the starting position and the current one. How to do such thing, when my only evaluation function is under training?</p>

<p>I'd like to avoid using other engines' scoring system, because I feel that would rather be supervised learning, which is not my goal.</p>
",5958724,,,,,2020-12-29 16:16:54,Training of chess evaluation function,<machine-learning><evaluation><chess><reinforcement-learning><reward>,2,0,,,,CC BY-SA 3.0,
2064,58473521,1,58474081,,2019-10-20 13:32:28,,4,943,"<p>I'making a implementation of Q-learning, specifically the Bellman equation.<a href=""https://i.stack.imgur.com/E1U4S.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/E1U4S.png"" alt=""Bellman equation""></a></p>

<p>I'm using the version from a <a href=""https://www.freecodecamp.org/news/an-introduction-to-q-learning-reinforcement-learning-14ac0b4493cc/"" rel=""nofollow noreferrer"">website</a> that guides he through the problem, but I have question:
For maxQ, do I calculate the max reward using all Q-table values of the new state (s') - in my case 4 possible action (a'), each with their respective value- or the sum of the Q-table values of all the positions when taking the action (a')?</p>

<p>In other words, do I use the highest Q-value of all the possible actions I can take, or the summed Q-values of all the ""neighbouring"" squares?</p>
",,user11105005,,,,2019-10-20 14:37:42,How do I calculate MaxQ in Q-learning?,<c++><reinforcement-learning><q-learning>,1,0,,,,CC BY-SA 4.0,
2068,3972812,1,3972927,,2010-10-19 21:12:35,,7,696,"<p>I like doing part-time research in reinforcement learning. In recent years (up to 2009) there was a reinforcement learning competition held at <a href=""http://www.rl-competition.org/"" rel=""noreferrer"">rl-competition.org</a> with some very interesting problems, but this seems to have been discontinued. I'd love to improve my skills and knowledge and measure it against other enthusiasts in the field - are there still any such competitions around?</p>
",481004,,,,,2010-10-19 21:27:57,Are there any active reinforcement learning competitions?,<artificial-intelligence><reinforcement-learning>,1,1,0,,,CC BY-SA 2.5,
2070,50189695,1,50190093,,2018-05-05 12:57:04,,3,596,"<p>What is the difference between policy gradient methods and neural network-based action-value methods?</p>
",1037845,,3924118,,2018-12-09 22:01:31,2018-12-09 22:32:48,What is the difference between policy gradient methods and neural network-based action-value methods?,<machine-learning><artificial-intelligence><reinforcement-learning><q-learning>,1,0,0,,,CC BY-SA 4.0,
2073,50297126,1,50302177,,2018-05-11 17:10:10,,2,81,"<p>I would like to cross check my understanding on reinforcement learning. How easy/difficult or common to train a policy and then reuse the learned policy later on? What I understood so far is that when we stop the training and if we would again start, it would need start from scratch i.e. not able to benefit from the learned policy. Thank you.</p>
",1014196,,,,,2018-05-12 01:47:36,Ways to utilize policy learned in reinforcement learning,<machine-learning><reinforcement-learning>,1,0,,,,CC BY-SA 4.0,
2074,50308750,1,50308765,,2018-05-12 17:06:38,,3,1196,"<p>I am following the tutorial <a href=""https://becominghuman.ai/lets-build-an-atari-ai-part-1-dqn-df57e8ff3b26"" rel=""nofollow noreferrer"">here</a> to learn Deep Reinforcement Learning (I am very new to the topic).</p>

<p>When I try to run the script below on my Macbook it returns me an Import Error (<code>ImportError: sys.meta_path is None, Python is likely shutting down</code>).
I tried to run it from the Mac terminal and also in Pycharm with the same result.
I tried also with other deep learning scripts and the error is the same (it seems not related to this script in particular).</p>

<p>Could you please help me understanding where I am wrong?</p>

<p>Script:</p>

<pre><code># Import the gym module
import gym

# Create a breakout environment
env = gym.make('BreakoutDeterministic-v4')
# Reset it, returns the starting frame
frame = env.reset()
# Render
env.render()

is_done = False
while not is_done:
  # Perform a random action, returns the new frame, reward and whether the game is over
  frame, reward, is_done, _ = env.step(env.action_space.sample())
  # Render
  env.render()
</code></pre>

<p>Error below:</p>

<pre><code>/usr/local/bin/python3.6 /Users/marcogdepinto/PycharmProjects/PlayPong/pong.py
2018-05-12 18:58:11.915 Python[567:12594] ApplePersistenceIgnoreState: Existing state will not be touched. New state will be written to (null)
Exception ignored in: &lt;bound method SimpleImageViewer.__del__ of &lt;gym.envs.classic_control.rendering.SimpleImageViewer object at 0x10b65bc88&gt;&gt;
Traceback (most recent call last):
  File ""/usr/local/lib/python3.6/site-packages/gym/envs/classic_control/rendering.py"", line 347, in __del__
  File ""/usr/local/lib/python3.6/site-packages/gym/envs/classic_control/rendering.py"", line 343, in close
  File ""/usr/local/lib/python3.6/site-packages/pyglet/window/cocoa/__init__.py"", line 281, in close
  File ""/usr/local/lib/python3.6/site-packages/pyglet/window/__init__.py"", line 770, in close
ImportError: sys.meta_path is None, Python is likely shutting down

Process finished with exit code 0
</code></pre>
",5539313,,,,,2018-08-16 13:26:50,Python - script made with gym does not work on Mac,<python><deep-learning><pycharm><reinforcement-learning>,2,0,,,,CC BY-SA 4.0,
2079,50360618,1,50360846,,2018-05-15 23:48:13,,0,280,"<p>I am coming across the SARSA algorithm in model-free reinforcement learning. Specifically, in each state, you would take an action <code>a</code>, and then observed a new state <code>s'</code>.</p>

<p>My question is, if you don't have the state transition probability equation <code>P{next state | current state = s0}</code>, how do you know what your next state will be?</p>

<p><strong>My attempt</strong>: do you simply try that action <code>a</code> out, and then observe from the enviroment?
<a href=""https://i.stack.imgur.com/EqCqr.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/EqCqr.png"" alt=""enter image description here""></a></p>
",7092612,,4685471,,2018-05-16 00:17:58,2019-01-17 20:51:55,SARSA in Reinforcement Learning,<algorithm><machine-learning><reinforcement-learning>,3,0,0,,,CC BY-SA 4.0,
2080,33011825,1,45033740,,2015-10-08 09:31:12,,6,9330,"<p>The question how the learning rate influences the convergence rate and convergence itself.
If the learning rate is constant, will Q function converge to the optimal on or learning rate should necessarily decay to guarantee convergence?</p>
",5166311,,,,,2017-07-11 11:48:59,Learning rate of a Q learning agent,<machine-learning><reinforcement-learning><q-learning>,3,2,0,,,CC BY-SA 3.0,
2085,51127979,1,51130431,,2018-07-02 00:35:49,,1,980,"<p>I've built a custom reinforcement learning <code>environment</code> and <code>agent</code> which is similar to a labyrinth game.</p>

<p>In labyrinth there're 5 possible actions: up, down, left, right, and stay. While if blocked, e.g. agent can't go up, then how do people design <code>env</code> and <code>agent</code> to simulate that?</p>

<p>To be specific, the agent is at current state <code>s0</code>, and by definition taking actions of down, left, and right will change the state to some other values with an immediate reward (>0 if at the exit). One possible approach is when taking action <code>up</code>, the state will stay at <code>s0</code> and the reward will be a large negative number. Ideally the agent will learn that and never go <code>up</code> again at this state. </p>

<p>However, my agent seems not learning this. Instead, it still goes <code>up</code>. Another approach is to hard code the agent and the environment that the agent will not be able to perform the action <code>up</code> when at <code>s0</code>, what I can think of is:</p>

<ol>
<li>when at some state when <code>up</code> is not allowed, we look at the Q values of different actions</li>
<li>pick the action with the largest Q value except <code>up</code></li>
<li>therefore, the agent will never perform an invalid action</li>
</ol>

<p>I'm asking is the above approach feasible? Will there be any issues related to that? Or is there a better design to deal with the boundary and invalid actions?</p>
",6750211,,6750211,,2018-07-02 01:41:20,2018-07-02 23:50:02,Deep reinforcement learning - how to deal with boundaries in action space,<machine-learning><reinforcement-learning><q-learning>,2,0,,,,CC BY-SA 4.0,
2086,40410384,1,40455581,,2016-11-03 20:09:42,,0,91,"<p>I tried to find what is pi* in many resources like <a href=""https://webdocs.cs.ualberta.ca/~sutton/book/ebook/node35.html"" rel=""nofollow noreferrer"">this link</a>. But, I can not find what is pi*. Is V* is same as V_pi*?</p>

<p><a href=""https://i.stack.imgur.com/EZ0oO.png"" rel=""nofollow noreferrer"">Screenshot of the question</a></p>
",5311361,,,,,2016-11-06 23:06:49,How do I describe optimal policy (pi*) of bellman's equation?,<optimization><machine-learning><reinforcement-learning>,1,0,,,,CC BY-SA 3.0,
2089,58447885,1,58456300,,2019-10-18 09:26:21,,3,799,"<p>I'm studying ""Deep Reinforcement Learning"" and build my own example after pytorch's REINFORCEMENT LEARNING (DQN) TUTORIAL.</p>

<p>I'm implement actor's strategy as follows:
1. model.eval()
2. get best action from a model
3. self.net.train()</p>

<p>The question is: 
Does going back and forth between eval() and train() modes cause any damage to optimization process?</p>

<p>The model includes only Linear and BatchNorm1d layers.
As far as I know when using BatchNorm1d one must perform model.eval() to use a model, because there is different results in eval() and train() modes.</p>

<p>When training Classification Neural Network the model.eval() performed only after training is finished, but in case of ""Deep Reinforcement Learning"" it is usual to use strategy and then continue the optimization process.</p>

<p>I'm wondering if going back and forth between modes is ""harmless"" to optimization process?</p>

<pre><code>def strategy(self, state):
    # Explore or Exploit
    if self.epsilon &gt; random():
        action = choice(self.actions)
    else:
        self.net.eval()
        action = self.net(state.unsqueeze(0)).max(1)[1].detach()
        self.net.train()
</code></pre>
",10844823,,,,,2022-09-18 11:02:33,Pytorch - going back and forth between eval() and train() modes,<python><neural-network><deep-learning><pytorch><reinforcement-learning>,2,1,,,,CC BY-SA 4.0,
2097,48740685,1,48749854,,2018-02-12 06:28:47,,1,97,"<p>I'm having trouble computing gradients for a transition function being repeatedly called. </p>

<p>Gradients being computed w/ respect to actions are None even though the loss depends on the selected actions chosen by a sum of the max values generated from repeated transition calls. If we change the value of the loss function to be a sum over v instead of a then we receive gradients for the transition. </p>

<p>Why is it the case that there are no gradients being computed for the transition when our loss is computed w/ respect to a sum over a?</p>

<p>Below is a chunk of sample code where you can replicate the issue.</p>

<pre><code>import tensorflow as tf
import numpy as np

ACTION_DIM = 1

# random input
x = tf.Variable(np.random.rand(1, 5))  # [b branches, state_dim]

depth = 3
b = 4
v_list, a_list = [], []  # value and action store
# make value estimates 3 steps into the future by predicting intermediate states
for i in range(depth):
    reuse = True if i &gt; 0 else False
    x = tf.tile(x, [b, 1])  # copy the state to be used for b different actions
    mu = tf.layers.dense(x, ACTION_DIM, name='mu', reuse=reuse)
    action_distribution = tf.distributions.Normal(loc=mu, scale=tf.ones_like(mu))
    a = tf.reshape(action_distribution.sample(1), [-1, ACTION_DIM])
    x_a = tf.concat([x, a], axis=1)  # concatenate action and state
    x = tf.layers.dense(x_a, x.shape[-1], name='transition', reuse=reuse)  # next state s'
    v = tf.layers.dense(x, 1, name='value', reuse=reuse)  # value of s'
    v_list.append(tf.reshape(v, [-1, b ** i]))
    a_list.append(tf.reshape(a, [-1, b ** i]))

# backup our sum of max values along trajectory
sum_v = [None]*depth
sum_v[-1] = v_list[-1]
for i in reversed(range(depth)):
    max_v_i = tf.reduce_max(v_list[i], axis=1)
    if i &gt; 0:
        sum_v[i-1] = tf.reduce_max(v_list[i-1], axis=1) + max_v_i

max_idx = tf.reshape(tf.argmax(sum_v[0]), [-1, 1])
v = tf.gather_nd(v_list[0], max_idx)
a = tf.gather_nd(a_list[0], max_idx)
loss = -tf.reduce_sum(a)
opt = tf.train.AdamOptimizer()
grads = opt.compute_gradients(loss)
</code></pre>
",3701294,,3701294,,2018-02-13 09:55:10,2018-02-13 09:55:10,gradients with respect to a repeating function,<python><tensorflow><deep-learning><reinforcement-learning>,1,0,0,,,CC BY-SA 3.0,
2104,49037430,1,49038031,,2018-02-28 19:47:12,,1,7179,"<p>In Colaboratory, we are forced to use pip install. Some third packages such as gym, PLE, etc, their installation should be </p>

<pre><code>git clone https://github.com/ntasfi/PyGame-Learning-Environment.git
cd PyGame-Learning-Environment/
pip install -e .
</code></pre>

<p>When I tried, here are some problems:</p>

<p>1) <code>!cd</code> doesn't work here. I'm still in my current folder in google drive.</p>

<p>2) Instead, I directly run:</p>

<pre><code>!git clone https://github.com/ntasfi/PyGame-Learning-Environment.git 
!pip install -e PyGame-Learning-Environment 
</code></pre>

<p>And it says successfully installed, but I cannot import it. I checked and it doesnt appear in the <code>/usr/local/lib/python3.6/dist-packages</code></p>

<p>I also checked the python by:</p>

<pre><code>import os
print(os.getcwd())
</code></pre>

<p>which gives me: <code>/content</code>, a directory I dont understand. And obviously I cannot import the package.</p>

<p>What should I do?</p>
",9052893,,8841057,,2018-02-28 23:28:11,2022-10-23 12:38:23,Colaboratory: how to install PyGame Learning Environment,<python><linux><jupyter-notebook><reinforcement-learning><google-colaboratory>,2,1,0,,,CC BY-SA 3.0,
2106,65142804,1,65143609,,2020-12-04 11:31:42,,1,506,"<p>I tried to install PTAN liblary using pip</p>
<pre><code>python -m pip install PTAN==0.6
</code></pre>
<p>But it shows error</p>
<pre><code>ERROR: Could not find a version that satisfies the requirement torch==1.3.0 (from ptan)
ERROR: No matching distribution found for torch==1.3.0
</code></pre>
<p>I can install it by running:</p>
<pre><code>python -m pip install PTAN
</code></pre>
<p>But then I get old version of this.</p>
<p><strong>How to install current version of PTAN without errors?</strong></p>
<p>I'm using Linux ubuntu and Python 3.8</p>
",13674281,,,,,2020-12-04 12:31:17,No matching distribution found for torch==1.3.0 while installing PTAN,<python><pip><reinforcement-learning>,1,0,,,,CC BY-SA 4.0,
2107,54051499,1,54078288,,2019-01-05 11:24:35,,2,4713,"<p><strong>Problem</strong></p>

<p>My goal is to apply Reinforcement Learning to predict the next state of an object under a known force in a 3D environment (the approach would be reduced to supervised learning, off-line learning).</p>

<p><strong>Details of my approach</strong></p>

<p>The current state is the vector representing the position of the object in the environment (3 dimensions), and the velocity of the object (3 dimensions). The starting position is randomly initialized in the environment, as well as the starting velocity.</p>

<p>The action is the vector representing the movement from state <em>t</em> to state <em>t+1</em>.</p>

<p>The reward is just the Euclidean distance between the predicted next state, and the real next state (I already have the target position).</p>

<p><strong>What have I done so far?</strong></p>

<p>I have been looking for many methods to do this. <em>Deep Deterministic Policy Gradients</em> works for a continuous action space, but in my case I also have a continuous state space. If you are interested in this approach, here's the original paper written at DeepMind: 
<a href=""http://proceedings.mlr.press/v32/silver14.pdf"" rel=""nofollow noreferrer"">http://proceedings.mlr.press/v32/silver14.pdf</a></p>

<p>The <em>Actor-Critic</em> approach should work, but it is usually (or always) applied to discrete and low-dimensional state space.</p>

<p><em>Q-Learning</em> and <em>Deep-Q Learning</em> cannot handle high dimensional state space, so my configuration would not work even if discretizing the state space.</p>

<p><em>Inverse Reinforcement Learning</em> (an instance of Imitation learning, with <em>Behavioral Cloning</em> and <em>Direct Policy Learning</em>) approximates a reward function when finding the reward function is more complicated than finding the policy function. Interesting approach, but I haven't seen any implementation, and in my case the reward function is pretty straightforward. 
Is there a methodology to deal with my configuration that I haven't explored?</p>
",9177422,,9177422,,2019-01-05 13:46:46,2019-04-18 23:37:41,Reinforcement learning for continuous state and action space,<python><machine-learning><artificial-intelligence><reinforcement-learning>,2,1,0,,,CC BY-SA 4.0,
2108,68391465,1,68400355,,2021-07-15 09:42:39,,0,114,"<p>I have a reinforcement learning game in which two agents interact with each other. I would now like to solve the following problem in Python.</p>
<p>I have created a <code>for-loop</code> that does something like this:</p>
<pre><code>self.rewards_hist = []
self.number_episodes = 1000

def Game(self):
 
   for episode in range(self.number_episodes):

       doSomething()

       rewards = self.save_rewards()

       self.rewards_hist.append(rewards)
</code></pre>
<p>Thus, with <code>self.rewards</code> I get the reward at time <code>t</code> and in <code>self.rewards_hist</code> I receive the history of all rewards, hence giving me access to the previous rewards.</p>
<p>Now I want to compare this reward with the rewards in <code>t + k</code>, i.e. I would have to wait <code>k</code> episodes in the iteration until I can compare the current reward with the upcoming rewards. I am aware that within the <code>for-loop</code> this does not work because it is not possible to access the next rewards in the current iteration. Therefore I would need a function that allows me to wait for the next <code>k</code> rewards and only then make the comparison.</p>
<p>What is the easiest way to solve this problem?</p>
",9401081,,,,,2021-07-15 20:23:31,Access the next k elements in a for loop,<python><for-loop><iteration><reinforcement-learning><next>,2,2,0,,,CC BY-SA 4.0,
2110,52376692,1,52431519,,2018-09-17 22:56:32,,1,1484,"<p>When I restore a saved model using:</p>

<pre><code>checkpoint = tf.train.get_checkpoint_state(config.pre_model_dir)
if checkpoint and checkpoint.model_checkpoint_path:
 saver.restore(session, checkpoint.model_checkpoint_path)
</code></pre>

<p>, I am getting this error: </p>

<pre><code>INFO:tensorflow:Restoring parameters from ./saved_model/10_zones/10/network--1685000
---------------------------------------------------------------------------
NotFoundError                             Traceback (most recent call last)
/usr/lib/python3.6/site-packages/tensorflow/python/client/session.py in _do_call(self, fn, *args)
   1321     try:
-&gt; 1322       return fn(*args)
   1323     except errors.OpError as e:

/usr/lib/python3.6/site-packages/tensorflow/python/client/session.py in _run_fn(feed_dict, fetch_list, target_list, options, run_metadata)
   1306       return self._call_tf_sessionrun(
-&gt; 1307           options, feed_dict, fetch_list, target_list, run_metadata)
   1308 

/usr/lib/python3.6/site-packages/tensorflow/python/client/session.py in _call_tf_sessionrun(self, options, feed_dict, fetch_list, target_list, run_metadata)
   1408           self._session, options, feed_dict, fetch_list, target_list,
-&gt; 1409           run_metadata)
   1410     else:

NotFoundError: Key Variable not found in checkpoint
     [[Node: save/RestoreV2 = RestoreV2[dtypes=[DT_INT32, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, ..., DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT], _device=""/job:localhost/replica:0/task:0/device:CPU:0""](_arg_save/Const_0_0, save/RestoreV2/tensor_names, save/RestoreV2/shape_and_slices)]]
     [[Node: save/RestoreV2/_21 = _Recv[client_terminated=false, recv_device=""/job:localhost/replica:0/task:0/device:GPU:0"", send_device=""/job:localhost/replica:0/task:0/device:CPU:0"", send_device_incarnation=1, tensor_name=""edge_18_save/RestoreV2"", tensor_type=DT_FLOAT, _device=""/job:localhost/replica:0/task:0/device:GPU:0""]()]]

During handling of the above exception, another exception occurred:

NotFoundError                             Traceback (most recent call last)
&lt;ipython-input-97-0cbd09927b40&gt; in &lt;module&gt;()
     42 checkpoint = tf.train.get_checkpoint_state(config.pre_model_dir)
     43 if checkpoint and checkpoint.model_checkpoint_path:
---&gt; 44     saver.restore(session, checkpoint.model_checkpoint_path)
     45     print(""loaded the model"")
     46 else:

/usr/lib/python3.6/site-packages/tensorflow/python/training/saver.py in restore(self, sess, save_path)
   1800     else:
   1801       sess.run(self.saver_def.restore_op_name,
-&gt; 1802                {self.saver_def.filename_tensor_name: save_path})
   1803 
   1804   @staticmethod

/usr/lib/python3.6/site-packages/tensorflow/python/client/session.py in run(self, fetches, feed_dict, options, run_metadata)
    898     try:
    899       result = self._run(None, fetches, feed_dict, options_ptr,
--&gt; 900                          run_metadata_ptr)
    901       if run_metadata:
    902         proto_data = tf_session.TF_GetBuffer(run_metadata_ptr)

/usr/lib/python3.6/site-packages/tensorflow/python/client/session.py in _run(self, handle, fetches, feed_dict, options, run_metadata)
   1133     if final_fetches or final_targets or (handle and feed_dict_tensor):
   1134       results = self._do_run(handle, final_targets, final_fetches,
-&gt; 1135                              feed_dict_tensor, options, run_metadata)
   1136     else:
   1137       results = []

/usr/lib/python3.6/site-packages/tensorflow/python/client/session.py in _do_run(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)
   1314     if handle is None:
   1315       return self._do_call(_run_fn, feeds, fetches, targets, options,
-&gt; 1316                            run_metadata)
   1317     else:
   1318       return self._do_call(_prun_fn, handle, feeds, fetches)

/usr/lib/python3.6/site-packages/tensorflow/python/client/session.py in _do_call(self, fn, *args)
   1333         except KeyError:
   1334           pass
-&gt; 1335       raise type(e)(node_def, op, message)
   1336 
   1337   def _extend_graph(self):

NotFoundError: Key Variable not found in checkpoint
     [[Node: save/RestoreV2 = RestoreV2[dtypes=[DT_INT32, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, ..., DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT], _device=""/job:localhost/replica:0/task:0/device:CPU:0""](_arg_save/Const_0_0, save/RestoreV2/tensor_names, save/RestoreV2/shape_and_slices)]]
     [[Node: save/RestoreV2/_21 = _Recv[client_terminated=false, recv_device=""/job:localhost/replica:0/task:0/device:GPU:0"", send_device=""/job:localhost/replica:0/task:0/device:CPU:0"", send_device_incarnation=1, tensor_name=""edge_18_save/RestoreV2"", tensor_type=DT_FLOAT, _device=""/job:localhost/replica:0/task:0/device:GPU:0""]()]]

Caused by op 'save/RestoreV2', defined at:
  File ""/usr/lib64/python3.6/runpy.py"", line 193, in _run_module_as_main
    ""__main__"", mod_spec)
  File ""/usr/lib64/python3.6/runpy.py"", line 85, in _run_code
    exec(code, run_globals)
  File ""/usr/lib/python3.6/site-packages/ipykernel_launcher.py"", line 16, in &lt;module&gt;
    app.launch_new_instance()
  File ""/usr/lib/python3.6/site-packages/traitlets/config/application.py"", line 658, in launch_instance
    app.start()
  File ""/usr/lib/python3.6/site-packages/ipykernel/kernelapp.py"", line 486, in start
    self.io_loop.start()
  File ""/usr/lib64/python3.6/site-packages/tornado/platform/asyncio.py"", line 127, in start
    self.asyncio_loop.run_forever()
  File ""/usr/lib64/python3.6/asyncio/base_events.py"", line 422, in run_forever
    self._run_once()
  File ""/usr/lib64/python3.6/asyncio/base_events.py"", line 1432, in _run_once
    handle._run()
  File ""/usr/lib64/python3.6/asyncio/events.py"", line 145, in _run
    self._callback(*self._args)
  File ""/usr/lib64/python3.6/site-packages/tornado/platform/asyncio.py"", line 117, in _handle_events
    handler_func(fileobj, events)
  File ""/usr/lib64/python3.6/site-packages/tornado/stack_context.py"", line 276, in null_wrapper
    return fn(*args, **kwargs)
  File ""/usr/lib64/python3.6/site-packages/zmq/eventloop/zmqstream.py"", line 450, in _handle_events
    self._handle_recv()
  File ""/usr/lib64/python3.6/site-packages/zmq/eventloop/zmqstream.py"", line 480, in _handle_recv
    self._run_callback(callback, msg)
  File ""/usr/lib64/python3.6/site-packages/zmq/eventloop/zmqstream.py"", line 432, in _run_callback
    callback(*args, **kwargs)
  File ""/usr/lib64/python3.6/site-packages/tornado/stack_context.py"", line 276, in null_wrapper
    return fn(*args, **kwargs)
  File ""/usr/lib/python3.6/site-packages/ipykernel/kernelbase.py"", line 283, in dispatcher
    return self.dispatch_shell(stream, msg)
  File ""/usr/lib/python3.6/site-packages/ipykernel/kernelbase.py"", line 233, in dispatch_shell
    handler(stream, idents, msg)
  File ""/usr/lib/python3.6/site-packages/ipykernel/kernelbase.py"", line 399, in execute_request
    user_expressions, allow_stdin)
  File ""/usr/lib/python3.6/site-packages/ipykernel/ipkernel.py"", line 208, in do_execute
    res = shell.run_cell(code, store_history=store_history, silent=silent)
  File ""/usr/lib/python3.6/site-packages/ipykernel/zmqshell.py"", line 537, in run_cell
    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)
  File ""/usr/lib/python3.6/site-packages/IPython/core/interactiveshell.py"", line 2662, in run_cell
    raw_cell, store_history, silent, shell_futures)
  File ""/usr/lib/python3.6/site-packages/IPython/core/interactiveshell.py"", line 2785, in _run_cell
    interactivity=interactivity, compiler=compiler, result=result)
  File ""/usr/lib/python3.6/site-packages/IPython/core/interactiveshell.py"", line 2903, in run_ast_nodes
    if self.run_code(code, result):
  File ""/usr/lib/python3.6/site-packages/IPython/core/interactiveshell.py"", line 2963, in run_code
    exec(code_obj, self.user_global_ns, self.user_ns)
  File ""&lt;ipython-input-97-0cbd09927b40&gt;"", line 26, in &lt;module&gt;
    saver = tf.train.Saver()
  File ""/usr/lib/python3.6/site-packages/tensorflow/python/training/saver.py"", line 1338, in __init__
    self.build()
  File ""/usr/lib/python3.6/site-packages/tensorflow/python/training/saver.py"", line 1347, in build
    self._build(self._filename, build_save=True, build_restore=True)
  File ""/usr/lib/python3.6/site-packages/tensorflow/python/training/saver.py"", line 1384, in _build
    build_save=build_save, build_restore=build_restore)
  File ""/usr/lib/python3.6/site-packages/tensorflow/python/training/saver.py"", line 835, in _build_internal
    restore_sequentially, reshape)
  File ""/usr/lib/python3.6/site-packages/tensorflow/python/training/saver.py"", line 472, in _AddRestoreOps
    restore_sequentially)
  File ""/usr/lib/python3.6/site-packages/tensorflow/python/training/saver.py"", line 886, in bulk_restore
    return io_ops.restore_v2(filename_tensor, names, slices, dtypes)
  File ""/usr/lib/python3.6/site-packages/tensorflow/python/ops/gen_io_ops.py"", line 1463, in restore_v2
    shape_and_slices=shape_and_slices, dtypes=dtypes, name=name)
  File ""/usr/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py"", line 787, in _apply_op_helper
    op_def=op_def)
  File ""/usr/lib/python3.6/site-packages/tensorflow/python/framework/ops.py"", line 3392, in create_op
    op_def=op_def)
  File ""/usr/lib/python3.6/site-packages/tensorflow/python/framework/ops.py"", line 1718, in __init__
    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access

NotFoundError (see above for traceback): Key Variable not found in checkpoint
     [[Node: save/RestoreV2 = RestoreV2[dtypes=[DT_INT32, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, ..., DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT], _device=""/job:localhost/replica:0/task:0/device:CPU:0""](_arg_save/Const_0_0, save/RestoreV2/tensor_names, save/RestoreV2/shape_and_slices)]]
     [[Node: save/RestoreV2/_21 = _Recv[client_terminated=false, recv_device=""/job:localhost/replica:0/task:0/device:GPU:0"", send_device=""/job:localhost/replica:0/task:0/device:CPU:0"", send_device_incarnation=1, tensor_name=""edge_18_save/RestoreV2"", tensor_type=DT_FLOAT, _device=""/job:localhost/replica:0/task:0/device:GPU:0""]()]]
</code></pre>

<p>I searched about this error, and there was a tf bug which requires to call the model using a full relative path, and I followed that path and tried values: <code>'./saved_model/10_zones/10'</code> and <code>os.path.abspath(config.pre_model_dir+'./../saved_model/10_zones/10')</code>
for <code>config.pre_model_dir</code>. Both resulted in a same error. </p>

<p>I also checked the name of the saved variables using 
from tensorflow.contrib.framework.python.framework import checkpoint_utils</p>

<pre><code>var_list = checkpoint_utils.list_variables(config.pre_model_dir)
for v in var_list:
    print(v)
</code></pre>

<p>which is:</p>

<pre><code>('actor/main_net/layer1/biases/Variable', [90])
('actor/main_net/layer1/biases/Variable/Adam', [90])
('actor/main_net/layer1/biases/Variable/Adam_1', [90])
('actor/main_net/layer1/weights/Variable', [30, 90])
('actor/main_net/layer1/weights/Variable/Adam', [30, 90])
('actor/main_net/layer1/weights/Variable/Adam_1', [30, 90])
('actor/main_net/layer2/biases/Variable', [60])
('actor/main_net/layer2/biases/Variable/Adam', [60])
('actor/main_net/layer2/biases/Variable/Adam_1', [60])
('actor/main_net/layer2/weights/Variable', [90, 60])
('actor/main_net/layer2/weights/Variable/Adam', [90, 60])
('actor/main_net/layer2/weights/Variable/Adam_1', [90, 60])
('actor/main_net/layer3/biases/Variable', [30])
('actor/main_net/layer3/biases/Variable/Adam', [30])
('actor/main_net/layer3/biases/Variable/Adam_1', [30])
('actor/main_net/layer3/weights/Variable', [60, 30])
('actor/main_net/layer3/weights/Variable/Adam', [60, 30])
('actor/main_net/layer3/weights/Variable/Adam_1', [60, 30])
('actor/main_net/layer4/biases/Variable', [10])
('actor/main_net/layer4/biases/Variable/Adam', [10])
('actor/main_net/layer4/biases/Variable/Adam_1', [10])
('actor/main_net/layer4/weights/Variable', [30, 10])
('actor/main_net/layer4/weights/Variable/Adam', [30, 10])
('actor/main_net/layer4/weights/Variable/Adam_1', [30, 10])
('actor/target_net/layer1/biases/Variable', [90])
('actor/target_net/layer1/weights/Variable', [30, 90])
('actor/target_net/layer2/biases/Variable', [60])
('actor/target_net/layer2/weights/Variable', [90, 60])
('actor/target_net/layer3/biases/Variable', [30])
('actor/target_net/layer3/weights/Variable', [60, 30])
('actor/target_net/layer4/biases/Variable', [10])
('actor/target_net/layer4/weights/Variable', [30, 10])
('beta1_power', [])
('beta1_power_1', [])
('beta2_power', [])
('beta2_power_1', [])
('critic/main_net/l1/biases', [90])
('critic/main_net/l1/biases/Adam', [90])
('critic/main_net/l1/biases/Adam_1', [90])
('critic/main_net/l1/weights', [40, 90])
('critic/main_net/l1/weights/Adam', [40, 90])
('critic/main_net/l1/weights/Adam_1', [40, 90])
('critic/main_net/l2/biases', [60])
('critic/main_net/l2/biases/Adam', [60])
('critic/main_net/l2/biases/Adam_1', [60])
('critic/main_net/l2/weights', [90, 60])
('critic/main_net/l2/weights/Adam', [90, 60])
('critic/main_net/l2/weights/Adam_1', [90, 60])
('critic/main_net/l3/biases', [30])
('critic/main_net/l3/biases/Adam', [30])
('critic/main_net/l3/biases/Adam_1', [30])
('critic/main_net/l3/weights', [60, 30])
('critic/main_net/l3/weights/Adam', [60, 30])
('critic/main_net/l3/weights/Adam_1', [60, 30])
('critic/main_net/l4/bias', [1])
('critic/main_net/l4/bias/Adam', [1])
('critic/main_net/l4/bias/Adam_1', [1])
('critic/main_net/l4/kernel', [30, 1])
('critic/main_net/l4/kernel/Adam', [30, 1])
('critic/main_net/l4/kernel/Adam_1', [30, 1])
('critic/target_net/l1/biases', [90])
('critic/target_net/l1/weights', [40, 90])
('critic/target_net/l2/biases', [60])
('critic/target_net/l2/weights', [90, 60])
('critic/target_net/l3/biases', [30])
('critic/target_net/l3/weights', [60, 30])
('critic/target_net/l4/bias', [1])
('critic/target_net/l4/kernel', [30, 1])
</code></pre>

<p>with what <code>tf.global_variables()</code> in my current model results in, and they are both similar:</p>

<pre><code>&lt;tf.Variable 'actor/main_net/layer1/weights/Variable:0' shape=(30, 90) dtype=float32_ref&gt;,
 &lt;tf.Variable 'actor/main_net/layer1/biases/Variable:0' shape=(90,) dtype=float32_ref&gt;,
 &lt;tf.Variable 'actor/main_net/layer2/weights/Variable:0' shape=(90, 60) dtype=float32_ref&gt;,
 &lt;tf.Variable 'actor/main_net/layer2/biases/Variable:0' shape=(60,) dtype=float32_ref&gt;,
 &lt;tf.Variable 'actor/main_net/layer3/weights/Variable:0' shape=(60, 30) dtype=float32_ref&gt;,
 &lt;tf.Variable 'actor/main_net/layer3/biases/Variable:0' shape=(30,) dtype=float32_ref&gt;,
 &lt;tf.Variable 'actor/main_net/layer4/weights/Variable:0' shape=(30, 10) dtype=float32_ref&gt;,
 &lt;tf.Variable 'actor/main_net/layer4/biases/Variable:0' shape=(10,) dtype=float32_ref&gt;,
 &lt;tf.Variable 'actor/target_net/layer1/weights/Variable:0' shape=(30, 90) dtype=float32_ref&gt;,
 &lt;tf.Variable 'actor/target_net/layer1/biases/Variable:0' shape=(90,) dtype=float32_ref&gt;,
 &lt;tf.Variable 'actor/target_net/layer2/weights/Variable:0' shape=(90, 60) dtype=float32_ref&gt;,
 &lt;tf.Variable 'actor/target_net/layer2/biases/Variable:0' shape=(60,) dtype=float32_ref&gt;,
 &lt;tf.Variable 'actor/target_net/layer3/weights/Variable:0' shape=(60, 30) dtype=float32_ref&gt;,
 &lt;tf.Variable 'actor/target_net/layer3/biases/Variable:0' shape=(30,) dtype=float32_ref&gt;,
 &lt;tf.Variable 'actor/target_net/layer4/weights/Variable:0' shape=(30, 10) dtype=float32_ref&gt;,
 &lt;tf.Variable 'actor/target_net/layer4/biases/Variable:0' shape=(10,) dtype=float32_ref&gt;,
 &lt;tf.Variable 'Variable:0' shape=() dtype=int32_ref&gt;,
 &lt;tf.Variable 'beta1_power:0' shape=() dtype=float32_ref&gt;,
 &lt;tf.Variable 'beta2_power:0' shape=() dtype=float32_ref&gt;,
 &lt;tf.Variable 'actor/main_net/layer1/weights/Variable/Adam:0' shape=(30, 90) dtype=float32_ref&gt;,
 &lt;tf.Variable 'actor/main_net/layer1/weights/Variable/Adam_1:0' shape=(30, 90) dtype=float32_ref&gt;,
 &lt;tf.Variable 'actor/main_net/layer1/biases/Variable/Adam:0' shape=(90,) dtype=float32_ref&gt;,
 &lt;tf.Variable 'actor/main_net/layer1/biases/Variable/Adam_1:0' shape=(90,) dtype=float32_ref&gt;,
 &lt;tf.Variable 'actor/main_net/layer2/weights/Variable/Adam:0' shape=(90, 60) dtype=float32_ref&gt;,
 &lt;tf.Variable 'actor/main_net/layer2/weights/Variable/Adam_1:0' shape=(90, 60) dtype=float32_ref&gt;,
 &lt;tf.Variable 'actor/main_net/layer2/biases/Variable/Adam:0' shape=(60,) dtype=float32_ref&gt;,
 &lt;tf.Variable 'actor/main_net/layer2/biases/Variable/Adam_1:0' shape=(60,) dtype=float32_ref&gt;,
 &lt;tf.Variable 'actor/main_net/layer3/weights/Variable/Adam:0' shape=(60, 30) dtype=float32_ref&gt;,
 &lt;tf.Variable 'actor/main_net/layer3/weights/Variable/Adam_1:0' shape=(60, 30) dtype=float32_ref&gt;,
 &lt;tf.Variable 'actor/main_net/layer3/biases/Variable/Adam:0' shape=(30,) dtype=float32_ref&gt;,
 &lt;tf.Variable 'actor/main_net/layer3/biases/Variable/Adam_1:0' shape=(30,) dtype=float32_ref&gt;,
 &lt;tf.Variable 'actor/main_net/layer4/weights/Variable/Adam:0' shape=(30, 10) dtype=float32_ref&gt;,
 &lt;tf.Variable 'actor/main_net/layer4/weights/Variable/Adam_1:0' shape=(30, 10) dtype=float32_ref&gt;,
 &lt;tf.Variable 'actor/main_net/layer4/biases/Variable/Adam:0' shape=(10,) dtype=float32_ref&gt;,
 &lt;tf.Variable 'actor/main_net/layer4/biases/Variable/Adam_1:0' shape=(10,) dtype=float32_ref&gt;,
 &lt;tf.Variable 'critic/main_net/l1/weights:0' shape=(40, 90) dtype=float32_ref&gt;,
 &lt;tf.Variable 'critic/main_net/l1/biases:0' shape=(90,) dtype=float32_ref&gt;,
 &lt;tf.Variable 'critic/main_net/l2/weights:0' shape=(90, 60) dtype=float32_ref&gt;,
 &lt;tf.Variable 'critic/main_net/l2/biases:0' shape=(60,) dtype=float32_ref&gt;,
 &lt;tf.Variable 'critic/main_net/l3/weights:0' shape=(60, 30) dtype=float32_ref&gt;,
 &lt;tf.Variable 'critic/main_net/l3/biases:0' shape=(30,) dtype=float32_ref&gt;,
 &lt;tf.Variable 'critic/main_net/l4/kernel:0' shape=(30, 1) dtype=float32_ref&gt;,
 &lt;tf.Variable 'critic/main_net/l4/bias:0' shape=(1,) dtype=float32_ref&gt;,
 &lt;tf.Variable 'critic/target_net/l1/weights:0' shape=(40, 90) dtype=float32_ref&gt;,
 &lt;tf.Variable 'critic/target_net/l1/biases:0' shape=(90,) dtype=float32_ref&gt;,
 &lt;tf.Variable 'critic/target_net/l2/weights:0' shape=(90, 60) dtype=float32_ref&gt;,
 &lt;tf.Variable 'critic/target_net/l2/biases:0' shape=(60,) dtype=float32_ref&gt;,
 &lt;tf.Variable 'critic/target_net/l3/weights:0' shape=(60, 30) dtype=float32_ref&gt;,
 &lt;tf.Variable 'critic/target_net/l3/biases:0' shape=(30,) dtype=float32_ref&gt;,
 &lt;tf.Variable 'critic/target_net/l4/kernel:0' shape=(30, 1) dtype=float32_ref&gt;,
 &lt;tf.Variable 'critic/target_net/l4/bias:0' shape=(1,) dtype=float32_ref&gt;,
 &lt;tf.Variable 'beta1_power_1:0' shape=() dtype=float32_ref&gt;,
 &lt;tf.Variable 'beta2_power_1:0' shape=() dtype=float32_ref&gt;,
 &lt;tf.Variable 'critic/main_net/l1/weights/Adam:0' shape=(40, 90) dtype=float32_ref&gt;,
 &lt;tf.Variable 'critic/main_net/l1/weights/Adam_1:0' shape=(40, 90) dtype=float32_ref&gt;,
 &lt;tf.Variable 'critic/main_net/l1/biases/Adam:0' shape=(90,) dtype=float32_ref&gt;,
 &lt;tf.Variable 'critic/main_net/l1/biases/Adam_1:0' shape=(90,) dtype=float32_ref&gt;,
 &lt;tf.Variable 'critic/main_net/l2/weights/Adam:0' shape=(90, 60) dtype=float32_ref&gt;,
 &lt;tf.Variable 'critic/main_net/l2/weights/Adam_1:0' shape=(90, 60) dtype=float32_ref&gt;,
 &lt;tf.Variable 'critic/main_net/l2/biases/Adam:0' shape=(60,) dtype=float32_ref&gt;,
 &lt;tf.Variable 'critic/main_net/l2/biases/Adam_1:0' shape=(60,) dtype=float32_ref&gt;,
 &lt;tf.Variable 'critic/main_net/l3/weights/Adam:0' shape=(60, 30) dtype=float32_ref&gt;,
 &lt;tf.Variable 'critic/main_net/l3/weights/Adam_1:0' shape=(60, 30) dtype=float32_ref&gt;,
 &lt;tf.Variable 'critic/main_net/l3/biases/Adam:0' shape=(30,) dtype=float32_ref&gt;,
 &lt;tf.Variable 'critic/main_net/l3/biases/Adam_1:0' shape=(30,) dtype=float32_ref&gt;,
 &lt;tf.Variable 'critic/main_net/l4/kernel/Adam:0' shape=(30, 1) dtype=float32_ref&gt;,
 &lt;tf.Variable 'critic/main_net/l4/kernel/Adam_1:0' shape=(30, 1) dtype=float32_ref&gt;,
 &lt;tf.Variable 'critic/main_net/l4/bias/Adam:0' shape=(1,) dtype=float32_ref&gt;,
 &lt;tf.Variable 'critic/main_net/l4/bias/Adam_1:0' shape=(1,) dtype=float32_ref&gt;
</code></pre>

<p>The only difference in these two lists, is <code>&lt;tf.Variable 'Variable:0' shape=() dtype=int32_ref&gt;</code>, which I do not know what is this for and how it is generated. But, I do not think if it is the problem, since any of my models that can be restored also has it. </p>

<p>I appreciate any help and comment to resolve this error. </p>
",5228890,,5228890,,2018-09-20 19:02:17,2018-09-20 19:02:17,NotFoundError (see above for traceback): Key Variable not found in checkpoint,<python><tensorflow><restore><reinforcement-learning>,1,0,,,,CC BY-SA 4.0,
2114,65082648,1,65133173,,2020-11-30 23:30:09,,1,538,"<p>I am trying to train an agent to play <a href=""https://www.kaggle.com/c/connectx"" rel=""nofollow noreferrer"">Connect4</a> game. I found an example of how it can be trained. Representation of board is 1x6x7 array:</p>
<pre><code>[[[0 0 0 0 0 0 0]
  [0 0 0 0 0 0 0]
  [0 0 0 0 0 0 0]
  [0 0 0 0 0 0 0]
  [0 0 0 0 0 0 2]
  [0 0 0 0 0 0 1]]]
</code></pre>
<p>This neural network architecture is used:</p>
<pre><code>class Net(BaseFeaturesExtractor):
    def __init__(self, observation_space: gym.spaces.Box, features_dim: int = 256):
        super(Net, self).__init__(observation_space, features_dim)
        # We assume CxHxW images (channels first)
        # Re-ordering will be done by pre-preprocessing or wrapper
        n_input_channels = observation_space.shape[0]
        self.cnn = nn.Sequential(
            nn.Conv2d(n_input_channels, 32, kernel_size=3, stride=1, padding=1),
            nn.ReLU(),
            nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1),
            nn.ReLU(),
            nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=0),
            nn.ReLU(),
            nn.Flatten(),
        )

        # Compute shape by doing one forward pass
        with th.no_grad():
            n_flatten = self.cnn(th.as_tensor(observation_space.sample()[None]).float()).shape[1]

        self.linear = nn.Sequential(nn.Linear(n_flatten, features_dim), nn.ReLU())

    def forward(self, observations: th.Tensor) -&gt; th.Tensor:
        return self.linear(self.cnn(observations))
</code></pre>
<p>And it scored not so bad on a game with an agent 2 which moves randomly:</p>
<pre><code>Agent 1 Win Percentage: 0.59 
Agent 2 Win Percentage: 0.38 
Number of Invalid Plays by Agent 1: 3 
Number of Invalid Plays by Agent 2: 0
Number of Draws (in 100 game rounds): 0
</code></pre>
<p><a href=""https://www.kaggle.com/c/connectx/discussion/168246"" rel=""nofollow noreferrer"">Here</a> 3 layers representation was suggested as one of the ways how an agent can be improved:</p>
<p><a href=""https://i.stack.imgur.com/sDkQo.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/sDkQo.png"" alt=""enter image description here"" /></a></p>
<p>I have tried to implement it and this is the example of the new 3 layer representation of board:</p>
<pre><code>[[[0 0 0 0 0 0 0]
  [0 0 0 0 0 0 0]
  [0 0 0 0 0 0 0]
  [0 0 0 0 0 0 0]
  [0 0 0 0 0 0 0]
  [0 0 0 0 0 0 1]]

 [[0 0 0 0 0 0 0]
  [0 0 0 0 0 0 0]
  [0 0 0 0 0 0 0]
  [0 0 0 0 0 0 0]
  [0 0 0 0 0 0 1]
  [0 0 0 0 0 0 0]]

 [[0 0 0 0 0 0 0]
  [0 0 0 0 0 0 0]
  [0 0 0 0 0 0 0]
  [0 0 0 0 0 0 1]
  [0 0 0 0 0 0 0]
  [1 1 1 1 1 1 0]]]
</code></pre>
<p>When I run this with the current neural network architecture, the agent is not able to train appropriately:</p>
<pre><code>Agent 1 Win Percentage: 0.0
Agent 2 Win Percentage: 0.0
Number of Invalid Plays by Agent 1: 100
Number of Invalid Plays by Agent 2: 0
Number of Draws (in 100 game rounds): 0
</code></pre>
<p><a href=""https://github.com/VSZM/ConnectX/compare/master...JoeRakhimov:master"" rel=""nofollow noreferrer"">Here</a> you can see my code.</p>
<p>As you can see now I have 3 layers instead of one. That's why I have tried to use Conv3d:</p>
<pre><code>class Net(BaseFeaturesExtractor):
    def __init__(self, observation_space: gym.spaces.Box, features_dim: int = 256):
        super(Net, self).__init__(observation_space, features_dim)
        # We assume CxHxW images (channels first)
        # Re-ordering will be done by pre-preprocessing or wrapper
        n_input_channels = observation_space.shape[0]
        self.cnn = nn.Sequential(
            nn.Conv3d(n_input_channels, 32, kernel_size=3, stride=1, padding=1),
            nn.ReLU(),
            nn.Conv3d(32, 64, kernel_size=3, stride=1, padding=1),
            nn.ReLU(),
            nn.Conv3d(64, 128, kernel_size=3, stride=1, padding=0),
            nn.ReLU(),
            nn.Flatten(),
        )

        # Compute shape by doing one forward pass
        with th.no_grad():
            n_flatten = self.cnn(th.as_tensor(observation_space.sample()[None]).float()).shape[1]

        self.linear = nn.Sequential(nn.Linear(n_flatten, features_dim), nn.ReLU())
</code></pre>
<p>When I try to run this code, it is showing this error:</p>
<blockquote>
<p>RuntimeError: Expected 5-dimensional input for 5-dimensional weight
[32, 1, 3, 3, 3], but got 4-dimensional input of size [1, 3, 6, 7]
instead</p>
</blockquote>
<p>My question: how can I use Conv3D layer with 3x6x7 shaped input?</p>
",2255924,,2255924,,2020-12-03 00:31:12,2020-12-03 20:03:39,How to change Pytorch model to work with 3d input instead 2d input?,<pytorch><reinforcement-learning><conv-neural-network><stable-baselines>,1,1,0,,,CC BY-SA 4.0,
2119,67808779,1,67821479,,2021-06-02 16:09:02,,0,3379,"<p>So I am looking to train a model on colab using a GPU/TPU as my local machine doesn't have one. I am not bothered about visualising the training I just want colab to do the bulk of the work.</p>
<p>When importing my .ipynb into colab and running as soon as i attempt to make an env using any of the atari games i get the error:</p>
<pre><code>---------------------------------------------------------------------------
Exception                                 Traceback (most recent call last)
&lt;ipython-input-21-8d6f8581e7f9&gt; in &lt;module&gt;()
----&gt; 1 env = gym.make('SpaceInvaders-v0')
      2 height, width, channels = env.observation_space.shape
      3 actions = env.action_space.n
      4 
      5 model = build_model(height,width, channels,actions)

4 frames
/usr/local/lib/python3.7/dist-packages/atari_py/games.py in get_game_path(game_name)
     18     path = os.path.join(_games_dir, game_name) + &quot;.bin&quot;
     19     if not os.path.exists(path):
---&gt; 20         raise Exception('ROM is missing for %s, see https://github.com/openai/atari-py#roms for instructions' % (game_name,))
     21     return path
     22 

Exception: ROM is missing for space_invaders, see https://github.com/openai/atari-py#roms for instructions
</code></pre>
<p>Locally I had this issue and went through the steps on the github to fix it which worked but I cannot seem to get colab to do a similar thing. I have found tons of tutorials online where it just works and when I run their colabs I get the same issue so I am at a bit of a loss as to what to do.</p>
<p>Thanks in advance</p>
",16108131,,,,,2021-10-18 03:09:58,Running gym atari in google colab?,<python><google-colaboratory><reinforcement-learning>,2,0,,,,CC BY-SA 4.0,
2122,32825178,1,33351458,,2015-09-28 14:10:58,,3,726,"<p>I am confused about the difference between batch and growing batch q learning. Also, if I only have historical data, can I implement growing batch q learning?</p>

<p>Thank you!</p>
",5065711,,5065711,,2015-09-28 19:25:44,2015-10-26 17:03:20,Difference between batch q learning and growing batch q learning,<reinforcement-learning><q-learning>,1,0,0,,,CC BY-SA 3.0,
2126,67828549,1,67834113,,2021-06-03 20:52:49,,0,38,"<p>I'm trying to solve the OpenAI gym Breakout-V0 with a Deep Q-Network Agent.<br />
Every time when my agent reaches the point where:</p>
<ul>
<li>The replay_memory is filled enough to start training</li>
<li>The copy_target_network interval is reached for the first time</li>
<li>The target_network predicts for the fist time</li>
</ul>
<p>Tensorflow throws following error:</p>
<pre><code>Error when checking input: expected dense_3_input to have shape (33600,) but got array with shape (1,)
</code></pre>
<p>When I print the shape of the incoming <code>state</code> array just 1 line before i call the <code>predict(state)</code>, it confirms that the shape of <code>state</code> is <code>(33600,)</code><br />
Before this error is shown the model is able to <code>predict_on_batch()</code> inside the training loop with the exact same data (but batched)</p>
<p>Does anybody know how to solve this? I can gladly give more details and information if I'm missing any</p>
<p>Versions:<br />
Python 3.8.7<br />
TensorFlow 2.4.1<br />
Gym 0.18.0</p>
",9428346,,4685471,,2021-06-03 20:59:25,2021-06-04 08:39:51,"Tensorflow gives ""ValueError: Error when checking input""",<python><tensorflow><deep-learning><reinforcement-learning>,1,2,,2021-06-04 11:52:43,,CC BY-SA 4.0,
2128,36610290,1,54562773,,2016-04-13 21:54:48,,30,30904,"<p>I have recently been working on a project that uses a neural network for virtual robot control. I used tensorflow to code it up and it runs smoothly. So far, I used sequential simulations to evaluate how good the neural network is, however, I want to run several simulations <strong>in parallel</strong> to reduce the amount of time it takes to get data.</p>

<p>To do this I am importing python's <code>multiprocessing</code> package. Initially I was passing the sess variable (<code>sess=tf.Session()</code>) to a function that would run the simulation. However, once I get to any statement that uses this <code>sess</code> variable, the process quits without a warning. After searching around for a bit I found these two posts:
<a href=""https://stackoverflow.com/questions/34900246/tensorflow-passing-a-session-to-a-python-multiprocess"">Tensorflow: Passing a session to a python multiprocess</a>
and <a href=""https://stackoverflow.com/questions/33758669/running-multiple-tensorflow-sessions-concurrently"">Running multiple tensorflow sessions concurrently</a></p>

<p>While they are highly related I haven't been able to figure out how to make it work. I tried creating a session for each individual process and assigning the weights of the neural net to its trainable parameters without success. I've also tried saving the session into a file and then loading it within a process, but no luck there either.</p>

<p>Has someone been able to pass a session (or clones of sessions) to several processes?</p>

<p>Thanks.</p>
",3691859,,-1,,2017-05-23 10:29:30,2022-05-03 12:22:14,Tensorflow and Multiprocessing: Passing Sessions,<python><parallel-processing><multiprocessing><tensorflow><reinforcement-learning>,2,7,0,,,CC BY-SA 3.0,
2129,36648996,1,36672599,,2016-04-15 13:49:19,,3,1580,"<p>I took an RL course recently and I am writing a Q-learning controller for a power management application where I have continuous states and discrete actions. I am using a neural network (Q-network) for approximation the action values and selecting the maximum action value. Like any control system, I have certain constraints or bounds over variables that cannot be violated by the agent. Say, if my controller's (agent) actions are to discharge or charge a battery, the resultant energy cannot be less than 0 or more than the maximum capacity respectively. </p>

<p>I want to understand how do add such constraints in the action selection or value approximation routine? Two approaches come to mind</p>

<p>(1) Say I am running one episode for T steps. At every step, I input my current state to the Q-network and select the maximum action value. Upon taking this action, if my constraints are violated I can assign a huge negative reward, if not I can assign the associated reward. Eventually all the actions that get huge negative rewards (corresponding to undesirable behaviour), will be avoided thus the agent will operate within the model constraints. However, if I think from an optimazation point of view, such actions should NEVER be taken since they don't fall in the allowed region. So ideally, I should stop the iterations right there because there is all sequential actions will be unacceptable. This will cause a severe waste of data. </p>

<p>(2) Second, I feed my current state to the Q-network, select the action corresponding to max Q-value and check the constraint. IF violated, I go take the action corresponding to the second highest Q-value and repeat until my constraint(s) is satisfied. But will this ever lead to optimality?</p>

<p>I posit this might be a recurring problem while training autonomous control systems that involve constraints over multiple variables. Will be really glad to get your feedback!</p>
",4284161,,,,,2016-04-17 07:36:05,Adding constraints in Q-learning and assigning rewards if constraints are violated,<machine-learning><artificial-intelligence><dynamic-programming><reinforcement-learning><q-learning>,1,0,0,,,CC BY-SA 3.0,
2131,36822951,1,36823267,,2016-04-24 12:14:30,,2,1173,"<p>I am trying to understand <strong>Q-Learning</strong>, </p>

<hr>

<p>My current algorithm operates as follows:</p>

<p><strong>1.</strong> A lookup table is maintained that maps a state to information about its immediate reward and utility for each action available.</p>

<p><strong>2.</strong> At each state, check to see if it is contained in the lookup table and initialise it if not (With a default utility of 0).</p>

<p><strong>3.</strong> Choose an action to take with a probability of:</p>

<pre><code>    (*Ïµ* = 0&gt;Ïµ&gt;1 - probability of taking a random action)
    1-Ïµ = Choosing the state-action pair with the highest utility.
    Ïµ = Choosing a random move.
    Ïµ decreases over time.
</code></pre>

<p><strong>4.</strong> Update the current state's utility based on:</p>

<pre><code>    Q(st, at) += a[rt+1, + d.max(Q(st+1, a)) - Q(st,at)]
</code></pre>

<hr>

<p>I am currently playing my agent against a simple <strong>heuristic player</strong>, who always takes the move that will give it the best <em>immediate reward</em>. </p>

<p><strong>The results</strong> - The results are very poor,  even after a couple hundred games, the Q-Learning agent is losing <strong>a lot more</strong> than it is winning. Furthermore, the change in win-rate is almost non-existent, especially after reaching a couple hundred games.  </p>

<p>Am I missing something? I have implemented a couple agents: </p>

<p><em>(Rote-Learning, TD(0), TD(Lambda), Q-Learning)</em></p>

<p>But they all seem to be yielding similar, <em>disappointing</em>, results. </p>

<p><a href=""https://i.stack.imgur.com/4uvi1.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/4uvi1.png"" alt=""enter image description here""></a></p>
",6244839,,6244839,,2016-04-24 12:32:32,2016-04-24 13:35:55,Is this a correct implementation of Q-Learning for Checkers?,<machine-learning><pseudocode><agent><reinforcement-learning><q-learning>,1,1,,,,CC BY-SA 3.0,
2132,36812719,1,36813146,,2016-04-23 15:30:23,,1,316,"<p>I'm trying to understand <strong>Q-Learning</strong></p>

<p>The basic update formula:</p>

<pre><code>Q(st, at) += a[rt+1, + d.max(Q(st+1, a)) - Q(st,at)]
</code></pre>

<p>I understand the formula, and what it does, but my question is:</p>

<p><strong>How does the agent know to choose Q(st, at)?</strong> </p>

<p>I understand that an agent follows some policy Ï€, but how do you create this policy in the first place? </p>

<ul>
<li>My agents are playing checkers, so I am focusing on model-free algorithms.</li>
<li>All the agent knows is the current state it is in.</li>
<li>I understand that <em>when</em> it performs an action, you update the utility, but how does it know to take that action in the first place. </li>
</ul>

<p><strong>At the moment I have:</strong> </p>

<ul>
<li>Check each move you could make from that state.</li>
<li>Pick whichever move has the highest utility.</li>
<li>Update the utility of the move made.</li>
</ul>

<p>However, this doesnt really solve much, you still get stuck in local minimum/maximums. </p>

<p>So, just to round things off, my main question is:</p>

<p>How, for an agent that knows nothing and is using a model-free algorithm, do you generate an initial policy, so it know which action to take?</p>
",6244839,,445131,,2016-04-23 19:47:26,2016-04-23 19:47:26,Reinforcement Learning - How does an Agent know which action to pick?,<machine-learning><policy><agent><reinforcement-learning><q-learning>,1,1,0,,,CC BY-SA 3.0,
2133,36850302,1,36858619,,2016-04-25 20:20:34,,3,1035,"<p>I'm trying to come up with a better representation for the state of a 2-d grid world for a Q-learning algorithm which utilizes a neural network for the Q-function.</p>

<p>In the tutorial, <a href=""http://outlace.com/rlpart3.html"" rel=""nofollow noreferrer"">Q-learning with Neural Networks</a>, the grid is represented as a 3-d array of integers (0 or 1). The first and second dimensions represent the position of an object in the grid world.  The third dimension encodes which object it is.</p>

<p>So, for a 4x4 grid with 4 objects in it, you would represent the state with a 3-d array with 64 elements in it (4x4x4). This means that the meural network would have 64 nodes in the input layer so it could accept the state of the grid world as input.  </p>

<p>I want to reduce the number of nodes in the Neural Network so that training does not take as long. So, can you represent the grid world as 2-d array of doubles instead?</p>

<p>I tried to represent a 4x4 grid world as a 2-d array of doubles and used different values to represent different objects. For example, I used 0.1 to represent the player and 0.4 to represent the goal. However, when I implemented this the algorithm stopped learning at all.</p>

<p>Right now I think my problem might be that I need to change which activation functions I'm using in my layers. I'm presently using the hyperbolic tangent activation function. My inputs values range from (0 - 1). My output values range from (-1 to 1). I've also tried the sigmoid function.</p>

<p>I realize this is a complex problem to be asking a question about. Any suggestion as to architecture of the network would be appreciated.</p>

<p><strong>UPDATE</strong></p>

<p>There are three variants to the game:
1. The world is static. All objects start in the same place.
2. The player starting position is random. All other objects stay the same.
3. Each grid is totally random.</p>

<p>With more testing I discovered I can complete the first two variants with my 2d array representation. So I think my network architecture might be fine. What I discovered is that my network is now extraordinarily susceptible to catastrophic forgetting (much more so than when I was using the 3d array).  I have to use ""experience replay"" to make it learn, but even then I still can't complete the third variant.  I'll keep trying.  I'm rather shocked how much of a difference changing the grid world representation made.  It hasn't improved performance at all.</p>
",901304,,1150698,,2017-07-10 04:50:40,2017-07-10 10:16:01,Grid World representation for a neural network,<neural-network><reinforcement-learning><q-learning>,1,1,0,,,CC BY-SA 3.0,
2134,36956906,1,37026950,,2016-04-30 15:43:50,,2,225,"<p>In every formalism of GTD(&#955;) seems to define it in terms of function approximation, using &#952; and some weight vector w.</p>

<p>I understand that the need for gradient methods widely came from their convergence properties for linear function approximators, but I would like to make use of GTD for the importance sampling.</p>

<p>Is it possible to take advantage of GTD without function approximation? If so, how are the update equations formalized?</p>
",2844792,,,,,2016-05-04 11:49:20,Gradient Temporal Difference Lambda without Function Approximation,<machine-learning><reinforcement-learning><temporal-difference>,1,0,0,,,CC BY-SA 3.0,
2135,70792122,1,70807448,,2022-01-20 19:11:18,,0,49,"<p>I need help with understanding the shaping theorem for MDPs. Here's the relevant paper: <a href=""https://people.eecs.berkeley.edu/%7Epabbeel/cs287-fa09/readings/NgHaradaRussell-shaping-ICML1999.pdf"" rel=""nofollow noreferrer"">https://people.eecs.berkeley.edu/~pabbeel/cs287-fa09/readings/NgHaradaRussell-shaping-ICML1999.pdf</a> it basically says that a markov decision process that has some reward function on transitions between states and actions R(s, a, s') has the same optimal policy as a different markov decision process with it's reward defined as R'(s, a, s') = R(s, a, s') + gamma*f(s') - f(s), where gamma is the time-discount-rate.</p>
<p>I understand the proof, but it seems like a trivial case where it breaks down is when R(s, a, s') = 0 for all states and actions, and the agent is faced with the path A -&gt; s -&gt; B versus A -&gt; r -&gt; t -&gt; B. With the original markov process we get an EV of 0 for both paths, so both paths are optimal. But with the potential added to each transition we get, gamma^2*f(B)-f(A) for the first path, and gamma^3*f(B) - f(A) for the second. So if gamma &lt; 1, and 0 &lt; f(B), f(A), then the second path is no longer optimal.</p>
<p>Am I misunderstanding the theorem, or am I making some other mistake?</p>
",6855956,,,,,2022-01-21 20:43:10,Shaping theorem for MDPs,<reinforcement-learning><markov-decision-process>,1,3,,2022-02-10 03:12:16,,CC BY-SA 4.0,
2139,37023460,1,37026903,,2016-05-04 09:18:52,,8,3110,"<p>I'm transitioning from discretization of a continuous state space to function approximation. My action and state space(3D) are both continuous. My problem suffers majorly from errors due to aliasing and nearly no convergene after training for a long time. Also I just cannot figure out how to choose the right step size for discretization. </p>

<p>Reading Sutton &amp; Barto helped me understand the power of tile coding i.e having the state space described by multiple offested tilings overlapping each other. Given a continuous query/state, it is discribed by N basis functions, each corresponding to a single block/square of the criss-cross tilings it belongs to.</p>

<p>1) How is the performance different from going for a highly discretized state space? </p>

<p>2) Can anyone please point me to a working example of tile coding in python? I am learning too many things at the same time and getting super confused! (Q learning, discretization dilemma, tile coding, function approximation and handling the problem itself)</p>

<p>There doesn't seem to be any exhaustive Python coding tutorials for continuous problems in RL. </p>
",4284161,,,,,2020-09-06 06:23:16,Function Approximation: How is tile coding different from highly discretized state space?,<python><machine-learning><artificial-intelligence><reinforcement-learning>,2,2,0,,,CC BY-SA 3.0,
2143,70826765,1,70826903,,2022-01-23 21:33:32,,0,52,"<p>Below is the sample code for simulation of atari games:</p>
<pre class=""lang-py prettyprint-override""><code>import numpy as np
import matplotlib.pyplot as plt
import gym
import time
env =gym.make('BreakoutNoFrameskip-v4')
print(&quot;Observation Space :&quot;,env.observation_space)
print(&quot;Action Space :&quot;,env.action_space)
env.reset()
for i in range(1000):
    action =env.action_space.sample()
    obs,reward,done,info =env.step(action)
    env.render()
    time.sleep(0.01)
    if done :
        env.reset()
env.close()
plt.show()
</code></pre>
<p>The question:</p>
<ul>
<li>Is it possible to create a simple video from the render? So my question is whether it is possible to convert render to mp4 format?</li>
</ul>
",,user466534,6632744,,2022-01-24 03:01:29,2022-01-24 03:01:29,Convert render to small video in Reinforcement learning,<python><mp4><reinforcement-learning>,1,0,,,,CC BY-SA 4.0,
2144,36994129,1,37001965,,2016-05-03 01:23:25,,0,279,"<p>Is there any algorithm for solving a finite-horizon semi-Markov-Decision-Process?</p>

<p>I want to find the optimal policy for a sequential decision problem with a finite action space, a finite state space, and a deadline. Critically, different actions take different amounts of time and for one of the actions this duration is stochastic. I can model time as being discrete or continuous depending on which methods are available.</p>

<p>I am aware of algorithms for discounted infinite-horizon semi-MDPs, but I cannot find any work on finite-horizon semi-MDPs. Has this class of problems been studied before? </p>
",2880800,,2880800,,2016-05-03 01:52:00,2016-05-03 10:42:22,Continuous-time finite-horizon MDP,<dynamic-programming><markov-chains><reinforcement-learning><markov-models><control-theory>,1,0,,,,CC BY-SA 3.0,
2145,61205082,1,61263414,,2020-04-14 10:00:52,,1,419,"<p>Hello I am getting an error when I am trying to run my model</p>

<ul>
<li>I am using tf2.1 and I have made a class for my model due to a few reasons</li>
<li>My model has two output layers called advantage and value and this is because I am making a duelling deep q networks.</li>
</ul>

<p>Here is my <code>__init__</code> method - </p>

<pre class=""lang-py prettyprint-override""><code>class model(Model):
    def __init__(self):
        super(model, self).__init__()
        self.lr = 0.01
        self.conv1 = Conv2D(filters=32, input_shape=(210, 160, 1), kernel_size=(3, 3), strides=1, padding='same', activation='elu')#(self.inp)

        self.conv2 = Conv2D(filters=32, kernel_size=(3, 3), strides=1, padding='same', activation='elu')#(self.conv1)
        self.mp2 = MaxPool2D(pool_size=(3, 3), strides=1, padding='same')#(self.conv2)

        self.conv3 = Conv2D(filters=64, kernel_size=(3, 3), strides=1, padding='same', activation='elu')#(self.mp2)
        self.mp3 = MaxPool2D(pool_size=(3, 3), strides=1, padding='same')#(self.conv3)

        self.conv4 = Conv2D(filters=64, kernel_size=(3, 3), strides=1, padding='same', activation='elu')#(self.mp3)
        self.mp4 = MaxPool2D(pool_size=(3, 3), strides=1, padding='same')#(self.conv4)

        self.flat = Flatten() #(self.mp6)
        self.value = Dense(1, activation=None)#(self.flat) # how good is a particular state
        self.advantage = Dense(env.action_space.n, activation=None)#(self.flat) # which is best action
        self.compile(optimizer=Adam(lr=self.lr), loss='mse', metrics=['accuracy'])
</code></pre>

<p>Then I have a function which is called <code>predict_advantage</code> where I am getting an error -</p>

<pre class=""lang-py prettyprint-override""><code>def predict_advantage(self, state):
        state = tf.cast(cv2.cvtColor(state, cv2.COLOR_RGB2GRAY), tf.float32)
        #x = self.inp(state)
        x = self.conv1(x)

        x=self.conv2(x)
        x=self.mp2(x)

        x=self.conv3(x)
        x=self.mp3(x)

        x=self.conv4(x)
        x=self.mp4(x)

        x = self.flat(x)
        # value = self.value(x)
        x = self.advantage(x)
        return x
</code></pre>

<ul>
<li>As you see I am using <code>tf.cast</code> to cast to make the dtype float32 as most posts were saying it is the only way to fix the error -
However I got the same the very same error as I got before I used that - </li>
</ul>

<pre class=""lang-py prettyprint-override""><code>tensorflow.python.framework.errors_impl.NotFoundError: Could not find valid device for node.
Node:{{node MatMul}}
</code></pre>

<p>And by the way it also printed out the device and dtypes for some particular layer or all the layers. I do not know of what it did but here it is -</p>

<pre class=""lang-py prettyprint-override""><code>All kernels registered for op MatMul :
  device='GPU'; T in [DT_FLOAT]
  device='GPU'; T in [DT_DOUBLE]
  device='GPU'; T in [DT_COMPLEX64]
  device='GPU'; T in [DT_COMPLEX128]
  device='GPU'; T in [DT_HALF]
  device='CPU'; label='eigen'; T in [DT_FLOAT]
  device='CPU'; label='eigen'; T in [DT_DOUBLE]
  ..........
  ..........
  ..........
  device='CPU'; T in [DT_COMPLEX64]
  device='CPU'; T in [DT_COMPLEX128]
  device='GPU'; label='cublas'; T in [DT_FLOAT]
  device='GPU'; label='cublas'; T in [DT_DOUBLE]
  device='GPU'; label='cublas'; T in [DT_COMPLEX64]
  device='GPU'; label='cublas'; T in [DT_COMPLEX128]
  device='GPU'; label='cublas'; T in [DT_HALF]
 [Op:MatMul] name: dense_1/Tensordot/MatMul/
</code></pre>

<ul>
<li><strong><em>As we see here we are having some parameters on gpu and some parameters on cpu.</em></strong> Why is it doing that? </li>
<li><strong><em>Also the dtypes of the parameters is different. I am not sure if they can be or they should not be.</em></strong> </li>
</ul>

<p>As far as I know I think it errored me out as things on gpu cannot interact with things on cpu. So why is it keeping my parameters on different devices.</p>

<p><strong>Edit:</strong></p>

<p>Here is a link to the full code - 
<a href=""https://pastebin.com/sd8L2xAM"" rel=""nofollow noreferrer"">https://pastebin.com/sd8L2xAM</a>
Here is also the full error I got if you want to find at what line it is occuring - <a href=""https://pastebin.com/C9Dy5NxL"" rel=""nofollow noreferrer"">https://pastebin.com/C9Dy5NxL</a></p>
",13218494,,13218494,,2020-04-16 11:08:32,2020-04-17 02:57:42,Tensorflow cannot find valid device for node. even after casting to float32,<tensorflow><machine-learning><reinforcement-learning>,1,8,0,,,CC BY-SA 4.0,
2146,61215966,1,61245117,,2020-04-14 19:47:48,,1,440,"<p>Using this code:</p>

<pre><code>import gym
import numpy as np
import time

""""""
SARSA on policy learning python implementation.
This is a python implementation of the SARSA algorithm in the Sutton and Barto's book on
RL. It's called SARSA because - (state, action, reward, state, action). The only difference
between SARSA and Qlearning is that SARSA takes the next action based on the current policy
while qlearning takes the action with maximum utility of next state.
Using the simplest gym environment for brevity: https://gym.openai.com/envs/FrozenLake-v0/
""""""

def init_q(s, a, type=""ones""):
    """"""
    @param s the number of states
    @param a the number of actions
    @param type random, ones or zeros for the initialization
    """"""
    if type == ""ones"":
        return np.ones((s, a))
    elif type == ""random"":
        return np.random.random((s, a))
    elif type == ""zeros"":
        return np.zeros((s, a))


def epsilon_greedy(Q, epsilon, n_actions, s, train=False):
    """"""
    @param Q Q values state x action -&gt; value
    @param epsilon for exploration
    @param s number of states
    @param train if true then no random actions selected
    """"""
    if train or np.random.rand() &lt; epsilon:
        action = np.argmax(Q[s, :])
    else:
        action = np.random.randint(0, n_actions)
    return action

def sarsa(alpha, gamma, epsilon, episodes, max_steps, n_tests, render = True, test=False):
    """"""
    @param alpha learning rate
    @param gamma decay factor
    @param epsilon for exploration
    @param max_steps for max step in each episode
    @param n_tests number of test episodes
    """"""
    env = gym.make('Taxi-v3')
    n_states, n_actions = env.observation_space.n, env.action_space.n
    Q = init_q(n_states, n_actions, type=""ones"")
    print('Q shape:' , Q.shape)

    timestep_reward = []
    for episode in range(episodes):
        print(f""Episode: {episode}"")
        total_reward = 0
        s = env.reset()
        print('s:' , s)
        a = epsilon_greedy(Q, epsilon, n_actions, s)
        t = 0
        done = False
        while t &lt; max_steps:
            if render:
                env.render()
            t += 1
            s_, reward, done, info = env.step(a)
            total_reward += reward
            a_ = epsilon_greedy(Q, epsilon, n_actions, s_)
            if done:
                Q[s, a] += alpha * ( reward  - Q[s, a] )
            else:
                Q[s, a] += alpha * ( reward + (gamma * Q[s_, a_] ) - Q[s, a] )
            s, a = s_, a_
            if done:
                if render:
                    print(f""This episode took {t} timesteps and reward {total_reward}"")
                timestep_reward.append(total_reward)
                break
#             print('Updated Q values:' , Q)
    if render:
        print(f""Here are the Q values:\n{Q}\nTesting now:"")
    if test:
        test_agent(Q, env, n_tests, n_actions)
    return timestep_reward

def test_agent(Q, env, n_tests, n_actions, delay=0.1):
    for test in range(n_tests):
        print(f""Test #{test}"")
        s = env.reset()
        done = False
        epsilon = 0
        total_reward = 0
        while True:
            time.sleep(delay)
            env.render()
            a = epsilon_greedy(Q, epsilon, n_actions, s, train=True)
            print(f""Chose action {a} for state {s}"")
            s, reward, done, info = env.step(a)
            total_reward += reward
            if done:  
                print(f""Episode reward: {total_reward}"")
                time.sleep(1)
                break


if __name__ ==""__main__"":
    alpha = 0.4
    gamma = 0.999
    epsilon = 0.9
    episodes = 200
    max_steps = 20
    n_tests = 20
    timestep_reward = sarsa(alpha, gamma, epsilon, episodes, max_steps, n_tests)
    print(timestep_reward)
</code></pre>

<p>from : </p>

<p><a href=""https://towardsdatascience.com/reinforcement-learning-temporal-difference-sarsa-q-learning-expected-sarsa-on-python-9fecfda7467e"" rel=""nofollow noreferrer"">https://towardsdatascience.com/reinforcement-learning-temporal-difference-sarsa-q-learning-expected-sarsa-on-python-9fecfda7467e</a></p>

<p>A sample Q table generated is :</p>

<pre><code>[[ 1.          1.          1.          1.          1.          1.        ]
 [ 0.5996      0.5996      0.5996      0.35936     0.5996      1.        ]
 [ 0.19936016  0.35936     0.10336026  0.35936     0.35936    -5.56063984]
 ...
 [ 0.35936     0.5996      0.35936     0.5996      1.          1.        ]
 [ 1.          0.5996      1.          1.          1.          1.        ]
 [ 0.35936     0.5996      1.          1.          1.          1.        ]]
</code></pre>

<p>The columns representing the actions and rows representing the corresponding states.</p>

<p>Can the state be represented by a vector ? The Q table cells are not contained by vectors of size > 1 so how should these states be represented ? For example if I'm in state [2] can this be represented as an n dimensional vector ?</p>

<p>Put another way if Q[1,3] = 4 can the Q state 1 with action 3 be represented as vector[1,3,2,12,3] ? If so then is the state_number->state_attributes mapping stored in a separate lookup table ?</p>
",470184,,470184,,2020-04-15 10:47:53,2020-04-16 07:47:06,How are n dimensional vectors state vectors represented in Q Learning?,<reinforcement-learning><q-learning>,1,0,,,,CC BY-SA 4.0,
2147,61231333,1,61231512,,2020-04-15 14:31:10,,0,32,"<p>How many states could I work with on my ordinary home computer when I want to implement a reinforcement learning algorithm such as Q-Learning? 1 thousand, 1 million, more?</p>
",6140139,,3924118,,2020-04-15 17:18:17,2020-04-15 17:18:17,How many states could I work with on my ordinary home computer when using Q-learning?,<machine-learning><reinforcement-learning><q-learning>,1,3,,,,CC BY-SA 4.0,
2148,43426409,1,43428900,,2017-04-15 13:07:12,,1,968,"<p>I want to implement Q-Learning for the Chrome dinosaur game (the one you can play when you are offline).</p>

<p>I defined my state as: distance to next obstacle, speed and the size of the next obstacle.</p>

<p>For the reward I wanted to use the number of successfully passed obstacles, but it could happen that the same state has different immediate rewards. The same type of obstacle could reappear later in the game, but the reward for passing it would be higher because more obstacles have already been passed.</p>

<p>My question now is: Is this a problem or would Q-Learning still work? If not is there a better way?</p>
",5166930,,1497463,,2017-04-15 17:26:31,2017-04-15 17:26:31,Different rewards for same state in reinforcement learning,<machine-learning><reinforcement-learning><q-learning>,1,1,,,,CC BY-SA 3.0,
2152,61495319,1,61590042,,2020-04-29 06:10:07,,1,128,"<p>I'm trying to learn RL and tensorflow.
Unfortunately there is a problem with the code that I can't figure out how to solve.
The following call fails:</p>

<p><code>train_loss, _, train_summary = session.run([loss, opt, all_summary], feed_dict={x_ph: X, y_ph: y})</code> </p>

<p>I get the following error:</p>

<blockquote>
  <p>TypeError: Fetch argument None has invalid type class 'NoneType'</p>
</blockquote>

<p>I'm using windows 10 OS.</p>

<p>What am I doing wrong?</p>

<p>I really need some help, thanks.</p>

<p>Here is the complete code:</p>

<pre><code>import tensorflow as tf
import tensorflow.compat.v1 as tfc
import numpy as np
from datetime import datetime

np.random.seed(10)
tfc.set_random_seed(10)

# Line: y = W*X + b
W, b = 0.5, 1.4

# 100 item data sample set
X = np.linspace(0, 100, num=100)
# add random noise to y
y = np.random.normal(loc=W * X + b, scale=2.0, size=len(X))

# Tensorflow
gr = tf.Graph()
with gr.as_default():
    x_ph = tfc.placeholder(shape=[None, ], dtype=tf.float32)
    y_ph = tfc.placeholder(shape=[None, ], dtype=tf.float32)

    v_weight = tfc.get_variable(""weight"", shape=[1], dtype=tf.float32)
    v_bias = tfc.get_variable(""bias"", shape=[1], dtype=tf.float32)

    # Line computation
    out = v_weight * x_ph + v_bias
    # compute mean squared error
    loss = tf.reduce_mean((out - y_ph) ** 2)
    # minimize MSE loss
    opt = tfc.train.AdamOptimizer(0.4).minimize(loss)

    tf.summary.scalar('MSEloss', loss)
    tf.summary.histogram('model_weight', v_weight)
    tf.summary.histogram('model_bias', v_bias)

    # merge summary
    all_summary = tfc.summary.merge_all()

    # log summary to file
    now = datetime.now()
    clock_time = f'{now.day}_{now.hour}.{now.minute}.{now.second}'
    file_writer = tfc.summary.FileWriter('log_dir\\' + clock_time, tfc.get_default_graph())

    # create session
    session = tfc.Session(graph=gr)
    session.run(tfc.global_variables_initializer())

    # loop to train the parameters
    for ep in range(210):
        # run optimizer
        train_loss, _, train_summary = session.run([loss, opt, all_summary], feed_dict={x_ph: X, y_ph: y})
        file_writer.add_summary(train_summary, ep)

        # print epoch and loss
        if ep % 40 == 0:
            print(f'Epoch: {ep}'.ljust(13) + f'MSE: {train_loss:.4f}'.ljust(16) + f'W: {session.run(v_weight)[0]:.3f}'.ljust(11) + f'b: {session.run(v_bias)[0]:.3f}')

    print(f'Final weight: {session.run(v_weight)[0]:.3f},  bias: {session.run(v_bias)[0]:.3f}')
    file_writer.close()

session.close()

</code></pre>
",13425196,,12411536,,2020-04-29 07:40:25,2020-05-04 10:25:28,Tensorflow session.run TypeError,<python><tensorflow><reinforcement-learning>,1,0,,,,CC BY-SA 4.0,
2154,40831896,1,40850664,,2016-11-27 17:41:18,,3,1647,"<p>One way to improve stability in deep Q-learning tasks is to maintain a set of target weights for the network that update slowly and are used for calculating Q-value targets. As a result at different times in the learning procedure, two different sets of weights are used in the forward pass. For normal DQN this is not difficult to implement, as the weights are tensorflow variables that can be set in a feed_dict ie:</p>

<pre><code>sess = tf.Session()
input = tf.placeholder(tf.float32, shape=[None, 5])
weights = tf.Variable(tf.random_normal(shape=[5,4], stddev=0.1)
bias = tf.Variable(tf.constant(0.1, shape=[4])
output = tf.matmul(input, weights) + bias
target = tf.placeholder(tf.float32, [None, 4])
loss = ...

...

#Here we explicitly set weights to be the slowly updated target weights
sess.run(output, feed_dict={input: states, weights: target_weights, bias: target_bias})

# Targets for the learning procedure are computed using this output.

....

#Now we run the learning procedure, using the most up to date weights,
#as well as the previously computed targets
sess.run(loss, feed_dict={input: states, target: targets})
</code></pre>

<p>I'd like to use this target network technique in a recurrent version of DQN, but I don't know how to access and set the weights used inside a recurrent cell. Specifically I'm using a tf.nn.rnn_cell.BasicLSTMCell, but I'd like to know how to do this for any type of recurrent cell.</p>
",4938706,,4938706,,2016-11-27 17:54:26,2018-07-31 17:53:27,How can I access the weights of a recurrent cell in Tensorflow?,<python><machine-learning><tensorflow><reinforcement-learning>,2,0,,,,CC BY-SA 3.0,
2155,7507004,1,7507094,,2011-09-21 21:39:32,,3,2104,"<p>I have an AI that is good at playing Connect 4 (using minimax). Now I want to use some machine learning algorithm to learn from this AI that I have, and I would like to do that by just letting them play against each other.</p>

<p>What algorithm would be good for this, and how would I train it? If someone could just name a way of doing this I can easily Google it by my self. But right now I don't know what to Google...</p>
",957738,,707111,,2011-09-21 21:42:54,2011-09-22 17:18:28,What machine learning algorithm should I use for Connect 4?,<artificial-intelligence><machine-learning><neural-network><reinforcement-learning>,2,0,,,,CC BY-SA 3.0,
2157,40827905,1,40828943,,2016-11-27 10:29:04,,1,1117,"<p>The <a href=""https://arxiv.org/pdf/1511.06342v4.pdf"" rel=""nofollow noreferrer"">Actor Mimic</a> paper talks about implementing an action-masking procedure. I quote</p>

<blockquote>
  <p>While playing a certain game, we
  mask out AMN action outputs that are not valid for that game and take the softmax over only the subset of valid actions</p>
</blockquote>

<p>Does anyone have an idea about how this action masking can be implemented in say Tensorflow? In specifc, how would one take a softmax only over specified subset of actions?</p>
",3654968,,,,,2018-08-29 08:55:06,How does one implement action masking?,<tensorflow><multitasking><reinforcement-learning>,2,0,,,,CC BY-SA 3.0,
2164,43728781,1,43755179,,2017-05-02 01:35:35,,8,5191,"<p>In reinforcement learning, I'm trying to understand the difference between policy iteration, and value iteration. There are some general answers to this out there, but I have two specific queries which I cannot find an answer to.</p>

<p>1) I have heard that policy iteration ""works forwards"", whereas value iteration ""works backwards"". What does this mean? I thought that both methods just take each state, then look at all the other states it can reach, and compute the value from this -- either by marginalising over the policy's action distribution (policy iteration) or by taking that argmax with regards to the action values (value iteration). So why is there any notion of the ""direction"" in which each method ""moves""?</p>

<p>2) Policy iteration requires an iterative process during policy evaluation, to find the value function -- by However, value iteration just requires one step. Why is this different? Why does value iteration converge in just one step?</p>

<p>Thank you!</p>
",3320135,,,,,2017-05-04 01:04:25,Policy Iteration vs Value Iteration,<machine-learning><reinforcement-learning>,2,0,0,,,CC BY-SA 3.0,
2168,59448001,1,62117589,,2019-12-22 21:02:04,,1,789,"<p>I'm not sure how to get the Q Values for a DDQN. </p>

<p>DQN is the normal network, TAR the target network.</p>

<pre><code>    q_values = self.DQN.predict(c_states) # DQN batch predict Q on states
    dqn_next = self.DQN.predict(n_states) # DQN batch predict Q on next_states
    tar_next = self.TAR.predict(n_states) # TAR batch predict Q on next_states
</code></pre>

<p>I mainly found 2 versions:</p>

<p><strong>Version 1:</strong></p>

<pre><code>q_values[i][actions[i]] = (rewards[i] + (GAMMA * np.amax(tar_next[i])))
</code></pre>

<p><strong>Version 2:</strong></p>

<pre><code>act = np.argmax(dqn_next[i])
q_values[i][actions[i]] = (rewards[i] + (GAMMA * tar_next[i][act]))
</code></pre>

<p>Which one is correct? And why?</p>

<p><strong>Version 1 Links:</strong></p>

<p><a href=""https://github.com/keon/deep-q-learning/blob/master/ddqn.py"" rel=""nofollow noreferrer"">https://github.com/keon/deep-q-learning/blob/master/ddqn.py</a></p>

<p><a href=""https://pythonprogramming.net/training-deep-q-learning-dqn-reinforcement-learning-python-tutorial"" rel=""nofollow noreferrer"">https://pythonprogramming.net/training-deep-q-learning-dqn-reinforcement-learning-python-tutorial</a></p>

<p><strong>Version 2 Links:</strong></p>

<p><a href=""https://github.com/germain-hug/Deep-RL-Keras/blob/master/DDQN/ddqn.py"" rel=""nofollow noreferrer"">https://github.com/germain-hug/Deep-RL-Keras/blob/master/DDQN/ddqn.py</a></p>

<p><a href=""https://github.com/rlcode/reinforcement-learning/blob/master/2-cartpole/2-double-dqn/cartpole_ddqn.py"" rel=""nofollow noreferrer"">https://github.com/rlcode/reinforcement-learning/blob/master/2-cartpole/2-double-dqn/cartpole_ddqn.py</a></p>

<p><a href=""https://jaromiru.com/2016/11/07/lets-make-a-dqn-double-learning-and-prioritized-experience-replay/"" rel=""nofollow noreferrer"">https://jaromiru.com/2016/11/07/lets-make-a-dqn-double-learning-and-prioritized-experience-replay/</a></p>

<hr>

<p><strong>EDIT:</strong>
Many thanks, to clarify this</p>

<pre><code>Q-learning: 
q_values[i][actions[i]] = (rewards[i] + (GAMMA * np.amax(tar_next[i])))

SARSA: 
act = np.argmax(dqn_next[i])
q_values[i][actions[i]] = (rewards[i] + (GAMMA * tar_next[i][act]))
</code></pre>

<p><strong>EDIT: re-open 03/2020</strong></p>

<p>I'm sorry but i have to re-open that question. Maybe I misunderstood something, but the following sources show that my Version 2 (SARSA) is Double Q Learning?</p>

<p><strong>Page 158 : Double Q-learning</strong>
<a href=""http://incompleteideas.net/book/RLbook2018.pdf"" rel=""nofollow noreferrer"">http://incompleteideas.net/book/RLbook2018.pdf</a></p>

<p><a href=""https://adventuresinmachinelearning.com/double-q-reinforcement-learning-in-tensorflow-2/"" rel=""nofollow noreferrer"">adventuresinML</a></p>

<p><a href=""https://github.com/adventuresinML/adventures-in-ml-code/blob/e661eeb5db86d2d0aa21621b68b5186d80e3d8b6/double_q_tensorflow2.py#L86"" rel=""nofollow noreferrer"">adventuresinML source</a></p>
",11122466,,11122466,,2020-03-25 20:49:32,2020-06-03 15:57:21,How to get Q Values in RL - DDQN,<python><deep-learning><neural-network><reinforcement-learning>,2,0,0,,,CC BY-SA 4.0,
2169,41242329,1,41288267,,2016-12-20 12:25:23,,1,247,"<p>I am implementing a SARSA(lambda) model in C++ to overcome some of the limitations (the sheer amount of time and space DP models require) of DP models, which hopefully will reduce the computation time (takes quite a few hours atm for similar research) and less space will allow adding more complexion to the model.</p>

<p>We do have explicit transition probabilities, and they do make a difference. So how should we incorporate them in a SARSA model? </p>

<p>Simply select the next state according to the probabilities themselves? Apparently SARSA models don't exactly expect you to use probabilities - or perhaps I've been reading the wrong books.</p>

<p>PS- Is there a way of knowing if the algorithm is properly implemented? First time working with SARSA.</p>
",4218673,,157726,,2018-12-23 23:04:53,2018-12-23 23:04:53,Incorporating Transition Probabilities in SARSA,<machine-learning><reinforcement-learning><sarsa>,2,13,,,,CC BY-SA 3.0,
2171,61805090,1,61839363,,2020-05-14 18:45:55,,2,467,"<p>I am trying to apply one idea proposed by Rusu et al. in <a href=""https://arxiv.org/pdf/1511.06295.pdf"" rel=""nofollow noreferrer"">https://arxiv.org/pdf/1511.06295.pdf</a>, which consists in training a NN changing the output layer according to the class of the input, i.e., provided that we know the id of the input, we would pick the corresponding output layer. This way, all the hidden layers would be trained with all the data, but each output layer would only be trained with its corresponding type of input data.</p>

<p>This is meant to achieve good results in a transfer learning framework.</p>

<p>How can I implement this ""change of the last layer"" in tensorflow 2.0?</p>
",5458054,,,,,2020-05-16 15:41:23,Transfer Learning - How can I change only the output layer in TensorFlow?,<python><tensorflow2.0><reinforcement-learning><transfer-learning>,1,0,,,,CC BY-SA 4.0,
2174,23078806,1,23079298,,2014-04-15 08:50:58,,5,4312,"<p>I am using rlglue based <a href=""https://github.com/amarack/python-rl"" rel=""noreferrer"">python-rl</a> framework for q-learning.
My understanding is that over number of episodes, the algorithm converges to an optimal policy (which is a mapping which says what action to take in what state).</p>

<p>Question1: Does this mean that after a number of episodes ( say 1000 or more ) I should essentially get the same state:action mapping? </p>

<p>When I plot the rewards (or rewards averaged over 100 episodes) I get a graph similar to Fig 6.13 in <a href=""http://webdocs.cs.ualberta.ca/~sutton/book/ebook/node65.html"" rel=""noreferrer"">this link</a>. </p>

<p>Question2: If the algorithm has converged to some policy why does the rewards fall down? Is there a possibility that the rewards vary drastically? </p>

<p>Question3: Is there some standard method which I can use to compare the results of various RL algorithms?</p>
",1122470,,,,,2019-03-11 10:54:04,Q-Learning convergence to optimal policy,<reinforcement-learning><q-learning>,1,1,0,,,CC BY-SA 3.0,
2177,41459498,1,41508818,,2017-01-04 08:43:25,,0,722,"<p>In Q-learning algorithm, the selection of an action depends on the current state and the values of the Q-matrix. I want to know if these Q-values are updated only during the exploration step or they change also in the exploitation step.</p>
",7373191,,,,,2017-01-06 15:28:50,exploration and exploitation in Q-learning,<reinforcement-learning><q-learning>,1,0,,,,CC BY-SA 3.0,
2178,59668333,1,59668622,,2020-01-09 16:29:28,,-1,198,"<p>I'm learning about Distributional RL from 'Deep Reinforcement Learning Hands On' <a href=""https://github.com/PacktPublishing/Deep-Reinforcement-Learning-Hands-On/blob/master/Chapter07/07_dqn_distrib.py"" rel=""nofollow noreferrer"">code</a>. And there is a method in model class: </p>

<pre><code>    def both(self, x):
        cat_out = self(x)
        probs = self.apply_softmax(cat_out)
        weights = probs * self.supports
        res = weights.sum(dim=2)
        return cat_out, res
</code></pre>

<p>What does that self(x) do/mean?</p>
",6635518,,2372064,,2020-01-09 16:30:48,2020-01-09 16:45:41,What does the notation self(x) do?,<python><deep-learning><reinforcement-learning><self>,1,4,,,,CC BY-SA 4.0,
2179,59709726,1,59724748,,2020-01-13 01:51:12,,5,5269,"<p>I am experimenting with the Q-learning algorithm. I have read from different sources and understood the algorithm, however, there seem to be no clear convergence criteria that is mathematically backed.</p>

<p>Most sources recommend iterating several times (example, N = 1000), while others say convergence is achieved when all state and action pairs (s, a) are visited infinitely often. But the question here is, how much is infinitely often. What is the best criteria for someone who wants to solve the algorithm by hand?</p>

<p>I would be grateful if someone could educate me on this. I would also appreciate any articles to this effect.</p>

<p>Regards.</p>
",12660007,,,,,2020-01-13 21:56:27,Criteria for convergence in Q-learning,<algorithm><machine-learning><artificial-intelligence><reinforcement-learning><q-learning>,2,2,0,,,CC BY-SA 4.0,
2180,23525804,1,23632515,,2014-05-07 18:44:15,,2,3997,"<p>I am implementing Softmax Action Selection policy for a reinforcement learning task (<a href=""http://www.incompleteideas.net/book/ebook/node17.html"" rel=""nofollow noreferrer"">http://www.incompleteideas.net/book/ebook/node17.html</a>).</p>

<p>I came with this solution, but I think there is room for improvement.</p>

<p>1-Here I evaluate the probabilities </p>

<pre><code>    prob_t = [0]*3
    denominator = 0
    for a in range(nActions):
        denominator += exp(Q[state][a] / temperature) 

    for a in range(nActions):
        prob_t[a] = (exp(Q[state][a]/temperature))/denominator  
</code></pre>

<p>2-Here I am comparing a random generated number in the range ]0,1[ to the probabilities value of the actions:</p>

<pre><code>    rand_action = random.random()
    if rand_action &lt; prob_t[0]:
        action = 0      
    elif rand_action &gt;= prob_t[0] and rand_action &lt; prob_t[1]+prob_t[0]:
        action = 1      
    else: #if rand_action &gt;= prob_t[1]+prob_t[0]
        action = 2
</code></pre>

<p>edit: </p>

<p>example: rand_action is 0.78, prob_t[0] is 0.25, prob_t[1] is 0.35, prob_t[2] is 0.4.
the probabilities sum to 1. 
0.78 is greater than the sum of the probabilities for action 0 and 1 (prob_t[0] + prob_t[1]) therefore action 2 is picked.</p>

<p>Is there a more efficient way of doing this?</p>
",1989141,,1989141,,2018-01-10 19:09:23,2019-09-26 08:07:17,Is there a better way than this to implement Softmax Action Selection for Reinforcement Learning?,<python-2.7><if-statement><random><reinforcement-learning><softmax>,3,2,,,,CC BY-SA 3.0,
2181,59947248,1,59947855,,2020-01-28 11:01:40,,-2,34,"<blockquote>
  <p>how are they represented in mathematical notation ?</p>
</blockquote>
",12647484,,1681985,,2020-09-10 12:06:39,2020-09-10 12:06:39,What is the example of a continous state space and continous action space in Reinforcement learning with mathematical notation?,<reinforcement-learning>,1,0,,2020-09-11 02:06:38,,CC BY-SA 4.0,
2182,23531117,1,23531418,,2014-05-08 01:18:28,,-2,696,"<p>I read <a href=""https://stackoverflow.com/questions/970060/machine-learning-in-game-ai"">this</a> </p>

<p>How can I make a AI learn to play a game from zero? A little example, let's say the AI goes to play blackjack, discount all the splits, cards in the deck and so on, the AI could either hit or stand, it doesn't know what it does until of course it starts to lose the game, it should learn that hitting too much make you lose and so does standing too early. I read this is called Reinforcement Learning. But I don't know how to implement it, what modules to use and the like...</p>

<p>Where should I start?</p>

<p>My ultimate goal is to create a kind of game where the user and the AI plays, not one against the other, but both against the game mechanics by themselves[not coop], and both learn playing it. The game would change every once in a while, new mechanics would arise making the game harder for both player and AI. The AI would learn both by playing the game but also by watching the player winning losing. I don't want the computer to learn too quickly tho, I would like to make like both are on the same 'ground'...Perhaps a final level would be the player can play agaisnt the AI. Am I going to the right place or I should try some other approach?</p>

<p>Edit: I thought that would be too broad. So I search a bit about ML and AI, and I found some modules that might help, scikit-learn, PyBrain, neurolab and also <a href=""http://incompleteideas.net/rlai.cs.ualberta.ca/RLAI/RLtoolkit/RLtoolkit1.0.html"" rel=""nofollow noreferrer"">RLToolkit</a>. The first two I didn't really understood how to get start it in it, the documentation is very unclear for a newcomer like me, neurolab I haven't tried yet, as I didn't really understood what is a Artificial Neural Networks[ANN] and how it can help me, and the last one, which is more specific to Reinforcement Learning doesn't have any documentation.</p>
",3014930,,-1,,2017-05-23 12:06:15,2014-05-08 04:10:07,Best/Easiest module for AI Learning?,<python><machine-learning><reinforcement-learning>,1,2,,2014-05-08 01:59:03,,CC BY-SA 3.0,
2185,62225037,1,62225113,,2020-06-05 22:33:08,,0,54,"<p>I am implementing policy iteration in python for the gridworld environment as a part of my learning. I have written the following code:</p>

<pre><code>### POLICY ITERATION ###
def policy_iter(grid, policy):
    '''
        Perform policy iteration to find the best policy and its value
    '''
    i = 1   
    while True:
        policy_converged = True # flag to check if the policy imporved and break out of the loop
        # evaluate the value function for the older policy
        old_v = value_eval(grid, policy)

        # evaluate the new policy
        for s in states:
            new_a = """"
            best_v = float(""-inf"")
            if grid.is_terminal(s):
                continue
            old_a = policy[s]
            for a in ACTION_SPACE:
                v = 0
                for s2 in states:
                    env_prob = transition_probs.get((s,a,s2), 0)
                    reward = rewards.get((s,a,s2), 0)

                    v += env_prob * (reward + gamma*old_v[s2])
                if v &gt; best_v:
                    new_a = a
                    best_v = v
            policy[s] = new_a
            if new_a != old_a:
                policy_converged = False
        print(i, ""th iteration"")
        i += 1
        if policy_converged == True:
            break

    return policy
</code></pre>

<p>This code works fine.
However, when I just change the placement of the '''policy_converged''' variable to be declared outside of the for loop, </p>

<pre><code>def policy_iter(grid, policy):
'''
    Perform policy iteration to find the best policy and its value
'''
i = 1  
policy_converged = True
while True:
</code></pre>

<p>and the rest of the code remains the same. In this case, the program starts to go in an infinite loop and never stops even though I am changing the value of the flag based on the performance after each iteration inside the primary while loop. Why does this happen?</p>
",11881261,,4240413,,2020-06-05 22:52:41,2020-06-05 22:58:02,Why does initialising the variable inside or outside of the loop change the code behaviour?,<python><deep-learning><reinforcement-learning><markov-decision-process><mdp>,2,0,0,,,CC BY-SA 4.0,
2186,23775766,1,23954443,,2014-05-21 06:41:00,,3,1241,"<p>My SARSA with gradient-descent keep escalating the weights exponentially. At Episode 4 step 17 the value is already nan</p>

<pre><code>Exception: Qa is nan
</code></pre>

<p>e.g:</p>

<pre><code>6) Qa:
Qa = -2.00890180632e+303

7) NEXT Qa:
Next Qa with west = -2.28577776413e+303

8) THETA:
1.78032402991e+303 &lt;= -0.1 + (0.1 * -2.28577776413e+303) - -2.00890180632e+303

9) WEIGHTS (sample)
5.18266630725e+302 &lt;= -1.58305782482e+301 + (0.3 * 1.78032402991e+303 * 1)
</code></pre>

<p>I don't know where to look for the mistake I made.
Here's some code FWIW:</p>

<pre><code>def getTheta(self, reward, Qa, QaNext):
    """""" let t = r + yQw(s',a') - Qw(s,a) """"""
    theta = reward + (self.gamma * QaNext) - Qa


def updateWeights(self, Fsa, theta):
    """""" wi &lt;- wi + alpha * theta * Fi(s,a) """"""
    for i, w in enumerate(self.weights):
        self.weights[i] += (self.alpha * theta * Fsa[i])
</code></pre>

<p>I have about 183 binary features.</p>
",184379,,,,,2018-12-17 06:03:42,How do you update the weights in function approximation with reinforcement learning?,<python><machine-learning><reinforcement-learning><function-approximation>,2,3,0,,,CC BY-SA 3.0,
2191,62208330,1,62458131,,2020-06-05 04:48:08,,1,273,"<p>python version as</p>

<pre><code>Python 3.6.10 :: Anaconda, Inc.
</code></pre>

<p>And was able to follow <a href=""https://github.com/Unity-Technologies/ml-agents/blob/release-0.15.0/docs/Learning-Environment-Create-New.md"" rel=""nofollow noreferrer"">this</a> docs successfully 
<br/>
But then i want to control environment with PYTHON-API so i followed <a href=""https://github.com/Unity-Technologies/ml-agents/blob/release-0.15.0/docs/Python-API.md"" rel=""nofollow noreferrer"">this</a> and with my code</p>

<pre><code>from mlagents_envs.environment import UnityEnvironment
import mlagents_envs
env = UnityEnvironment(file_name=""v1-ball-cube-game.x86_64"", 
                       base_port=5004, 
                       seed=1, 
                       side_channels=[])
# env = UnityEnvironment(file_name=None, base_port=5004, seed=1,worker_id=0, side_channels=[])

print(mlagents_envs.__version__)  # outputs 0.16.1
print(env.reset()) # outputs None
print(env) # outputs &lt;mlagents_envs.environment.UnityEnvironment object at 0x7f3ed001c278&gt;
print(str(env.get_agent_groups())) # outputs error
</code></pre>

<p>Out put of above code</p>

<pre><code>0.16.1
None
&lt;mlagents_envs.environment.UnityEnvironment object at 0x7f3ed001c278&gt;
Traceback (most recent call last):
  File ""index.py"", line 12, in &lt;module&gt;
    print(str(env.get_agent_groups()))
AttributeError: 'UnityEnvironment' object has no attribute 'get_agent_groups'
</code></pre>

<p>I do have this code and <strong>why is function <code>get_agent_groups</code></strong> not defined ? I am unable to find the solution. Plus it is written in docs </p>
",6543342,,7111561,,2020-06-05 07:26:59,2020-06-18 19:41:15,'UnityEnvironment' object has no attribute 'get_agent_groups' ( mlagents_envs 0.16.1 ),<python><unity3d><machine-learning><reinforcement-learning><ml-agent>,1,1,,,,CC BY-SA 4.0,
2194,23559050,1,24325875,,2014-05-09 07:36:47,,1,1105,"<p>I'm trying to implement eligibility traces (forward looking), whose pseudocode can be found in the following image</p>

<p><img src=""https://i.stack.imgur.com/TSr4m.png"" alt=""enter image description here""></p>

<p>I'm uncertain what the <code>For all s, a</code> means (5th line from below). Where do they get that collection of <code>s, a</code> from?</p>

<p>If it's forward-looking, do loop forward from the current state to observe <code>s'</code>?</p>

<p>Do you adjust every single <code>e(s, a)</code>?</p>
",184379,,3924118,,2018-08-20 19:11:16,2018-08-20 19:11:16,How are eligibility traces with SARSA calculated?,<machine-learning><reinforcement-learning><sarsa>,1,0,0,,,CC BY-SA 4.0,
2200,51614542,1,51629504,,2018-07-31 13:27:26,,0,75,"<p>Does anyone know of some notes from the book <a href=""http://www.incompleteideas.net/book/the-book.html"" rel=""nofollow noreferrer"">R. S. Sutton: Reinforcement Learning: An Introduction</a>? It is rather long and not very dense in information so it would be nice to have a more compressed version. </p>
",9790988,,,,,2018-08-01 09:13:43,Sutton: Reinforcement Learning - notes reference request,<reinforcement-learning>,1,0,0,,,CC BY-SA 4.0,
2211,34181056,1,34263310,,2015-12-09 14:17:54,,23,20799,"<p>I'm in a course called ""Intelligent Machines"" at the university. We were introduced with 3 methods of reinforced learning, and with those we were given the intuition of when to use them, and I quote:</p>

<ol>
<li>Q-Learning - Best when MDP can't be solved. </li>
<li>Temporal Difference Learning - best when MDP is known or can be learned but can't be solved.</li>
<li>Model-based - best when MDP can't be learned.</li>
</ol>

<p>Are there any good examples explaining when to choose one method over the other?</p>
",2204803,,3924118,,2018-11-22 15:39:35,2022-02-24 12:41:40,Q-learning vs temporal-difference vs model-based reinforcement learning,<machine-learning><reinforcement-learning><q-learning><temporal-difference>,1,5,0,,,CC BY-SA 4.0,
2212,52655995,1,52665601,,2018-10-04 22:25:34,,0,274,"<h3>@Edit:</h3>
<p>I'm trying to create an agent to play the game of Tetris, using a convolutional nnet that takes the board state + current piece as input. From what I've read, Deep Q-learning is not very good at this, which I just confirmed.</p>
<h3>@end Edit</h3>
<p>Suppose that an agent is learning a policy to play a game, where each game step can be represented as</p>
<blockquote>
<p><em>s, a, r, s', done</em></p>
</blockquote>
<p>representing</p>
<blockquote>
<p><em>state, action, reward, next state, game over</em></p>
</blockquote>
<p>In the <em>Deep Q-learning</em> algorithm, the agent is in state <em>s</em> and takes some action <em>a</em> (following an <em>epsilon-greedy</em> policy), observes a reward <em>r</em> and gets to the next state <em>s'</em>.</p>
<p>The agent acts like this:</p>
<pre><code># returns an action index
get_action(state, epsilon)
   if random() &lt; epsilon
       return random_action_index
   else
      return argmax(nnet.predict(state))
</code></pre>
<p>The parameters are updated by greedily observing the max Q-value in state <em>s'</em>, so we have</p>
<pre><code># action taken was 'a' in state 's' leading to 's_'
prediction = nnet.predict(s)
if done
   target = reward
else
   target = reward + gamma * max(nnet.predict(s_))

prediction[a] = target
</code></pre>
<p>The [prediction, target] is feed to some nnet for weight update. So this nnet gets a state <em>s</em> as input, and outputs a vector of q-values with dimension <em>n_actions</em>. This is all clear to me.</p>
<p>Now, suppose that my state-actions are so noise, that this approach will simply not work. So, instead of outputting a vector of dimension <em>n_actions</em>, my nnet output is a single value, representing the &quot;state-quality&quot; (how desirable is that state).</p>
<p>Now my agent acts like this:</p>
<pre><code># returns an action based on how good the next state is
get_action(state, epsilon):
    actions = []
    for each action possible in state:
         game.deepcopy().apply(action)
         val = nnet.predict(game.get_state())
         action.value = val
         actions.append(action)

    if random() &lt; epsilon
        return randomChoice(actions)
    else
        return action with_max_value from actions
</code></pre>
<p>And my [prediction, target] is like this:</p>
<pre><code># action taken was 'a' in state 's' leading to 's_'
prediction = nnet.predict(s)
if done
   target = reward
else
   target = reward + gamma * nnet.predict(s_)
</code></pre>
<p>I have some questions regarding this second algorithm:</p>
<blockquote>
<p>a) Does it makes sense to act non greedily sometimes?</p>
</blockquote>
<p>Intuitively no, because if I land in a bad state, it was probably because of a bad random action, not because the previous state was 'bad'. The Q-learning update will adjust the bad action, but this second algorithm will wrongly adjust the previous state.</p>
<blockquote>
<p>b) What kind of algorithm is this? Where does it fits in Reinforcement Learning?</p>
<p>c) In the case of Tetris, the state almost never repeats, so what can I do in this case? Is that the reason deep q-learning fails here?</p>
</blockquote>
<p>This may look confusing, but the algorithm actually works. I can provide extra details if necessary, thank you!</p>
",814601,,-1,,2020-06-20 09:12:55,2018-10-05 19:28:45,Deep Q-learning modification,<python><machine-learning><deep-learning><reinforcement-learning>,1,1,,,,CC BY-SA 4.0,
2213,62867291,1,62905956,,2020-07-12 23:03:58,,0,223,"<p>I have a problem with TensorFlow Code. Here is a piece of code that I used in my previous environment - Cart-pole problem</p>
<pre><code>initializer = tf.contrib.layers.variance_scaling_initializer()

X = tf.placeholder(tf.float32, shape=[None, n_inputs])

hidden = tf.layers.dense(X, n_hidden, activation=tf.nn.elu, kernel_initializer=initializer)
logits = tf.layers.dense(hidden, n_outputs)
outputs = tf.nn.sigmoid(logits)  

p_left_and_right = tf.concat(axis=1, values=[outputs, 1 - outputs])
action = tf.multinomial(tf.log(p_left_and_right), num_samples=1)

y = 1. - tf.to_float(action)

cross_entropy = tf.nn.sigmoid_cross_entropy_with_logits(labels=y, logits=logits)
optimizer = tf.train.AdamOptimizer(learning_rate)
grads_and_vars = optimizer.compute_gradients(cross_entropy)
</code></pre>
<p>There was two possible discrete decisions (right and left move).</p>
<p>I had decision given by sigmoid layer, which later was randomly selected with probabilities given by that layer.</p>
<p>Now I have environment with three discrete possible decisions, so I tried with softmax layer and it not work. When I start TensorFlow session.
The code is like that:</p>
<pre><code>initializer = tf.contrib.layers.variance_scaling_initializer()

X = tf.placeholder(tf.float32, shape=[None, n_inputs])

hidden = tf.layers.dense(X, n_hidden, activation=tf.nn.elu, kernel_initializer=initializer)

logits = tf.layers.dense(hidden, n_outputs)

outputs = tf.nn.softmax(logits)  

p_left_and_right = tf.concat(axis=3, values=[outputs])
action = tf.multinomial(tf.log(p_left_and_right), num_samples=1)

y = 1. - tf.to_float(action)
cross_entropy = tf.nn.softmax_cross_entropy_with_logits_v2(labels=y, logits=logits)
optimizer = tf.train.AdamOptimizer(learning_rate)
grads_and_vars = optimizer.compute_gradients(cross_entropy)
</code></pre>
<p>How should I change or improve it, to achieve suitable result and correct/better TensorFlow's code</p>
",11493867,,11493867,,2020-07-12 23:22:53,2020-07-15 00:46:06,TensorFlow reinforcement learning softmax layer,<python><tensorflow><neural-network><reinforcement-learning><softmax>,2,2,,,,CC BY-SA 4.0,
2218,52682521,1,52682718,,2018-10-06 19:15:06,,1,160,"<pre><code>x = tf.Placeholder(shape=[1,31,5,1])
def func(x):
    operations...
    return output

convolutionFunction = func(x)
sess = tf.Session()
gradientConv1 = gradientConv1 + sess.run(tf.gradients(tf.square(reward-convolutionFunction), weightsConv1))
</code></pre>

<p>gradientConv1 (numpy array of shape [2,2,1,32])
weightsConv1 (tensor variable of shape [2,2,1,32])</p>

<p>I'm getting an error such that ""Placeholder should have a dtype of float and shape of [1,31,5,1]"". It seems that it is showing me that I have not given a feed_dict to the function in sess.run? Please point me out to the error.
Also is my way of differentiating with respect to each value correct.</p>

<p>reward is a scalar</p>
",7779411,,,,,2018-10-06 19:38:21,tf.gradients application on a function,<python-3.x><tensorflow><reinforcement-learning><low-level-api>,1,4,,,,CC BY-SA 4.0,
2223,62963441,1,62963673,,2020-07-18 00:40:29,,1,486,"<p>I saw a python code where the value from a matrix could be grabbed using a &quot;index&quot; and the both python operators &quot;floor division&quot; and &quot;modulus&quot;.</p>
<p>Given the (3,3) matrix below.</p>
<pre class=""lang-py prettyprint-override""><code>&gt;&gt;&gt; m = np.array([['0&gt;','1&gt;','2&gt;'],['3&gt;','4&gt;','5&gt;'],['6&gt;','7&gt;','8&gt;']])
&gt;&gt;&gt; m
array([['0&gt;', '1&gt;', '2&gt;'],
       ['3&gt;', '4&gt;', '5&gt;'],
       ['6&gt;', '7&gt;', '8&gt;']], dtype='&lt;U2')
</code></pre>
<p>If we &quot;flat&quot; the given matrix we will have:</p>
<pre class=""lang-py prettyprint-override""><code>&gt;&gt;&gt; m.reshape(-1)
array(['0&gt;', '1&gt;', '2&gt;', '3&gt;', '4&gt;', '5&gt;', '6&gt;', '7&gt;', '8&gt;'], dtype='&lt;U2')
</code></pre>
<p>Let's suppose that I want to read the value '3&gt;' that is the value in the 4th position in the array.</p>
<p>If I use the index <code>3</code> I can get the respective value from the matrix, using:</p>
<pre class=""lang-py prettyprint-override""><code>&gt;&gt;&gt; idx = int(np.where(m.reshape(-1) == '3&gt;')[0])
&gt;&gt;&gt; idx
3
&gt;&gt;&gt; x = idx // m.shape[0]
&gt;&gt;&gt; y = idx % m.shape[0]
&gt;&gt;&gt; 
&gt;&gt;&gt; m[x][y]
'3&gt;'
&gt;&gt;&gt;
</code></pre>
<p>I can't see how this work.</p>
<p>What is the explanation for that ?</p>
",2135819,,,,,2020-07-18 15:54:43,"Indexing a matrix using ""floor division"" and ""modulus"" operators",<python><numpy><deep-learning><linear-algebra><reinforcement-learning>,1,2,,,,CC BY-SA 4.0,
2229,70605210,1,70822559,,2022-01-06 09:47:18,,0,30,"<p>In the environment tutorial of tensorflow agents (<a href=""https://www.tensorflow.org/agents/tutorials/2_environments_tutorial"" rel=""nofollow noreferrer"">https://www.tensorflow.org/agents/tutorials/2_environments_tutorial</a>), the state is stored as an integer. When the state is required, it is converted to a numpy array:</p>
<pre><code>from tf_agents.environments import py_environment
import numpy as np

class CardGameEnv(py_environment.PyEnvironment):

    def __init__(self):
        self._state = 0

    def _step(self,action):
        state_array = np.array([self._state], dtype=np.int32)
        return np.transition(state_array, reward=1.0, discount=0.9)
</code></pre>
<p>Is there any reason why they do this, instead of just storing the state directly as a numpy array? So like this:</p>
<pre><code>from tf_agents.environments import py_environment
import numpy as np
class CardGameEnv(py_environment.PyEnvironment):

    def __init__(self):
        self._state = np.array([0], dtype=np.int32)

    def _step(self,action):
        return np.transition(self._state, reward=1.0, discount=0.9)
</code></pre>
<p>Is there any downside to using the second method? Or is this equally valid?</p>
",3053216,,,,,2022-01-23 13:31:10,Benefit of storing state as a list/integer in tensorflow agents,<python><tensorflow><reinforcement-learning><tensorflow-agents>,1,0,,,,CC BY-SA 4.0,
2230,70615514,1,70637235,,2022-01-07 01:17:44,,0,1647,"<p>I am new to pytorch and I am working on DQN for a timeseries using Reinforcement Learning and I needed to have a complex observation of timeseries and some sensor readings, so I merged two neural networks and I am not sure if that's what is ruining my loss.backward or something else.
I know there is multiple questions with the same title but none worked for me, maybe I am missing something.<br />
First of all, this is my network:</p>
<pre><code>class DQN(nn.Module):
  def __init__(self, list_shape, score_shape, n_actions):
    super(DQN, self).__init__()

    self.FeatureList =  nn.Sequential(
            nn.Conv1d(list_shape[1], 32, kernel_size=8, stride=4),
            nn.ReLU(),
            nn.Conv1d(32, 64, kernel_size=4, stride=2),
            nn.ReLU(),
            nn.Conv1d(64, 64, kernel_size=3, stride=1),
            nn.ReLU(),
            nn.Flatten()
        )
    
    self.FeatureScore = nn.Sequential(
            nn.Linear(score_shape[1], 512),
            nn.ReLU(),
            nn.Linear(512, 128)
        )
    
    t_list_test = torch.zeros(list_shape)
    t_score_test = torch.zeros(score_shape)
    merge_shape = self.FeatureList(t_list_test).shape[1] + self.FeatureScore(t_score_test).shape[1]
    
    self.FinalNN =  nn.Sequential(
            nn.Linear(merge_shape, 512),
            nn.ReLU(),
            nn.Linear(512, 128),
            nn.ReLU(),
            nn.Linear(128, n_actions),
    )
    
  def forward(self, list, score):
    listOut = self.FeatureList(list)
    scoreOut = self.FeatureScore(score)
    MergedTensor = torch.cat((listOut,scoreOut),1)
    return self.FinalNN(MergedTensor)
</code></pre>
<p>I have a function called calc_loss, and at its end it return the MSE loss as below</p>
<pre><code>  print(state_action_values.dtype)
  print(expected_state_action_values.dtype) 
  return nn.MSELoss()(state_action_values, expected_state_action_values)
</code></pre>
<p>and the print shows float32 and float64 respectively.<br />
I get the error when I run the loss.backward() as below</p>
<pre><code>LEARNING_RATE = 0.01
optimizer = optim.Adam(net.parameters(), lr=LEARNING_RATE)

for i in range(50):
  optimizer.zero_grad()
  loss_v = calc_loss(sample(obs, 500, 200, 64), net, tgt_net)
  print(loss_v.dtype)
  print(loss_v)
  loss_v.backward()
  optimizer.step()
</code></pre>
<p>and the print output is as below:<br />
torch.float64<br />
tensor(1887.4831, dtype=torch.float64, grad_fn=)</p>
<p>Update 1:<br />
I tried using a simpler model, yet the same issue, when I tried to cast the inputs to Float, I got an error:</p>
<pre><code>RuntimeError: expected scalar type Double but found Float
</code></pre>
<p>What makes the model expects double ?</p>
<p>Update 2:<br />
I tried to add the below line on top after the torch import but same issue of <code>RuntimeError: Found dtype Double but expected Float</code></p>
<pre><code>&gt;&gt;&gt; torch.set_default_tensor_type(torch.FloatTensor)
</code></pre>
<p>But when I used the DoubleTensor I got:
<code>RuntimeError: Input type (torch.FloatTensor) and weight type (torch.DoubleTensor) should be the same or input should be a MKLDNN tensor and weight is a dense tensor</code></p>
",2791816,,2791816,,2022-01-08 08:57:09,2022-01-08 23:25:07,RuntimeError: Found dtype Double but expected Float - PyTorch,<python><python-3.x><pytorch><reinforcement-learning>,1,4,,,,CC BY-SA 4.0,
2231,62941625,1,63040588,,2020-07-16 18:48:54,,0,134,"<p>I am trying to implement the cross-entropy policy-based method to the classic CartPole-v0 environment. I am actually reformatting a working implementation of this algorithm on the MountainCarContinuous-v0, but when I try to get the agent learning, I get this error message:</p>
<pre><code>---------------------------------------------------------------------------
AssertionError                            Traceback (most recent call last)
 in 
      4 
      5 agent = Agent(env)
----&gt; 6 scores = agent.learn()
      7 
      8 # plot the scores

~/cross_entropy.py in learn(self, n_iterations, max_t, gamma, print_every, pop_size, elite_frac, sigma)
     83         for i_iteration in range(1, n_iterations+1): # loop over all the training iterations
     84             weights_pop = [best_weight + (sigma*np.random.randn(self.get_weights_dim())) for i in range(pop_size)] # population of the weights/policies
---&gt; 85             rewards = np.array([self.evaluate(weights, gamma, max_t) for weights in weights_pop]) # rewards from the policies resulting from all individual weights
     86 
     87             # get the best policies

~/cross_entropy.py in (.0)
     83         for i_iteration in range(1, n_iterations+1): # loop over all the training iterations
     84             weights_pop = [best_weight + (sigma*np.random.randn(self.get_weights_dim())) for i in range(pop_size)] # population of the weights/policies
---&gt; 85             rewards = np.array([self.evaluate(weights, gamma, max_t) for weights in weights_pop]) # rewards from the policies resulting from all individual weights
     86 
     87             # get the best policies

~/cross_entropy.py in evaluate(self, weights, gamma, max_t)
     56             action = self.forward(state)
     57             #action = torch.argmax(action_vals).item()
---&gt; 58             state, reward, done, _ = self.env.step(action)
     59             episode_return += reward * math.pow(gamma, t)
     60             if done:

/gym/wrappers/time_limit.py in step(self, action)
     14     def step(self, action):
     15         assert self._elapsed_steps is not None, &quot;Cannot call env.step() before calling reset()&quot;
---&gt; 16         observation, reward, done, info = self.env.step(action)
     17         self._elapsed_steps += 1
     18         if self._elapsed_steps &gt;= self._max_episode_steps:

/gym/envs/classic_control/cartpole.py in step(self, action)
    102     def step(self, action):
    103         err_msg = &quot;%r (%s) invalid&quot; % (action, type(action))
--&gt; 104         assert self.action_space.contains(action), err_msg
    105 
    106         x, x_dot, theta, theta_dot = self.state

AssertionError: tensor([ 0.3987,  0.6013]) () invalid
</code></pre>
<p>I found this is because the MountainCarContinuous-v0 environment has an action_space of type Box(2) whereas CartPole-v0 is Discrete(2), meaning that I only want an integer as action selection.</p>
<p>I have tried working around this notion by applying a softmax activation function and then took the index of the higher value as the action.</p>
<pre><code>action_vals = self.forward(state)
action = torch.argmax(action_vals).item()
</code></pre>
<p>This gets rid of the error but when I train the agent, it seems to learn incredibly fast which is kind of an indicator that something is wrong. This is my full agent class:</p>
<pre><code>class Agent(nn.Module):
    def __init__(self, env, h_size=16):
        super().__init__()
        self.env = env
        # state, hidden layer, action sizes
        self.s_size = env.observation_space.shape[0]
        self.h_size = h_size
        self.a_size = env.action_space.n
        # define layers
        self.fc1 = nn.Linear(self.s_size, self.h_size)
        self.fc2 = nn.Linear(self.h_size, self.a_size)

        self.device = torch.device('cpu')
        
    def set_weights(self, weights):
        s_size = self.s_size
        h_size = self.h_size
        a_size = self.a_size
        # separate the weights for each layer
        fc1_end = (s_size*h_size)+h_size
        fc1_W = torch.from_numpy(weights[:s_size*h_size].reshape(s_size, h_size))
        fc1_b = torch.from_numpy(weights[s_size*h_size:fc1_end])
        fc2_W = torch.from_numpy(weights[fc1_end:fc1_end+(h_size*a_size)].reshape(h_size, a_size))
        fc2_b = torch.from_numpy(weights[fc1_end+(h_size*a_size):])
        # set the weights for each layer
        self.fc1.weight.data.copy_(fc1_W.view_as(self.fc1.weight.data))
        self.fc1.bias.data.copy_(fc1_b.view_as(self.fc1.bias.data))
        self.fc2.weight.data.copy_(fc2_W.view_as(self.fc2.weight.data))
        self.fc2.bias.data.copy_(fc2_b.view_as(self.fc2.bias.data))
    
    def get_weights_dim(self):
        return (self.s_size+1)*self.h_size + (self.h_size+1)*self.a_size
        
    def forward(self, x):
        x = F.relu(self.fc1(x))
        x = F.softmax(self.fc2(x))
        return x
        
    def evaluate(self, weights, gamma=1.0, max_t=5000):
        self.set_weights(weights)
        episode_return = 0.0
        state = self.env.reset()
        for t in range(max_t):
            state = torch.from_numpy(state).float().to(self.device)
            action_vals = self.forward(state)
            action = torch.argmax(action_vals).item()
            state, reward, done, _ = self.env.step(action)
            episode_return += reward * math.pow(gamma, t)
            if done:
                break
        return episode_return

    def learn(self, n_iterations=500, max_t=1000, gamma=1.0, print_every=10, pop_size=50, elite_frac=0.2, sigma=0.5):
        &quot;&quot;&quot;PyTorch implementation of the cross-entropy method.
        
        Params
        ======
            n_iterations (int): maximum number of training iterations
            max_t (int): maximum number of timesteps per episode
            gamma (float): discount rate
            print_every (int): how often to print average score (over last 100 episodes)
            pop_size (int): size of population at each iteration
            elite_frac (float): percentage of top performers to use in update
            sigma (float): standard deviation of additive noise
        &quot;&quot;&quot;
        n_elite=int(pop_size*elite_frac) # number of elite policies from the population

        scores_deque = deque(maxlen=100) # list of the past 100 scores
        scores = [] # list of all the scores
        best_weight = sigma*np.random.randn(self.get_weights_dim()) # initialize the first best weight randomly

        for i_iteration in range(1, n_iterations+1): # loop over all the training iterations
            weights_pop = [best_weight + (sigma*np.random.randn(self.get_weights_dim())) for i in range(pop_size)] # population of the weights/policies
            rewards = np.array([self.evaluate(weights, gamma, max_t) for weights in weights_pop]) # rewards from the policies resulting from all individual weights

            # get the best policies
            ##
            elite_idxs = rewards.argsort()[-n_elite:] 
            elite_weights = [weights_pop[i] for i in elite_idxs]
            ##

            best_weight = np.array(elite_weights).mean(axis=0) # take the average of the best weights

            reward = self.evaluate(best_weight, gamma=1.0) # evaluate this new policy
            scores_deque.append(reward) # append the reward
            scores.append(reward) # also append the reward
            
            torch.save(self.state_dict(), 'checkpoint.pth') # save the agent
            
            if i_iteration % print_every == 0: # print every 100 steps
                print('Episode {}\tAverage Score: {:.2f}'.format(i_iteration, np.mean(scores_deque)))

            if np.mean(scores_deque)&gt;=195.0: # print if environment is solved
                print('\nEnvironment solved in {:d} iterations!\tAverage Score: {:.2f}'.format(i_iteration-100, np.mean(scores_deque)))
                break
        return scores
</code></pre>
<p>If anyone has an idea on how to get the agent training properly, please give me any suggestions.</p>
",13561941,,13561941,,2020-07-16 22:10:26,2020-07-22 18:07:44,Problem with output of neural network in a cross-entropy method attempt at solving CartPole-v0,<python><deep-learning><pytorch><reinforcement-learning>,1,1,,,,CC BY-SA 4.0,
2232,55245875,1,55248243,,2019-03-19 16:34:51,,1,41,"<p>I am simulating an inventory management system for a retail shop; therefore, I have a (15,15) matrix of zeros in which states are rows and actions columns:</p>

<pre><code>Q = np.matrix(np.zeros([15, 15]) )
</code></pre>

<p>Specifically, 0 is the minimum and 14 the maximum inventory level, states are current inventory level and actions stock orders (quantity). </p>

<p>Consequently, I would like to substitute zeros with ""-1"", where the sum of state and action > 14: </p>

<pre><code>print(final_Q)

#First row, from which I can order everything (since 0 + 14 == 14)
[[0 0   0   0   0   0   0   0   0   0   0   0   0   0   0]
#Second row, from which I can order max. 13 products (1 + 14 &gt; 14)
[[0 0   0   0   0   0   0   0   0   0   0   0   0   0   -1]]
#Third row, from which the max is 12    
[[0 0   0   0   0   0   0   0   0   0   0   0   0   -1  -1]]

(...)
</code></pre>

<p>I tried implementing that manually, but how can I get the <strong>final</strong> <strong>matrix</strong> automatically?</p>
",8618380,,712995,,2019-10-12 08:16:28,2019-10-12 08:16:28,Build a matrix of available actions for Q-Learning,<numpy><reinforcement-learning><q-learning>,2,2,,,,CC BY-SA 4.0,
2233,46422845,1,50663200,,2017-09-26 09:36:42,,49,26485,"<p>I know the basics of Reinforcement Learning, but what terms it's necessary to understand to be able read <a href=""https://arxiv.org/abs/1707.06347"" rel=""noreferrer"">arxiv PPO paper</a> ?</p>

<p>What is the roadmap to learn and use <a href=""https://blog.openai.com/openai-baselines-ppo/"" rel=""noreferrer"">PPO</a> ?</p>
",2844907,,,,,2021-07-13 15:54:47,What is the way to understand Proximal Policy Optimization Algorithm in RL?,<machine-learning><reinforcement-learning>,4,1,0,,,CC BY-SA 3.0,
2235,36160248,1,36499550,,2016-03-22 16:40:43,,0,1082,"<p>I would like to get some helpful instructions about how to use the Q-learning algorithm with function approximation. For the basic Q-learning algorithm I have found examples and I think I did understand it. In case of using function approximation I get into trouble. Can somebody give me an explanation through a short example how it works?</p>

<p>What I know:</p>

<ol>
<li>Istead of using matrix for Q-values we use features and parameters.</li>
<li>Make approximation with the linear combination of feauters and parameters.</li>
<li>Update the parameters.</li>
</ol>

<p>I have checked this paper: <a href=""http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.81.7833&amp;rep=rep1&amp;type=pdf"" rel=""nofollow"">Q-learning with function approximation</a></p>

<p>But I cant find any useful tutorial how to use it.</p>

<p>Thanks for help!</p>
",6099702,,,,,2016-04-08 12:26:27,Q-learning with linear function approximation,<algorithm><reinforcement-learning><q-learning><function-approximation>,1,0,,,,CC BY-SA 3.0,
2241,64642712,1,64642821,,2020-11-02 09:15:58,,2,226,"<p>I am learning markov decision process.
Am I don't know where to mark terminal states.</p>
<p>In 4x3 grid world, I marked the terminal state that I think correct(I might be wrong) with T.
<a href=""https://i.stack.imgur.com/8c1CD.png"" rel=""nofollow noreferrer"">Pic</a></p>
<p>I saw an instruction mark terminal states as follow.</p>
<pre><code>terminals=[(3, 2), (3, 1)]
</code></pre>
<p>Can someone explain how does it work?</p>
",,user13612530,,,,2020-11-02 09:29:10,What is terminal state in gridworld?,<reinforcement-learning><markov><markov-decision-process>,1,1,,,,CC BY-SA 4.0,
2242,46965925,1,46970812,,2017-10-27 00:53:34,,1,73,"<p>I am trying to implement Q-Learning algorithm but I do not have enough time to select the action by e-greedy.For simplicity I am choosing a random action,without any proper justification.Will this work ? </p>
",7779148,,7541612,,2017-10-29 03:45:26,2018-10-16 10:28:21,Will Q Learning algorithm produce the same result if I do not use e-greedy?,<machine-learning><reinforcement-learning>,1,0,,,,CC BY-SA 3.0,
2243,47512110,1,47579598,,2017-11-27 13:28:08,,2,375,"<p>I have some questions related to POMDPs.</p>

<ol>
<li><p>What do we mean by <em>controllable actions</em> in a partially observable Markov decision process? Or no controllable actions in hidden Markov states? </p></li>
<li><p>When computing policies through value or policy iteration, could we say that the POMDP is an expert system (because we model the environment)? While, when using <em>Q-learning</em>, it is a more flexible system in terms of intelligence or <em>adaptability to a changing environment</em>?</p></li>
</ol>
",7985899,,3924118,,2019-09-01 00:47:49,2019-09-01 00:47:49,"What do we mean by ""controllable actions"" in a POMDP?",<artificial-intelligence><probability><reinforcement-learning><expert-system><markov-decision-process>,1,0,0,,,CC BY-SA 4.0,
2247,64649944,1,64802992,,2020-11-02 17:00:22,,2,535,"<p>Recently, I have tried to apply the naive policy gradient method to my problem. However, I found that the difference between different outputs of the last layer of the neural network is huge, which means that after applying the softmax layer, only one action will be marked as 1, and other actions will be marked as 0. For instance, the output of the last layer is shown below:</p>
<pre class=""lang-py prettyprint-override""><code>[ 242.9629, -115.6593,   63.3984,  226.1815,  131.5903, -316.6087,
 -205.9341,   98.7216,  136.7644,  266.8708,   19.2289,   47.7531]
</code></pre>
<p>After applying the softmax function, it is clear that only one action will be chosen.</p>
<pre class=""lang-py prettyprint-override""><code>[4.1395e-11, 0.0000e+00, 0.0000e+00, 2.1323e-18, 0.0000e+00, 0.0000e+00,
 0.0000e+00, 0.0000e+00, 0.0000e+00, 1.0000e+00, 0.0000e+00, 0.0000e+00]
</code></pre>
<p>This problem severely affects the final performance as the neural network will try constant action only after a few steps. Therefore, is there any way to solve this problem?</p>
<p>(By the way, even if I have tried to give some negative rewards to the neural network, the actions chosen by it are still unchanged.)</p>
<p>My training curve is shown as follows:
<a href=""https://i.stack.imgur.com/rBviW.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/rBviW.png"" alt=""Training curve"" /></a></p>
",11636648,,11636648,,2020-11-03 14:01:19,2020-11-12 11:24:14,How to solve the zero probability problem in the policy gradient?,<reinforcement-learning><policy-gradient-descent>,2,0,0,,,CC BY-SA 4.0,
2248,40760576,1,40867590,,2016-11-23 09:32:50,,1,226,"<p>I am working on a project to combine reinforcement learning with traffic light simulations using the package Pybrain. I have read the tutorial and implemented my own subclasses of <code>Environment</code> and <code>Task</code>. I am using an <code>ActionValueNetwork</code> as controller because I want my state to be a vector with continuous values such that it can contain information about for example the number of cars waiting on each lane, the total waiting times per lane and more.</p>

<p>I set the input dimensions of the <code>ActionValueNetwork</code> to the dimensions of my state vector, which would suggest that it's possible to use vectors as state variables. When I use the Q-learner or the SARSA learner the code runs fine at first but I obtain an error message as soon as the method <code>learn()</code> is called. This function contains the line</p>

<pre><code>state = int(state)
</code></pre>

<p>and the error message is</p>

<pre><code>TypeError: only length-1 arrays can be converted to Python scalars
</code></pre>

<p>which would suggest that only scalar shaped states can be used.</p>

<p>Does the pybrain reinforcement learning environment support vector shaped states? If so, how can I modify my code such that it will work with their implementations of Q-learning or other methods?</p>
",6467569,,,,,2017-08-22 09:17:04,Pybrain reinforcement learning,dimension of state,<python><neural-network><pybrain><reinforcement-learning><q-learning>,1,0,,,,CC BY-SA 3.0
2249,22572624,1,22576346,,2014-03-22 02:17:27,,1,492,"<p>I'm looking over a sample exam and there is a question on Q-learning, I have included it below. In the 3rd step, how come the action taken is 'right' rather than 'up' (back to A2). It appears the Q value to go back up to A2 would be 0.18, and the Q value to go right would be 0.09. So why wouldn't the agent go back to A2 instead of going to B3?</p>

<p><a href=""http://i.imgur.com/3S9gFiM"" rel=""nofollow"">Maze &amp; Q-Table</a></p>

<p><a href=""http://i.imgur.com/RNtyLtz.png"" rel=""nofollow"">Solution</a></p>

<p>Edit: Also, how come 2,C has a reward value of 2 for action 'right' even though there's a wall there and not possible to go right? Do we just assume thats not a possible move and ignore its Q value?</p>

<p>Edit2: Then in step 6, the Q values for going 'down' and 'right' at state 1,C are equal. At that point does the agent just pick randomly? So then for this question I would just pick the best move since it's possible the agent would pick it?</p>

<p>Edit3: Would it be true to say the agent doesn't return to the state he previously came from? Will an agent ever explore the same state more than once (not including starting a new instance of the maze)?</p>
",2172284,,484129,,2014-03-22 10:15:00,2014-04-21 18:44:57,Q-Learning: Can you move backwards?,<machine-learning><artificial-intelligence><computer-science><reinforcement-learning><q-learning>,1,5,0,,,CC BY-SA 3.0,
2253,59376108,1,59379506,,2019-12-17 14:13:48,,0,32,"<p>what are the memory_size and the memory_counter in the DeepQNetwork:</p>

<pre><code>class DeepQNetwork:
def __init__(
        self,
        n_actions,
        n_features,
        learning_rate=0.01,              
        reward_decay=0.9,
        e_greedy=0.9,                     
        replace_target_iter=300,          
        memory_size=500,                  
        batch_size=32,                    
        e_greedy_increment=None,
        output_graph=True,
        memory_counter=48    
):
</code></pre>
",12548884,,,,,2019-12-17 17:44:41,Memory_size and memory_counter in DeepQNetwork,<machine-learning><deep-learning><reinforcement-learning>,1,2,,,,CC BY-SA 4.0,
2254,47537563,1,47553278,,2017-11-28 17:28:14,,1,719,"<p>I want know how to add a constraint to Q-learning. I have an action resulting in two rewards every time (reward 1= delivery cost , reward 2= delivery time). I want to minimize the cost while ensuring max delivery time limit is not violated. 
Is there a standard/formalized way to do this?</p>
",8876170,,,,,2017-11-29 12:48:07,How to add constraint to reinforcement learning (Q-learning),<machine-learning><constraints><reinforcement-learning><q-learning>,1,0,,,,CC BY-SA 3.0,
2256,40951613,1,40963198,,2016-12-03 19:26:40,,0,395,"<p>I've been studying up on reinforcement learning, but the thing I don't understand is how a Q value is ever calculated. If you use the Bellman equation <code>Q(s,a) = r + Î³*max(Q(s',a'))</code>, would't it just go on forever? Because <code>Q(s',a')</code> would need the Q value of one timestep further, and that would just continue on and on. How does it end?</p>
",,user5702166,,,,2016-12-04 20:20:42,"In Q Learning, how can you ever actually get a Q value? Wouldn't Q(s,a) just go on forever?",<reinforcement-learning><q-learning>,1,0,,,,CC BY-SA 3.0,
2259,59222431,1,59443160,,2019-12-07 02:14:57,,0,63,"<p>In PPOâ€™s objective function second term introduces squared error loss of the value function neural network. Is that term is essentially the squared advantage values, right?</p>
",10437878,,,,,2019-12-22 09:49:41,Objective function in proximal policy optimization,<reinforcement-learning>,1,0,,,,CC BY-SA 4.0,
2261,22805323,1,22805482,,2014-04-02 08:13:35,,2,467,"<h1>Q learning - rewards</h1>

<p>I'm struggling to interpret the pseudocode for the Q learning algorithm:</p>

<pre><code>1  For each s, a initialize table entry Q(a, s) = 0
2  Observe current state s
3  Do forever:
4     Select an action a and execute it
5     Receive immediate reward r
6     Observe the new state sâ€² â† Î´(a, s)
7     Update the table entry for Q(a, s) as follows:
8        Q( a, s ) â† R( s ) + Î³ * max Q( aâ€², sâ€² )
9     ï¿¼ï¿¼ï¿¼ï¿¼ï¿¼ï¿¼s â† sâ€²
</code></pre>

<p>Should the rewards be collected from the subsequent state <code>s'</code> or the current state <code>s</code>?</p>
",3488341,,,,,2014-04-02 08:20:57,Q-learning: What is the correct state for reward calculation,<reinforcement-learning><q-learning>,1,0,0,,,CC BY-SA 3.0,
2264,41317030,1,41520712,,2016-12-24 21:41:59,,2,561,"<h2>Background (may be skipped):</h2>

<p>In training neural networks, usually stochastic gradient descent (SGD) is used: instead of computing the network's error on all members of the training set and updating the weights by gradient descent (which means waiting a long time before each weight update), use each time a minbatch of members, and treat the resulting error as an unbiased estimation of the true error.</p>

<p>In reinforcement learning, sometimes Q-learning is implemented with a neural network (as in deep Q-learning), and experience replay is used: Instead of updating the weights by the previous (state,action,reward) of the agent, update using a minibatch of random samples of old (states,actions,rewards), so that there is no correlation between subsequent updates.</p>

<h2>The Question:</h2>

<p>Is the following assertion correct?: When minibatching in SGD, one weights update is performed per the whole minibatch, while when minibatching in Q-learning, one weights update is performed per each member in the minibatch?</p>

<h2>One more thing:</h2>

<p>I think this question is more suitable for <a href=""https://stats.stackexchange.com/"">Cross Validated</a>, as it is a conceptual question about machine learning and has nothing to do with programming, but by looking at questions tagged <a href=""https://stackoverflow.com/questions/tagged/reinforcement-learning"">reinforcement-learning</a> on Stackoverflow, I conclude that it is normative to ask this question here, and the number of responses I can get is larger.</p>
",5737630,,-1,,2017-05-23 10:30:37,2017-01-07 11:03:14,Minibatching in Stochastic Gradient Descent and in Q-Learning,<machine-learning><neural-network><reinforcement-learning><q-learning>,1,2,,,,CC BY-SA 3.0,
2265,59223646,1,59223806,,2019-12-07 06:40:46,,1,51,"<p>In the original paper on Proximal Policy Optimization Algorithms</p>

<blockquote>
  <p><a href=""https://arxiv.org/pdf/1707.06347.pdf"" rel=""nofollow noreferrer"">https://arxiv.org/pdf/1707.06347.pdf</a></p>
</blockquote>

<p>in equation (4) the authors use an operation denoted by <code>KL[]</code>. Unfortunately, they never give a definition for it. </p>

<p>My question:</p>

<blockquote>
  <p>What does the <code>KL[]</code> operation stand for?</p>
</blockquote>
",4114325,,,,,2019-12-07 19:50:31,"Proximal Policy Optimization Algorithms paper - definition of ""KL"" operation?",<machine-learning><reinforcement-learning>,1,0,,,,CC BY-SA 4.0,
2266,22805872,1,22923101,,2014-04-02 08:39:59,,20,21883,"<h1>Ïµ-greedy policy</h1>

<p>I know the Q-learning algorithm should try to balance between <em>exploration</em> and <em>exploitation</em>. Since I'm a beginner in this field, I wanted to implement a simple version of exploration/exploitation behavior.</p>

<strong>Optimal epsilon value</strong>

<p>My implementation uses the Ïµ-greedy policy, but I'm at a loss when it comes to deciding the epsilon value. Should the epsilon be bounded by the number of times the algorithm have visited a given (state, action) pair, or should it be bounded by the number of iterations performed?</p>

My suggestions:

<ol>
<li>Lower the epsilon value for each time a given (state, action) pair has been encountered.</li>
<li>Lower the epsilon value after a complete iteration has been performed.</li>
<li>Lower the epsilon value for each time we encounter a state s.</li>
</ol>

<p>Much appreciated!</p>
",3488341,,3488341,,2014-04-02 08:45:02,2019-12-02 01:25:37,Optimal epsilon (Ïµ-greedy) value,<machine-learning><reinforcement-learning><q-learning>,2,2,0,,,CC BY-SA 3.0,
2267,4845489,1,4846138,,2011-01-30 20:59:01,,4,1321,"<p>I have an artificial neural network which plays Tic-Tac-Toe - but it is not complete yet.</p>

<hr>

<p><strong>What I have yet:</strong></p>

<ul>
<li>the reward array ""R[t]"" with integer values for every timestep or move ""t"" (1=player A wins, 0=draw, -1=player B wins)</li>
<li>The input values are correctly propagated through the network.</li>
<li>the formula for adjusting the weights:</li>
</ul>

<p><img src=""https://i.stack.imgur.com/HAHfX.gif"" alt=""enter image description here""></p>

<hr>

<p><strong>What is missing:</strong></p>

<ul>
<li>the TD learning: I still need a procedure which ""backpropagates"" the network's errors using the TD(Î») algorithm.</li>
</ul>

<p>But I don't really understand this algorithm.</p>

<hr>

<p><strong>My approach so far ...</strong></p>

<p>The trace decay parameter Î» should be ""0.1"" as distal states should not get that much of the reward.</p>

<p>The learning rate is ""0.5"" in both layers (input and hidden).</p>

<p>It's a case of delayed reward: The reward remains ""0"" until the game ends. Then the reward becomes ""1"" for the first player's win, ""-1"" for the second player's win or ""0"" in case of a draw.</p>

<hr>

<p><strong>My questions:</strong></p>

<ul>
<li>How and when do you calculate the net's error (TD error)?</li>
<li>How can you implement the ""backpropagation"" of the error?</li>
<li>How are the weights adjusted using TD(Î»)?</li>
</ul>

<hr>

<p>Thank you so much in advance :)</p>
",89818,,89818,,2011-02-18 08:50:56,2011-02-18 08:50:56,TD(Î») in Delphi/Pascal (Temporal Difference Learning),<artificial-intelligence><neural-network><reinforcement-learning><temporal-difference>,3,0,0,,,CC BY-SA 2.5,
2269,59425390,1,59443007,,2019-12-20 12:36:43,,0,1124,"<p>I'm reading Reinforcement Learning by Sutton and Barto, and for an example of Dyna-Q, they use a maze problem.  The example shows that with n=50 steps of planning, the algorithm reaches the optimal path in only 3 episodes.  </p>

<p>Is this an improvement over 50-step Q-learning?  It seems like you are really just running a bunch of 50-step Q-learning algorithms in each episode, so saying it finds the optimal path in 3 episodes is misleading.  </p>

<p>Also, I guess the big question is, I thought Dyna-Q was useful when you don't have a model of the environment, but in this example don't we have a model of the environment?  Why use all of the memory to save all our previous moves if we already have a model?  I'm having trouble understanding why this is a good example for Dyna-Q.</p>
",12528567,,,,,2019-12-22 09:23:26,Dyna-Q with planning vs. n-step Q-learning,<machine-learning><reinforcement-learning>,1,0,,,,CC BY-SA 4.0,
2271,59569710,1,59599319,,2020-01-02 20:38:41,,4,1559,"<p>I have a Python class that conforms to OpenAI's environment API, but it's written in non-vectorized form i.e. it receives one input action per step and returns one reward per step. How do I vectorize the environment? I haven't been able to find any clear explanation on GitHub.</p>
",4570472,,,,,2020-01-06 07:04:18,How does one vectorize reinforcement learning environments?,<pytorch><vectorization><reinforcement-learning>,1,5,0,,,CC BY-SA 4.0,
2272,59582677,1,59776674,,2020-01-03 17:16:39,,0,792,"<p>This last days I'm trying to train a contextual bandit algorithm throw Vowpalwabbit, so I'm doing some toy-model that can help me understand how the algorithm works. </p>

<p>So I imagined a state with 4 possible action and I train my model on two different context.
Each context has only one optimal action among the 4 actions. </p>

<p>That's how I did it.</p>

<pre><code>vw = pyvw.vw(""--cb_explore 4 -q UA --epsilon 0.1"")
vw.learn('1:-2:0.5 | 5')
vw.learn('3:2:0.5 | 5')
vw.learn('1:2:0.5 | 15')
vw.learn('3:-2:0.5 | 15')
vw.learn('4:2:0.5 | 5')
vw.learn('4:2:0.5 | 15')
vw.learn('2:2:0.5 | 5')
vw.learn('2:2:0.5 | 15')
</code></pre>

<p>So for my example for the context with his feature equal to 5 the optimal action is 2 and for the other one the optimal action is  3. </p>

<p>When I predict on those two context, there is no problem since the algorithm meet them already once and had get a reward conditioning his choice. </p>

<p>But when I arrive with a new context I expect the algorithm to make me the most relevant action, for example by taking into account the similarity of the context features.</p>

<p>So for example if I give a feature that equal to 29, I'm expecting to get action 3, since 29 is more near to 15 than 5. </p>

<p>So that my interrogations right now.</p>

<p>Thanks ! </p>
",12094383,,,,,2020-01-16 19:39:22,(vowpal wabbit) contextual bandit dealing with new context,<python><reinforcement-learning><vowpalwabbit>,1,0,,,,CC BY-SA 4.0,
2273,5112531,1,5112552,,2011-02-25 01:12:20,,1,466,"<p>This question is to do with Q-learning.</p>

<p>Please consider the following:</p>

<ol>
<li>A loop(absorbing) state J- with reward 100 to go from J to J(J is the final state-the reward from going from I to J is also 100)</li>
<li>gamma value of 1</li>
<li>alpha value 0.5</li>
</ol>

<p>say the transition J to J has already got a Q value of 100. The new Q value is given by:
100+0.5(100+1(100)-100) where Q(max next possible states) is 100 as if you are in state J, to get the max possible next Q value, you would loop(so the max next poss Q value is what it currently is-100). This gives you a new Q value of 150. Taking this to a logical conclusion,every time you loop on J, the Q value goes up by 50 and that particular Q value will never converge and this seems wrong to me(is this wrong?).(the others values coverge). I've done this experiment so many times already and am still unsure about this. Please clarify the above point if you can. We have been taught Q learning very badly at my university, and I have a coursework to hand in in a week and a half.</p>

<p>Thanks!</p>
",277846,,,,,2012-02-23 10:12:21,Q learning algorithm-convergence on a loop(absorbing) state,<reinforcement-learning>,1,0,,,,CC BY-SA 2.5,
2274,41684801,1,41763847,,2017-01-16 20:52:01,,0,548,"<p>I'm doing policy gradient and I'm trying to figure out what the best objective function is for the task. The task is the open ai CartPole-v0 environment in which the agent receives a reward of 1 for each timestep it survives and a reward of 0 upon termination. I'm trying to figure out which is the best way to model the objective function. I've come up with 3 possible functions:</p>

<pre><code>def total_reward_objective_function(self, episode_data) :
    return sum([timestep_data['reward'] for timestep_data in timestep_data])

def average_reward_objective_function(self, episode_data):
    return total_reward_objective_function(episode_data) / len(episode_data)

def sum_of_discounted_rewards_objective_function(self, episode_data, discount_rate=0.7)
    return sum([episode_data[timestep]['reward'] * pow(discount_rate, timestep) 
        for timestep in enumerate(episode_data)])
</code></pre>

<p>Note that for the average reward objective function will always return 1 unless I intervene and modify the reward function to return a negative value upon termination. The reason I'm asking rather than just running a few experiments is because there's errors elsewhere. So if someone could point me towards a good practice in this area I could focus on the more significant mistakes in the algorithm.</p>
",1596288,,,,,2017-01-20 12:35:48,What's the best objective function for the CartPole task?,<machine-learning><gradient-descent><reinforcement-learning>,1,0,,,,CC BY-SA 3.0,
2276,41685575,1,41715173,,2017-01-16 21:51:10,,0,833,"<p>I've read on wikipedia
<a href=""https://en.wikipedia.org/wiki/Q-learning"" rel=""nofollow noreferrer"">https://en.wikipedia.org/wiki/Q-learning</a></p>

<blockquote>
  <p>Q-learning may suffer from slow rate of convergence, especially when the discount factor {\displaystyle \gamma } \gamma  is close to one.[16] Speedy Q-learning, a new variant of Q-learning algorithm, deals with this problem and achieves a slightly better rate of convergence than model-based methods such as value iteration</p>
</blockquote>

<p>So I wanted to try speedy q-learning, and see how better it is.</p>

<p>The only source about it I could find on the internet is this:
<a href=""https://papers.nips.cc/paper/4251-speedy-q-learning.pdf"" rel=""nofollow noreferrer"">https://papers.nips.cc/paper/4251-speedy-q-learning.pdf</a></p>

<p>That's the algorithm they suggest.</p>

<p><a href=""https://i.stack.imgur.com/GfqYx.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/GfqYx.png"" alt=""enter image description here""></a></p>

<p>Now, I didn't understand it. what excactly is TkQk, Am I supposed to have another list of q-values? Is there any clearer explanation than this?</p>

<pre><code> Q[previousState][action] = ((Q[previousState][action]+(learningRate * ( reward + discountFactor * maxNextExpectedReward - Q[previousState][action]) )));
</code></pre>

<p>this is my current QLearning algorithm, I want to replace it to speedy Q-learning.</p>
",1216343,,1216343,,2017-01-17 05:51:46,2017-01-18 09:06:39,Speedy Q-Learning,<machine-learning><reinforcement-learning><q-learning>,1,2,,,,CC BY-SA 3.0,
2277,60139303,1,60141863,,2020-02-09 17:27:19,,1,292,"<p>This code :</p>

<pre><code>R = ql.matrix([ [0,0,0,0,1,0],
        [0,0,0,1,0,1],
        [0,0,100,1,0,0],
        [0,1,1,0,1,0],
        [1,0,0,1,0,0],
        [0,1,0,0,0,0] ])
</code></pre>

<p>is from :</p>

<p><a href=""https://github.com/PacktPublishing/Artificial-Intelligence-By-Example/blob/47bed1a88db2c9577c492f950069f58353375cfe/Chapter01/MDP.py"" rel=""nofollow noreferrer"">https://github.com/PacktPublishing/Artificial-Intelligence-By-Example/blob/47bed1a88db2c9577c492f950069f58353375cfe/Chapter01/MDP.py</a></p>

<p>R is defined as the ""Reward matrix for each state"" . What are the states and rewards in this matrix ?</p>

<pre><code># Reward for state 0
print('R[0,]:' , R[0,])

# Reward for state 0
print('R[1,]:' , R[1,])
</code></pre>

<p>prints : </p>

<pre><code>R[0,]: [[0 0 0 0 1 0]]
R[1,]: [[0 0 0 1 0 1]]
</code></pre>

<p>Is <code>[0 0 0 0 1 0]</code> state0 &amp; <code>[0 0 0 1 0 1]</code> state1 ?</p>
",470184,,,,,2020-02-09 22:15:16,What are the states and rewards in the reward matrix?,<reinforcement-learning><markov-chains><markov-models>,1,0,,,,CC BY-SA 4.0,
2278,42336415,1,42340666,,2017-02-20 04:18:17,,1,736,"<p>I want to implement Ïµ-greedy policy action-selection policy in Q-learning. Here many people have used, following equation for decreasing rate of exploration, </p>

<p>É› = e^(-En)</p>

<p>n = the age of the agent </p>

<p>E = exploitation parameter</p>

<p>But I am not clear what does this ""n"" means? is that number of visits to a particular state-action pair OR is that the number of iterations?</p>

<p>Thanks a lot</p>
",4702833,,,,,2017-02-20 09:23:06,Ïµ-greedy policy with decreasing rate of exploration,<machine-learning><greedy><reinforcement-learning><q-learning>,1,0,,,,CC BY-SA 3.0,
2279,5427188,1,5735271,,2011-03-25 00:18:00,,1,428,"<p>I am working on a project that has a simulated robot exploring an unknown, but patterned environment (such as an office building) by moving around to predefined ""sensing locations"". In other words, at each point the robot must choose a new location to move to from the available visible locations. Our ultimate goal is to have the robot learn how to exploit the patterns in the environment to optimize global exploration time.</p>

<p>The robot chooses which location to move to next by giving it a utility score based on a linear combination of a number of known features about the location (such as distance to the point, average distance from the point to all others, area around the point already explored, etc.). My goal is to optimize the weights of this utility function to give the fastest time to explore the whole environment.</p>

<p>Because the score depends on the entire exploration path, I do not want to alter the weights mid-exploration. To test a combination of weights, I want the simulated robot to run through the entire environment with those weights, and get the resulting score. Therefore, I can create an |w|+1 x n array of data, where |w| is the number of weights, such as the following:</p>

<pre><code>w1    w2    w3     w4      score
0.23, 4.30, -0.33, -2.001, 17030
-1.3, 2.03, -10.1, -0.021, 21983
3.65, -1.1, 5.021, 0.2301, 19508
etc...
</code></pre>

<p>My question is, what sort of reinforcement learning algorithm would be best for this? Most of what I find in the literature and my research has to do with classification, and obviously multivariate regression wont work. I also tried implementing a q-learning algorithm, but this does not really work as there are a variable number of states and actions depending on the path taken and the structure of the environment. What I really want is some sort of structure that takes in row after row of the data, and determines the values of weights and their combinations that maximize the expected score. Any help/ideas? Thanks.</p>
",674558,,,,,2011-06-29 21:22:29,Reinforcement Learning - Optimizing Weights Given Scores,<optimization><neural-network><reinforcement-learning>,1,0,0,,,CC BY-SA 2.5,
2280,60164460,1,60176597,,2020-02-11 08:12:13,,1,557,"<blockquote>
  <p>What do we mean by 1 step/state MDP(Markov decision process) ?</p>
</blockquote>
",11714043,,,,,2020-02-18 07:33:55,Why the bandit problem is also called a one-step/state MDP in Reinforcement learning?,<machine-learning><reinforcement-learning><markov-decision-process><mdp><bandit>,2,0,0,,,CC BY-SA 4.0,
2281,42317356,1,42322260,,2017-02-18 15:57:33,,7,4835,"<p>As far as I know, NEAT (NeuroEvolution of Augmenting Topologies) is an algorithm that uses the concept of evolution to train a neural network. On the other hand, reinforcement learning is a type of machine learning with the concept of ""rewarding"" more successful nodes.</p>

<p>What is the difference between these two fields as they seem to be quite similar? Or is NEAT derived from reinforcement learning?</p>
",3600395,,3924118,,2018-11-11 13:29:52,2018-11-11 13:29:52,What is the relation between NEAT and reinforcement learning?,<machine-learning><artificial-intelligence><difference><reinforcement-learning><evolutionary-algorithm>,1,0,,,,CC BY-SA 4.0,
2282,60419966,1,60428722,,2020-02-26 18:06:14,,2,541,"<p>I am relatively new in this field, but I couldn't find anything similar to this problem.</p>

<p>The problem: An agent can move from state s1 to state s2 in many ways (in one step).
For example if states represent locations, assume that an agent can move from location represented by s1 to the one of s2 in one step, by taking one of the actions a1 or a2.
This means that multiple actions taken in some state lead to the same state.</p>

<p>Is there anything similar in the literature?</p>
",7484228,,,,,2020-02-27 08:04:15,Multiple actions that lead to the same state in Reinforcement Learning,<machine-learning><reinforcement-learning>,1,0,,,,CC BY-SA 4.0,
2285,6542274,1,6564294,,2011-06-30 23:47:29,,148,39159,"<p>I'm currently trying to get an ANN to play a video game and  and I was hoping to get some help from the wonderful community here.</p>

<p>I've settled on Diablo 2. Game play is thus in real-time and from an isometric viewpoint, with the player controlling a single avatar whom the camera is centered on. </p>

<p>To make things concrete, the task is to get your character x experience points without having its health drop to 0, where experience point are gained through killing monsters. Here is an example of the gameplay:</p>

<p><img src=""https://i.stack.imgur.com/2NslT.jpg"" alt=""here""></p>

<p>Now, since I want the net to operate based solely on the information it gets from the pixels on the screen, it must learn a very rich representation in order to play efficiently, since this would presumably require it to know (implicitly at least) how divide the game world up into objects and how to interact with them.</p>

<p>And all of this information must be taught to the net somehow. I can't for the life of me think of how to train this thing. My only idea is have a separate program visually extract something innately good/bad in the game (e.g. health, gold, experience) from the screen, and then use that stat in a reinforcement learning procedure. I think that will be <em>part</em> of the answer, but I don't think it'll be enough; there are just too many levels of abstraction from raw visual input to goal-oriented behavior for such limited feedback to train a net within my lifetime.</p>

<p>So, my question: what other ways can you think of to train a net to do at least some part of this task? preferably without making thousands of labeled examples.</p>

<p>Just for a little more direction: I'm looking for some other sources of reinforcement learning and/or any unsupervised methods for extracting useful information in this setting. Or a supervised algorithm if you can think of a way of getting labeled data out of a game world without having to manually label it.</p>

<p><strong>UPDATE(04/27/12):</strong></p>

<p>Strangely, I'm still working on this and seem to be making progress. The biggest secret to getting a ANN controller to work is to use the most advanced ANN architectures appropriate to the task. Hence I've been using a <a href=""http://www.scholarpedia.org/article/Deep_belief_networks"" rel=""noreferrer"">deep belief net</a> composed of factored <a href=""http://cs.nyu.edu/~gwtaylor/thesis/4/"" rel=""noreferrer"">conditional restricted Boltzmann machines</a> that I've trained in an unsupervised manner (on video of me playing the game) before fine tuning with <a href=""http://www.stanford.edu/group/pdplab/pdphandbook/handbookch10.html#x26-1290009"" rel=""noreferrer"">temporal difference back-propagation</a> (i.e. reinforcement learning with standard feed-forward ANNs).</p>

<p><em>Still looking for more valuable input though, especially on the problem of action selection in real-time and how to encode color images for ANN processing :-)</em> </p>

<p><strong>UPDATE(10/21/15):</strong></p>

<p>Just remembered I asked this question back-in-the-day, and thought I should mention that this is no longer a crazy idea. Since my last update, DeepMind published their nature <a href=""http://www.nature.com/nature/journal/v518/n7540/abs/nature14236.html"" rel=""noreferrer"">paper on getting neural networks to play Atari games from visual inputs</a>. Indeed, the  only thing preventing me from using their architecture to play, a limited subset, of Diablo 2 is the lack of access to the underlying game engine. Rendering to the screen and then redirecting it to the network is just far too slow to train in a reasonable amount of time. Thus we probably won't see this sort of bot playing Diablo 2 anytime soon, but only because it'll be playing something either open-source or with API access to the rendering target. (Quake perhaps?)</p>
",821806,,4157124,,2020-03-28 22:56:55,2020-03-28 22:56:55,How to train an artificial neural network to play Diablo 2 using visual input?,<machine-learning><computer-vision><neural-network><video-processing><reinforcement-learning>,7,7,0,,,CC BY-SA 4.0,
2288,59940538,1,60330136,,2020-01-27 23:51:21,,0,134,"<p>In A2C, the actor and critic algorithm, the weights are updated via equations:</p>

<p>delta = TD Error and</p>

<p>theta = theta + alpha*delta*[Grad(log(PI(a|s,theta)))] and</p>

<p>w = w + beta*delta*[Grad(V(s,w))]</p>

<p>So my question is, when using neural networks to implement this, </p>

<ol>
<li><p>how are the gradients calculated and </p></li>
<li><p>am I correct that the weights are updated via the optimization fmethods in TensorFlow or PyTorch?</p></li>
</ol>

<p>Thanks, Jon</p>
",6283174,,,,,2020-02-20 23:44:16,Gradient calculation in A2C,<tensorflow><pytorch><reinforcement-learning>,1,0,,,,CC BY-SA 4.0,
2291,60682153,1,60682640,,2020-03-14 11:20:51,,1,75,"<p>Reading <a href=""https://towardsdatascience.com/reinforcement-learning-temporal-difference-sarsa-q-learning-expected-sarsa-on-python-9fecfda7467e"" rel=""nofollow noreferrer"">https://towardsdatascience.com/reinforcement-learning-temporal-difference-sarsa-q-learning-expected-sarsa-on-python-9fecfda7467e</a> <code>epsilon_greedy</code> is defined as :</p>

<pre><code>def epsilon_greedy(Q, epsilon, n_actions, s, train=False):
    """"""
    @param Q Q values state x action -&gt; value
    @param epsilon for exploration
    @param s number of states
    @param train if true then no random actions selected
    """"""
    if train or np.random.rand() &lt; epsilon:
        action = np.argmax(Q[s, :])
    else:
        action = np.random.randint(0, n_actions)
    return action
</code></pre>

<p>Is the parameter <code>n_actions</code> the number of actions available to an agent ? So if an agent is learning to play football and the actions available are {kick, don't kick} <code>n_actions</code> = 2</p>
",470184,,,,,2020-03-14 12:18:53,reinforcement learning - number of actions,<python><reinforcement-learning>,1,0,0,,,CC BY-SA 4.0,
2298,42823213,1,42827405,,2017-03-16 00:26:54,,0,244,"<p>I'm having trouble understanding Monte Carlo policy evaluation algorithm. What I am reading is that <code>G</code> is the average return after visiting a particular state, lets say <code>s1</code>, for the first time. Does this mean averaging all rewards following that state <code>s1</code> to the end of the episode and then assigning the resulting value to <code>s1</code>? Or does it mean the immediate reward received for taking an action in <code>s1</code> averaged over multiple episodes?</p>
",3901400,,1497463,,2017-03-16 06:56:50,2017-03-16 07:07:43,Monte Carlo policy evaluation confusion,<montecarlo><reinforcement-learning>,1,0,,,,CC BY-SA 3.0,
2299,6848828,1,6852935,,2011-07-27 17:46:29,,139,67570,"<p>Although I know that <a href=""https://en.wikipedia.org/wiki/State%E2%80%93action%E2%80%93reward%E2%80%93state%E2%80%93action"" rel=""noreferrer"">SARSA</a> is on-policy while <a href=""https://en.wikipedia.org/wiki/Q-learning"" rel=""noreferrer"">Q-learning</a> is off-policy, when looking at their formulas it's hard (to me) to see any difference between these two algorithms.</p>
<p>According to the book <a href=""http://incompleteideas.net/book/bookdraft2017nov5.pdf"" rel=""noreferrer"">Reinforcement Learning: An Introduction</a> (by Sutton and Barto). In the SARSA algorithm, given a policy, the corresponding action-value function Q (in the state s and action a, at timestep t), i.e. Q(s<sub>t</sub>, a<sub>t</sub>), can be updated as follows</p>
<blockquote>
<p>Q(s<sub>t</sub>, a<sub>t</sub>) = Q(s<sub>t</sub>, a<sub>t</sub>) + Î±*(r<sub>t</sub> + Î³*Q(s<sub>t+1</sub>, a<sub>t+1</sub>) - Q(s<sub>t</sub>, a<sub>t</sub>))</p>
</blockquote>
<p>On the other hand, the update step for the Q-learning algorithm is the following</p>
<blockquote>
<p>Q(s<sub>t</sub>, a<sub>t</sub>) = Q(s<sub>t</sub>, a<sub>t</sub>) + Î±*(r<sub>t</sub> + Î³*max<sub>a</sub> Q(s<sub>t+1</sub>, a) - Q(s<sub>t</sub>, a<sub>t</sub>))</p>
</blockquote>
<p>which can also be written as</p>
<blockquote>
<p>Q(s<sub>t</sub>, a<sub>t</sub>) = (1 - Î±) * Q(s<sub>t</sub>, a<sub>t</sub>) + Î± * (r<sub>t</sub> + Î³*max<sub>a</sub> Q(s<sub>t+1</sub>, a))</p>
</blockquote>
<p>where Î³ (gamma) is the discount factor and r<sub>t</sub> is the reward received from the environment at timestep t.</p>
<p>Is the difference between these two algorithms the fact that SARSA only looks up the next policy value while Q-learning looks up the next <em>maximum</em> policy value?</p>
<p><strong>TLDR (and my own answer)</strong></p>
<p>Thanks to all those answering this question since I first asked it. I've made a <a href=""http://alexge233.github.io/relearn/"" rel=""noreferrer"">github repo</a> playing with Q-Learning and empirically understood what the difference is. It all amounts to how <em><strong>you select your next best action</strong></em>, which from an algorithmic standpoint can be a <em>mean</em>, <em>max</em> or <em>best</em> action depending on how you chose to implement it.</p>
<p>The other main difference is <em>when</em> this selection is happening (e.g., <em>online</em> vs <em>offline</em>) and how/why that affects learning. If you are reading this in 2019 and are more of a hands-on person, playing with a RL toy problem is probably the best way to understand the differences.</p>
<p>One last <strong>important</strong> note is that both Suton &amp; Barto as well as Wikipedia often have <em>mixed, confusing</em> or <em>wrong</em> formulaic representations with regards to the <em>next state best/max action and reward</em>:</p>
<blockquote>
<p>r(t+1)</p>
</blockquote>
<p>is in fact</p>
<blockquote>
<p>r(t)</p>
</blockquote>
<p>Hope this helps anyone ever getting stuck at this.</p>
",499699,,-1,,2020-06-20 09:12:55,2022-01-18 15:21:44,What is the difference between Q-learning and SARSA?,<artificial-intelligence><reinforcement-learning><q-learning><sarsa>,7,0,0,,,CC BY-SA 4.0,
2301,60917814,1,60920003,,2020-03-29 17:07:18,,0,87,"<p>Here a very basic model : </p>

<pre><code>class LinearDeepQNetwork(nn.Module):
    def __init__(self, lr, n_actions, input_dims):
        super(LinearDeepQNetwork, self).__init__()

        self.fc1 = nn.Linear(*input_dims, 128)
        self.fc2 = nn.Linear(128, n_actions)

        self.optimizer = optim.Adam(self.parameters(), lr=lr)
        self.loss = nn.MSELoss()
        self.device = T.device('cuda:0' if T.cuda.is_available() else 'cpu')
        self.to(self.device)

    def forward(self, state):
        layer1 = F.relu(self.fc1(state))
        actions = self.fc2(layer1)

        return actions
</code></pre>

<p>Be aware that I am using <code>Pytorch</code>, not <code>Keras</code> or <code>Tensorflow</code>. In my <code>Agent()</code> class, I instantiate <code>self.Q_eval = LinearDeepQNetwork(self.lr, self.n_actions, self.input_dims)</code>. Once I have trained my agent for several episodes, I need to output the weights of <code>self.Q_eval</code>. How can I do that? </p>
",13144280,,13144280,,2020-03-29 17:54:12,2020-03-29 19:58:59,Output the weights from a Pytorch model,<model><reinforcement-learning>,1,1,,,,CC BY-SA 4.0,
2303,42844728,1,42884690,,2017-03-16 21:03:00,,-1,371,"<p>So I'm trying to implement a reinforcement learning agent that plays tic-tac-toe. To do this, I need to create a data structure that stores the state of the game, the actions available at that state, and the reward for taking said actions given that state. Also, it has to be able to return the maximum reward of taking an available action given the state. Specifically, the state of the game here is given by a 9-vector that can have the values 0,1,2 (empty,player 1, player2). There are up to 9 possible actions to be taken at each step (Less if those squares have already been occupied)</p>

<p>I've come up with this:</p>

<pre><code>    #Input np.array([0,0,0,0,0,0,0,0]),1
class Tree():
    def __init__(self):
        self.data=defaultdict(nested_dict)
    def set(self,key,key2,value):        
        self.data[key[0]][key[1]][key[2]][key[3]][key[4]][key[5]][key[6]][key[7]][key[8]][key2]=value
    def get(self,key,key2=''):
        if(key2==''):
             return self.data[key[0]][key[1]][key[2]][key[3]][key[4]][key[5]][key[6]][key[7]][key[8]]
        return self.data[key[0]][key[1]][key[2]][key[3]][key[4]][key[5]][key[6]][key[7]][key[8]][key2]
    def get_max_child(self,state):
        return np.max(list(a.get(state).values()))
#Example of use
a=Tree()
a.set([0,0,0,0,0,0,0,0,0],7,1)
a.set([0,0,0,0,0,0,0,0,0],6,2)
a.get_max_child([0,0,0,0,0,0,0,0,0])
#Returns 2
</code></pre>

<p>How could this be made better? Some restrictions I am imposing myself are:</p>

<ol>
<li>Not precalculating all possible states and assigning them an
integer</li>
<li>I want to do this with a lookup table, not a neural network. That will come later.</li>
</ol>
",6126692,,,,,2017-03-19 08:54:58,Best way to store state space in python,<python><tree><reinforcement-learning>,1,0,,,,CC BY-SA 3.0,
2304,6910859,1,6919508,,2011-08-02 10:48:14,,6,975,"<p>I am planning to use <strong>neural networks</strong> for approximating a value function in a <strong>reinforcement learning</strong> algorithm. I want to do that to introduce some generalization and flexibility on how I represent states and actions. </p>

<p>Now, it looks to me that neural networks are the right tool to do that, however I have limited visibility here since I am not an AI expert. In particular, it seems that neural networks are being replaced by other technologies these days, e.g. support vector machines, but I am unsure if this is a fashion matter or if there is some real limitation in neural networks that could doom my approach. Do you have any suggestion?</p>

<p>Thanks,<br />
Tunnuz</p>
",25418,,,,,2014-04-23 06:22:37,Are neural networks really abandonware?,<neural-network><reinforcement-learning>,4,0,0,,,CC BY-SA 3.0,
2305,45199590,1,45486387,,2017-07-19 19:37:30,,2,133,"<p>I want my RL agent to reach the goal as quickly as possible and at the same time to minimize the number of times it uses a specific resource T (which sometimes though is necessary).</p>

<p>I thought of setting up the immediate rewards as -1 per step, an additional -1 if the agent uses T and 0 if it reaches the goal.</p>

<p>But the additional -1 is completely arbitrary, how do I decide how much punishment should the agent get for using T?</p>
",1273987,,,,,2017-08-03 13:57:16,How should one set up the immediate reward in a RL program?,<neural-network><artificial-intelligence><reinforcement-learning>,1,0,,,,CC BY-SA 3.0,
2306,61201800,1,61341123,,2020-04-14 06:24:39,,1,1106,"<p>I am rewriting a Reinforcement Learning Framework from serial code execution to parallel (multiprocessing) to reduce training time. It works but after a few hours of training a <code>MemoryError</code> is thrown. I tried adding <code>gc.collect</code> after each loop with no change.</p>

<p>Here's the for loop, that utilizes multiprocessing:</p>

<pre><code>for episode in episodes:
    env.episode = episode
    flex_list = [0,1,2]                                                                                          
    for machine in env.list_of_machines:                                                                            
        flex_plan = []                                                                                              
        for time_step in range(0,env.steplength):
            flex_plan.append(random.choice(flex_list))
        machine.flex_plan = flex_plan
    env.current_step = 0                                                                                            
    steps = []
    state = env.reset(restricted=True)                                                                              
    steps.append(state)

    # multiprocessing part, has condition to use a specific amount of CPUs or 'all' of them
    ####################################################
    func_part = partial(parallel_pool, episode=episode, episodes=episodes, env=env, agent=agent, state=state, log_data_qvalues=log_data_qvalues, log_data=log_data, steps=steps)
    if CPUs_used == 'all':
        mp.Pool().map(func_part, range(env.steplength-1))
    else:
        mp.Pool(CPUs_used).map(func_part, range(env.steplength-1))
    ############################################################
    # model is saved periodically, not only in the end
    save_interval = 100 #set episode interval to save models
    if (episode + 1) % save_interval == 0:
        agent.save_model(f'models/model_{filename}_{episode + 1}')
        print(f'model saved at episode {episode + 1}')

    plt.close()
    gc.collect()
</code></pre>

<p>Output after 26 episodes of training:</p>

<pre><code>Episode: 26/100   Action: 1/11    Phase: 3/3    Measurement Count: 231/234   THD fake slack: 0.09487   Psoll: [0.02894068 0.00046048 0.         0.        ]    Laptime: 0.181
Episode: 26/100   Action: 1/11    Phase: 3/3    Measurement Count: 232/234   THD fake slack: 0.09488   Psoll: [0.02894068 0.00046048 0.         0.        ]    Laptime: 0.181
Episode: 26/100   Action: 1/11    Phase: 3/3    Measurement Count: 233/234   THD fake slack: 0.09489   Psoll: [0.02894068 0.00046048 0.         0.        ]    Laptime: 0.179
Traceback (most recent call last):
  File ""C:/Users/Artur/Desktop/RL_framework/train.py"", line 87, in &lt;module&gt;
    main()
  File ""C:/Users/Artur/Desktop/RL_framework/train.py"", line 77, in main
    duration = cf.training(episodes, env, agent, filename, topology=topology, multi_processing=multi_processing, CPUs_used=CPUs_used)
  File ""C:\Users\Artur\Desktop\RL_framework\help_functions\custom_functions.py"", line 166, in training
    save_interval = parallel_training(range(episodes), env, agent, log_data_qvalues, log_data, filename, CPUs_used)
  File ""C:\Users\Artur\Desktop\RL_framework\help_functions\custom_functions.py"", line 81, in parallel_training
    mp.Pool().map(func_part, range(env.steplength-1))
  File ""C:\Users\Artur\Anaconda\lib\multiprocessing\pool.py"", line 268, in map
    return self._map_async(func, iterable, mapstar, chunksize).get()
  File ""C:\Users\Artur\Anaconda\lib\multiprocessing\pool.py"", line 657, in get
    raise self._value
  File ""C:\Users\Artur\Anaconda\lib\multiprocessing\pool.py"", line 431, in _handle_tasks
    put(task)
  File ""C:\Users\Artur\Anaconda\lib\multiprocessing\connection.py"", line 206, in send
    self._send_bytes(_ForkingPickler.dumps(obj))
  File ""C:\Users\Artur\Anaconda\lib\multiprocessing\reduction.py"", line 51, in dumps
    cls(buf, protocol).dump(obj)
MemoryError
</code></pre>

<p>Is there a way to fix this?</p>
",7826511,,,,,2020-04-21 10:17:41,multiprocessing.Pool.map throws MemoryError,<python><memory><multiprocessing><python-multiprocessing><reinforcement-learning>,1,1,,,,CC BY-SA 4.0,
2307,34364246,1,34368407,,2015-12-18 21:32:13,,6,813,"<p>I'm writing an assisted learning algorithm in Java.</p>

<p>I've run into a mathematical problem that I can probably solve, but because the processing will be heavy I need an optimum solution.</p>

<p>That being said, if anyone knows a optimized library that will be totally awesome, but the language is Java so that will need to be taken into consideration.</p>

<p>The idea is fairly simple:</p>

<p>Objects will store combination of variables such as ABDC, ACDE, DE, AE.</p>

<p>The max number of combination will be based on how many I can run without slowing down the program, so theoretically 100 lets say.</p>

<p>The decision process will generate one random variable per iteration. If the variable generated is part of one of the combinations, eg. 'A' which is part of ABDC and ACDE, than the propensity for C and B (or any following letter in a stored combination) will increase.</p>

<p>To make things a little more clear, lets assume that 'A', 'B', 'C', 'D', and 'E', are the only possible variables. The truth is, there will be more like 12 or 14, but that maximum will also depend on how many I can process without lag.</p>

<p>Since there are five possible variables, it will generate a weighted 1/5 random roll for the first iteration. If that roll turns out to be 'A', than in the next iteration 'B' and 'C' will now have 2/5 propensity instead of 1/5.</p>

<p>If the next iteration were to generate 'B', the 'D' propensity will increase to 3/5. Note: the relationship is exponential; realistically, it won't be 1/5 but a slight boost like 10%, which will snowball to say 50% if it reaches the 4th variable in a sequence. </p>

<p>Now, in Java, I can probably achieve this functionality by tracking all of the stored combinations for each object. I was thinking that by distributing the tracking process in small steps across each iteration, it shouldn't be too slow.</p>

<p>Another solution would be mapping all of the possible combinations and their potential propensities. This will of course simply require a search function, but also presents problems in calculating all of the possibilities and storing somewhere, probably in a file.</p>

<p>It has been suggested that I should use a Markov Model and/or library, though I am not too familiar with this type mathematics.</p>

<p>How can I compute this process quickly in Java?<br>
.  </p>

<p>Example >>> </p>

<p>Only one sequence ABC.</p>

<p>For three numbers, chances begin equal so it would look something like rand(1,3)</p>

<p>If A is the result, we increase the likelihood of B, because it is the next letter in the sequence. Lets say we double it. </p>

<p>So now the chances are: A=1/4, C=1/4, B=2/4</p>

<p>The function will now look like rand(1,4) where the results of 3 and 4 both represent option B.</p>

<p>If the next result is B we want to increase the likelihood of C because it is the next character in the sequence, but twice as much as it was increased last time (exponentially)</p>

<p>Chances are now something like: A=1/6, C=1/6, B=4/6 </p>

<p>The function is now rand(1/6) where values 3, 4, 5, 6 represent C.</p>
",5171360,,5171360,,2015-12-19 07:10:49,2015-12-19 15:14:21,Markov Model descision process in Java,<java><performance><artificial-intelligence><reinforcement-learning><markov-models>,1,11,0,,,CC BY-SA 3.0,
2310,53198503,1,53202849,,2018-11-07 22:00:42,,17,31393,"<p>I understand that epsilon marks the trade-off between exploration and exploitation. At the beginning, you want epsilon to be high so that you take big leaps and learn things. As you learn about future rewards, epsilon should decay so that you can exploit the higher Q-values you've found. </p>

<p>However, does our learning rate also decay with time in a stochastic environment? The posts on SO that I've seen only discuss epsilon decay. </p>

<p>How do we set our epsilon and alpha such that values converge? </p>
",6637004,,4099593,,2019-02-24 07:30:00,2022-09-12 13:17:35,Epsilon and learning rate decay in epsilon greedy q learning,<machine-learning><reinforcement-learning><q-learning>,3,0,0,,,CC BY-SA 4.0,
2313,68070368,1,68112472,,2021-06-21 15:08:20,,1,520,"<p>I am using <strong>Ray 1.3.0</strong> (<strong>for RLlib</strong>) with a combination of <strong>SUMO version 1.9.2</strong> for the simulation of a multi-agent scenario. I have configured RLlib to use a single <strong>PPO network</strong> that is commonly updated/used by all <strong>N</strong> agents. My evaluation settings look like this:</p>
<pre><code># === Evaluation Settings ===
# Evaluate with every `evaluation_interval` training iterations.
# The evaluation stats will be reported under the &quot;evaluation&quot; metric key.
# Note that evaluation is currently not parallelized, and that for Ape-X
# metrics are already only reported for the lowest epsilon workers.

&quot;evaluation_interval&quot;: 20,

# Number of episodes to run per evaluation period. If using multiple
# evaluation workers, we will run at least this many episodes total.

&quot;evaluation_num_episodes&quot;: 10,

# Whether to run evaluation in parallel to a Trainer.train() call
# using threading. Default=False.
# E.g. evaluation_interval=2 -&gt; For every other training iteration,
# the Trainer.train() and Trainer.evaluate() calls run in parallel.
# Note: This is experimental. Possible pitfalls could be race conditions
# for weight synching at the beginning of the evaluation loop.

&quot;evaluation_parallel_to_training&quot;: False,

# Internal flag that is set to True for evaluation workers.

&quot;in_evaluation&quot;: True,

# Typical usage is to pass extra args to evaluation env creator
# and to disable exploration by computing deterministic actions.
# IMPORTANT NOTE: Policy gradient algorithms are able to find the optimal
# policy, even if this is a stochastic one. Setting &quot;explore=False&quot; here
# will result in the evaluation workers not using this optimal policy!

&quot;evaluation_config&quot;: {
    # Example: overriding env_config, exploration, etc:
    &quot;lr&quot;: 0, # To prevent any kind of learning during evaluation
    &quot;explore&quot;: True # As required by PPO (read IMPORTANT NOTE above)
},

# Number of parallel workers to use for evaluation. Note that this is set
# to zero by default, which means evaluation will be run in the trainer
# process (only if evaluation_interval is not None). If you increase this,
# it will increase the Ray resource usage of the trainer since evaluation
# workers are created separately from rollout workers (used to sample data
# for training).

&quot;evaluation_num_workers&quot;: 1,

# Customize the evaluation method. This must be a function of signature
# (trainer: Trainer, eval_workers: WorkerSet) -&gt; metrics: dict. See the
# Trainer.evaluate() method to see the default implementation. The
# trainer guarantees all eval workers have the latest policy state before
# this function is called.

&quot;custom_eval_function&quot;: None,
</code></pre>
<p>What happens is <strong>every 20 iterations</strong> (each iteration collecting &quot;X&quot; training samples), there is an evaluation run of a <strong>minimum of 10 episodes</strong>. The sum of reward received by all <strong>N</strong> agents is summed over these episodes and that is set as the reward sum for that particular evaluation run. Over time, I notice that there is a pattern with the reward sums that repeats over the same interval of evaluation runs continuously, and the learning goes nowhere.</p>
<p><strong>UPDATE (23/06/2021)</strong></p>
<p>Unfortunately, I did not have TensorBoard activated for that particular run but from the mean rewards that were collected during evaluations (that happens every 20 iterations) of 10 episodes each, it is clear that there is a repeating pattern as shown in the annotated plot below:</p>
<p><a href=""https://i.stack.imgur.com/LpfKh.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/LpfKh.png"" alt=""Mean reward vs. number of iterations"" /></a></p>
<p>The 20 agents in the scenario should be learning to avoid colliding but instead continue to somehow stagnate at a certain policy and end up showing the exact same reward sequence during evaluation?</p>
<p>Is this a characteristic of how I have configured the evaluation aspect, or should I be checking something else? I would be grateful if anyone could advise or point me in the right direction.</p>
<p>Thank you.</p>
",3623131,,3623131,,2021-06-23 08:54:05,2021-06-24 08:47:31,How to prevent my reward sum received during evaluation runs repeating in intervals when using RLlib?,<reinforcement-learning><ray><multi-agent><reward><rllib>,2,2,0,,,CC BY-SA 4.0,
2316,52808170,1,52879498,,2018-10-15 00:06:40,,1,234,"<p>I am reading <a href=""http://www0.cs.ucl.ac.uk/staff/d.silver/web/Publications_files/tdsearch.pdf"" rel=""nofollow noreferrer"">Silver et al (2012) ""Temporal-Difference Search in Computer Go""</a>, and trying to understand the update order for the eligibility trace algorithm.
In the Algorithm 1 and 2 of the paper, weights are updated before updating the eligibility trace.  I wonder if this order is correct (Line 11 and 12 in the Algorithm 1, and Line 12 and 13 of the Algorithm 2).
Thinking about an extreme case with <code>lambda=0</code>, the parameter is not updated with the initial state-action pair (since <code>e</code> is still 0). So I doubt the order possibly should be the opposite.</p>

<p>Can someone clarify the point?</p>

<p>I find the paper very instructive for learning the reinforcement learning area, so would like to understand the paper in detail.</p>

<p>If there is a more suitable platform to ask this question, please kindly let me know as well.</p>

<p><a href=""https://i.stack.imgur.com/1MkxQ.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/1MkxQ.png"" alt=""enter image description here""></a>
<a href=""https://i.stack.imgur.com/YeGK7.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/YeGK7.png"" alt=""enter image description here""></a></p>
",5680348,,5680348,,2018-10-15 00:15:55,2018-10-18 17:28:20,"Eligibility trace algorithm, the update order",<algorithm><machine-learning><reinforcement-learning><sarsa><monte-carlo-tree-search>,1,1,,,,CC BY-SA 4.0,
2318,34536340,1,34544424,,2015-12-30 19:56:12,,13,3187,"<p>EDIT(1/3/16): <a href=""https://github.com/tensorflow/tensorflow/issues/672"" rel=""nofollow noreferrer"">corresponding github issue</a></p>

<p>I'm using Tensorflow (Python interface) to implement a <code>q-learning</code> agent with function approximation trained using <code>stochastic gradient descent</code>.</p>

<p>At each iteration of the experiment, a step function in the agent is called that updates the parameters of the approximator based on the new reward and activation, and then chooses a new action to perform.</p>

<p>Here is the problem(with reinforcement learning jargon):</p>

<ul>
<li>The agent computes its state-action value predictions to choose an action.</li>
<li>Then gives control back to another program that simulates a step in the environment.</li>
<li>Now the agent's step function is called for the next iteration.  I want to use Tensorflow's Optimizer class to compute the gradients for me.  However, this requires both the state-action value predictions that I computed the last step AND their graph.  So:

<ul>
<li>If I run the optimizer on the whole graph, then it has to recompute the state-action value predictions.</li>
<li>But, if I store the prediction (for the chosen action) as a variable, then feed it to the optimizer as a placeholder, it no longer has the graph necessary to compute the gradients.</li>
<li>I can't just run it all in the same <code>sess.run()</code> the statement, because I have to give up control and return the chosen action in order to get the next observation and reward (to use in the target for the loss function).</li>
</ul></li>
</ul>

<p>So, is there a way that I can (without reinforcement learning jargon):</p>

<ol>
<li>Compute part of my graph, returning value1.</li>
<li>Return value1 to the calling program to compute value2</li>
<li>In the next iteration, use value2 to as part of my loss function for gradient descent WITHOUT recomputing the part of the graph that computes value1.</li>
</ol>

<p>Of course, I've considered the obvious solutions:</p>

<ol>
<li><p>Just hardcode the gradients:  This would be easy for the really simple approximators I'm using now but would be really inconvenient if I were experimenting with different filters and activation functions in a big convolutional network.  I'd really like to use the Optimizer class if possible.</p></li>
<li><p>Call the environment simulation from within the agent:  <a href=""https://github.com/asrivat1/DeepLearningVideoGames/blob/master/deep_q_network.py#L135"" rel=""nofollow noreferrer"">This system</a> does this, but it would make mine more complicated, and remove a lot of the modularity and structure.  So, I don't want to do this.</p></li>
</ol>

<p>I've read through the API and whitepaper several times, but can't seem to come up with a solution.  I was trying to come up with some way to feed the target into a graph to calculate the gradients, but couldn't come up with a way to build that graph automatically.</p>

<p>If it turns out this isn't possible in TensorFlow yet, do you think it would be very complicated to implement this as a new operator? (I haven't used C++ in a couple of years, so the TensorFlow source looks a little intimidating.)  Or would I be better off switching to something like Torch, which has the imperative differentiation Autograd, instead of symbolic differentiation?</p>

<p>Thanks for taking the time to help me out with this.  I was trying to make this as concise as I could.</p>

<p>EDIT:  After doing some further searching I came across <a href=""https://stackoverflow.com/questions/32082506/neural-network-reinforcement-learning-requiring-next-state-propagation-for-backp"">this previously asked question</a>.  It's a little different than mine (they are trying to avoid updating an LSTM network twice every iteration in Torch), and doesn't have any answers yet.</p>

<p>Here is some code if that helps:</p>

<pre><code>'''
-Q-Learning agent for a grid-world environment.
-Receives input as raw RGB pixel representation of the screen.
-Uses an artificial neural network function approximator with one hidden layer

2015 Jonathon Byrd
'''

import random
import sys
#import copy
from rlglue.agent.Agent import Agent
from rlglue.agent import AgentLoader as AgentLoader
from rlglue.types import Action
from rlglue.types import Observation

import tensorflow as tf
import numpy as np

world_size = (3,3)
total_spaces = world_size[0] * world_size[1]

class simple_agent(Agent):

    #Contants
    discount_factor = tf.constant(0.5, name=""discount_factor"")
    learning_rate = tf.constant(0.01, name=""learning_rate"")
    exploration_rate = tf.Variable(0.2, name=""exploration_rate"")  # used to be a constant :P
    hidden_layer_size = 12

    #Network Parameters - weights and biases
    W = [tf.Variable(tf.truncated_normal([total_spaces * 3, hidden_layer_size], stddev=0.1), name=""layer_1_weights""), 
    tf.Variable(tf.truncated_normal([hidden_layer_size,4], stddev=0.1), name=""layer_2_weights"")]
    b = [tf.Variable(tf.zeros([hidden_layer_size]), name=""layer_1_biases""), tf.Variable(tf.zeros([4]), name=""layer_2_biases"")]

    #Input placeholders - observation and reward
    screen = tf.placeholder(tf.float32, shape=[1, total_spaces * 3], name=""observation"") #input pixel rgb values
    reward = tf.placeholder(tf.float32, shape=[], name=""reward"")

    #last step data
    last_obs = np.array([1, 2, 3], ndmin=4)
    last_act = -1

    #Last step placeholders
    last_screen = tf.placeholder(tf.float32, shape=[1, total_spaces * 3], name=""previous_observation"")
    last_move = tf.placeholder(tf.int32, shape = [], name=""previous_action"")

    next_prediction = tf.placeholder(tf.float32, shape = [], name=""next_prediction"")

    step_count = 0

    def __init__(self):
        #Initialize computational graphs
        self.q_preds = self.Q(self.screen)
        self.last_q_preds = self.Q(self.last_screen)
        self.action = self.choose_action(self.q_preds)
        self.next_pred = self.max_q(self.q_preds)
        self.last_pred = self.act_to_pred(self.last_move, self.last_q_preds) # inefficient recomputation
        self.loss = self.error(self.last_pred, self.reward, self.next_prediction)
        self.train = self.learn(self.loss)
        #Summaries and Statistics
        tf.scalar_summary(['loss'], self.loss)
        tf.scalar_summary('reward', self.reward)
        #w_hist = tf.histogram_summary(""weights"", self.W[0])
        self.summary_op = tf.merge_all_summaries()
        self.sess = tf.Session()
        self.summary_writer = tf.train.SummaryWriter('tensorlogs', graph_def=self.sess.graph_def)


    def agent_init(self,taskSpec):
        print(""agent_init called"")
        self.sess.run(tf.initialize_all_variables())

    def agent_start(self,observation):
        #print(""agent_start called, observation = {0}"".format(observation.intArray))
        o = np.divide(np.reshape(np.asarray(observation.intArray), (1,total_spaces * 3)), 255)
        return self.control(o)

    def agent_step(self,reward, observation):
        #print(""agent_step called, observation = {0}"".format(observation.intArray))
        print(""step, reward: {0}"".format(reward))
        o = np.divide(np.reshape(np.asarray(observation.intArray), (1,total_spaces * 3)), 255)

        next_prediction = self.sess.run([self.next_pred], feed_dict={self.screen:o})[0]

        if self.step_count % 10 == 0:
            summary_str = self.sess.run([self.summary_op, self.train], 
                feed_dict={self.reward:reward, self.last_screen:self.last_obs, 
                self.last_move:self.last_act, self.next_prediction:next_prediction})[0]

            self.summary_writer.add_summary(summary_str, global_step=self.step_count)
        else:
            self.sess.run([self.train], 
                feed_dict={self.screen:o, self.reward:reward, self.last_screen:self.last_obs, 
                self.last_move:self.last_act, self.next_prediction:next_prediction})

        return self.control(o)

    def control(self, observation):
        results = self.sess.run([self.action], feed_dict={self.screen:observation})
        action = results[0]

        self.last_act = action
        self.last_obs = observation

        if (action==0):  # convert action integer to direction character
            action = 'u'
        elif (action==1):
            action = 'l'
        elif (action==2):
            action = 'r'
        elif (action==3):
            action = 'd'
        returnAction=Action()
        returnAction.charArray=[action]
        #print(""return action returned {0}"".format(action))
        self.step_count += 1
        return returnAction

    def Q(self, obs):  #calculates state-action value prediction with feed-forward neural net
        with tf.name_scope('network_inference') as scope:
            h1 = tf.nn.relu(tf.matmul(obs, self.W[0]) + self.b[0])
            q_preds = tf.matmul(h1, self.W[1]) + self.b[1] #linear activation
            return tf.reshape(q_preds, shape=[4])

    def choose_action(self, q_preds):  #chooses action epsilon-greedily
        with tf.name_scope('action_choice') as scope:
            exploration_roll = tf.random_uniform([])
            #greedy_action = tf.argmax(q_preds, 0)  # gets the action with the highest predicted Q-value
            #random_action = tf.cast(tf.floor(tf.random_uniform([], maxval=4.0)), tf.int64)

            #exploration rate updates
            #if self.step_count % 10000 == 0:
                #self.exploration_rate.assign(tf.div(self.exploration_rate, 2))

            return tf.select(tf.greater_equal(exploration_roll, self.exploration_rate), 
                tf.argmax(q_preds, 0),   #greedy_action
                tf.cast(tf.floor(tf.random_uniform([], maxval=4.0)), tf.int64))  #random_action

        '''
        Why does this return NoneType?:

        flag = tf.select(tf.greater_equal(exploration_roll, self.exploration_rate), 'g', 'r')
        if flag == 'g':  #greedy
            return tf.argmax(q_preds, 0) # gets the action with the highest predicted Q-value
        elif flag == 'r':  #random
            return tf.cast(tf.floor(tf.random_uniform([], maxval=4.0)), tf.int64)
        '''

    def error(self, last_pred, r, next_pred):
        with tf.name_scope('loss_function') as scope:
            y = tf.add(r, tf.mul(self.discount_factor, next_pred)) #target
            return tf.square(tf.sub(y, last_pred)) #squared difference error


    def learn(self, loss): #Update parameters using stochastic gradient descent
        #TODO:  Either figure out how to avoid computing the q-prediction twice or just hardcode the gradients.
        with tf.name_scope('train') as scope:
            return tf.train.GradientDescentOptimizer(self.learning_rate).minimize(loss, var_list=[self.W[0], self.W[1], self.b[0], self.b[1]])


    def max_q(self, q_preds):
        with tf.name_scope('greedy_estimate') as scope:
            return tf.reduce_max(q_preds)  #best predicted action from current state

    def act_to_pred(self, a, preds): #get the value prediction for action a
        with tf.name_scope('get_prediction') as scope:
            return tf.slice(preds, tf.reshape(a, shape=[1]), [1])


    def agent_end(self,reward):
        pass

    def agent_cleanup(self):
        self.sess.close()
        pass

    def agent_message(self,inMessage):
        if inMessage==""what is your name?"":
            return ""my name is simple_agent"";
        else:
            return ""I don't know how to respond to your message"";

if __name__==""__main__"":
    AgentLoader.loadAgent(simple_agent())
</code></pre>
",5547331,,10413749,,2020-01-20 14:47:34,2020-01-20 14:47:34,How to use Tensorflow Optimizer without recomputing activations in reinforcement learning program that returns control after each iteration?,<python><tensorflow><machine-learning><reinforcement-learning><q-learning>,1,0,0,,,CC BY-SA 4.0,
2319,34553205,1,34558484,,2016-01-01 02:59:13,,8,2790,"<p>I came across <a href=""https://www.youtube.com/watch?v=u2t77mQmJiY"" rel=""noreferrer"">this interesting video on YouTube on genetic algorithms</a>.</p>

<p>As you can see in the video, the bots learn to fight.<br>
Now, I have been studying neural networks for a while and I wanted to start learning genetic algorithms.. This somehow combines both. </p>

<p>How do you combine genetic algorithms and neural networks to do this?<br>
And also how does one know the error in this case which you use to back-propagate and update your weights and train the net? And also how do you think the program in the video calculated its fitness function ? I guess mutation is definitely happening in the program in the video but what about crossover ?</p>

<p>Thanks!</p>
",5345443,,2838606,,2016-01-01 19:50:50,2019-10-15 20:19:24,How do neural networks use genetic algorithms and backpropagation to play games?,<neural-network><genetic-algorithm><reinforcement-learning>,3,3,0,,,CC BY-SA 3.0,
2321,52838439,1,52941588,,2018-10-16 14:59:43,,-1,483,"<p>In reinforcement learning, we empirically know using discrete actions is easier to train than using continuous actions.</p>

<p>But theoretically, continuous actions is more accurate and fast, just like our human, most of our actions are continuous. </p>

<p>So is there any method or related research that train a discrete action policy for easier start and then transfer that policy to output continuous actions for better precision?</p>

<p>Thanks.</p>
",10513608,,,,,2018-10-23 05:12:42,Transfer Discrete action to Continuous action in Reinforcement Learning,<machine-learning><reinforcement-learning>,1,0,0,,,CC BY-SA 4.0,
2322,27340967,1,35667432,,2014-12-07 08:27:27,,8,1113,"<p>I have implemented Q-Learning as described in,</p>

<p><a href=""http://web.cs.swarthmore.edu/~meeden/cs81/s12/papers/MarkStevePaper.pdf"" rel=""noreferrer"">http://web.cs.swarthmore.edu/~meeden/cs81/s12/papers/MarkStevePaper.pdf</a></p>

<p>In order to approx. Q(S,A) I use a neural network structure like the following,</p>

<ul>
<li>Activation sigmoid</li>
<li>Inputs, number of inputs + 1 for Action neurons (All Inputs Scaled 0-1)</li>
<li>Outputs, single output. Q-Value</li>
<li>N number of M Hidden Layers.</li>
<li>Exploration method random 0 &lt; rand() &lt; propExplore</li>
</ul>

<p>At each learning iteration using the following formula,</p>

<p><img src=""https://i.stack.imgur.com/e3hgc.png"" alt=""enter image description here""></p>

<p>I calculate a Q-Target value then calculate an error using,</p>

<pre><code>error = QTarget - LastQValueReturnedFromNN
</code></pre>

<p>and back propagate the error through the neural network.</p>

<p>Q1, Am I on the right track? I have seen some papers that implement a NN with one output neuron for each action.</p>

<p>Q2, My reward function returns a number between -1 and 1. Is it ok to return a number between -1 and 1 when the activation function is sigmoid (0 1)</p>

<p>Q3, From my understanding of this method given enough training instances it should be quarantined to find an optimal policy wight? When training for XOR sometimes it learns it after 2k iterations sometimes it won't learn even after 40k 50k iterations.</p>
",89904,,,,,2016-02-28 11:39:24,Questions about Q-Learning using Neural Networks,<machine-learning><artificial-intelligence><neural-network><reinforcement-learning><q-learning>,1,4,0,,,CC BY-SA 3.0,
2323,60958362,1,60964859,,2020-03-31 19:26:32,,1,418,"<p>I am trying to plot two plots next to each other in python, one a linear result of the experiment and one a logarithmic transformation. The goal would be to place the plots next to each other similar to <code>par(mfrow=c(1,2))</code> in R .</p>

<pre><code>if __name__ == '__main__':
  c_1 = run_experiment(1.0, 2.0, 3.0, 0.1, 100000) # run_experiment(m1, m2, m3, eps, N):
  c_05 = run_experiment(1.0, 2.0, 3.0, 0.05, 100000)
  c_01 = run_experiment(1.0, 2.0, 3.0, 0.01, 100000)

  # log scale plot
  plt.plot(c_1, label='eps = 0.10')
  plt.plot(c_05, label='eps = 0.05')
  plt.plot(c_01, label='eps = 0.01')
  plt.legend()
  plt.xscale('log')
  plt.title(label=""Log Multi-Arm Bandit"")
  plt.show()


  # linear plot
  plt.plot(c_1, label='eps = 0.10')
  plt.plot(c_05, label='eps = 0.05')
  plt.plot(c_01, label='eps = 0.01')
  plt.legend()
  plt.show()
</code></pre>

<p>I have tried many methods but seem to keep getting an error. Could someone implement this with the code. I'm relatively new in Python and mostly have experience in R but any help would mean the world to me. Below I will provide some code for an alteration that I attempted.</p>

<pre><code>  # log scale plot
  fig, axes = plt.subplots(122)
  ax1, ax2 = axes[0], axes[1]
  ax1.plot(c_1, label='eps = 0.10')
  ax1.plot(c_05,label='eps = 0.05')
  ax1.plot(c_01,label='eps = 0.01')
  ax1.legend()
  ax1.xscale('log')
  #plt.title(label=""Log Multi-Arm Bandit"")
  #plt.show()

  # linear plot
  ax2.plot(c_1, label='eps = 0.10')
  ax2.plot(c_05,label='eps = 0.05')
  ax2.plot(c_01, label='eps = 0.01')
  ax2.legend()
  plt.show()
</code></pre>

<p>But I received an error.</p>

<p>The full code that I am using to run an experiment is below from start to end. It is the multi-arm bandit problem from reinforcement learning.</p>

<pre><code># Premable
from __future__ import print_function, division
from builtins import range
import numpy as np
import matplotlib.pyplot as plt


class Bandit:
  def __init__(self, m):  # m is the true mean
    self.m = m
    self.mean = 0
    self.N = 0

  def pull(self): # simulated pulling bandits arm
    return np.random.randn() + self.m

  def update(self, x):
    self.N += 1
    # look at the derivation above of the mean
    self.mean = (1 - 1.0/self.N)*self.mean + 1.0/self.N*x  


def run_experiment(m1, m2, m3, eps, N):
  bandits = [Bandit(m1), Bandit(m2), Bandit(m3)]

  data = np.empty(N)

  for i in range(N): # Implement epsilon greedy shown above
    # epsilon greedy
    p = np.random.random()
    if p &lt; eps:
      j = np.random.choice(3) # Explore
    else:
      j = np.argmax([b.mean for b in bandits]) # Exploit
    x = bandits[j].pull()  # Pull and update
    bandits[j].update(x)

    # Results for the plot
    data[i] = x  # Store the results in an array called data of size N
    # Calculate cumulative average
  cumulative_average = np.cumsum(data) / (np.arange(N) + 1)


  # plot moving average ctr
  plt.plot(cumulative_average) # plot cumulative average
  # Plot bars with each of the means so we can see where are 
  # cumulative averages relative to means
  plt.plot(np.ones(N)*m1) 
  plt.title('Slot Machine ')
  plt.plot(np.ones(N)*m2)
  plt.title('Slot Machine ')
  plt.plot(np.ones(N)*m3)
  plt.title('Slot Machine ')
  # We do this on a log scale so that you can see the 
  # fluctuations in earlier rounds more clearly
  plt.xscale('log') 
  plt.show()

  for b in bandits:
    print(b.mean)

  return cumulative_average

if __name__ == '__main__':
  c_1 = run_experiment(1.0, 2.0, 3.0, 0.1, 100000) # run_experiment(m1, m2, m3, eps, N):
  c_05 = run_experiment(1.0, 2.0, 3.0, 0.05, 100000)
  c_01 = run_experiment(1.0, 2.0, 3.0, 0.01, 100000)

  # log scale plot
  plt.plot(c_1, label='eps = 0.10')
  plt.plot(c_05, label='eps = 0.05')
  plt.plot(c_01, label='eps = 0.01')
  plt.legend()
  plt.xscale('log')
  plt.title(label=""Log Multi-Arm Bandit"")
  plt.show()


  # linear plot
  plt.plot(c_1, label='eps = 0.10')
  plt.plot(c_05, label='eps = 0.05')
  plt.plot(c_01, label='eps = 0.01')
  plt.legend()
  plt.show()
</code></pre>
",4796942,,10659910,,2020-04-01 05:44:25,2020-04-01 06:29:35,"Plot linear plot and log plot next to each other in Python. Similar to mfrow=c(2,1) in R",<python><matplotlib><reinforcement-learning>,1,1,,,,CC BY-SA 4.0,
2324,34445590,1,34455305,,2015-12-24 00:09:40,,7,5365,"<p>I want to make a Neural Network that is trained using reinforcement learning in python. </p>

<pre><code>X -&gt; [ANN] -&gt; yEstimate -&gt; score! -&gt; (repeat until weights are optimised)
</code></pre>

<p>I'm using Scikit-learn at the moment but there doesn't seem to be all the neural networks stuff tries to fit <code>yEstimate</code> to <code>yTarget</code>.</p>

<p>Are there secrets to scikit-learn or are there other libraries that I don't know about for accomplishing this?</p>

<p>Thank you!</p>
",2399894,,,,,2015-12-25 09:10:06,Python Neural Network Reinforcement Learning,<python><machine-learning><scikit-learn><reinforcement-learning>,2,0,0,2015-12-25 10:05:21,,CC BY-SA 3.0,
2326,63362078,1,63362343,,2020-08-11 16:05:10,,1,83,"<p>I need to take one row from the CSV file to be used in the reinforcement learning class environment as an observation tuple. I have used generator function first it's not retrieving any data and secondly it will provide all the data iteratively which doesn't match with the requirement of my problem. Also, I need the currently selected observation(CSV row) to be used in multiple methods in the class environment for instance in the reward function.</p>
<p>Any idea or suggestion is highly appreciated on how to do this. Thanks</p>
<pre><code> class Environment1: 
    def __init__(self, data, max_ticks=300):
      self.data = data 
      self.application_latency=1342
      self.reward = 0
      #self.done = False
      self.MAX_TICKS = max_ticks
      self.episode_over = False

    def step(self, act):
      self.take_action(action)
      reward = self.get_reward()
      ob = self.get_state()
      return ob, reward, self.episode_over
      #return  ob, reward, self.done # obs, reward, done

    def get_state(self):
     &quot;&quot;&quot;Get the observation.  it is a tuple &quot;&quot;&quot;
     lst = [tuple(x) for x in data.values]
     def gen(last):
       for i in last:
         print(yield i)
         #observation_space= yield i
         #ob = (observation_space.Edge_Latency, observation_space.Cloud_latency )
         #print(ob)
      #return ob 
</code></pre>
",1566490,,,,,2020-08-11 16:28:55,How to retrieve one row at a time from the csv file using generator functions,<python-3.x><pandas><machine-learning><reinforcement-learning>,1,0,,,,CC BY-SA 4.0,
2328,68113725,1,68114690,,2021-06-24 10:08:27,,1,3491,"<p>I have a question around the representation of an observation in a gym environment.
I have actually several observation spaces with different  dimensions, let's say for example I have one camera with 24x24 pixels, then a xray machine with a 1x25 values, then 10 temperature sensors so 1x1 10 times.
So currently I have represented that with a spaces.Dict encapsulating the continuous values with some space.Box</p>
<pre><code>class MyEnv(gym.Env):
    def __init__(self, ...):
        spaces = {
                'xray': gym.spaces.Box(low=-np.inf, high=np.inf, shape=(nbcaptors, )),
                'cam1': gym.spaces.Box(low=-np.inf, high=np.inf, shape=(cam1width, cam1height)),
                'cam2': gym.spaces.Box(low=-np.inf, high=np.inf, shape=(cam2width, cam2height)),
                'thermal': gym.spaces.Box(low=-np.inf, high=np.inf, shape=(thermalwidth, thermalheight))
                    }
        self.observation_space = gym.spaces.Dict(spaces)
</code></pre>
<p>A custom agent can then process the data with:
observation['cam1'] or obervation['xray'] etc...</p>
<p>The problem is when i want to use third party algorithm, for example from stable-baselines3, they don't support spaces.Dict.
So my question is: how to solve that? should I just represent my obervation_space with a 1xn Box such as:</p>
<pre><code>self.observation_space = 
    gym.spaces.Box(low=-np.inf, high=np.inf, 
                   shape=(nbcaptors*cam1width*cam1height*cam2width*cam2height*thermalwidth*thermalheight,)
</code></pre>
<p>Does that make sense?
Even if it does I see 3 problems with this approach:</p>
<ol>
<li>the low and high of my 1d space might be not good  enough since for example other spaces could have some defined bounds.</li>
<li>it will be easier to make a mistake in the implementation</li>
<li>really those are 2d matrices, so i would have to convert 4 matrices to a location in the 1d obervation_space, and a custom agent would have then to rebuild the 4 matrices from the 1d observation. The original fast non RL based implementation already takes forever to run, so I'm afraid this overhead is going to slow down things.</li>
</ol>
<p>At this point I see only 2 ways to go:</p>
<ol>
<li>to map all my 4 matrices to a 1d array</li>
<li>to encapsulate my spaces.Dict gym.Env with another gym.Env which will handle the conversion from spaces.Dict to spaces.Box and use one agent or the other depending if I want to use a custom agent or a third party one.</li>
</ol>
<p>Would be grateful from some input on how to best tackle this problem, in term of performance and simplicity.</p>
<p>Thanks!</p>
",9973177,,,,,2021-12-09 07:20:23,openai gym observation space representation,<python><reinforcement-learning>,2,0,0,,,CC BY-SA 4.0,
2337,68141855,1,68211567,,2021-06-26 11:22:42,,1,265,"<p>I am trying to implement reinforcement learning in Anylogic using pathmind library, the RL agent can take either of two actions which is changing the priority rule of a Queue block.</p>
<p>I have a Queue block where I'm using priority-based queueing. I have two priority rules: using <em>agent's departure date</em> &amp; <em>agent's wait time</em>. I want to either of these rules during runtime using another function called doAction(action). A value 0 or 1 will be passed to this function. The function body would be like this:</p>
<pre><code>doAction(action){

if(action==0){
//set departure_date as priority rule of Queueblock}

else{
//set wait_time as priority rule of Queueblock}

}
</code></pre>
<p>The expression of my queue block is given here. <a href=""https://i.stack.imgur.com/wCT87.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/wCT87.png"" alt=""QueueBlock"" /></a>.</p>
<p>RL parameters are mentioned here. <a href=""https://i.stack.imgur.com/jZxdj.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/jZxdj.png"" alt=""RL Params"" /></a></p>
<p>What should be the code to set priority rule dynamically from the doAction(action) function?</p>
",12946205,,12946205,,2021-06-27 12:28:07,2021-07-01 13:57:52,Change priority rule of a Queue block at runtime in Anylogic,<simulation><reinforcement-learning><anylogic><agent-based-modeling><discrete>,3,0,,,,CC BY-SA 4.0,
2339,53307599,1,53348627,,2018-11-14 19:36:35,,6,3466,"<p>I am learning about the approach employed in Reinforcement Learning for robotics and I came across the concept of Evolutionary Strategies. But I couldn't understand how  RL and ES are different. Can anyone please explain?</p>
",9761439,,,,,2022-07-19 10:41:31,Difference between Evolutionary Strategies and Reinforcement Learning?,<deep-learning><reinforcement-learning><robotics><evolutionary-algorithm>,3,0,0,,,CC BY-SA 4.0,
2340,53378284,1,53378514,,2018-11-19 15:52:42,,0,70,"<p>I am trying to build a neural network to output a probabilistic distribution over set of whole action space. </p>

<p>My action space is a vector of 3 individual actions : <code>[a,b,c]</code> </p>

<p><code>a</code> can have 3 possible actions within itself <code>a1,a2,a3</code> and similarly <code>b</code> has <code>b1,b2,b3</code>, <code>c</code> has <code>c1,c2,c3</code>. So In total i can have 27 different combinations of these actions <code>3^3 = 27</code>. Ultimately the neural network should output 27 combinations of these actions (which is a matrix of 27 x 3) : <code>[[a1,b1,c1],[a2,b2,c2],[a3,b3,c3],[a1,b1,c2],[a1,b1,c3],.....]</code> and so on for all 27 combinations. Just to mention the input to my network is a state which is a vector of 5 elements. </p>

<p>I want a probability associated to each of these 27 combinations. </p>

<p>I know I can associate probability by using  a softmax with 27 outputs but I don't understand how the network can output a matrix in this case where every row has a probability associated to it. </p>
",8954691,,,,,2018-11-19 16:04:20,Return distribution over set of action space from Neural Network,<python><tensorflow><neural-network><probability><reinforcement-learning>,1,0,,,,CC BY-SA 4.0,
2341,63728800,1,64223512,,2020-09-03 17:27:14,,8,1618,"<p>I'm working in <strong>A2C</strong> reinforcement learning where my environment has an increasing and decreasing in the number of agents. As a result of the increasing and decreasing the number of agents, the state space will also change.  I have tried to solve the problem of changing the state space this way:</p>
<ul>
<li><p>If the state space exceeds the maximum state space that selected
as <code>n_input</code>, the excess state space will be selected by
<code>np.random.choice</code> where random choice provides a way of creating random samples from the state space after converting the state space into probabilities.</p>
</li>
<li><p>If the state space is less than the maximum state I padded the state
space with zeros.</p>
<pre><code>def get_state_new(state):
 n_features =  n_input-len(get_state(env))
 # print(&quot;state&quot;,len(get_state(env)))
 p = np.array(state)
 p = np.exp(p)
 if p.sum() != 1.0:
     p = p * (1. / p.sum())
 if len(get_state(env)) &gt; n_input:
     statappend = np.random.choice(state, size=n_input, p=p)
     # print(statappend)
 else:
     statappend = np.zeros(n_input)
     statappend[:state.shape[0]] = state
 return statappend
</code></pre>
</li>
</ul>
<p>It works but the results are not as expected and I don't know if this correct or not.</p>
<p><strong>My question</strong></p>
<p>Are there any reference papers that deal with such a problem and how to deal with the changing of state space?</p>
",4694757,,4694757,,2020-09-05 17:49:38,2021-02-06 12:22:21,How to deal with different state space size in reinforcement learning?,<python><tensorflow><reinforcement-learning>,2,0,0,,,CC BY-SA 4.0,
2346,46031578,1,46256607,,2017-09-04 06:43:05,,2,805,"<p>I'm new to reinforcement learning and q-learning and I'm trying to understand concepts and try to implement them. Most of material I have found use CNN layers to process image input. I think I would rather start with something simpler than than, so I use grid world.</p>

<p>This is what I have already implemented. I implemented an environment by following MDP and have 5x5 grid, with fixed agent position (A) and target position (T). Start state could look like this.</p>

<pre><code>-----
---T-
-----
-----
A----
</code></pre>

<p>Currently I represent my state as a 1-dimensional vector of length 25 (5x5) where 1 is on position where Agent is, otherwise 0, so for example
the state above will be repsented as vector</p>

<pre><code>[1, 0, 0, ..., 0]
</code></pre>

<p>I have successfully implemented solutions with Q table and simple NN with no hidden layer.</p>

<p>Now, I want to move little further and make the task more complicated by making Target position random each episode. Because now there is no correlation between my current representation of state and actions, my agents act randomly. In order to solve my problem, first I need to adjust my state representation to contain some information like distance to the target, direction or both. The problem is, that I don't how to represent my state now. I have come up with some ideas:</p>

<ol>
<li>[x, y, distance_T]</li>
<li>[distance_T]</li>
<li><p>two 5x5 vectors, one for Agent's position, one for Target's position</p>

<p>[1, 0, 0, ..., 0], [0, 0, ..., 1, 0, ..., 0]</p></li>
</ol>

<p>I know that even if I will figure out the state representation, my implemented model will not be able to solve the problem and I will need to move toward hidden layers, experience replay, frozen target network and so on, but I only want to verify the model failure.</p>

<p>In conclusion, I want to ask how to represent such state as an input for neural network. If there are any sources of informations, articles, papers etc. which I have missed, feel free to post them.</p>

<p>Thank you in advance.</p>
",5216517,,,,,2017-10-04 11:25:14,State representation for grid world,<neural-network><reinforcement-learning><q-learning>,2,0,0,,,CC BY-SA 3.0,
2347,53403311,1,53432579,,2018-11-20 23:47:30,,0,323,"<p>I try to implement an <a href=""https://arxiv.org/pdf/1602.01783.pdf"" rel=""nofollow noreferrer"">experience replay memory</a> with the <a href=""https://www.tensorflow.org/api_docs/python/tf/estimator/Estimator"" rel=""nofollow noreferrer"">tf.estimator.Estimator</a> API. However, i am not sure what is the best way to achieve a result which works atleast for all modes (<code>TRAIN</code>, <code>EVALUATE</code>, <code>PREDICT</code>). I tried the following:</p>

<ul>
<li>Implement the memory with a <code>tf.Variable</code>, which causes issues with the batching and the input pipeline (i cannot input a custom experience in testing or prediction phase)</li>
</ul>

<p>and currently try to:</p>

<ul>
<li>Implement the memory outside the <code>tf.Graph</code>. Set the values after each run with a <code>tf.train.SessionRunHook</code>. Load the experiences with <code>tf.data.Dataset.from_generator()</code> during training and testing. Manage the state on your own.</li>
</ul>

<p>I am failing on several points and starting to believe that the tf.estimator.Estimator API does not provide me with the necessary interfaces to easily write this down.</p>

<p>Some code (first approach, which fails with the batch_size, since it is fixed for the slicing of the exp, i cannot use the model for prediction or evaluation):</p>

<pre><code> def model_fn(self, features, labels, mode, params):
    batch_size = features[""matrix""].get_shape()[0].value

    # get prev_exp
    if mode == tf.estimator.ModeKeys.TRAIN:
        erm = tf.get_variable(""erm"", shape=[30000, 10], initializer=tf.constant_initializer(self.erm.initial_train_erm()), trainable=False)
        prev_exp = tf.slice(erm, [features[""index""][0], 0], [batch_size, 10])

    # model
    pred = model(features[""matrix""], prev_exp, params) 
</code></pre>

<p>However: it would be better to have the erm inside the feature dict. But then i have to manage the erm outside the graph and also write back my newest experience with a SessionRunHook. Is there any better way or am i missing something?</p>
",1171204,,2846062,,2018-11-21 05:26:49,2018-11-22 13:57:28,tensorflow - implementing experience replay memory with the estimator api,<python><tensorflow><deep-learning><reinforcement-learning><tensorflow-estimator>,1,0,,,,CC BY-SA 4.0,
2352,23235181,1,23235578,,2014-04-23 05:07:02,,6,1990,"<p>I have a read few papers and lectures on temporal difference learning (some as they pertain to neural nets, such as the Sutton tutorial on TD-Gammon) but I am having a difficult time understanding the equations, which leads me to my questions. </p>

<p>-Where does the prediction value V_t come from? And subsequently, how do we get V_(t+1)?</p>

<p>-What exactly is getting back propagated when TD is used with a neural net? That is, where does the error that gets back propagated come from when using TD?</p>
",3554641,,3554641,,2014-04-23 05:17:11,2014-04-23 05:35:12,Neural Network and Temporal Difference Learning,<artificial-intelligence><neural-network><backpropagation><reinforcement-learning><temporal-difference>,1,0,0,,,CC BY-SA 3.0,
2358,41826887,1,41915017,,2017-01-24 11:19:37,,0,328,"<p><a href=""https://classroom.udacity.com/courses/ud600/lessons/3780788560/concepts/40374085350923"" rel=""nofollow noreferrer"">https://classroom.udacity.com/courses/ud600/lessons/3780788560/concepts/40374085350923</a></p>

<p>At the above link it refers that in order to create the initial state of a graph domain you execute this command:
GraphDefinedDomain.getState(domain, 0)</p>

<p>But <strong>getState</strong> does <strong>not exist</strong> as a static method at the current Burlap library.</p>

<p>So how could you create the initial state node of a graph domain in Burlap (<a href=""http://burlap.cs.brown.edu/"" rel=""nofollow noreferrer"">http://burlap.cs.brown.edu/</a>)?</p>

<p>(what is the version I am seeing, how much Burlac has changed since then and where could I find a migration guide? could also help)</p>
",720484,,,,,2017-01-28 21:27:52,How could you create the initial state node of a graph domain in Burlap?,<java><reinforcement-learning>,1,0,0,,,CC BY-SA 3.0,
2361,42044608,1,42052804,,2017-02-04 19:23:08,,1,147,"<p>Are there well understood guidelines of when to use dropout versus simply get more data? I had previously understood that, given sufficient data, one wouldn't want to use dropout. However, I recently had a model (4 layer LSTM processing audio input) that empirically converged to a certain loss no matter how much data I provided - and was then improved significantly when I added dropout.</p>

<p>Is this phenomena well understood? Should one always use dropout then, even when there is more (potentially infinite) data available?</p>

<p>Follow-up: if so, I haven't seen much mention of dropout in RL papers. I'd assumed this is because there is infinite generable data. Are there other nuances to consider here about the fraction of the state space explored, or the heterogeneity of the training data available etc?</p>
",5365672,,,,,2017-02-05 13:46:43,Does dropout improve models even with access to infinite data?,<tensorflow><deep-learning><lstm><reinforcement-learning>,2,0,,,,CC BY-SA 3.0,
2362,47559290,1,47559403,,2017-11-29 18:19:24,,1,2229,"<p>As part of Q learning an objective is to maximize the expected utility. I know</p>
<p>Reading wikipedia  :
<a href=""https://en.wikipedia.org/wiki/Q-learning"" rel=""nofollow noreferrer"">https://en.wikipedia.org/wiki/Q-learning</a> describes expected utility in following contexts :</p>
<blockquote>
<p>It works by learning an action-value function that ultimately gives
the expected utility of taking a given action in a given state and
following the optimal policy thereafter.</p>
<p>One of the strengths of Q-learning is that it is able to compare the
expected utility of the available actions without requiring a model of
the environment.</p>
</blockquote>
<p>But does not define what utility is, what is meant by utility ?</p>
<p>When maximizing <code>utility</code> what exactly is being maximized ?</p>
",470184,,-1,,2020-06-20 09:12:55,2017-11-30 07:59:34,What is utility?,<reinforcement-learning>,2,0,,,,CC BY-SA 3.0,
2363,64816793,1,64816841,,2020-11-13 07:05:52,,2,253,"<p>As stated in the Wikipedia <a href=""https://en.wikipedia.org/wiki/Q-learning#Learning_Rate"" rel=""nofollow noreferrer"">https://en.wikipedia.org/wiki/Q-learning#Learning_Rate</a>,
for a stochastic problem, using the learning rate is important for convergence. Although I tried to find the &quot;intuition&quot; behind the reason without any mathematical proof, I could not find it.</p>
<p>Specifically, it is difficult for me to understand why updating q-values slowly is beneficial for a stochastic environment. Could anyone please explain the intuition or motivation?</p>
",7533175,,,,,2020-11-13 07:12:25,Why the learning rate for Q-learning is important for stochastic environments?,<reinforcement-learning><q-learning><stochastic-process>,1,1,,,,CC BY-SA 4.0,
2364,42547787,1,42548042,,2017-03-02 05:52:49,,0,865,"<p>As far as I understand Q-learning, a Q-value is a measure of ""how good"" a particular state-action pair is. This is usually represented in a table in one of the following ways (see fig.):</p>

<p><a href=""https://i.stack.imgur.com/ahSMb.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/ahSMb.png"" alt=""enter image description here""></a></p>

<ol>
<li>Are both representations valid?</li>
<li>How do you determine the best action if the Q-table is given as a state to state transition table (as shown in the top q-table in the figure), especially if the state transitions are not deterministic (i.e. taking an action from a state can land you in different states at different times?)</li>
</ol>
",7598050,,3782161,,2017-03-02 10:48:24,2017-03-02 10:48:24,Q-table representation,<reinforcement-learning><q-learning>,1,1,,,,CC BY-SA 3.0,
2367,53429724,1,53432748,,2018-11-22 11:13:37,,1,303,"<p>I am initializing the state of my environment with some value <code>s'</code>. 
Also i reinitialize the state of the environment everytime a new epsiode starts. But I have noticed that when I make the environment and initialize the state as lets say <code>[10,3]</code> , the Policy obtained after the training is not close to the optimal at all. However with other states lets say <code>[20,3].[20,7]....</code> etc I get results quite close to the optimal. So the question is , is it possible that starting from a state <code>[10,3]</code> might result in the network getting stuck at local minimas ? </p>
",8954691,,,,,2018-11-22 14:06:46,Initialization state in DQN,<deep-learning><reinforcement-learning>,1,0,,,,CC BY-SA 4.0,
2374,47750291,1,48088365,,2017-12-11 09:58:27,,0,260,"<p>So i am using a deepQ implementation using tensorflow to solve the CartPole-v0, however the output sometimes (40% of all runs) stays stuck at 9. I tried fixing the seed, using tf.set_random_seed, but that still doesn't ensure that the output wont be stuck. This is my code:</p>

<pre><code>from collections import deque
import tensorflow as tf
import numpy as np
import random
import gym
import matplotlib.pyplot as plt
import pickle
from time import time
t = int(time())
class DQNAgent:

    def __init__(self, state_size, action_size):
        self.state_size = state_size
        self.action_size = action_size
        self.memory = deque(maxlen = 2000)
        self.gamma = 0.95
        #self.epsilon = 1.0
        #self.epsilon_min = 0.01
        #self.epsilon_decay = 0.995
        self.learning_rate = 0.001
        self.model = self._build_model()

    def _build_model(self):
        graph = tf.Graph()
        with graph.as_default():
            inp = tf.placeholder(tf.float32, [None, self.state_size])
            out = tf.placeholder(tf.float32, [None, self.action_size])
            w1 = tf.Variable(tf.truncated_normal([self.state_size, 24]))
            b1 = tf.Variable(tf.zeros([24]))

            hidden = tf.nn.tanh(tf.matmul(inp, w1) + b1)

            w2 = tf.Variable(tf.truncated_normal([24, 24]))
            b2 = tf.Variable(tf.zeros([24]))

            hidden1 = tf.nn.tanh(tf.matmul(hidden, w2) + b2)

            w3 = tf.Variable(tf.truncated_normal([24, 24]))
            b3 = tf.Variable(tf.zeros([24]))

            hidden2 = tf.nn.tanh(tf.matmul(hidden1, w3) + b3)

            wo = tf.Variable(tf.truncated_normal([24, self.action_size]))
            bo = tf.Variable(tf.zeros([self.action_size]))

            prediction = tf.matmul(hidden2, wo) + bo

            loss = tf.losses.mean_squared_error(out, prediction)
            train = tf.train.AdamOptimizer().minimize(loss)
            init = tf.global_variables_initializer()

        return graph, inp, out, prediction, train, init

    def remember(self, state, action, reward, next_state, done):
        self.memory.append((state, action, reward, next_state, done))

    def act(self, state, sess):
        act_values = sess.run(self.model[3], feed_dict = { self.model[1]: state})
        return np.argmax(act_values[0])

    def replay(self, batch_size, sess):
        try:
            minibatch = random.sample(self.memory, batch_size)
        except ValueError:
            minibatch = self.memory
        for state, action, reward, next_state, done in minibatch:
            target = reward
            if not done:
                target = reward + self.gamma * np.amax(sess.run(self.model[3], feed_dict = { self.model[1]: next_state}))
            target_f = sess.run(self.model[3], feed_dict = { self.model[1]: state})
            target_f[0][action] = target
            #print(target_f)
            sess.run(self.model[4], feed_dict = { self.model[1]: state, self.model[2]: target_f})

if __name__ == ""__main__"":
    environment = 'CartPole-v0'
    env = gym.make(environment)
    avgs = deque(maxlen = 50)
    rewardLA = []
    agent = DQNAgent(env.observation_space.shape[0], env.action_space.n)
    sess = tf.Session(graph = agent.model[0])
    sess.run(agent.model[5])
    episodes = 10000
    rewardL = []
    for e in range(episodes):
        state = env.reset()
        state = np.reshape(state, [1, 4])
        for time_t in range(500):
            #env.render()
            action = agent.act(state, sess)
            next_state, reward, done, _ = env.step(action)
            next_state = np.reshape(next_state, [1, 4])
            agent.remember(state, action, reward, next_state, done)
            state = next_state
            if done:
                break
        avgs.append(time_t)
        rewardLA.append(sum(avgs)/len(avgs))
        print(""episode: "", e, ""score: "", time_t)
        rewardL.append(time_t)
        agent.replay(32, sess)
    #pickle.dump(rewardL, open(environment + ""_"" + str(t) + ""_rewardL.pickle"", ""wb""))
    plt.plot(rewardLA)
    plt.show()
</code></pre>

<p>I tried changing the optimiser to GD, rmsProp, but nothing works, but if i simply restart the code, it works better (gets to 199, in 200 epochs). Why is this happening? How do i fix it.</p>
",5096669,,1000551,,2017-12-11 10:04:54,2018-01-04 03:45:23,Deep Q score stuck at 9 for CartPole,<python><python-3.x><machine-learning><tensorflow><reinforcement-learning>,1,0,0,,,CC BY-SA 3.0,
2379,61149054,1,61618566,,2020-04-10 21:37:34,,2,252,"<p>I try to save the model using the saver method (I use the save function in the DDPG class to save), but when restoring the model, the result is far from the one I saved (I save the model when the episodic award is zero, the restor method in the code is commented out ) My code is below with all the features. I use Python 3.7, gym 0.16.0 and TensorFlow version 1.13.1</p>

<pre><code>import tensorflow as tf
import numpy as np
import gym

epsiode_steps = 500

# learning rate for actor
lr_a = 0.001

# learning rate for critic
lr_c = 0.002

gamma = 0.9
alpha = 0.01
memory = 10000
batch_size = 32
render = True

class DDPG(object):
    def __init__(self, no_of_actions, no_of_states, a_bound, ):
        self.memory = np.zeros((memory, no_of_states * 2 + no_of_actions + 1), dtype=np.float32)

        # initialize pointer to point to our experience buffer
        self.pointer = 0

        self.sess = tf.Session()

        # initialize the variance for OU process for exploring policies
        self.noise_variance = 3.0

        self.no_of_actions, self.no_of_states, self.a_bound = no_of_actions, no_of_states, a_bound,

        self.state = tf.placeholder(tf.float32, [None, no_of_states], 's')
        self.next_state = tf.placeholder(tf.float32, [None, no_of_states], 's_')
        self.reward = tf.placeholder(tf.float32, [None, 1], 'r')

        with tf.variable_scope('Actor'):
            self.a = self.build_actor_network(self.state, scope='eval', trainable=True)
            a_ = self.build_actor_network(self.next_state, scope='target', trainable=False)

        with tf.variable_scope('Critic'):
            q = self.build_crtic_network(self.state, self.a, scope='eval', trainable=True)
            q_ = self.build_crtic_network(self.next_state, a_, scope='target', trainable=False)

        self.ae_params = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope='Actor/eval')
        self.at_params = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope='Actor/target')

        self.ce_params = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope='Critic/eval')
        self.ct_params = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope='Critic/target')

        # update target value
        self.soft_replace = [
            [tf.assign(at, (1 - alpha) * at + alpha * ae), tf.assign(ct, (1 - alpha) * ct + alpha * ce)]
            for at, ae, ct, ce in zip(self.at_params, self.ae_params, self.ct_params, self.ce_params)]

        q_target = self.reward + gamma * q_

        # compute TD error i.e  actual - predicted values
        td_error = tf.losses.mean_squared_error(labels=(self.reward + gamma * q_), predictions=q)

        # train the critic network with adam optimizer
        self.ctrain = tf.train.AdamOptimizer(lr_c).minimize(td_error, name=""adam-ink"", var_list=self.ce_params)

        a_loss = - tf.reduce_mean(q)

        # train the actor network with adam optimizer for minimizing the loss
        self.atrain = tf.train.AdamOptimizer(lr_a).minimize(a_loss, var_list=self.ae_params)

        tf.summary.FileWriter(""logs2"", self.sess.graph)

        # initialize all variables
        self.sess.run(tf.global_variables_initializer())

        # saver
        self.saver = tf.train.Saver()
        # self.saver.restore(self.sess, ""Pendulum/nn.ckpt"")  

    def choose_action(self, s):
        a = self.sess.run(self.a, {self.state: s[np.newaxis, :]})[0]
        a = np.clip(np.random.normal(a, self.noise_variance), -2, 2)

        return a

    def learn(self):
        # soft target replacement
        self.sess.run(self.soft_replace)

        indices = np.random.choice(memory, size=batch_size)
        batch_transition = self.memory[indices, :]
        batch_states = batch_transition[:, :self.no_of_states]
        batch_actions = batch_transition[:, self.no_of_states: self.no_of_states + self.no_of_actions]
        batch_rewards = batch_transition[:, -self.no_of_states - 1: -self.no_of_states]
        batch_next_state = batch_transition[:, -self.no_of_states:]

        self.sess.run(self.atrain, {self.state: batch_states})
        self.sess.run(self.ctrain, {self.state: batch_states, self.a: batch_actions, self.reward: batch_rewards,
                                    self.next_state: batch_next_state})

    def store_transition(self, s, a, r, s_):
        trans = np.hstack((s, a, [r], s_))

        index = self.pointer % memory
        self.memory[index, :] = trans
        self.pointer += 1

        if self.pointer &gt; memory:
            self.noise_variance *= 0.99995
            self.learn()

    def build_actor_network(self, s, scope, trainable):
        # Actor DPG
        with tf.variable_scope(scope):
            l1 = tf.layers.dense(s, 30, activation=tf.nn.tanh, name='l1', trainable=trainable)
            a = tf.layers.dense(l1, self.no_of_actions, activation=tf.nn.tanh, name='a', trainable=trainable)
            return tf.multiply(a, self.a_bound, name=""scaled_a"")

    def build_crtic_network(self, s, a, scope, trainable):
        with tf.variable_scope(scope):
            n_l1 = 30
            w1_s = tf.get_variable('w1_s', [self.no_of_states, n_l1], trainable=trainable)
            w1_a = tf.get_variable('w1_a', [self.no_of_actions, n_l1], trainable=trainable)
            b1 = tf.get_variable('b1', [1, n_l1], trainable=trainable)
            net = tf.nn.tanh(tf.matmul(s, w1_s) + tf.matmul(a, w1_a) + b1)

            q = tf.layers.dense(net, 1, trainable=trainable)
            return q

    def save(self):
        self.saver.save(self.sess, ""Pendulum/nn.ckpt"")

env = gym.make(""Pendulum-v0"")
env = env.unwrapped
env.seed(1)

no_of_states = env.observation_space.shape[0]
no_of_actions = env.action_space.shape[0]

a_bound = env.action_space.high
ddpg = DDPG(no_of_actions, no_of_states, a_bound)

total_reward = []

# set the number of episodes
no_of_episodes = 300
for i in range(no_of_episodes):
    # initialize the environment
    s = env.reset()
    ep_reward = 0

    for j in range(epsiode_steps):
        env.render()

        # select action by adding noise through OU process
        a = ddpg.choose_action(s)

        # peform the action and move to the next state s
        s_, r, done, info = env.step(a)

        # store the the transition to our experience buffer
        # sample some minibatch of experience and train the network
        ddpg.store_transition(s, a, r, s_)

        # update current state as next state
        s = s_

        # add episodic rewards
        ep_reward += r

        if int(ep_reward) == 0 and i &gt; 150:
            ddpg.save()
            print(""save"")
            quit()

        if j == epsiode_steps - 1:
            total_reward.append(ep_reward)
            print('Episode:', i, ' Reward: %i' % int(ep_reward))
            break
</code></pre>
",10846582,,,,,2020-05-05 16:54:08,How can I save DDPG model?,<python><tensorflow><machine-learning><reinforcement-learning>,1,0,,,,CC BY-SA 4.0,
2380,61162864,1,61169469,,2020-04-11 19:52:18,,0,431,"<p>I programmed a very easy game which works the following way:</p>

<p>Given an 4x4 field of squares, a player can move (up, right, down or left). </p>

<ul>
<li><p>Going on a square the agent never visited before gives the reward 1.</p></li>
<li><p>Stepping on ""dead-field"" is rewarded with -5 and then the game will be resetted.</p></li>
<li><p>Moving on a field that was already visited is rewarded with -1</p></li>
<li><p>Going on the ""win-field"" (there's exactly one) gives the reward 5 and the game will be resetted as well.</p></li>
</ul>

<hr>

<p>Now I want an AI to learn to play that game via Q-Learning. </p>

<p>How I organized the Inputs / feature engineering:</p>

<p>An input for the net is an array with the shape 1x4 where arr[0] represents the field above (when moving up), arr[1] represents the field to the right, arr[2] the one below, arr[3] the one to the left.</p>

<p>Possible values the array can hold: 0, 1, 2, 3</p>

<p>0 = ""dead field"", so the worst case</p>

<p>1 = this would be outside of the 4x4 field (so you can't step there) or the field was already visited</p>

<p>2 = unvisited field (so that is something good)</p>

<p>3 = ""win field"", so the best-case</p>

<p>As you see, I ordered them by their reward. </p>

<p>Since the game takes an input the same way (0 = move up, 1 = move to the right, 2 = move down, 3 = move to the left), the only thing the AI would have to learn is basically: Choose the array index that holds the greatest value.</p>

<p>But unforntunately it doesn't work, the net just doesn't learn, not even after 30.000 episodes.</p>

<hr>

<p>Here's my code (including the game at the beginning):</p>

<pre><code>import numpy as np
import random
Import tensorflow as tf
import matplotlib.pyplot as plt

from time import sleep

episoden = 0

felder = []
schon_besucht = []

playerx = 0
playery = 0

grafik = False

def gib_zustand():
    # besonderes feature engineering:
    # input besteht nur aus einer richtung, die one-hot-encoded ist; also 4 inputneuronen
    # (glut, wand/besucht, unbesucht, sieg)
    #
    # es ist die richtung, die bewertet werden soll (also 1 outputneuron fuer eine richtung)

    # rueckgabe hier: array, shape: 4x4 (s.o.)

    global playerx
    global playery

    # oben 
    if playery == 0:
        oben = 1
    else:
        oben = felder[playery-1][playerx]

    # rechts
    if playerx == 4:
        rechts = 1
    else:
        rechts = felder[playery][playerx+1]

    # unten
    if playery == 4:
        unten = 1
    else:
        unten = felder[playery+1][playerx]

    # links
    if playerx == 0:
        links = 1
    else:
        links = felder[playery][playerx-1]

    return np.array([oben, rechts, unten, links])

def grafisch():
    if grafik:

        # encoding:
        # glut = G, besucht = b, unbesucht = , sieg = S, Spieler = X
        global felder
        global playerx
        global playery

        print('')

        for y in range(0,5):
            print('|', end='')
            for x in range(0,5):
                if felder[y][x] == 0:
                    temp = 'G'
                if felder[y][x] == 1:
                    temp = 'b'
                if felder[y][x] == 2:
                    temp = ' '
                if felder[y][x] == 3:
                    temp = 'S'
                if y == playery and x == playerx:
                    temp = 'X'

                print(temp, end='')
                print('|', end='')
            print('')

def reset():
    print('--- RESET ---')

    global playery
    global playerx
    global felder
    global schon_besucht

    playerx = 1
    playery = 3

    # anordnung
    # glut = 0, wand/besucht = 1, unbesucht = 2, sieg = 3

    felder = [[2 for x in range(0,5)] for y in range(0,5)]
    # zwei mal glut setzen
    gl1 = random.randint(1,3)
    gl1_1 = random.randint(2,3) if gl1==3 else (random.randint(1,2) if gl1==1 else random.randint(1,3))
    felder[gl1][gl1_1] = 0 # glut

    # zweites mal
    gl1 = random.randint(1,3)
    gl1_1 = random.randint(2,3) if gl1==3 else (random.randint(1,2) if gl1==1 else random.randint(1,3))
    felder[gl1][gl1_1] = 0 # glut

    # pudding
    felder[1][3] = 3

    # ruecksetzen
    schon_besucht = []

    grafisch()

    return gib_zustand()

def step(zug):
    # 0 = oben, 1 = rechts, 2 = unten, 3 = links
    global playerx
    global playery
    global felder
    global schon_besucht

    if zug == 0:
        if playery != 0:
            playery -= 1
    if zug == 1:
        if playerx != 4:
            playerx += 1
    if zug == 2:
        if playery != 4:
            playery += 1
    if zug == 3:
        if playerx != 0:
            playerx -= 1

    # belohnung holen
    wert = felder[playery][playerx]

    if wert==0:
        belohnung = -5
    if wert==1:
        belohnung = -1
    if wert==2:
        belohnung = 1
    if wert==3:
        belohnung = 5

    # speichern wenn nicht verloren
    if belohnung != -5:
        schon_besucht.append((playery,playerx))
        felder[playery][playerx] = 1

    grafisch()

    return gib_zustand(), belohnung, belohnung==5, 0 # 0 damits passt

episoden = 0

tf.reset_default_graph()

#These lines establish the feed-forward part of the network used to choose actions
inputs1 = tf.placeholder(shape=[1,4],dtype=tf.float32)
#W1 = tf.Variable(tf.random_uniform([16,8],0,0.01))
W2 = tf.Variable(tf.random_uniform([4,4],0,0.01))
#schicht2 = tf.matmul(inputs1,W1)
Qout = tf.matmul(inputs1,W2)
predict = tf.argmax(Qout,1)

#Below we obtain the loss by taking the sum of squares difference between the target and prediction Q values.
nextQ = tf.placeholder(shape=[1,4],dtype=tf.float32)
loss = tf.reduce_sum(tf.square(nextQ - Qout))
trainer = tf.train.GradientDescentOptimizer(learning_rate=0.1)
updateModel = trainer.minimize(loss)

init = tf.initialize_all_variables()

# Set learning parameters
y = .99
e = 0.1
num_episodes = 10_000
#create lists to contain total rewards and steps per episode
jList = []
rList = []
with tf.Session() as sess:
    sess.run(init)
    for i in range(num_episodes):             
        #Reset environment and get first new observation
        s = reset()
        rAll = 0
        d = False
        j = 0
        #The Q-Network        
        while j &lt; 99:
            j+=1
            #Choose an action by greedily (with e chance of random action) from the Q-network
            a,allQ = sess.run([predict,Qout],feed_dict={inputs1:s.reshape(1,4)}) # berechnet prediction fuer input (input scheint hier one hot encoded zu sein)
            if np.random.rand(1) &lt; e:
                a[0] = random.randint(0,3)                 

            #Get new state and reward from environment
            s1,r,d,_ = step(a[0])
            #Obtain the Q' values by feeding the new state through our network
            Q1 = sess.run(Qout,feed_dict={inputs1:s1.reshape(1,4)})
            #Obtain maxQ' and set our target value for chosen action.
            maxQ1 = np.max(Q1)


            targetQ = allQ
            targetQ[0,a[0]] = r + y*maxQ1
            #Train our network using target and predicted Q values

            _,W1 = sess.run([updateModel,W2],feed_dict={inputs1:s.reshape(1,4),nextQ:targetQ})
            rAll += r
            s = s1

            if r == -5 or r == 5:
                if r == 5:
                    episoden+=1

                reset()

                #Reduce chance of random action as we train the model.
                e = 1./((i/50) + 10)
                break
        jList.append(j)
        #print(rAll)
        rList.append(rAll)
print(""Percent of succesful episodes: "" + str((episoden/num_episodes)*100) + ""%"")
plt.plot(rList)
plt.plot(jList)
</code></pre>

<p>I read in a simular question, that a reason for too high Q-Values can be, that it is in fact possible for the agent to get unlimited high total rewards in a game. That would be the case here, if the agent could step on already visited fields and would get a reward of 1. Then of course, the possible total reward would be infinity. But that isn't the case here: The player gets a bad reward (-1) when he does that. Little calculation: win-field is rewarded with 5. Unvisited field with 1. There's at least one dead-field. All in all there are 16 fields. Max possible total reward: 14*1 + 1*5 = 19</p>
",12232720,,,,,2020-04-12 12:56:05,"Reinforcement Learning doesn't work for this VERY EASY game, why? Q Learning",<python><tensorflow><reinforcement-learning>,1,0,,,,CC BY-SA 4.0,
2382,61118197,1,61162133,,2020-04-09 09:41:01,,1,600,"<p>I programmed a very easy game which works the following way:</p>

<p>Given an 4x4 field of squares, a player can move (up, right, down or left). </p>

<ul>
<li><p>Going on a square the agent never visited before gives the reward 1.</p></li>
<li><p>Stepping on ""dead-field"" is rewarded with -5 and then the game will be resetted.</p></li>
<li><p>Moving on a field that was already visited is rewarded with -1</p></li>
<li><p>Going on the ""win-field"" (there's exactly one) gives the reward 5 and the game will be resetted as well.</p></li>
</ul>

<hr>

<p>Now I want an AI to learn to play that game via Q-Learning. </p>

<p>How I organized the Inputs / feature engineering:</p>

<p>An input for the net is an array with the shape 1x4 where arr[0] represents the field above (when moving up), arr[1] represents the field to the right, arr[2] the one below, arr[3] the one to the left.</p>

<p>Possible values the array can hold: 0, 1, 2, 3</p>

<p>0 = ""dead field"", so the worst case</p>

<p>1 = this would be outside of the 4x4 field (so you can't step there) or the field was already visited</p>

<p>2 = unvisited field (so that is something good)</p>

<p>3 = ""win field"", so the best-case</p>

<p>As you see, I ordered them by their reward. </p>

<p>Since the game takes an input the same way (0 = move up, 1 = move to the right, 2 = move down, 3 = move to the left), the only thing the AI would have to learn is basically: Choose the array index that holds the greatest value.</p>

<p>But unforntunately it doesn't work, the expected Q-Values that are fed to the neural net are getting higher and higher. They rise to NaN.</p>

<hr>

<p>Here's my code (including the game at the beginning):</p>

<pre><code>import numpy as np
import random
Import tensorflow as tf
import matplotlib.pyplot as plt

from time import sleep

episoden = 0

felder = []
schon_besucht = []

playerx = 0
playery = 0

grafik = False

def gib_zustand():
    # besonderes feature engineering:
    # input besteht nur aus einer richtung, die one-hot-encoded ist; also 4 inputneuronen
    # (glut, wand/besucht, unbesucht, sieg)
    #
    # es ist die richtung, die bewertet werden soll (also 1 outputneuron fuer eine richtung)

    # rueckgabe hier: array, shape: 4x4 (s.o.)

    global playerx
    global playery

    # oben 
    if playery == 0:
        oben = 1
    else:
        oben = felder[playery-1][playerx]

    # rechts
    if playerx == 4:
        rechts = 1
    else:
        rechts = felder[playery][playerx+1]

    # unten
    if playery == 4:
        unten = 1
    else:
        unten = felder[playery+1][playerx]

    # links
    if playerx == 0:
        links = 1
    else:
        links = felder[playery][playerx-1]

    return np.array([oben, rechts, unten, links])

def grafisch():
    if grafik:

        # encoding:
        # glut = G, besucht = b, unbesucht = , sieg = S, Spieler = X
        global felder
        global playerx
        global playery

        print('')

        for y in range(0,5):
            print('|', end='')
            for x in range(0,5):
                if felder[y][x] == 0:
                    temp = 'G'
                if felder[y][x] == 1:
                    temp = 'b'
                if felder[y][x] == 2:
                    temp = ' '
                if felder[y][x] == 3:
                    temp = 'S'
                if y == playery and x == playerx:
                    temp = 'X'

                print(temp, end='')
                print('|', end='')
            print('')

def reset():
    print('--- RESET ---')

    global playery
    global playerx
    global felder
    global schon_besucht

    playerx = 1
    playery = 3

    # anordnung
    # glut = 0, wand/besucht = 1, unbesucht = 2, sieg = 3

    felder = [[2 for x in range(0,5)] for y in range(0,5)]
    # zwei mal glut setzen
    gl1 = random.randint(1,3)
    gl1_1 = random.randint(2,3) if gl1==3 else (random.randint(1,2) if gl1==1 else random.randint(1,3))
    felder[gl1][gl1_1] = 0 # glut

    # zweites mal
    gl1 = random.randint(1,3)
    gl1_1 = random.randint(2,3) if gl1==3 else (random.randint(1,2) if gl1==1 else random.randint(1,3))
    felder[gl1][gl1_1] = 0 # glut

    # pudding
    felder[1][3] = 3

    # ruecksetzen
    schon_besucht = []

    grafisch()

    return gib_zustand()

def step(zug):
    # 0 = oben, 1 = rechts, 2 = unten, 3 = links
    global playerx
    global playery
    global felder
    global schon_besucht

    if zug == 0:
        if playery != 0:
            playery -= 1
    if zug == 1:
        if playerx != 4:
            playerx += 1
    if zug == 2:
        if playery != 4:
            playery += 1
    if zug == 3:
        if playerx != 0:
            playerx -= 1

    # belohnung holen
    wert = felder[playery][playerx]

    if wert==0:
        belohnung = -5
    if wert==1:
        belohnung = -1
    if wert==2:
        belohnung = 1
    if wert==3:
        belohnung = 5

    # speichern wenn nicht verloren
    if belohnung != -5:
        schon_besucht.append((playery,playerx))
        felder[playery][playerx] = 1

    grafisch()

    return gib_zustand(), belohnung, belohnung==5, 0 # 0 damits passt

episoden = 0

tf.reset_default_graph()

#These lines establish the feed-forward part of the network used to choose actions
inputs1 = tf.placeholder(shape=[1,4],dtype=tf.float32)
#W1 = tf.Variable(tf.random_uniform([16,8],0,0.01))
W2 = tf.Variable(tf.random_uniform([4,4],0,0.01))
#schicht2 = tf.matmul(inputs1,W1)
Qout = tf.matmul(inputs1,W2)
predict = tf.argmax(Qout,1)

#Below we obtain the loss by taking the sum of squares difference between the target and prediction Q values.
nextQ = tf.placeholder(shape=[1,4],dtype=tf.float32)
loss = tf.reduce_sum(tf.square(nextQ - Qout))
trainer = tf.train.GradientDescentOptimizer(learning_rate=0.1)
updateModel = trainer.minimize(loss)

init = tf.initialize_all_variables()

# Set learning parameters
y = .99
e = 0.1
num_episodes = 10_000
#create lists to contain total rewards and steps per episode
jList = []
rList = []
with tf.Session() as sess:
    sess.run(init)
    for i in range(num_episodes):             
        #Reset environment and get first new observation
        s = reset()
        rAll = 0
        d = False
        j = 0
        #The Q-Network        
        while j &lt; 99:
            j+=1
            #Choose an action by greedily (with e chance of random action) from the Q-network
            a,allQ = sess.run([predict,Qout],feed_dict={inputs1:s.reshape(1,4)}) # berechnet prediction fuer input (input scheint hier one hot encoded zu sein)
            if np.random.rand(1) &lt; e:
                a[0] = random.randint(0,3)                 

            #Get new state and reward from environment
            s1,r,d,_ = step(a[0])
            #Obtain the Q' values by feeding the new state through our network
            Q1 = sess.run(Qout,feed_dict={inputs1:s1.reshape(1,4)})
            #Obtain maxQ' and set our target value for chosen action.
            maxQ1 = np.max(Q1)


            targetQ = allQ
            targetQ[0,a[0]] = r + y*maxQ1
            #Train our network using target and predicted Q values

            _,W1 = sess.run([updateModel,W2],feed_dict={inputs1:s.reshape(1,4),nextQ:targetQ})
            rAll += r
            s = s1

            if r == -5 or r == 5:
                if r == 5:
                    episoden+=1

                reset()

                #Reduce chance of random action as we train the model.
                e = 1./((i/50) + 10)
                break
        jList.append(j)
        #print(rAll)
        rList.append(rAll)
print(""Percent of succesful episodes: "" + str((episoden/num_episodes)*100) + ""%"")
plt.plot(rList)
plt.plot(jList)
</code></pre>

<p>As I said, after a certain time I get outputs like this:</p>

<pre><code>##
[[1 1 1 1]]
[[nan nan nan nan]]
##
[[1 1 1 1]]
[[nan nan nan nan]]
##
[[1 2 1 1]]
[[nan nan nan nan]]
</code></pre>
",12232720,,12232720,,2020-04-10 10:47:35,2020-04-12 10:28:31,"Q-values get too high, values become NaN, Q-Learning Tensorflow",<python><tensorflow><machine-learning><reinforcement-learning><q-learning>,1,6,,,,CC BY-SA 4.0,
2383,7098625,1,7100856,,2011-08-17 19:54:44,,49,37327,"<p>I'm trying to get an agent to learn the mouse movements necessary to best perform some task in a reinforcement learning setting (i.e. the reward signal is the only feedback for learning).</p>

<p>I'm hoping to use the Q-learning technique, but while I've found <a href=""http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.17.2539&amp;rep=rep1&amp;type=pdf"">a way to extend this method to continuous state spaces</a>, I can't seem to figure out how to accommodate a problem with a continuous action space.</p>

<p>I could just force all mouse movement to be of a certain magnitude and in only a certain number of different directions, but any reasonable way of making the actions discrete would yield a huge action space. Since standard Q-learning requires the agent to evaluate <em>all</em> possible actions, such an approximation doesn't solve the problem in any practical sense.</p>
",821806,,3924118,,2019-02-19 08:56:47,2019-07-09 05:39:43,How can I apply reinforcement learning to continuous action spaces?,<algorithm><machine-learning><reinforcement-learning><q-learning>,6,0,0,,,CC BY-SA 4.0,
2384,47838665,1,47840536,,2017-12-15 19:21:15,,1,473,"<p>According to <a href=""https://medium.com/applied-data-science/alphago-zero-explained-in-one-diagram-365f5abf67e0"" rel=""nofollow noreferrer"">AlphaGo Cheat Sheet</a>, AlphaGo Zero uses a <em>sequence of consecutive board configurations</em> to encode its <strong>game state</strong>. </p>

<p>In theory, all the necessary information is contained in the latest state, and yet they include the previous 7 configurations.</p>

<p>Why did they choose to inject so much complexity ? </p>

<p>What are they listening for ??</p>

<p><a href=""https://deepmind.com/blog/alphago-zero-learning-scratch/"" rel=""nofollow noreferrer"">AlphaGoZero</a> </p>

<p><a href=""https://i.stack.imgur.com/tWLoH.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/tWLoH.png"" alt=""Game State""></a></p>
",3303844,,,,,2017-12-15 22:11:56,AlphaGo Zero board evaluation function uses multiple time steps as an input... Why?,<neural-network><deep-learning><artificial-intelligence><torch><reinforcement-learning>,1,0,,,,CC BY-SA 3.0,
2386,47933380,1,50013537,,2017-12-21 22:08:40,,1,267,"<p>I'm trying to install mujoco-py, when I try <code>pip install mujoco_py</code> in my maxOS 10.12 with python 3.5 in anaconda, I got </p>

<pre><code>  /usr/local/bin/gcc-7 -Wno-unused-result -Wsign-compare -Wunreachable-code -DNDEBUG -g -fwrapv -O3 -Wall -I/Users/Shangtong/anaconda2/envs/Python3.5/include -I/Users/Shangtong/anaconda2/envs/Python3.5/include -Wno-error=unused-command-line-argument-hard-error-in-future -Wno-error=unused-command-line-argument-hard-error-in-future -Wno-error=unused-command-line-argument-hard-error-in-future -DONMAC -Imujoco_py -I/private/var/folders/6j/gtq1cx25611ck3bft9rnfxkh0000gn/T/pip-build-3r460esi/mujoco-py/mujoco_py -I/Users/Shangtong/.mujoco/mjpro150/include -I/Users/Shangtong/anaconda2/envs/Python3.5/lib/python3.5/site-packages/numpy/core/include -I/Users/Shangtong/anaconda2/envs/Python3.5/include/python3.5m -c /private/var/folders/6j/gtq1cx25611ck3bft9rnfxkh0000gn/T/pip-build-3r460esi/mujoco-py/mujoco_py/cymj.c -o /private/var/folders/6j/gtq1cx25611ck3bft9rnfxkh0000gn/T/pip-build-3r460esi/mujoco-py/mujoco_py/generated/_pyxbld_MacExtensionBuilder/temp.macosx-10.6-x86_64-3.5/private/var/folders/6j/gtq1cx25611ck3bft9rnfxkh0000gn/T/pip-build-3r460esi/mujoco-py/mujoco_py/cymj.o -fopenmp -w
  cc1: error: -Werror=unused-command-line-argument-hard-error-in-future: no option -Wunused-command-line-argument-hard-error-in-future
  cc1: error: -Werror=unused-command-line-argument-hard-error-in-future: no option -Wunused-command-line-argument-hard-error-in-future
  cc1: error: -Werror=unused-command-line-argument-hard-error-in-future: no option -Wunused-command-line-argument-hard-error-in-future
  error: command '/usr/local/bin/gcc-7' failed with exit status 1
</code></pre>

<p>I have to install <code>gcc</code> because it doesn't accept <code>clang</code>. It seems <code>gcc</code> doesn't ignore this unknown flag. I tried </p>

<pre><code>ARCHFLAGS=-Wno-error=unused-command-line-argument-hard-error-in-future pip install mujoco_py
</code></pre>

<p>But it didn't help. Is there any way that <code>gcc</code> can ignore this unknown flag?</p>
",3650053,,,,,2018-04-25 03:16:44,How to ignore compiler flags in pip?,<python><gcc><pip><reinforcement-learning>,1,0,,,,CC BY-SA 3.0,
2396,61741641,1,61772628,,2020-05-12 00:20:15,,0,324,"<p>I am very new to RL and wondering about the capabilities of RL. In my understand RL is a kind of Neural Network that feeding into a state and output the probability of each action. The training process is to mitigate the difference between the predicted value and the value of the real rewards (may be wrong here). </p>

<p>However, my problem is very tricky. In the beginning, there is an action space [x1, x2, x3, x4,..,x5], after each step, the action cannot be repeated. In other words, the action space is shrinking after each iterate. The 'game' is done when the action space is 0. The goal of this game is to get the highest accumulated rewards. </p>

<p>I did some search online but I failed to find any useful information. Thanks so much!</p>

<p>Added:
Sorry for the unclear problem. For a classical RL such as CartPole game. The action is repeatable, so the agent is learning the reward of each action at each state. The goal is to get a reward '1' rather than '0'. But for my question, since the game will be done anyway (because the action space is decreasing every iterate and the game is done when action space is empty), I hope the agent can do the action with the highest reward firstly, and then second-highest reward.....</p>

<p>So I believe this is some kind of optimization problem. But I don't know how to modify the classical architecture of RL learning for this problem? Or could anyone help me find some related sources or tutorials?</p>

<p>Added Again:
Currently, my solution is to change the part of how an action is picked. I fed a list of previous actions. And avoid these actions to be picked again. For example, if the action is picked by neural network, I made the output of the neural network by </p>

<pre><code>with torch.no_grade():
   action_from_nn = nn(state)
   action_from_nn[actions_already_taken] = 0
   action = torch.max(action_from_nn,1)[1]
</code></pre>

<p>if the random value smaller than epsilon, the action is just randomly chosen, it will be:</p>

<pre><code>action = random.sample([i for i in action_space if i not in actions_already_taken], 1)[0]
</code></pre>

<p>As can be seen, I only forced the agent not to choose the repeat actions. But I didn't really change the output of the neural network. I am wondering is that OK? Or is there any space for future improvement?</p>
",11429035,,11429035,,2020-05-13 13:21:33,2020-05-13 13:21:33,Reinforcement learning with non repeatable actions,<reinforcement-learning>,1,1,,,,CC BY-SA 4.0,
2397,48476039,1,48476751,,2018-01-27 12:57:45,,1,349,"<p>I am implementing a Go playing program roughly according to the architecture of earlier versions of AlphaGo(AlphaGo Fan or AlphaGo Lee), e.g. using policy network, value network, and Monte Carlo tree search(MCTS). Currently I have trained a decent policy network and an insensitive value network, and I don't have a fast roll-out policy. By ""insensitive"" I mean, the value network is not able to judge complicated situations, only outputing a win rate around 50% unless the situation is concise. The value network can judge concise board(no big fight going on) correctly.</p>

<p>Using this policy network and value network, I also implemented MCTS algorithm(The evaluation of a tree node is done only by value network). Since the value network is not accurate, I am afraid MCTS is prone to be trapped in bad moves before the time of MCTS is up. In order to better fine-tune the hyper parameters of MCTS to remedy the bad influence brought by inaccurate value network, I have two questions to ask:</p>

<ol>
<li>Node selection is done by <code>arg max (p_value + lambda * p_policy/visit_cnt)</code>. Does fine-tune the parameter <code>lambda</code> help? </li>
<li>Intuitively I want MCTS to explore as further as possible. In node expansion stage, does setting the expansion condition as <code>expand a leaf once it is visited a very small number of times, like 3</code> help? What expansion method should I use? </li>
</ol>

<p>EDIT: The second question is about the 'expand' stage of typical 'selection, expand, evaluation, backup' MCTS algorithm. I reckon by expand as quickly as possible, the MCTS can explore deeper, and give more accurate value approximations. I set a parameter <code>n</code> as <code>how many times a leaf node is visited before it is expanded</code>. I want to know intuitively, what a large <code>n</code> and a small <code>n</code> would influence the performance of MCTS.</p>
",4542163,,4542163,,2018-01-27 14:16:08,2018-01-27 14:25:27,Reinforcement Learning: fine-tuning MCTS node selection and expansion stage with inaccurate values,<montecarlo><reinforcement-learning><monte-carlo-tree-search>,1,2,,,,CC BY-SA 3.0,
2402,48933294,1,48964383,,2018-02-22 17:05:36,,3,2222,"<p>I am just getting start with deep reinforcement learning and i am trying to crasp this concept.</p>

<p>I have this deterministic bellman equation </p>

<p><a href=""https://i.stack.imgur.com/HAqpu.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/HAqpu.png"" alt=""deterministic bellman equation""></a></p>

<p>When i implement stochastacity from the MDP then i get 2.6a</p>

<p><a href=""https://i.stack.imgur.com/WUrW0.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/WUrW0.png"" alt=""Implement MDP in deterministic bellman""></a></p>

<p>My equation is this assumption correct. I saw this implementation 2.6a without a policy sign on the state value function. But to me this does not make sense due to i am using the probability of which different next steps i could end up in. Which is the same as saying policy, i think. and if yes 2.6a is correct, can i then assume that the rest (2.6b and 2.6c) because then i would like to write the action state function like this:</p>

<p><a href=""https://i.stack.imgur.com/dNeJ7.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/dNeJ7.png"" alt=""State action function with policy""></a></p>

<p>The reason why i am doing it like this is because i would like to explain myself from a deterministic point of view to a non-deterministic point of view.</p>

<p>I hope someone out there can help on this one!</p>

<p>Best regards SÃ¸ren Koch</p>
",5950877,,,,,2018-02-25 20:56:02,State value and state action values with policy - Bellman equation with policy,<equation><policy><reinforcement-learning><mdp><markov-decision-process>,2,0,0,,,CC BY-SA 3.0,
2403,48495733,1,48497097,,2018-01-29 06:44:12,,1,2050,"<p>When using DQN, I was told that it's better to fill up the entire replay memory before learning. I am wondering how to anneal the epsilon if I use epsilon-greedy policy. Say the replay memory size is 10000 so the agent should run 10000 steps before learning. Should I start to anneal the epsilon during the 10000 steps or after the learning starts? Thanks in advance.</p>
",6181069,,,,,2018-01-29 08:25:24,Annealing epsilon in epsilon-greedy policy when using DQN,<reinforcement-learning>,1,0,0,,,CC BY-SA 3.0,
2407,65003111,1,65004129,,2020-11-25 10:49:04,,0,28,"<p>I want to extend the PPO agent class in ChainerRL. I did the following:</p>
<pre><code>class exPPO(chainerrl.agents.PPO):
    
    def act_and_train(self, obs, reward):
        action = chainerrl.agents.PPO(self, obs, reward)
        print(&quot;this is my exPPO act and train&quot;)
        return action
</code></pre>
<p>I tried with cartpole env of gym, but when doing</p>
<pre><code>obs, reward = env.step(action)
</code></pre>
<p>it just crashes with the following output</p>
<pre><code>this is my exPPO act and train
Traceback (most recent call last):
  File &quot;C:\personal_if\extendPPO.py&quot;, line 184, in &lt;module&gt;
    obs, reward, done, _ = env.step(action)
  File &quot;C:\Users\PareekHi\AppData\Local\Programs\Python\Python37\lib\site-packages\gym\wrappers\time_limit.py&quot;, line 16, in step
    observation, reward, done, info = self.env.step(action)
  File &quot;C:\Users\PareekHi\AppData\Local\Programs\Python\Python37\lib\site-packages\gym\envs\classic_control\cartpole.py&quot;, line 104, in step
    assert self.action_space.contains(action), err_msg
AssertionError: &lt;chainerrl.agents.ppo.PPO object at 0x000002A986D2F508&gt; (&lt;class 'chainerrl.agents.ppo.PPO'&gt;) invalid
</code></pre>
<p>Please help how I can extend PPO class here.</p>
",5307557,,466862,,2020-11-25 11:52:06,2020-11-25 11:52:06,How to extend an agent class in ChainerRL in Python,<python><reinforcement-learning><chainer>,1,0,,,,CC BY-SA 4.0,
2409,48937434,1,49068636,,2018-02-22 21:24:13,,1,54,"<p>After reading some tutorials I am still unsure about the definition of any episode. Is episode defined as one walk through from the start state to an exit/goal state?</p>
",6186449,,6735980,,2018-03-02 14:32:18,2018-03-02 14:32:18,Confusion with Q learning Episode Definition,<artificial-intelligence><reinforcement-learning><q-learning>,1,0,,,,CC BY-SA 3.0,
2410,65177161,1,65177432,,2020-12-07 06:44:17,,0,48,"<p>In the Reinforcement learning algorithm, I need to build and train an NN with multiple outputs, where each output is a probability vector of dimension 7. I used categorical cross-entropy as the loss function. However, I am unable to train the network.</p>
<p>The NN is</p>
<pre><code>num_actions = 7
input_actor = keras.layers.Input(shape = [4], name = 'states')
layer_1 = keras.layers.Dense(30, activation=&quot;relu&quot;, kernel_initializer=keras.initializers.he_normal())(input_actor)
layer_2 = keras.layers.Dense(30, activation=&quot;relu&quot;, kernel_initializer=keras.initializers.he_normal())(layer_1)
out_1 = keras.layers.Dense(num_actions, activation='softmax')(layer_2)
out_2 = keras.layers.Dense(num_actions, activation='softmax')(layer_2)
out_3 = keras.layers.Dense(num_actions, activation='softmax')(layer_2)
out_4 = keras.layers.Dense(num_actions, activation='softmax')(layer_2)
actor = keras.Model(inputs = [input_actor], outputs = [out_1, out_2, out_3, out_4])
actor.compile(loss=['categorical_crossentropy', 'categorical_crossentropy', 'categorical_crossentropy', 'categorical_crossentropy'], optimizer=keras.optimizers.Adam())
</code></pre>
<p>training with a single sample:</p>
<pre><code>s = np.arange(1,5).reshape(1,-1)
out = [keras.utils.to_categorical(a, 7).tolist() for a in iter([4,0,1,4])]
actor.train_on_batch(s, out , sample_weight=[-1.])
</code></pre>
<p>which results in the following error</p>
<pre><code>/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py in train_on_batch(self, x, y, sample_weight, class_weight, reset_metrics, return_dict)
   1693                                                     class_weight)
   1694       train_function = self.make_train_function()
-&gt; 1695       logs = train_function(iterator)
   1696 
   1697     if reset_metrics:

/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py in __call__(self, *args, **kwds)
    778       else:
    779         compiler = &quot;nonXla&quot;
--&gt; 780         result = self._call(*args, **kwds)
    781 
    782       new_tracing_count = self._get_tracing_count()

/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py in _call(self, *args, **kwds)
    805       # In this case we have created variables on the first call, so we run the
    806       # defunned version which is guaranteed to never create variables.
--&gt; 807       return self._stateless_fn(*args, **kwds)  # pylint: disable=not-callable
    808     elif self._stateful_fn is not None:
    809       # Release the lock early so that multiple threads can perform the call

TypeError: 'NoneType' object is not callable
</code></pre>
",3701747,,,,,2020-12-07 07:13:49,Tensorflow Multiple outputs Classification error,<python-3.x><tensorflow><reinforcement-learning>,1,0,,,,CC BY-SA 4.0,
2416,65202522,1,65250561,,2020-12-08 16:08:05,,2,493,"<p>I'm using TF-Agents library for reinforcement learning,
and I would like to take into account that, for a given state,
some actions are invalid.</p>
<p>How can this be implemented?</p>
<p>Should I define a &quot;observation_and_action_constraint_splitter&quot; function when
creating the DqnAgent?</p>
<p>If yes: do you know any tutorial on this?</p>
",3306091,,3306091,,2022-06-30 05:50:48,2022-06-30 05:50:48,TFAgents: how to take into account invalid actions,<tensorflow><reinforcement-learning><tensorflow-agents>,1,0,,,,CC BY-SA 4.0,
2417,65243290,1,65245472,,2020-12-10 22:37:06,,2,1293,"<p>In the <a href=""https://cdn.openai.com/dota-2.pdf"" rel=""nofollow noreferrer"">OpenAI Five paper</a> it is mentioned that the &quot;Gradients are additionally clipped per parameter to be within between Â±5âˆš<em>v</em> where <em>v</em> is the running estimate of the second moment of the (unclipped) gradient.&quot;. This is something I would like to implement in my project, but I am not sure how to do it neihter in theory nor in practice.</p>
<p>From <a href=""https://en.wikipedia.org/wiki/Moment_(mathematics)"" rel=""nofollow noreferrer"">wikipedia</a> I found out that the &quot;The second central moment is the variance. The positive square root of the variance is the standard deviation [...]&quot;. My best guess regarding the &quot;running estimate&quot; is that it is the Exponential Moving Average. The gradients of a network can be accessed as <a href=""https://discuss.pytorch.org/t/explicitly-obtain-gradients/4486/2"" rel=""nofollow noreferrer"">this comment suggests</a>.</p>
<p>From these I would assume that âˆš<em>v</em> is the Exponential Running Average of the standard dev. of the gradients and could be calculated via:
<code>estimate = alpha * torch.std(list(param.grad for param in model.parameters())) + (1-alpha) * estimate</code></p>
<p>Is my theory correct? Is there a better way to do it?
Thanks in advance.</p>
<p>Edit: fixed gradient gathering after <a href=""https://stackoverflow.com/users/13509540/mr-for-example"">Mr. For Example</a>&quot;s answer.</p>
",12351436,,12351436,,2020-12-11 08:39:13,2021-01-09 03:05:26,How to get the second moment of the gradient,<machine-learning><pytorch><reinforcement-learning>,1,1,0,,,CC BY-SA 4.0,
2418,65282936,1,65877186,,2020-12-14 02:36:36,,0,170,"<p>Given the following enviroment, why does the reinforce algorithm (or any other easy policy gradient algorithm) converge to the optimal solution of taking action b, even if the starting probability for taking action a is much higher ?</p>
<ol>
<li><p>Start in state S0</p>
</li>
<li><p>Take action a ---&gt; reward of 5</p>
<p>Take action b ---&gt; reward of 10</p>
</li>
<li><p>Episode ends, start again in state s0</p>
</li>
</ol>
",11945707,,11945707,,2022-05-20 02:50:26,2022-05-20 02:50:26,Why does the reinforce algorithm converge when initialized with uneven probabilites?,<machine-learning><state><reinforcement-learning><montecarlo>,1,0,,,,CC BY-SA 4.0,
2419,47287033,1,48595524,,2017-11-14 13:26:34,,1,124,"<p>For example can the State at timestep t actually be made of the state at t and t-1. </p>

<pre><code>S_t = [s_t, s_t-1]
</code></pre>

<p>i.e. Does Proximal Policy Optimization already incorporate the state history, or can it be implicit in the State (or neither).</p>
",1506969,,,,,2019-08-30 15:02:24,Can state in Proximal Policy Optimization contain history?,<machine-learning><state><reinforcement-learning>,1,0,,,,CC BY-SA 3.0,
2420,49083999,1,50598296,,2018-03-03 12:15:40,,2,8483,"<p>I find either theories or python example which is not satisfactory as a beginner. I just need to understand a simple example for understanding the step by step iterations. Could anyone please show me the 1st and 2nd iterations for the Image that I have uploaded for value iteration?
<a href=""https://i.stack.imgur.com/0P9kx.png"" rel=""nofollow noreferrer"">Grid world problem</a></p>
",9401069,,9401069,,2018-03-03 12:21:25,2020-09-24 06:02:38,How to Solve reinforcement learning Grid world examples using value iteration?,<reinforcement-learning><value-iteration>,2,0,0,,,CC BY-SA 3.0,
2422,49065222,1,49720172,,2018-03-02 08:22:27,,0,2116,"<p>I have some troubles finding some example on the great www to how i implement a recurrent neural network with LSTM layer into my current Deep q-network in Pytorch so it become a DRQN.. Bear with me i am just getting started.. 
Futhermore, I am <strong>NOT</strong> working with images processing, thereby CNN so do <strong>not</strong> worry about this. My states are purely temperatures values.</p>

<p>Here is my code that i am currently train my DQN with:</p>

<pre><code># Importing the libraries

import numpy as np
import random # random samples from different batches (experience replay)
import os # For loading and saving brain
import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim # for using stochastic gradient descent
import torch.autograd as autograd # Conversion from tensor (advanced arrays) to avoid all that contains a gradient
# We want to put the tensor into a varaible taht will also contain a
# gradient and to this we need:
from torch.autograd import Variable
# to convert this tensor into a variable containing the tensor and the gradient


# Creating the architecture of the Neural Network
class Network(nn.Module): #inherinting from nn.Module

    #Self - refers to the object that will be created from this class
    #     - self here to specify that we're referring to the object
    def __init__(self, input_size, nb_action): #[self,input neuroner, output neuroner]
        super(Network, self).__init__() #inorder to use modules in torch.nn
        # Input and output neurons
        self.input_size = input_size
        self.nb_action = nb_action
        # Full connection between different layers of NN
        # In this example its one input layer, one hidden layer and one output layer
        # Using self here to specify that fc1 is a variable of my object
        self.fc1 = nn.Linear(input_size, 40)
        self.fc2 = nn.Linear(40, 30)
        #Example of adding a hiddenlayer
        # self.fcX = nn.Linear(30,30)
        self.fc3 = nn.Linear(30, nb_action) # 30 neurons in hidden layer

    # For function that will activate neurons and perform forward propagation
    def forward(self, state):
        # rectifier function
        x = F.relu(self.fc1(state))
        x = F.relu(self.fc2(x))
        q_values = self.fc3(x)
        return q_values

# Implementing Experience Replay
# We know that RL is based on MDP
# So going from one state(s_t) to the next state(s_t+1)
# We gonna put 100 transition between state into what we call the memory
# So we can use the distribution of experience to make a decision
class ReplayMemory(object):

    def __init__(self, capacity):
        self.capacity = capacity #100 transitions
        self.memory = [] #memory to save transitions

    # pushing transitions into memory with append
    #event=transition
    def push(self, event):
        self.memory.append(event)
        if len(self.memory) &gt; self.capacity: #memory only contain 100 events
            del self.memory[0] #delete first transition from memory if there is more that 100

    # taking random sample
    def sample(self, batch_size):
        #Creating variable that will contain the samples of memory
        #zip =reshape function if list = ((1,2,3),(4,5,6)) zip(*list)= (1,4),(2,5),(3,6)
        #                (state,action,reward),(state,action,reward)  
        samples = zip(*random.sample(self.memory, batch_size))
        #This is to be able to differentiate with respect to a tensor
        #and this will then contain the tensor and gradient
        #so for state,action and reward we will store the seperately into some
        #bytes which each one will get a gradient
        #so that eventually we'll be able to differentiate each one of them
        return map(lambda x: Variable(torch.cat(x, 0)), samples)

# Implementing Deep Q Learning

class Dqn():

    def __init__(self, input_size, nb_action, gamma, lrate, T):
        self.gamma = gamma #self.gamma gets assigned to input argument
        self.T = T
        # Sliding window of the evolving mean of the last 100 events/transitions
        self.reward_window = []
        #Creating network with network class
        self.model = Network(input_size, nb_action)
        #creating memory with memory class
        #We gonna take 100000 samples into memory and then we will sample from this memory to 
        #to get a snakk number of random transitions
        self.memory = ReplayMemory(100000)
        #creating optimizer (stochastic gradient descent)
        self.optimizer = optim.Adam(self.model.parameters(), lr = lrate) #learning rate
        #input vector which is batch of input observations
        #by unsqeeze we create a fake dimension to this is
        #what the network expect for its inputs
        #have to be the first dimension of the last_state
        self.last_state = torch.Tensor(input_size).unsqueeze(0)
        #Inilizing
        self.last_action = 0
        self.last_reward = 0

    def select_action(self, state):
        #Q value depends on state
        #Temperature parameter T will be a positive number and the closer
        #it is to ze the less sure the NN will when taking an action
        #forexample
        #softmax((1,2,3))={0.04,0.11,0.85} ==&gt; softmax((1,2,3)*3)={0,0.02,0.98} 
        #to deactivate brain then set T=0, thereby it is full random
        probs = F.softmax((self.model(Variable(state, volatile = True))*self.T),dim=1) # T=100
        #create a random draw from the probability distribution created from softmax
        action = probs.multinomial()
        print(probs.multinomial())
        return action.data[0,0]

    # See section 5.3 in AI handbook
    def learn(self, batch_state, batch_next_state, batch_reward, batch_action):
        outputs = self.model(batch_state).gather(1, batch_action.unsqueeze(1)).squeeze(1)
        #next input for target see page 7 in attached AI handbook
        next_outputs = self.model(batch_next_state).detach().max(1)[0]
        target = self.gamma*next_outputs + batch_reward
        #Using hubble loss inorder to obtain loss
        td_loss = F.smooth_l1_loss(outputs, target)
        #using  lass loss/error to perform stochastic gradient descent and update weights 
        self.optimizer.zero_grad() #reintialize the optimizer at each iteration of the loop
        #This line of code that backward propagates the error into the NN
        #td_loss.backward(retain_variables = True) #userwarning
        td_loss.backward(retain_graph = True)
        #And this line of code uses the optimizer to update the weights
        self.optimizer.step()

    def update(self, reward, new_signal):
        #Updated one transition and we have dated the last element of the transition
        #which is the new state
        new_state = torch.Tensor(new_signal).float().unsqueeze(0)
        self.memory.push((self.last_state, new_state, torch.LongTensor([int(self.last_action)]), torch.Tensor([self.last_reward])))
        #After ending in a state its time to play a action
        action = self.select_action(new_state)
        if len(self.memory.memory) &gt; 100:
            batch_state, batch_next_state, batch_action, batch_reward = self.memory.sample(100)
            self.learn(batch_state, batch_next_state, batch_reward, batch_action)
        self.last_action = action
        self.last_state = new_state
        self.last_reward = reward
        self.reward_window.append(reward)
        if len(self.reward_window) &gt; 1000:
            del self.reward_window[0]
        return action

    def score(self):
        return sum(self.reward_window)/(len(self.reward_window)+1.)

    def save(self):
        torch.save({'state_dict': self.model.state_dict(),
                    'optimizer' : self.optimizer.state_dict(),
                   }, 'last_brain.pth')

    def load(self):
        if os.path.isfile('last_brain.pth'):
            print(""=&gt; loading checkpoint... "")
            checkpoint = torch.load('last_brain.pth')
            self.model.load_state_dict(checkpoint['state_dict'])
            self.optimizer.load_state_dict(checkpoint['optimizer'])
            print(""done !"")
        else:
            print(""no checkpoint found..."")
</code></pre>

<p>I hope there is someone out there that can help me and could implement a RNN and a LSTM layer into my code! I believe in you stackflow!</p>

<p>Best regards SÃ¸ren Koch</p>
",5950877,,,,,2018-04-08 16:36:37,Implementing RNN and LSTM into DQN Pytorch code,<python-3.x><machine-learning><recurrent-neural-network><pytorch><reinforcement-learning>,1,0,,,,CC BY-SA 3.0,
2424,65222734,1,65250126,,2020-12-09 18:25:57,,1,103,"<p>I try to implement self play with PPO.
Suppose we have a game with 2 agents. We control one player on each side and get information like observation and reward after each step. As far as I know, you can use the information of the right and left player to generate training data and to optimize the model. But that is only possible for off-policy, isn't it?
Because with on-policy e.g. PPO, you expect that the training data to be generated by the current network version and that is usually not the case during self play?</p>
<p>Thanks!</p>
",12196179,,12196179,,2020-12-09 19:10:49,2020-12-11 11:03:43,RL: Self-Play with On-Policy and Off-Policy,<machine-learning><reinforcement-learning>,1,0,,,,CC BY-SA 4.0,
2425,65338120,1,65603866,,2020-12-17 09:39:45,,0,4747,"<p>I ran the following code in Google Colab:</p>
<pre><code>    from baselines.common.atari_wrappers import make_atari, wrap_deepmind
</code></pre>
<p>And got the following error:</p>
<pre><code>    ModuleNotFoundError: No module named 'baselines.common'
</code></pre>
<p>I tried !pip install baselines, but it didn't fixed the issue.....</p>
",,user14349917,,,,2021-01-07 08:25:41,ModuleNotFoundError: No module named 'baselines.common',<python><machine-learning><google-colaboratory><reinforcement-learning><baseline>,3,0,,,,CC BY-SA 4.0,
2430,65448313,1,65528519,,2020-12-25 13:03:28,,3,759,"<p>I saved trained policy with policy saver as following:</p>
<pre><code>  tf_policy_saver = policy_saver.PolicySaver(agent.policy)
  tf_policy_saver.save(policy_dir)
</code></pre>
<p>I want to continue training with the saved policy. So I tried initializing the training with the saved policy, which caused some error.</p>
<pre><code>agent = dqn_agent.DqnAgent(
tf_env.time_step_spec(),
tf_env.action_spec(),
q_network=q_net,
optimizer=optimizer,
td_errors_loss_fn=common.element_wise_squared_loss,
train_step_counter=train_step_counter)

agent.initialize()

agent.policy=tf.compat.v2.saved_model.load(policy_dir)
</code></pre>
<p>ERROR:</p>
<pre><code>  File &quot;C:/Users/Rohit/PycharmProjects/pythonProject/waypoint.py&quot;, line 172, in &lt;module&gt;
agent.policy=tf.compat.v2.saved_model.load('waypoints\\Two_rewards')


File &quot;C:\Users\Rohit\anaconda3\envs\btp36\lib\site-packages\tensorflow\python\training\tracking\tracking.py&quot;, line 92, in __setattr__
    super(AutoTrackable, self).__setattr__(name, value)
AttributeError: can't set attribute
</code></pre>
<p>I just want to save time retraining from first every time. How can I load saved policy and continue training??</p>
<p>Thanks in advance</p>
",14464348,,,,,2021-01-01 10:50:13,Cant load saved policy (TF-agents),<tensorflow><deep-learning><reinforcement-learning>,2,0,0,,,CC BY-SA 4.0,
2432,12411197,1,12785302,,2012-09-13 16:56:12,,13,6967,"<p>What is <em>evolutionary computation</em>? Is it a method of reinforcement learning? Or a separate method of machine learning? Or maybe none?</p>

<p>Please, cite references used to answer this question.</p>
",906350,,3924118,,2018-01-25 00:44:56,2018-01-25 00:44:56,Can evolutionary computation be a method of reinforcement learning?,<machine-learning><artificial-intelligence><reinforcement-learning><evolutionary-algorithm>,5,0,0,,,CC BY-SA 3.0,
2433,47491072,1,48086547,,2017-11-25 22:21:30,,0,1109,"<p>I'm learning Q-Learning and trying to build a Q-learner on the FrozenLake-v0 problem in OpenAI Gym. Since the problem has only 16 states and 4 possible actions it should be fairly easy, but looks like my algorithm is not updating the Q-table correctly.</p>

<p>The following is my Q-learning algorithm:</p>

<pre><code>import gym
import numpy as np
from gym import wrappers


def run(
    env,
    Qtable,
    N_STEPS=10000,
    alpha=0.2,  # 1-alpha the learning rate
    rar=0.4,  # random exploration rate
    radr=0.97  # decay rate
):

    # Initialize pars::
    TOTAL_REWARD = 0
    done = False
    action = env.action_space.sample()
    state = env.reset()

    for _ in range(N_STEPS):
        if done:
            print('TW', TOTAL_REWARD)
            break

        s_prime, reward, done, info = env.step(action)
        # Update Q Table:
        Qtable[state, action] = (1 - alpha) * Qtable[state, action] + alpha * (reward + Qtable[s_prime,np.argmax(Qtable[s_prime,])])

        # Prepare for the next step:
        # Next New Action:
        if rand.uniform(0, 1) &lt; rar:
            action = env.action_space.sample()
        else:
            action = np.argmax(Qtable[s_prime, :])

        # Update new state:
        state = s_prime
        # Update Decay:
        rar *= radr
        # Update Stats
        TOTAL_REWARD += reward
        if reward &gt; 0:
            print(reward)

    return Qtable, TOTAL_REWARD
</code></pre>

<p>Then run the Q-learner 1000 iterations:</p>

<pre><code>if __name__==""__main__"":
    # Required Pars:
    N_ITER = 1000
    REWARDS = []
    # Setup the Maze:
    env = gym.make('FrozenLake-v0')

    # Initialize Qtable:
    num_actions = env.unwrapped.nA
    num_states = env.unwrapped.nS
    # Qtable = np.random.uniform(0, 1, size=num_states * num_actions).reshape((num_states, num_actions))
    Qtable = np.zeros((env.observation_space.n, env.action_space.n))

    for _ in range(N_ITER):
        res = run(env, Qtable)
        Qtable = res[0]
        REWARDS.append(res[1])
    print(np.mean(REWARDS))
</code></pre>

<p>Any advice will be appreciated!</p>
",4757432,,,,,2018-01-03 23:22:04,FrozenLake Q-Learning Update Issue,<python><reinforcement-learning><q-learning>,1,0,,,,CC BY-SA 3.0,
2434,47494105,1,47499751,,2017-11-26 07:58:15,,0,327,"<p>As far as I know, for a specific policy \pi, temporal difference learning let us compute the expected value following that policy \pi, but what's the meaning of knowing a specific policy?</p>

<p>Shouldn't we try finding the optimal policy for a given environment? What's the point of doing a specific \pi using temporal difference learning at all?</p>
",5217363,,5217363,,2017-11-26 08:03:23,2017-11-26 18:49:32,What's the point of using Temporal difference learning at all?,<reinforcement-learning><temporal-difference>,1,0,,,,CC BY-SA 3.0,
2436,47333240,1,47345857,,2017-11-16 15:18:25,,0,67,"<p>I am struggling to understand one aspect of the Markov Decison Process.</p>

<p>When I am in state s and do action a, is it deterministic or stochastic to arrive in state s+1?</p>

<p>In most examples it seems to be deterministic. However I found one example in the picture below (David Silvers lecture on RL) where the transistion is stochastic. Namely following action ""Pub"".</p>

<p><img src=""https://i.imgur.com/8sp3NlC.png"" alt=""graph""></p>
",7362388,,,,,2017-11-17 08:00:06,"Following action a from state s, is the outcome probablisitc or deterministic?",<reinforcement-learning><stochastic-process><markov-decision-process>,1,0,,,,CC BY-SA 3.0,
2441,49547264,1,49551095,,2018-03-29 02:52:21,,3,832,"<p>I am implementing Q-learning algorithm and I observed that my Q-values are not converging to optimal Q-values even though the policy seems to be converging. I defined the action selection strategy as epsilon-greedy and epsilon is decreasing by 1/N starting from 1(N being the total number of iterations). That way in the earlier iterations the algorithm explores random states then this rate gradually decreases leading to exploitation. In addition, I defined the learning rate as 1/N_t(s,a)  where N_t(s,a) is the total number of times (s,a) is visited. </p>

<p>Everything seems to be correct but since I can't get to the optimal Q-values I started looking into different strategies and in the meantime got super confused. I know that convergence is achieved when all (s,a) pairs are visited infinitely often. Isn't this equivalent to saying all (s,a) pairs are explored many times? In other words, why do we need exploitation for convergence? What if we don't exploit and just focus on exploring? If we do that we search all of the solution space, hence shouldn't that be enough to find an optimal policy? </p>

<p>Also, when its said the Q-values converge to optimal, does only the max_a[Q(s,a)] converge to its optimal value or all Q(s,a) values converge to their optimal value? </p>

<p>Probably there is a simple answer to all of these however even though I checked a lot of resources and similar threads I still couldn't figure out the logic behind exploitation. Thanks a lot for your time in advance!</p>
",8099284,,,,,2018-03-29 09:58:40,Why do we need exploitation in RL(Q-Learning) for convergence?,<reinforcement-learning><q-learning><convergence><markov-decision-process>,2,0,0,,,CC BY-SA 3.0,
2444,49533349,1,49535450,,2018-03-28 11:30:04,,2,364,"<p>I'm trying to implement the Episodic Semi-gradient Sarsa for estimating q* with a Neural Network as a function approximator.
My question is: does the weight vector w in q(S, A, w) refer to the weights in the Neural Network?</p>

<p>See:
<a href=""http://incompleteideas.net/book/bookdraft2018jan1.pdf"" rel=""nofollow noreferrer""> Sutton and Barto </a> page 197/198 for a concrete algorithm.</p>

<p>If yes: then how to deal with the fact that there are multiple weight vectors in a multilayer Neural Network?</p>

<p>If no: How would I use it in the algorithm?
My suggestion would be to append it to the state s and action a and plug it into the Neural Network to get an approximation of the state with the chosen action. Is this correct?</p>

<p>How is the dimension of the weight vector w determined?</p>

<p>Thanks in advance!</p>
",6911745,,6911745,,2018-03-28 12:44:56,2018-03-28 13:07:07,Reinforcement Learning function approximation with Neural Networks,<machine-learning><neural-network><artificial-intelligence><reinforcement-learning>,1,2,0,,,CC BY-SA 3.0,
2447,67177184,1,67346604,,2021-04-20 10:33:44,,0,131,"<p>I would like to generate some data (position of the snake, available moves, distance from the food...) to create a neural network model so that it can be trained on the data to play the snake game. However, I don't know how to do that. My current ideas are:</p>
<ul>
<li>Play manually (by myself) the game for many iterations and store the data (drawback: I should play the game a lot of times).</li>
<li>Make the snake do some random movements track and track their outcomes.</li>
<li>Play the snake with depth-fist search or similar algorithms many times and store the data.</li>
</ul>
<p>Can you suggest to me some other method or should I choose from one of those? Which one in that case?</p>
<p>P.S. I don't know if it is the right place to ask such a question. However, I don't know whom/where to ask such a question hence, I am here.</p>
",8801862,,5577765,,2021-04-22 17:56:14,2021-05-01 13:04:27,Generate the data for A.I. to play the Snake game,<python><neural-network><reinforcement-learning>,1,3,0,,,CC BY-SA 4.0,
2451,53039048,1,53041074,,2018-10-29 05:08:50,,1,184,"<p>I am working on an RL project, but got stuck at one point: The task is continuous (Non-episodic). Following some suggestion from Sutton's <a href=""http://incompleteideas.net/book/bookdraft2017nov5.pdf"" rel=""nofollow noreferrer"">RL book</a>, I am using a value function approximation method with average reward (differential return instead of discount return). For some state (represented by some features), only one action is legal. I am not sure how to design a reward for such action. Is it ok to just assign the reward in the previous step? Or assign the average reward (take the average of all reward collected so far)ï¼Ÿ Could anyone tell me the best way to decide the reward for the only legal action? Thank you! </p>

<p>UPDATE:
To give more details, I added one simplified example:
Let me explain this by a simplified example: the state space consists of a job queue with fix size and a single server. The queue state is represented by the duration of jobs and the server state is represented by the time left to finish the currently running job.  When the queue is not full and the server is idle, the agent can SCHEDULE a job to the server for execution and see a state transition(taking next job into the queue) or the agent can TAKE NEXT JOB into the queue. But when the job queue is full and the server is still running a job, the agent can do nothing except take a BLOCKING action and witness a state transit (time left to finish running job gets decreased by one unit time). The BLOCKING action is the only action that the agent can take in that state.</p>
",6437725,,6437725,,2018-10-29 05:24:31,2018-10-29 21:57:11,How to design the reward for an action which is the only legal action at some state,<reinforcement-learning>,1,0,0,,,CC BY-SA 4.0,
2452,48213366,1,48230960,,2018-01-11 18:06:56,,0,541,"<p>consider the Deep Q-Learning algorithm </p>

<pre><code>1   initialize replay memory D
2   initialize action-value function Q with random weights
3   observe initial state s
4   repeat
5       select an action a
6           with probability Îµ select a random action
7           otherwise select a = argmaxaâ€™Q(s,aâ€™)
8       carry out action a
9       observe reward r and new state sâ€™
10      store experience &lt;s, a, r, sâ€™&gt; in replay memory D
11
12      sample random transitions &lt;ss, aa, rr, ssâ€™&gt; from replay memory D
13      calculate target for each minibatch transition
14          if ssâ€™ is terminal state then tt = rr
15          otherwise tt = rr + Î³maxaâ€™Q(ssâ€™, aaâ€™)
16      train the Q network using (tt - Q(ss, aa))^2 as loss
17
18      s = s'
19  until terminated
</code></pre>

<p>In step 16 the value of Q(ss, aa) is used to calculate the loss. When is this Q value calculated? At the time the action was taken or during the training itself?</p>

<p>Since replay memory only stores &lt; s,a,r,s' > and not the q-value, is it safe to assume the q value will be calculated during the time of training?</p>
",1513863,,1513863,,2018-01-12 13:22:35,2018-01-12 17:09:56,Calculating Q value in dqn with experience replay,<neural-network><reinforcement-learning><q-learning>,1,3,0,,,CC BY-SA 3.0,
2455,51223187,1,51224520,,2018-07-07 12:22:43,,0,904,"<p>I am trying to set up Starcraft II for Deep Reinforcement Learning, following this <a href=""https://github.com/deepmind/pysc2"" rel=""nofollow noreferrer"">Tutorial</a>.</p>

<p>At some point I am adviced to download the maps:</p>

<blockquote>
  <p><strong>Get the maps</strong></p>
  
  <p>PySC2 has many maps pre-configured, but they need to be downloaded
  into the SC2 Maps directory before they can be played.</p>
  
  <p>Download the ladder maps and the mini games and extract them to your
  StarcraftII/Maps/ directory.</p>
</blockquote>

<p>Thing is I am not able to locate my Maps directory.
Any advice or hints are welcome.</p>
",5763590,,5763590,,2018-10-07 20:41:35,2018-10-07 20:41:35,Where can I find the maps folder within StarcraftII?,<directory><reinforcement-learning><starcraftgym>,1,0,,2018-10-09 08:51:20,,CC BY-SA 4.0,
2459,51221952,1,51232515,,2018-07-07 09:50:11,,1,1477,"<p>My teacher gave the following problem:
Consider the following MDP with 3 states and rewards. There are two possible actions - RED and BLUE. The state transitions probabilites are given on the edges, and S2 is a terminal state.  Assume that the initial policy is: Ï€(S0) = B; Ï€(S1) = R. 
<a href=""https://i.stack.imgur.com/i6cDX.jpg"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/i6cDX.jpg"" alt=""MDP""></a></p>

<p>We were asked for what Î³ values (0&lt;Î³&lt;1) the optimal policy would be:</p>

<p>(a) Ï€âˆ—(S0) = R; Ï€âˆ—(S1) = B;</p>

<p>(b) Ï€âˆ—(S0) = B; Ï€âˆ—(S1) = R;</p>

<p>(c) Ï€âˆ—(S0) = R; Ï€âˆ—(S1) = R;</p>

<p>I've shown that for (a) the answer is Î³ = 0.1, and couldn't find Î³ values for (b) and (c). The teacher said that for (b) any Î³ > 0.98 would work, and for (c) Î³ = 0.5. I think he's wrong, and have written<a href=""https://ideone.com/JLl5e4"" rel=""nofollow noreferrer"">the following python script</a> , which follows the algorithm in the textbook (Russell and Norvig AIMA), and indeed for any Î³ value the only policy I get is (a).  However the teacher says he's not wrong, and that my script must be buggy. How can i definitely show that such policies are impossible?</p>

<pre><code>S0 = ""S0""
S1 = ""S1""
S2 = ""S2""
BLUE = ""blue""
RED = ""red""
gamma = 0.5  # TODO MODIFY GAMMA HERE

# P(s'|s,a)
P_destination_start_action = \
{
(S0,S0, BLUE):0.5,(S0,S0,RED):0.9, (S0,S1,BLUE):0.8,(S0,S1,RED):0, (S0,S2, BLUE):0,(S0,S2,RED):0,
(S1,S0, BLUE):0.5,(S1,S0,RED):0, (S1,S1,BLUE):0.2,(S1,S1,RED):0.6, (S1,S2, BLUE):0,(S1,S2,RED):0,
(S2,S0, BLUE):0, (S2,S0,RED):0.1, (S2,S1,BLUE):0  ,(S2,S1,RED):0.4,(S2,S2, BLUE):1,(S2,S2,RED):1
}

class MDP:
    def __init__(self):
        self.states = [S0, S1, S2]
        self.actions = [BLUE, RED]


        self.P_dest_start_action = P_destination_start_action
        self.rewards = {S0: -2, S1: -5, S2: 0}

def POLICY_EVALUATION(policy_vec, utility_vec, mdp):
    new_utility_vector = {}
    for s in mdp.states:
        to_sum = [(mdp.P_dest_start_action[(s_tag, s, policy_vec[s])] * utility_vec[s_tag])
                  for s_tag in mdp.states]
        new_utility_vector[s] = mdp.rewards[s] + gamma * sum(to_sum)
    return new_utility_vector

def POLICY_ITERATION(mdp):
    utility_vector = {state: 0 for state in mdp.states}
    policy_vector = {S0: BLUE, S1: RED, S2: RED}
    unchanged = False

    while not unchanged:
        utility_vector = POLICY_EVALUATION(policy_vector, utility_vector, mdp)
        unchanged = True
        for s in mdp.states:
            BLUE_sum = sum([(mdp.P_dest_start_action[(s_tag, s, BLUE)] * utility_vector[s_tag])
                            for s_tag in mdp.states])
            RED_sum = sum([(mdp.P_dest_start_action[(s_tag, s, RED)] * utility_vector[s_tag])
                           for s_tag in mdp.states])
            if policy_vector[s] == RED and BLUE_sum &gt; RED_sum:
                policy_vector[s] = BLUE
                unchanged = False

            elif policy_vector[s] == BLUE and RED_sum &gt; BLUE_sum:
                policy_vector[s] = RED
                unchanged = False

    return policy_vector

if __name__ == ""__main__"":
    Q2_mdp = MDP()
    new_policy_vec = POLICY_ITERATION(Q2_mdp)
    print(""===========================END==============================="")
    print(""S_O policy ="", new_policy_vec[S0], "" ,S_1 Policy ="", new_policy_vec[S1])
</code></pre>
",4399305,,4399305,,2018-07-08 13:14:45,2018-07-08 13:34:00,How to find out values of Policy Iteration?,<python><machine-learning><reinforcement-learning><markov>,1,0,,,,CC BY-SA 4.0,
2460,17336069,1,17592020,,2013-06-27 06:32:27,,8,10348,"<p>In any of the standard Reinforcement learning algorithms that use generalized temporal differencing (e.g. SARSA, Q-learning), the question arises as to what values to use for the lambda and gamma hyper-parameters for a specific task. </p>

<p>I understand that lambda is tied to the length of the eligibility traces and gamma can be interpreted as how much to discount future rewards, but how do I know when my lambda value is too low for a given task, or my gamma too high? </p>

<p>I realize these questions don't have well defined answers, but knowing some 'red flags' for having inappropriate values would be very useful.</p>

<p>Take the standard <a href=""https://en.wikipedia.org/wiki/Inverted_pendulum"" rel=""noreferrer"">cart-pole, or inverted pendulum</a> task for example. Should I set gamma to be high, since it requires many steps to fail the task, or low because the state information is completely <a href=""http://en.wikipedia.org/wiki/Markov_process#Markov_property"" rel=""noreferrer"">Markovian</a>? And I can't even fathom rationals for lambda values... </p>
",821806,,2152558,,2013-06-27 17:31:52,2013-07-11 11:20:39,Setting gamma and lambda in Reinforcement Learning,<machine-learning><artificial-intelligence><reinforcement-learning><markov>,1,0,0,,,CC BY-SA 3.0,
2461,51260136,1,51290479,,2018-07-10 08:07:56,,1,329,"<p>In the code of <a href=""https://github.com/dennybritz/reinforcement-learning/blob/master/PolicyGradient/Continuous%20MountainCar%20Actor%20Critic%20Solution.ipynb?short_path=6b34a0b#L150"" rel=""nofollow noreferrer"">Actor-Critic with Gaussian</a>, </p>

<pre class=""lang-py prettyprint-override""><code>class PolicyEstimator():
    """"""
    Policy Function approximator. 
    """"""

    def __init__(self, learning_rate=0.01, scope=""policy_estimator""):
        with tf.variable_scope(scope):
            self.state = tf.placeholder(tf.float32, [400], ""state"")
            self.target = tf.placeholder(dtype=tf.float32, name=""target"")

            # This is just linear classifier
            self.mu = tf.contrib.layers.fully_connected(
                inputs=tf.expand_dims(self.state, 0),
                num_outputs=1,
                activation_fn=None,
                weights_initializer=tf.zeros_initializer)
            self.mu = tf.squeeze(self.mu)

            self.sigma = tf.contrib.layers.fully_connected(
                inputs=tf.expand_dims(self.state, 0),
                num_outputs=1,
                activation_fn=None,
                weights_initializer=tf.zeros_initializer)

            self.sigma = tf.squeeze(self.sigma)
            self.sigma = tf.nn.softplus(self.sigma) + 1e-5
            self.normal_dist = tf.contrib.distributions.Normal(self.mu, self.sigma)
            self.action = self.normal_dist._sample_n(1)
</code></pre>

<p>Initializing an instance of Normal distribution</p>

<pre class=""lang-py prettyprint-override""><code>self.normal_dist = tf.contrib.distributions.Normal(self.mu, self.sigma)
</code></pre>

<p>Sampling</p>

<pre class=""lang-py prettyprint-override""><code>self.action = self.normal_dist._sample_n(1)
</code></pre>

<p>the code samples only one action since the dimension of the env is 1. However, if the action space is 40 or more, how can I sample the action?</p>

<pre class=""lang-py prettyprint-override""><code>self.action = self.normal_dist._sample_n(40)
</code></pre>

<p>I think it means sampling 40 actions of which dimension space is 1 not sampling an action with 40 dimension value.</p>

<p>How can I sample one action of which dimension value is 40 or more?</p>
",5046896,,5046896,,2018-07-10 08:13:46,2018-09-26 11:15:18,"Reinforcement Learning, how can I sample action from Gaussian distribution with action dimension space larger than one?",<tensorflow><reinforcement-learning>,1,0,,,,CC BY-SA 4.0,
2464,44301591,1,44314007,,2017-06-01 07:49:08,,0,1098,"<p>Is there a mapping between direct/indirect and supervised/unsupervised/reinforcement learning? To me it looks like direct learning â‰ˆ supervised learning and indirect learning â‰ˆ reinforcement learning, but I couldn't find a good reference for this.</p>
",3429133,,3429133,,2017-06-01 08:04:14,2017-06-01 17:53:51,Direct/indirect and supervised/unsupervised/reinforcement learning,<machine-learning><artificial-intelligence><reinforcement-learning><supervised-learning><unsupervised-learning>,1,0,,,,CC BY-SA 3.0,
2465,62008235,1,62074031,,2020-05-25 18:11:36,,1,195,"<pre><code>def create_example_model():
    tf.keras.backend.set_floatx('float64')
    model = Sequential()
    model.add(LSTM(128, input_shape=((60, len(df_train.columns)))))

    model.add(Dense(64, activation='relu'))

    model.add(Dense(3, activation=None))

    return model

def choose_action(model, observation):
    observation = np.expand_dims(observation, axis=0)

    logits = model.predict(observation)

    prob_weights = tf.nn.softmax(logits).numpy()

    action = np.random.choice(3, size=1, p=prob_weights.flatten())[0]

    return action

def train_step(model, optimizer, observations, actions, discounted_rewards):
    with tf.GradientTape() as tape:

        logits = model(observations)

        loss = compute_loss(logits, actions, discounted_rewards)

        grads = tape.gradient(loss, model.trainable_variables)
        optimizer.apply_gradients(zip(grads, model.trainable_variables))

learning_rate = 1e-3
optimizer = tf.keras.optimizers.Adam(learning_rate)
env = TradingEnv(rnn_ready_array)

model = create_example_model()
memory = Memory()
info_list = []

for i_episode in range(10):
    observation = env.reset()
    memory.clear()

    while True:
        action = choose_action(model, observation)
        next_observation, reward, done, info = env.step(action)
        info_list.append(info)
        memory.add_to_memory(observation, action, reward)
        if done:
            total_reward = sum(memory.rewards)
            train_step(model, optimizer,
                 observations=np.array(memory.observations),
                 actions=np.array(memory.actions),
                 discounted_rewards = discount_rewards(memory.rewards))

            memory.clear()
            break
        observation = next_observation
</code></pre>

<p>I am working on a reinforcement learning project with Tensorflow 2.0; the format of the code comes from an online MIT course of which I am attempting to adapt to my own project. I am new to Tensorflow 2.0 and I can't glean from the documentation why this problem is occurring. The issue is that when I run the reinforcement learning process, </p>

<ol>
<li>The first episode will always complete successfully.</li>
<li>A new observation will always be generated from the model successfully.</li>
<li>During the second episode, the network will always output: [NaN, NaN, NaN]</li>
</ol>

<p>Some debugging info I have found that should be helpful:
If I comment out the optimization lines 'grads = tape.gradient(...)' and 'optimizer.apply_gradients(...)' the script will run to completion error free (though it is obviously not doing anything useful without optimization). This indicates to me the optimization process is changing the model in a way that is causing the problem. I've tried to include only the necessary functions for debugging; if there is any further information one might need for debugging, I'd be happy to add additional info in an edit.</p>
",12757857,,,,,2020-05-28 20:34:11,Tensorflow Reinforcement Learning RNN returning NaN's after Optimization with GradientTape,<python><deep-learning><tensorflow2.0><recurrent-neural-network><reinforcement-learning>,1,0,,,,CC BY-SA 4.0,
2468,62027289,1,62027399,,2020-05-26 16:49:16,,1,55,"<p>I'm wondering if there is any reinforcement learning technique capable of learning how to play a game and some strategies from it simply by analyzing matches played by others instead of playing it himself. </p>
",13526464,,4685471,,2020-05-26 19:22:31,2020-05-26 19:22:31,Can a computer learn strategies of a game by analyzing others' game?,<machine-learning><reinforcement-learning>,1,1,0,2020-05-26 18:03:56,,CC BY-SA 4.0,
2473,51354107,1,51359267,,2018-07-16 03:12:37,,2,704,"<p>I need to create a state space for my RL problem which has about 10 state variables each which contains about 2 or 3 values for the variables. That would make the state space about 600,000 states. How do I implement this in python?</p>
",3359134,,,,,2018-07-16 10:03:36,how to define a state in python for reinforcement learning,<reinforcement-learning><q-learning>,1,0,,,,CC BY-SA 4.0,
2474,62022924,1,62025338,,2020-05-26 13:09:55,,1,31,"<p>I am reading Sutton and Barto and want to make sure I am clear.</p>

<p>For Off Policy learning can we think of a robot in a particular terrain - say on sand - as the target policy but use the robot's policy for walking in snow as the behaviour policy? We are using our experience of walking on snow to approximate the optimal policy for walking on sand?</p>
",4367851,,,,,2020-05-26 15:10:17,Is this example of off policy correct?,<reinforcement-learning><function-approximation>,1,0,,,,CC BY-SA 4.0,
2476,62048441,1,62052686,,2020-05-27 16:48:21,,0,104,"<p>I'm trying to learn to train a double-DQN algorithm on tensorflow and it doesn't work. to make sure everything is fine I wanted to test something. I wanted to make sure that using tf.gather on the argmax is exactly the same as taking the max: let's say I have a network called target_network:</p>

<p>first let's take the max:</p>

<pre><code>next_qvalues_target1 = target_network.get_symbolic_qvalues(next_obs_ph) #returns tensor of qvalues
next_state_values_target1 = tf.reduce_max(next_qvalues_target1, axis=1)
</code></pre>

<p>let's try it in a different way- using argmax and gather:</p>

<pre><code>next_qvalues_target2 = target_network.get_symbolic_qvalues(next_obs_ph) #returns same tensor of qvalues
chosen_action = tf.argmax(next_qvalues_target2, axis=1)
next_state_values_target2 = tf.gather(next_qvalues_target2, chosen_action)

diff = tf.reduce_sum(next_state_values_target1) - tf.reduce_sum(next_state_values_target2)
</code></pre>

<p>next_state_values_target2 and next_state_values_target1 are supposed to be completely identical. so running the session should output diff = . but it does not.</p>

<p>What am I missing?</p>

<p>Thanks. </p>
",10146664,,4685471,,2020-05-27 18:15:57,2020-05-27 20:54:10,using gather on argmax is different than taking max,<tensorflow><deep-learning><tensorflow2.0><reinforcement-learning>,1,0,,,,CC BY-SA 4.0,
2480,44381198,1,44419834,,2017-06-06 03:59:25,,4,548,"<p>According to Sutton's book - Reinforcement Learning: An Introduction, the update equation of Network weights is given by:</p>

<p><a href=""https://i.stack.imgur.com/c0kac.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/c0kac.png"" alt=""theta = theta + alpha * delta * e""></a></p>

<p>where e<sub>t</sub> is the eligibility trace.
This is similar to a Gradient Descent update with an extra e<sub>t</sub>.<br>
Can this eligibility trace be included in the <code>tf.train.GradientDescentOptimizer</code> in TensorFlow?</p>
",2306662,,2306662,,2017-06-06 15:05:36,2017-06-07 18:05:11,Eligibility traces in TensorFlow,<tensorflow><gradient-descent><reinforcement-learning>,1,4,,,,CC BY-SA 3.0,
2481,33573663,1,34069637,,2015-11-06 18:51:49,,0,457,"<p>I'm trying to use GA to train an ANN whose job is to move a bar vertically so that it makes a ball bounce without hitting the wall behind the bar, in other words, a single bar pong.
I'm going to ask it directly because i think to know what the problem is.
The game window is 200x200 pixels, so i created 40000 input neurons.
The obvious doubt is: can GA handle chromosomes of 40000(input)*10(hidden)*2 elements(genes)?
Since i think the answer is no(i implemented this solution and doesn't seem to work), the solution seems simple, i feed the NN with only 4 parameters which are the coordinates x,y of bar and ball, nailed it.
Nice solution, but the problem is: how can i apply such a solution in a game like supermario where the number of enemies in the screen is not fixed? Surely i cannot create a NN with dynamic numbers of inputs.
I hope you can help me.</p>
",5185763,,,,,2015-12-03 15:20:19,Using a neural network with genetic algorithm for pong or supermario,<neural-network><genetic-algorithm><reinforcement-learning>,1,0,,,,CC BY-SA 3.0,
2483,62088430,1,65472767,,2020-05-29 14:37:10,,1,1325,"<p>I am trying to train ML-Agents on Google colab but every time it fails with the same given error.</p>
<pre><code>WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/compat/v2_compat.py:65: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.
Instructions for updating:
non-resource variables are not supported in the long term


                        â–„â–„â–„â–“â–“â–“â–“
                   â•“â–“â–“â–“â–“â–“â–“â–ˆâ–“â–“â–“â–“â–“
              ,â–„â–„â–„mâ–€â–€â–€'  ,â–“â–“â–“â–€â–“â–“â–„                           â–“â–“â–“  â–“â–“â–Œ
            â–„â–“â–“â–“â–€'      â–„â–“â–“â–€  â–“â–“â–“      â–„â–„     â–„â–„ ,â–„â–„ â–„â–„â–„â–„   ,â–„â–„ â–„â–“â–“â–Œâ–„ â–„â–„â–„    ,â–„â–„
          â–„â–“â–“â–“â–€        â–„â–“â–“â–€   â–â–“â–“â–Œ     â–“â–“â–Œ   â–â–“â–“ â–â–“â–“â–“â–€â–€â–€â–“â–“â–Œ â–“â–“â–“ â–€â–“â–“â–Œâ–€ ^â–“â–“â–Œ  â•’â–“â–“â–Œ
        â–„â–“â–“â–“â–“â–“â–„â–„â–„â–„â–„â–„â–„â–„â–“â–“â–“      â–“â–€      â–“â–“â–Œ   â–â–“â–“ â–â–“â–“    â–“â–“â–“ â–“â–“â–“  â–“â–“â–Œ   â–â–“â–“â–„ â–“â–“â–Œ
        â–€â–“â–“â–“â–“â–€â–€â–€â–€â–€â–€â–€â–€â–€â–€â–“â–“â–„     â–“â–“      â–“â–“â–Œ   â–â–“â–“ â–â–“â–“    â–“â–“â–“ â–“â–“â–“  â–“â–“â–Œ    â–â–“â–“â–â–“â–“
          ^â–ˆâ–“â–“â–“        â–€â–“â–“â–„   â–â–“â–“â–Œ     â–“â–“â–“â–“â–„â–“â–“â–“â–“ â–â–“â–“    â–“â–“â–“ â–“â–“â–“  â–“â–“â–“â–„    â–“â–“â–“â–“`
            'â–€â–“â–“â–“â–„      ^â–“â–“â–“  â–“â–“â–“       â””â–€â–€â–€â–€ â–€â–€ ^â–€â–€    `â–€â–€ `â–€â–€   'â–€â–€    â–â–“â–“â–Œ
               â–€â–€â–€â–€â–“â–„â–„â–„   â–“â–“â–“â–“â–“â–“,                                      â–“â–“â–“â–“â–€
                   `â–€â–ˆâ–“â–“â–“â–“â–“â–“â–“â–“â–“â–Œ
                        Â¬`â–€â–€â–€â–ˆâ–“

        
 Version information:
  ml-agents: 0.15.1,
  ml-agents-envs: 0.15.1,
  Communicator API: 0.15.0,
  TensorFlow: 2.0.2
Found path: /content/build/test/ball.x86_64
Mono path[0] = '/content/build/test/ball_Data/Managed'
Mono config path = '/content/build/test/ball_Data/MonoBleedingEdge/etc'
Preloaded 'lib_burst_generated.so'
Preloaded 'libgrpc_csharp_ext.x64.so'
Initialize engine version: 2018.4.21f1 (fd3915227633)
Forcing GfxDevice: Null
GfxDevice: creating device client; threaded=0
NullGfxDevice:
    Version:  NULL 1.0 [1.0]
    Renderer: Null Device
    Vendor:   Unity Technologies
Begin MonoManager ReloadAssembly
- Completed reload, in  0.147 seconds
WARNING: Shader Unsupported: 'Autodesk Interactive' - Pass 'FORWARD' has no vertex shader
WARNING: Shader Unsupported: 'Autodesk Interactive' - Pass 'FORWARD_DELTA' has no vertex shader
WARNING: Shader Unsupported: 'Autodesk Interactive' - Pass 'ShadowCaster' has no vertex shader
WARNING: Shader Unsupported: 'Autodesk Interactive' - All passes removed
UnloadTime: 1.492000 ms
Fallback handler could not load library /content/build/test/ball_Data/Mono/libcoreclr.so
Fallback handler could not load library /content/build/test/ball_Data/Mono/libcoreclr.so
Fallback handler could not load library /content/build/test/ball_Data/Mono/libcoreclr.so
Fallback handler could not load library /content/build/test/ball_Data/Mono/libSystem.dylib
Fallback handler could not load library /content/build/test/ball_Data/Mono/libSystem.dylib.so
Fallback handler could not load library /content/build/test/ball_Data/Mono/libSystem.dylib
Fallback handler could not load library /content/build/test/ball_Data/Mono/libcoreclr.so
Fallback handler could not load library /content/build/test/ball_Data/Mono/libcoreclr.so
Fallback handler could not load library /content/build/test/ball_Data/Mono/libcoreclr.so
Fallback handler could not load library /content/build/test/ball_Data/Mono/libSystem.dylib
Fallback handler could not load library /content/build/test/ball_Data/Mono/libSystem.dylib.so
Fallback handler could not load library /content/build/test/ball_Data/Mono/libSystem.dylib
2020-05-29 13:53:12 INFO [environment.py:160] Connected to Unity environment with package version 0.15.1-preview and communication version 0.15.0
2020-05-29 13:53:12 INFO [environment.py:305] Connected new brain:
3DBall?team=0
2020-05-29 13:53:12 INFO [trainer_controller.py:167] Hyperparameters for the PPOTrainer of brain 3DBall: 
    trainer:    ppo
    ... (Hyperparameter list)
    summary_path:   test-1_3DBall
    model_path: ./models/test-1/3DBall
    keep_checkpoints:   5
2020-05-29 13:53:12.718558: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
2020-05-29 13:53:12.732085: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2200000000 Hz
2020-05-29 13:53:12.732627: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x1818f40 executing computations on platform Host. Devices:
2020-05-29 13:53:12.732677: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): Host, Default Version
Receiving unhandled NULL exception
2020-05-29 13:54:17 INFO [subprocess_env_manager.py:150] UnityEnvironment worker 0: environment stopping.
2020-05-29 13:54:17 INFO [trainer_controller.py:104] Learning was interrupted. Please wait while the graph is generated.
2020-05-29 13:54:17 INFO [environment.py:455] Environment shut down with return code -11 (SIGSEGV).
2020-05-29 13:54:18 INFO [trainer_controller.py:100] Saved Model
2020-05-29 13:54:18 INFO [model_serialization.py:222] List of nodes to export for brain :3DBall?team=0
2020-05-29 13:54:18 INFO [model_serialization.py:224]   is_continuous_control
2020-05-29 13:54:18 INFO [model_serialization.py:224]   version_number
2020-05-29 13:54:18 INFO [model_serialization.py:224]   memory_size
2020-05-29 13:54:18 INFO [model_serialization.py:224]   action_output_shape
2020-05-29 13:54:18 INFO [model_serialization.py:224]   action
2020-05-29 13:54:18 INFO [model_serialization.py:224]   action_probs
Converting ./models/test-1/3DBall/frozen_graph_def.pb to ./models/test-1/3DBall.nn
IGNORED: Cast unknown layer
IGNORED: Shape unknown layer
IGNORED: StopGradient unknown layer
GLOBALS: 'is_continuous_control', 'version_number', 'memory_size', 'action_output_shape'
IN: 'vector_observation': [-1, 1, 1, 8] =&gt; 'sub_2'
OUT: 'action', 'action_probs'
DONE: wrote ./models/test-1/3DBall.nn file.
2020-05-29 13:54:18 INFO [model_serialization.py:76] Exported ./models/test-1/3DBall.nn file
</code></pre>
<p>ML-Agents version = 0.15.1 <a href=""https://github.com/Unity-Technologies/ml-agents/tree/release-0.15.1"" rel=""nofollow noreferrer"">check the repo</a></p>
<p>Unity Version = 2018.4.21f1 (Used to create a Linux build)</p>
<p><strong>Steps used:</strong></p>
<ol>
<li>Create a build file for Linux from Unity Editor.</li>
<li>Zipped the build file and uploaded it to google colab.</li>
<li>Unzipped in google colab and enabled the execution permissions.</li>
<li>Cloned and installed the ml-agents repo. <a href=""https://github.com/Unity-Technologies/ml-agents/tree/release-0.15.1"" rel=""nofollow noreferrer"">same as above repo</a></li>
<li>Using this command to start the training process</li>
</ol>
<pre><code>ENV_BINARY = '/content/build/test/ball.x86_64'

mlagents-learn /content/config.yaml --run-id=test-1 --env=$ENV_BINARY --train --no-graphics
</code></pre>
<p>Another method which I followed:</p>
<p>Install unity in google colab to create a build file in colab itself but the build didn't work in unity.</p>
<p>To implement this I used <a href=""https://colab.research.google.com/drive/1mVIuqSCmgWVA84kMsWPKmh8ygbhERWfn"" rel=""nofollow noreferrer"">notebook</a></p>
<p><em>It seems that there are no resources on the internet related to this topic.
So please help me to get this working, I have already wasted my two days in search and trying to run ML-Agents in google colab.</em></p>
<p><strong>Updates:
The colab notebook started working after I updated my environment build using the Unity engine's 2019.3.15f1 version.</strong></p>
",13642199,,1364007,,2020-12-28 16:05:51,2020-12-30 05:39:10,Why Unity's ML-Agents are not working with Google Colab,<unity3d><google-colaboratory><reinforcement-learning><ml-agent>,1,2,0,,,CC BY-SA 4.0,
2491,44444923,1,44445077,,2017-06-08 20:23:53,,0,306,"<p>I am working on a DDPG implementation, which requires the computation of one network's (below: <code>critic</code>) gradients with respect to another network's (below: <code>actor</code>) output. My code already makes use of queues instead of feed dicts for the most part, but I could not do so for this specific part yet:</p>

<pre><code>import tensorflow as tf
tf.reset_default_graph()

states = tf.placeholder(tf.float32, (None,))
actions = tf.placeholder(tf.float32, (None,))

actor = states * 1
critic = states * 1 + actions

grads_indirect = tf.gradients(critic, actions)
grads_direct = tf.gradients(critic, actor)

with tf.Session() as sess:
    sess.run(tf.global_variables_initializer())

    act = sess.run(actor, {states: [1.]})
    print(act)  # -&gt; [1.]
    cri = sess.run(critic, {states: [1.], actions: [2.]})
    print(cri)  # -&gt; [3.]
    grad1 = sess.run(grads_indirect, {states: [1.], actions: act})
    print(grad1)  # -&gt; [[1.]]
    grad2 = sess.run(grads_direct, {states: [1.], actions: [2.]})
    print(grad2)  # -&gt; TypeError: Fetch argument has invalid type 'NoneType'
</code></pre>

<p><code>grad1</code> here computes the gradients w.r.t. to the fed-in actions, which were previously computed by the <code>actor</code>. <code>grad2</code> should do the same, but directly inside of the graph without the need for feeding the actions back in but by evaluating <code>actor</code> directly. The problem is that <code>grads_direct</code> is <code>None</code>:</p>

<pre><code>print(grads_direct)  # [None]
</code></pre>

<p>How can I achieve this? Is there a dedicated ""evaluate this tensor"" operation I could make use of? Thanks!</p>
",1447384,,1447384,,2017-06-08 22:09:05,2017-06-08 22:09:05,Tensorflow: tf.gradients between different paths of the graph,<graph><tensorflow><reinforcement-learning><gradient>,1,0,,,,CC BY-SA 3.0,
2496,67466527,1,67482275,,2021-05-10 07:37:35,,2,440,"<p>I have been learning Reinforcement Learning for few days now, and I have seen example problems like Mountain Car problem and Cart Pole problem.</p>
<p>In these problems, the way action space is described is discrete. For example in Cart Pole Problem, the agent can either move left or move right.</p>
<ol>
<li><p>But the examples don't talk about how much? How does the agent decide how much to move left, how much to move right, after all these movements are continuous space actions. So I want to know how does the agent decide what real value to choose from a continuous action space.</p>
</li>
<li><p>Also I have been using <a href=""https://juliareinforcementlearning.org/"" rel=""nofollow noreferrer"">ReinforcementLearning.jl</a> in Julia and wanted to know a way i could represent range constraints on action space in it. Example, the real value that the agent chooses as it's action should lie in a range like [10.00, 20.00[ for example. I want to know how this can  be done.</p>
</li>
</ol>
",,user12619063,,,,2021-05-11 07:27:18,Continuous action spaces in Reinforcement Learning - How does the agent choose action value from a continuous space?,<julia><reinforcement-learning>,1,1,,2021-05-23 00:47:10,,CC BY-SA 4.0,
2499,62119988,1,62120163,,2020-05-31 17:28:08,,0,301,"<p>I am new in Python and I faced with a problem in my code. I try to build my custom environment for a Deep Q-Network program. The name of my environment is ""FooEnv"".But when I run the main code, I faced with this error in line <strong>FooEnv.reset()</strong></p>

<p><strong><em>type object 'FooEnv' has no attribute 'reset'</em></strong></p>

<p>This is my main code, That I call ""FooEnv"" here:</p>

<pre><code>import json
import numpy as np
from keras.models import Sequential
from keras.layers.core import Dense
from keras.optimizers import sgd
from FooEnv import FooEnv
class ExperienceReplay(object):
  def __init__(self, max_memory=100, discount=.9):
    self.max_memory = max_memory
    self.memory = list()
    self.discount = discount

  def remember(self, states, game_over):
    # memory[i] = [[state_t, action_t, reward_t, state_t+1], game_over?]
    self.memory.append([states, game_over])
    if len(self.memory) &gt; self.max_memory:
        del self.memory[0]

  def get_batch(self, model, batch_size=10):
    len_memory = len(self.memory)
    num_actions = model.output_shape[-1]
    # env_dim = self.memory[0][0][0].shape[1]
    env_dim = self.memory[0][0][0].shape[1]
    inputs = np.zeros((min(len_memory, batch_size), env_dim))
    targets = np.zeros((inputs.shape[0], num_actions))
    for i, idx in enumerate(np.random.randint(0, len_memory,
                                              size=inputs.shape[0])):
        state_t, action_t, reward_t, state_tp1 = self.memory[idx][0]
        game_over = self.memory[idx][1]

        inputs[i:i+1] = state_t
        # There should be no target values for actions not taken.
        # Thou shalt not correct actions not taken #deep
        targets[i] = model.predict(state_t)[0]
        Q_sa = np.max(model.predict(state_tp1)[0])
        if game_over:  # if game_over is True
            targets[i, action_t] = reward_t
        else:
            # reward_t + gamma * max_a' Q(s', a')
            targets[i, action_t] = reward_t + self.discount * Q_sa
    return inputs, targets
if __name__ == ""__main__"":
    # parameters
    epsilon = .1  
    num_actions = 2  
    epoch = 1000
    max_memory = 500
    hidden_size = 100
    batch_size = 50
    input_size = 2
    f_c=[2.4*10**9]
    eta_Los=[1]
    eta_NLos=[2]
    x_threshold = [5]
    model = Sequential()
    model.add(Dense(hidden_size, input_shape=(2, ), activation='relu'))
    model.add(Dense(hidden_size, activation='relu'))
    model.add(Dense(num_actions))
    model.compile(sgd(lr=.2), ""mse"")
    # Define environment/game
    env = FooEnv(f_c, eta_Los, eta_NLos)
    # Initialize experience replay object
    exp_replay = ExperienceReplay(max_memory=max_memory)
    FooEnv.reset()
</code></pre>

<p>And this is my FooEnv code:</p>

<pre><code>import numpy as  np
import math
class FooEnv(object):
  def __init__(self, f_c, eta_Los, eta_NLos):
    self.f_c = f_c
    self.eta_Los = eta_Los
    self.eta_NLos = eta_NLos
    self.num_actions = 2
  def reset(self):
    state=self.state
    E_Consumtion, Average_Delay_UAV, Average_DeLay_FAP = state
    E_Consumtion=0
    Average_Delay_UAV=0
    Average_DeLay_FAP=0
    self.state = np.append(E_Consumtion,Average_Delay_UAV,Average_DeLay_FAP)
    self.steps_beyond_done = None
    return np.array(self.state)
</code></pre>

<p>I would greatly appreciated it if you could help me with this.</p>
",13653661,,,,,2020-05-31 17:40:29,AttributeError: type object 'FooEnv' has no attribute 'reset',<python-3.x><object><reset><reinforcement-learning><attributeerror>,1,0,,,,CC BY-SA 4.0,
2500,62126327,1,62225923,,2020-06-01 05:42:01,,0,1731,"<p>I am following this online tutorial for coding a DQN,<a href=""https://github.com/philtabor/Youtube-Code-Repository/blob/master/ReinforcementLearning/DeepQLearning/torch_deep_q_model.py"" rel=""nofollow noreferrer"">https://github.com/philtabor/Youtube-Code-Repository/blob/master/ReinforcementLearning/DeepQLearning/torch_deep_q_model.py</a>
, however I am running into this Runtime Error that I am unsure of how to debug or modify to prevent this error. Thanks!</p>

<pre><code>---------------------------------------------------------------------------
RuntimeError                              Traceback (most recent call last)
&lt;ipython-input-196-00975d66fd2d&gt; in &lt;module&gt;
     28         agent.storeTransition(preprocess(obs),action,reward,preprocess(obs_))
     29         obs= obs_
---&gt; 30         agent.learn(batch_size)
     31         lastAction = action
     32     scores.append(score)

&lt;ipython-input-191-f6b163cc3a8a&gt; in learn(self, batch_size)
     72         Qtarget = Qpred.clone()
     73         print(Qnext[1])
---&gt; 74         Qtarget[:,maxA] = rewards + self.GAMMA*torch.max(Qnext[1])
     75         # epsilon decay action
     76         if self.steps &gt; 2000:

RuntimeError: the derivative for 'indices' is not implemented
</code></pre>

<p>These are my code blocks in my jupyter notebook</p>

<pre><code>class DeepQNetwork(nn.Module):
    def __init__(self,Alpha):
        super(DeepQNetwork,self).__init__()
        self.conv1 = nn.Conv2d(1,32,8,stride=4, padding=1)
        self.conv2 = nn.Conv2d(32,64,4,stride=2)
        self.conv3 = nn.Conv2d(64,128,3)
        self.fc1 = nn.Linear(128* 21* 12,512)
        self.fc2 = nn.Linear(512,6)

        self.optimizer = optim.RMSprop(self.parameters(), lr = Alpha)
        self.loss = nn.MSELoss()
        self.device =  torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')
        self.to(self.device)

    def forward(self,obs): 
        '''Passing in a sequence of arrays'''
        obs = torch.Tensor(obs).to(self.device) # send to the GPU
        ''' Feed forward the Network Parameters'''
        obs = obs.view(-1, 1,200,125)
        #print(obs.shape)
        obs = F.relu(self.conv1(obs))
        #print(obs.shape)
        obs = F.relu(self.conv2(obs))
        #print(obs.shape)
        obs = F.relu(self.conv3(obs))
        #print(obs.shape)
        obs = obs.view(-1,128* 21* 12)
        obs = F.relu(self.fc1(obs))
        # 4 Rows and 6 columns
        actions = self.fc2(obs)
        return actions
</code></pre>

<p>This is the Agent Code, and it contains the error causing line of code </p>

<pre><code>class DQNAgent(object):
    def __init__(self, gamma, epsilon, alpha, maxMemory, 
                 epsEnd = 0.05, replace =10000, actionSpace = [0,1,2,3,4,5]):
        '''
        Gamma -&gt; discount factor of valuing current reward over future reward
        Epsilon -&gt; for trade off between exploration-exploitation
        alpha -&gt; learn rate
        maxMemory -&gt; max size of Memory buffer
        epsEnd -&gt; smallest value of Exploration
        repace -&gt; how often to replace target network
        '''
        self.GAMMA = gamma
        self.EPSILON = epsilon
        self.EPS_END = epsEnd
        self.actionSpace = actionSpace
        self.maxMemory = maxMemory
        self.steps = 0
        self.learn_step_counter = 0
        self.memory = []
        self.memCount = 0
        self.replace_tgt_count = replace
        self.Q_eval = DeepQNetwork(alpha)
        self.Q_next = DeepQNetwork(alpha)

    def storeTransition(self, state, action, reward, state_):
        '''Stores Transition states'''
        if self.memCount &lt; self.maxMemory:
            self.memory.append([state,action,reward,state_])
        else:
            self.memory[self.memCount%self.maxMemory] = [state,action,reward,state_]
        self.memCount +=1

    def chooseAction(self,obs):
        '''
        Exploration if np.random &gt; epsilon
        else take epsilon greedy action
        '''
        rand = np.random.random()
        # Get the value for all actions for the current set of states
        # Forward pass the stack of frames to get value of each action given subset of staes in obs
        actions = self.Q_eval.forward(obs)
        if rand&lt;1-self.EPSILON:
            action = torch.argmax(actions[1]).item()
        else:
            action = np.random.choice(self.actionSpace)
        self.steps += 1
        return action

    def learn(self, batch_size):
        self.Q_eval.optimizer.zero_grad()
        #0 gradient to do batch optimisation
        if self.replace_tgt_count is not None and self.learn_step_counter % self.replace_tgt_count==0:
            self.Q_next.load_state_dict(self.Q_eval.state_dict())

        # memory subsampling
        if self.memCount + batch_size &lt; self.maxMemory:
            memStart = int(np.random.choice(range(self.memCount)))
        else:
            memStart = int(np.random.choice(range(self.maxMemory-batch_size-1)))

        miniBatch = self.memory[memStart:memStart+batch_size]
        memory = np.array(miniBatch)

        #feed forward current state and successor state conv to list as memory is array of numpy objects
        Qpred = self.Q_eval.forward(list(memory[:,0][:])).to(self.Q_eval.device)
        Qnext = self.Q_next.forward(list(memory[:,3][:])).to(self.Q_eval.device)

        maxA = torch.argmax(Qnext,dim = 1).to(self.Q_eval.device)
        #calculate rewards
        rewards = torch.Tensor(list(memory[:,2])).to(self.Q_eval.device)
        # loss for every action except max action to be 0
        Qtarget = Qpred.clone()
        print(Qnext.shape)
        Qtarget[:,maxA] = rewards + self.GAMMA*torch.max(Qnext[1])# PROBLEMATIC LINE
        # epsilon decay action
        if self.steps &gt; 2000:
            if self.EPSILON-1e-4 &gt;self.EPS_END:
                self.EPSILON-= 1e-4
            else:
                self.EPSILON = self.EPS_END
        loss = self.Q_eval.loss(Qtarget,Qpred).to(self.Q_eval.device)
        loss.backward()
        self.Q_eval.optimizer.step()
        self.learn_step_counter +=1
</code></pre>

<pre><code>env = gym.make(""Invader-v0"")
agent = DQNAgent(gamma=0.95,epsilon = 1.0,alpha = 0.003, maxMemory = 5000,replace = None)
while agent.memCount &lt; agent.maxMemory:
        obs = env.reset()
        done = False
        lives = 3
        while not done:
            action = env.action_space.sample()
            obs_ , reward, done, info = env.step(action)
            if done and info['lives']&lt;lives:
                lives = info['lives']
                reward -= 200
            agent.storeTransition(preprocess(obs),action,reward,preprocess(obs_))
            obs= obs_
initialised = True

scores = []
epsHistory = []
numGames = 50
batch_size = 16

for i in range(numGames):

    print(f'starting game {i+1}, epsilon = {agent.EPSILON}')
    epsHistory.append(agent.EPSILON)
    done = False
    obs = env.reset()

    frames = [np.sum(obs)]
    score = 0
    lastAction = 0
    lives = 3
    while not done:
        if len(frames) == 4:
            action = agent.chooseAction(frames)
            frames = []
        else:
            action = lastAction
        obs_, reward, done, info = env.step(action)
        score += score-reward
        frames.append(preprocess(obs_))
        if done and info['lives'] &lt; lives:
            reward -=200
        agent.storeTransition(preprocess(obs),action,reward,preprocess(obs_))
        obs= obs_
        agent.learn(batch_size)
        lastAction = action
    scores.append(score)
    print('score: ', score)
    x = [i+1 for i in range(numGames)]
</code></pre>
",8412864,,8412864,,2020-06-01 06:09:06,2020-06-06 09:40:34,RuntimeError: the derivative for 'indices' is not implemented,<pytorch><reinforcement-learning>,1,1,,,,CC BY-SA 4.0,
2508,65694416,1,65697986,,2021-01-13 01:23:35,,0,147,"<p>I apologize in advance for the question in the title not being very clear. I'm trying to train a reinforcement learning policy using tf-agents in which there exists some unobservable stochastic variable that affects the state.</p>
<p>For example, consider the standard CartPole problem, but we add wind where the velocity changes over time. I don't want to train an agent that relies on having observed the wind velocity at each step; I instead want the wind to affect the position and angular velocity of the pole, and the agent to learn to adapt just as it would in the wind-free environment. In this example however, we would need the wind velocity at the current time to be correlated with the wind velocity at the previous time e.g. we wouldn't want the wind velocity to change from 10m/s at time t to -10m/s at time t+1.</p>
<p>The problem I'm trying to solve is how to track the state of the exogenous variable without making it part of the observation spec that gets fed into the neural network when training the agent. Any guidance would be appreciated.</p>
",4588459,,,,,2021-01-13 08:17:31,Can a tf-agents environment be defined with an unobservable exogenous state?,<tensorflow><reinforcement-learning><tensorflow-agents>,1,0,,,,CC BY-SA 4.0,
2509,65750675,1,65755566,,2021-01-16 14:19:43,,4,178,"<p>I was following this tutorial (<a href=""https://www.tensorflow.org/agents/tutorials/1_dqn_tutorial?hl=en"" rel=""nofollow noreferrer"">https://www.tensorflow.org/agents/tutorials/1_dqn_tutorial?hl=en</a>) on how to implement a Deep Q Network algorithm with TF Agents to solve the Cart Pole using RL.</p>
<p>I create the <code>q_net</code>:</p>
<pre><code>fc_layer_params = (100,)

q_net = q_network.QNetwork(
    train_env.observation_spec(),
    train_env.action_spec(),
    fc_layer_params=fc_layer_params)
</code></pre>
<p>And when i use <code>q_net.summary()</code> it shows that the network has 500 input layers:</p>
<pre><code>    Model: &quot;QNetwork&quot;
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
EncodingNetwork (EncodingNet multiple                  500       
_________________________________________________________________
dense_1 (Dense)              multiple                  202       
=================================================================
Total params: 702
Trainable params: 702
Non-trainable params: 0
_________________________________________________________________
time: 3.63 ms (started: 2021-01-16 13:44:09 +00:00)
</code></pre>
<p>I would like to know why the value of input layers are 500 if for the cart pole environment we have the observation_spec and action_spec as:</p>
<pre><code>Observation Spec:
BoundedArraySpec(shape=(4,), dtype=dtype('float32'), name='observation', minimum=[-4.8000002e+00 -3.4028235e+38 -4.1887903e-01 -3.4028235e+38], maximum=[4.8000002e+00 3.4028235e+38 4.1887903e-01 3.4028235e+38])

Action Spec:
BoundedTensorSpec(shape=(), dtype=tf.int64, name='action', minimum=array(0), maximum=array(1))time: 5.24 ms (started: 2021-01-16 13:48:27 +00:00)
</code></pre>
<p>This problem has a maximum time step per episode of 200, shouldn't the input layer be 200?</p>
",13449757,,,,,2021-01-16 22:53:39,Why do q_net has this number of input layers?,<python><tensorflow><neural-network><artificial-intelligence><reinforcement-learning>,1,0,,,,CC BY-SA 4.0,
2510,49174342,1,49180318,,2018-03-08 13:32:12,,16,9321,"<p>Recently i looked into reinforcement learning and there was one question bugging me, that i could not find an answer for: How is training effectively done using GPUs? To my understanding constant interaction with an environment is required, which for me seems like a huge bottleneck, since this task is often non-mathematical / non-parallelizable. Yet for example Alpha Go uses multiple TPUs/GPUs. So how are they doing it?</p>
",4812335,,,,,2018-03-15 21:47:37,How to effectively make use of a GPU for reinforcement learning?,<gpu><reinforcement-learning>,2,0,0,,,CC BY-SA 3.0,
2511,65704854,1,65705017,,2021-01-13 15:23:05,,1,89,"<p>My server's conditions:</p>
<p>CUDA: 10, cudnn: 7.6   GPU:NVIDIA GeForce RTX 2080Ti  system: Ubuntu18.04</p>
<p>When I try to reproduce the repository <em><a href=""https://github.com/grantsrb/PyTorch-A2C"" rel=""nofollow noreferrer"">A2C</a></em>, I get an error. I tried many ways, but failed. How can I fix this?</p>
<p>Output:</p>
<pre><code>(myconda) root@9eac5f8cc084:~/PyTorch-A2C# python3.6 -m pip install --user -r requirements.txt
Looking in indexes: https://repo.huaweicloud.com/repository/pypi/simple
Requirement already satisfied: numpy in /root/miniconda3/envs/myconda/lib/python3.6/site-packages (from -r requirements.txt (line 1)) (1.16.3)
Requirement already satisfied: torch in /root/miniconda3/envs/myconda/lib/python3.6/site-packages (from -r requirements.txt (line 2)) (1.0.1.post2)
Requirement already satisfied: tqdm in /root/miniconda3/envs/myconda/lib/python3.6/site-packages (from -r requirements.txt (line 3)) (4.25.0)
Requirement already satisfied: matplotlib in /root/miniconda3/envs/myconda/lib/python3.6/site-packages (from -r requirements.txt (line 4)) (2.1.0)
Requirement already satisfied: gym in /root/miniconda3/envs/myconda/lib/python3.6/site-packages (from -r requirements.txt (line 6)) (0.12.1)
ERROR: Could not find a version that satisfies the requirement copy
ERROR: No matching distribution found for copy
</code></pre>
",12311212,,63550,,2021-08-19 06:57:30,2021-08-19 06:57:30,ERROR: Could not find a version that satisfies the requirement copy (from -r requirements.txt (line 10)),<python><deep-learning><pip><reinforcement-learning>,1,0,,,,CC BY-SA 4.0,
2514,49801638,1,50574354,,2018-04-12 16:27:37,,9,9206,"<p>The question is about vanilla, <strong>non-batched</strong> reinforcement learning. Basically what is defined <a href=""https://i.imgur.com/tZrylr9.png"" rel=""noreferrer"">here</a> in <a href=""http://incompleteideas.net/book/bookdraft2017nov5.pdf"" rel=""noreferrer"">Sutton's book</a>.
My model trains, (woohoo!) though there is an element that confuses me.</p>

<p><strong>Background:</strong></p>

<p>In an environment where duration is rewarded (like pole-balancing), we have rewards of (say) 1 per step. After an episode, before sending this array of 1's to the train step, we do the standard discounting and normalization to get returns:</p>

<pre><code>returns = self.discount_rewards(rewards)
returns = (returns - np.mean(returns)) / (np.std(returns) + 1e-10) // usual normalization
</code></pre>

<p>The discount_rewards is the usual method, but <a href=""https://gist.github.com/keithmgould/2e17a44fa253ca294a18f81ccf42468e"" rel=""noreferrer"">here is gist</a> if curious. </p>

<p>So an array of rewards [1,1,1,1,1,1,1,1,1] becomes an array of returns [1.539, 1.160, 0.777, 0.392, 0.006, -0.382, -0.773, -1.164, -1.556].</p>

<p><strong>Given that basic background I can ask my question:</strong></p>

<p>If positive returns are enforced, and negative returns are discouraged (in the optimize step), then no matter the length of the episode, roughly the first half of the actions will be encouraged, and the latter half will be discouraged. Is that true, or am I misunderstanding something?</p>

<p>If its <strong>NOT</strong> true, would love to understand what I got wrong.</p>

<p>If it <strong>IS</strong> true, then I don't understand why the model trains, since even a good-performing episode will have the latter half of its actions discouraged.</p>

<p>To reiterate, this is non-batched learning (so the returns are <strong>not</strong> relative to returns in another episode in the training step). After each episode, the model trains, and again, it trains well :)</p>

<p>Hoping this makes sense, and is short enough to feel like a proper clear question.</p>
",2429335,,,,,2021-10-30 07:27:51,Normalizing Rewards to Generate Returns in reinforcement learning,<python><tensorflow><machine-learning><reinforcement-learning>,2,0,0,,,CC BY-SA 3.0,
2517,49822078,1,49823242,,2018-04-13 17:10:38,,5,3111,"<p>I think I am messing something up.  </p>

<p>I always thought that:<br>
- 1-step TD on-policy = Sarsa<br>
- 1-step TD off-policy = Q-learning</p>

<p>Thus I conclude:
- n-step TD on-policy = n-step Sarsa<br>
- n-step TD off-policy = n-step Q-learning </p>

<p>In Sutton's book, however, he never introduces n-step Q-Learning, but he does introduce n-step off-policy Sarsa. Now I feel confused.</p>

<p>Can someone help me with the naming?</p>

<p><a href=""http://incompleteideas.net/book/the-book-2nd.html"" rel=""noreferrer"">Link to Sutton's book</a> (Off-Policy n-step Sarsa on page 149)</p>
",7362388,,,,,2018-04-13 18:34:42,Why is there no n-step Q-learning algorithm in Sutton's RL book?,<reinforcement-learning><q-learning><sarsa>,1,0,,,,CC BY-SA 3.0,
2518,49709397,1,49711238,,2018-04-07 16:17:24,,2,896,"<p>I use n-step Sarsa/sometimes Sarsa(lambda)</p>

<p>After experimenting a bit with different epsilon schedules I found out that the agent learns faster when I change the epsilon during an episode based on the number of steps already taken and the mean length of the last 10 episodes.</p>

<p>Low number of steps/beginning of episode => Low epsilon<br>
High number of steps/end of episode => High epsilon</p>

<p>This works far better than just an epsilon decay over time from episode to episode.</p>

<p>Does the theory allow this?</p>

<p>I think yes because all states are still visited regularly.</p>
",7362388,,3782161,,2018-04-07 19:38:30,2018-04-07 19:38:30,Does Sarsa still converge even when epsilon changes during each episode?,<reinforcement-learning>,1,0,,,,CC BY-SA 3.0,
2528,50393969,1,50473855,,2018-05-17 14:25:45,,0,474,"<p>I just implemented Q-Learning without neural networks but I am stuck at implementing them with neural networks.</p>

<p>I will give you a pseudo code showing how my Q-Learning is implemented:</p>

<pre><code>train(int iterations)
    buffer = empty buffer
    for i = 0 while i &lt; iterations:

         move = null
         if random(0,1) &gt; threshold:
             move = random_move()                 
         else
             move = network_calculate_move()

         input_to_network = game.getInput()
         output_of_network = network.calculate(input_to_network)

         game.makeMove(move)
         reward = game.getReward()

         maximum_next_q_value = max(network.calculate(game.getInput()))

         if reward is 1 or -1:            //either lost or won
             output_of_network[move] = reward
         else:
             output_of_network[move] = reward + discount_factor * max


         buffer.add(input_to_network, output_of_network)
         if buffer is full: 
             buffer.remove_oldest()
             train_network()


train_network(buffer b):
     batch = b.extract_random_batch(batch_size) 
     for each input,output in batch:
          network.train(input, output, learning_rate)  //one forward/backward pass
</code></pre>

<p>My problem right now is that this code works for a buffer size of less than 200. 
For any buffer over 200, my code does not work anymore so I've got a few questions:</p>

<ol>
<li>Is this implementation correct? (In theory)</li>
<li>How big should the batch size be compared to the buffer size</li>
<li>How would one usually train the network? For how long? Until a specific MSE of the whole batch is reached?</li>
</ol>
",4944986,,712995,,2019-10-12 08:29:59,2019-10-12 08:29:59,How to train a neural network with Q-Learning,<neural-network><reinforcement-learning><q-learning>,1,0,0,,,CC BY-SA 4.0,
2529,50397098,1,50397961,,2018-05-17 17:14:35,,1,219,"<p>To familiarize myself with reinforcement learning, I am implementing the basic RL algorithm to play the game <a href=""https://github.com/sourabhv/FlapPyBird"" rel=""nofollow noreferrer"">Flappy Bird</a>. I have everything set up, the only problem I am having is with implementing the reward function. I want to be able to process the screen and recognize whether a point has been scored or the bird has died. </p>

<p>Processing the screen is done using <a href=""https://pypi.org/project/mss/#description"" rel=""nofollow noreferrer"">mss</a> and opencv, which returns a <a href=""https://docs.scipy.org/doc/numpy-1.14.0/reference/generated/numpy.dstack.html"" rel=""nofollow noreferrer"">stacked numpy array</a>. The reward function then needs to assign a reward to the provided array, but I have no idea how to go about this. </p>

<p>This is how a single processed image looks like:</p>

<p><a href=""https://i.stack.imgur.com/cIOui.jpg"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/cIOui.jpg"" alt=""This is how a processed image looks like""></a></p>

<p>The idea I had for implementing the reward function was that if the background stops moving, the bird has died. And that if the bird is in a gap inbetween two pipes, the agent has scored a point. Any ideas in how can I express this in the numpy calculations ?</p>

<pre><code>def _calculate_reward(self, state):
    """"""""
    calculate the reward of the state. Flappy is dead when the screen has stopped moving, so when two consecutive frames
    are equal. A point is scored when an obstacle is above flappy, and before it wasn't. An object is above Flappy when
    there are two white pixels in the first 50 pixels on the first row.

    :param state: np.array shape = (1, height, width, 4) - &gt; four consecutive processed frames
    :return reward: int representing the reward if a point is scored or if flappy has died.
    """"""
    if np.sum((state[0,:,:,3] - state[0,:,:,2])) == 0 and np.sum((state[0,:,:,2] - state[0,:,:,1])) == 0:
        print(""flappy is dead"")
        return -1000
    elif sum(state[0,0,:50,3]) == 510 and sum(state[0,0,:50,2]) == 510 and sum(state[0,0,:50,1]) != 510 and sum(state[0,0,:50,0]) != 510:
        print(""point!"")
        return 1000
    else:
        return 0
</code></pre>
",5682512,,,,,2018-05-17 18:10:07,Game image recognition (Recognising Point Scored or Game over in Flappy Bird),<python><numpy><image-processing><reinforcement-learning>,1,0,,,,CC BY-SA 4.0,
2532,50392231,1,50394582,,2018-05-17 13:05:39,,0,408,"<p>Currently trying to implement a Q table algorithm in my environment created using turtle graphics. When i try running the algorithm which uses Q learning I get an error stating:</p>

<pre><code>  File ""&lt;ipython-input-1-cf5669494f75&gt;"", line 304, in &lt;module&gt;
    rl()

  File ""&lt;ipython-input-1-cf5669494f75&gt;"", line 282, in rl
    A = choose_action(S, q_table)

  File ""&lt;ipython-input-1-cf5669494f75&gt;"", line 162, in choose_action
    state_actions = q_table.iloc[state, :]

  File ""/Users/himansuodedra/anaconda3/lib/python3.6/site-packages/pandas/core/indexing.py"", line 1367, in __getitem__
    return self._getitem_tuple(key)

  File ""/Users/himansuodedra/anaconda3/lib/python3.6/site-packages/pandas/core/indexing.py"", line 1737, in _getitem_tuple
    self._has_valid_tuple(tup)

  File ""/Users/himansuodedra/anaconda3/lib/python3.6/site-packages/pandas/core/indexing.py"", line 204, in _has_valid_tuple
    if not self._has_valid_type(k, i):

  File ""/Users/himansuodedra/anaconda3/lib/python3.6/site-packages/pandas/core/indexing.py"", line 1674, in _has_valid_type
    return self._is_valid_list_like(key, axis)

  File ""/Users/himansuodedra/anaconda3/lib/python3.6/site-packages/pandas/core/indexing.py"", line 1723, in _is_valid_list_like
    raise IndexingError('Too many indexers')

IndexingError: Too many indexers
</code></pre>

<p>I cannot seem to pinpoint the problem. The logic to me looks fine. Also I am able to build the environment thereafter the script gets stuck and i am forced to terminate it. Any help would be great. The code is below:</p>

<pre><code>""""""
Reinforcement Learning using table lookup Q-learning method.
An agent ""Blue circle"" is positioned in a grid and must make its way to the 
green square. This is the end goal. Each time the agent should improve its 
strategy to reach the final Square. There are two traps the red and the wall 
which will reset the agent. 
""""""
import turtle
import pandas as pd
import numpy as np
import time

np.random.seed(2)

"""""" Setting Parameters """"""

#N_STATES = 12   # the size of the 2D world
ACTIONS = ['left', 'right', 'down','up']     # available actions
EPSILON = 0.9   # greedy police (randomness factor)
ALPHA = 0.1     # learning rate 
GAMMA = 0.9    # discount factor
MAX_EPISODES = 13   # maximum episodes
FRESH_TIME = 0.3    # fresh time for one move


def isGoal():
    if player.xcor() == -25 and player.ycor() == 225:
        player.goto(-175,125)
        status_func(1)
        S_ = 'terminal'
        R = 1
        interaction = 'Episode %s: total_steps = %s' %(episode+1, step_counter)
        print('\r{}'.format(interaction), end='')
        time.sleep(2)
        print('\r', end='')
        return S_, R
    else:
        pass


def isFire():
    if player.xcor() == -25 and player.ycor() == 175:
        player.goto(-175,125)
        status_func(3)
        S_ = 'terminal'
        R = -1
        interaction = 'Episode %s: total_steps = %s' %(episode+1, step_counter)
        print('\r{}'.format(interaction), end='')
        time.sleep(2)
        print('\r', end='')
        return S_, R
    else:
        pass 


def isWall():
    if player.xcor() == -125 and player.ycor() == 175:
        player.goto(-175,125)
        status_func(2)
        S_ = 'terminal'
        R = -1
        interaction = 'Episode %s: total_steps = %s' %(episode+1, step_counter)
        print('\r{}'.format(interaction), end='')
        time.sleep(2)
        print('\r', end='')
        return S_, R
    else:
        pass


"""""" Player Movement """"""

playerspeed = 50

"""""" Create the token object """"""

player = turtle.Turtle()
player.color(""blue"")
player.shape(""circle"")
player.penup()
player.speed(0)
player.setposition(-175,125)
player.setheading(90)



#Move the player left and right
def move_left():
    x = player.xcor()
    x -= playerspeed
    if x &lt; -175:
        x = -175
    player.setx(x)
    isGoal()
    isFire()
    isWall()
    S_ = player.pos()
    R = 0

def move_right():
    x = player.xcor()
    x += playerspeed
    if x &gt; -25:
        x = -25
    player.setx(x)
    isGoal()
    isFire()
    isWall()
    S_ = player.pos()
    R = 0

def move_up():
    y = player.ycor()
    y += playerspeed
    if y &gt; 225:
        y = 225
    player.sety(y)
    isGoal()
    isFire()
    isWall()
    S_ = player.pos()
    R = 0

def move_down():
    y = player.ycor()
    y -= playerspeed
    if y &lt; 125:
        y = 125
    player.sety(y)
    isGoal()
    isFire()
    isWall()
    S_ = player.pos()
    R = 0

#Create Keyboard Bindings
turtle.listen()
turtle.onkey(move_left, ""Left"")
turtle.onkey(move_right, ""Right"")
turtle.onkey(move_up, ""Up"")
turtle.onkey(move_down, ""Down"")

def build_q_table(n_states, actions):
    table = pd.DataFrame(
        np.zeros((n_states, len(actions))),     # q_table initial values
        columns=actions,    # actions's name
    )
    # print(table)    # show table
    return table


def choose_action(state, q_table):
    # This is how to choose an action
    state_actions = q_table.iloc[state, :]
    # act non-greedy or state-action have no value
    if (np.random.uniform() &gt; EPSILON) or ((state_actions == 0).all()): 
        action_name = np.random.choice(ACTIONS)
    else:   # act greedy
        # replace argmax to idxmax as argmax means a different function 
        action_name = state_actions.idxmax()    
    return action_name



def get_env_feedback(S, A):
    if A == 'right':
        move_right()
    elif A == 'left':
        move_left()
    elif A == 'up':
        move_up()
    else: #down 
        move_down()
    return S_, R



def update_env(S, episode, step_counter):
    wn = turtle.Screen()
    wn.bgcolor(""white"")
    wn.title(""test"")

    """""" Create the Grid """"""

    greg = turtle.Turtle()
    greg.speed(0)

    def create_square(size,color=""black""):
        greg.color(color)
        greg.pd()
        for i in range(4):
            greg.fd(size)
            greg.lt(90)
        greg.pu()
        greg.fd(size)

    def row(size,color=""black""):
        for i in range(4):
            create_square(size)

    def board(size,color=""black""):
        greg.pu()
        greg.goto(-(size*4),(size*4))
        for i in range(3):
            row(size)
            greg.bk(size*4)
            greg.rt(90)
            greg.fd(size)
            greg.lt(90)

    def color_square(start_pos,distance_sq, sq_width, color):
        greg.pu()
        greg.goto(start_pos)
        greg.fd(distance_sq)
        greg.color(color)
        greg.begin_fill()
        for i in range(4):
            greg.fd(sq_width)
            greg.lt(90)
        greg.end_fill()
        greg.pu()

    def initiate_grid(): 
        board(50)
        color_square((-200,200),150, 50,color=""green"")
        color_square((-200,150),50, 50,color=""black"")
        color_square((-200,150),150, 50,color=""red"")
        greg.hideturtle()

    initiate_grid()

    """""" Create the token object """"""

    player = turtle.Turtle()
    player.color(""blue"")
    player.shape(""circle"")
    player.penup()
    player.speed(0)
    player.setposition(S)
    player.setheading(90)




def rl():
    possible_states = {0:(-175,125),
                      1:(-175,175),
                      2:(-175,225),
                      3:(-125,125),
                      4:(-125,175),
                      5:(-125,225),
                      6:(-75,125),
                      7:(-75,175),
                      8:(-75,225),
                      9:(-25,125),
                      10:(-25,175),
                      11:(-25,225)}

    inv_possible_states = {v:k for k,v in possible_states.items()}

    #build the qtable 
    q_table = build_q_table(len(possible_states),ACTIONS)
    for episode in range(MAX_EPISODES):
        step_counter = 0
        which_state = 0
        S = possible_states[which_state]
        is_terminated = False
        update_env(S,episode,step_counter)
        while not is_terminated:

            A = choose_action(S, q_table)
            # take action &amp; get next state and reward
            S_, R = get_env_feedback(S, A) 
            q_predict = q_table.loc[S, A]
            if S_ != 'terminal':
                S_ = inv_possible_states[S_]
                # next state is not terminal
                q_target = R + GAMMA * q_table.iloc[S_, :].max()   
            else:
                q_target = R     # next state is terminal
                is_terminated = True    # terminate this episode

            q_table.loc[S, A] += ALPHA * (q_target - q_predict)  # update
            S = S_  # move to next state

            update_env(S, episode, step_counter+1)
            step_counter += 1
    return q_table



rl()
</code></pre>
",8946962,,,,,2018-05-17 14:57:28,Reinforcement learning algorithm using turtle graphics not functioning,<python><algorithm><turtle-graphics><reinforcement-learning>,1,0,,,,CC BY-SA 4.0,
2536,50444342,1,50452319,,2018-05-21 07:57:53,,0,253,"<p>This will probably be a dumb question to many. I rented a Linux (CentOS) server to run some simulation experiments using <a href=""http://www.mujoco.org/index.html"" rel=""nofollow noreferrer"">MuJoCo</a>. I have got most of the things installed, and I am running the python script to spin up an environment to collect data. <a href=""https://github.com/berkeleydeeprlcourse/homework/blob/master/hw1/run_expert.py"" rel=""nofollow noreferrer"">Here</a> is the python script. It gives me this error: </p>

<pre><code>Exception: glfw failed to initialize
Fatal Python error: Couldn't create autoTLSkey mapping
Aborted
</code></pre>

<p>I guess I was expecting this because there is no way that a pure shell environment will start rendering colourful graphics. As far as I understood from reading online, <code>glfw</code> is an OpenGL framework to render graphics. My pure shell environment is incapable of such tasks.</p>

<p>Is there a way that I can get the graphics to work? Can I tunnel the graphics layer to my laptop?</p>

<p>Note: I have no idea how graphics work so this question might annoy some experts, so getting some online pointers or a succinct description will be extremely helpful.</p>
",3413239,,2166798,,2018-05-21 12:53:40,2018-05-21 15:45:27,How to render graphics on Linux server,<linux><graphics><glfw><reinforcement-learning>,1,1,,,,CC BY-SA 4.0,
2537,62325794,1,62326661,,2020-06-11 13:41:31,,-2,130,"<p>I'm just getting into reinforcement learning and q-learning, and I wanted to try and create a Tic-Tac-Toe AI. With a Q-Table, I need to find the ""state"" of the board, and I was having trouble finding a way to do this.</p>

<p>For extra clarification, a state is a number that represents the current board, including the value of each of the nine squares.</p>

<p>A board that looks like:</p>

<pre class=""lang-py prettyprint-override""><code>[[0, 0, 0],
 [0, 0, 0],
 [0, 0, 0]]
</code></pre>

<p>would be state 0, as it is the first board. Beyond this, I am not sure how to calculate the state of the board based on the array.</p>

<p>[EDIT]
I'm coming here because I honestly don't know where to start; I can't find anything on the web, and if you dislike my question you could at least tell me why.</p>
",13314450,,13314450,,2020-06-11 13:48:16,2020-06-11 14:26:02,Get state of TicTacToe board in Q-Learning,<python><arrays><numpy><reinforcement-learning><q-learning>,1,0,,,,CC BY-SA 4.0,
2543,50453490,1,50453833,,2018-05-21 17:02:22,,2,547,"<p>Currently working on a reinforcement algorithm using Q tables and turtle graphics. The agent is within a grid of 6 squares and needs to reach the far right hand side as its goal. I have constructed this and then I run my algorithm in order for the agent to learn. I am facing the following problems. The script eventually gets stuck and as a result I can only seem to go through one episode. The agent (blue marker) is flashing around the 0,0 coordinate mark although I have set specific coordinates for it. Finally, the agent basically leaves a trail of its steps. My logic seems fine but can't pinpoint what is causing these problems</p>

<pre><code>"""""" Basic Reinforcement Learning environment using Turtle Graphics """"""

#imported libraries required for this project
import turtle
import pandas as pd
import numpy as np
import time
#import numpy as np


"""""" Environment """"""

#initialise the screen using a turtle object
wn = turtle.Screen()
wn.bgcolor(""black"")
wn.title(""Basic_Reinforcement_Learning_Environment"")
#wn.bgpic(""game_background.gif"")

#this function initializes the 2D environment
def grid(size): 
    #this function creates one square
    def create_square(size,color=""white""):
        greg.color(color)
        greg.pd()
        for i in range(4):
            greg.fd(size)
            greg.lt(90)
        greg.pu()
        greg.fd(size)
    #this function creates a row of sqaures based on simply one square
    def row(size,color=""white""):
            for i in range(6):
                create_square(size)
            greg.hideturtle()

    row(size)       

greg = turtle.Turtle()
greg.speed(0)
greg.setposition(-150,0)
grid(50)


def player_set(S):
    player = turtle.Turtle()
    player.color(""blue"")
    player.shape(""circle"")
    player.penup()
    player.speed(0)
    player.setposition(S)
    player.setheading(90)

N_STATES = 6   # the length of the 1 dimensional world
ACTIONS = ['left', 'right']     # available actions
EPSILON = 0.9   # greedy police
ALPHA = 0.1     # learning rate
GAMMA = 0.9    # discount factor
MAX_EPISODES = 13   # maximum episodes
FRESH_TIME = 0.3    # fresh time for one move

#this functions builds a Q-table and initializes all values to 0
def build_q_table(n_states, actions):
    table = pd.DataFrame(
        np.zeros((n_states, len(actions))),     # q_table initial values
        columns=actions,    # actions's name
    )
    # print(table)    # show table
    return table

def choose_action(state, q_table):
    # This is how to choose an action
    state_actions = q_table.iloc[state, :]
    # act non-greedy or state-action have no value
    if (np.random.uniform() &gt; EPSILON) or ((state_actions == 0).all()): 
        action_name = np.random.choice(ACTIONS)
    else:   # act greedy
        # replace argmax to idxmax as argmax means a different function 
        action_name = state_actions.idxmax()    
    return action_name


def get_env_feedback(S, A):
    # This is how agent will interact with the environment
    if A == 'right':    # move right
        if S == N_STATES - 2:   # terminate
            S_ = 'terminal'
            R = 1
        else:
            S_ = S + 1
            R = 0
    else:   # move left
        R = 0
        if S == 0:
            S_ = S  # reach the wall
        else:
            S_ = S - 1
    return S_, R

def update_env(S, episode, step_counter):            
    coords = [(-125,25),(-75,25),(-25,25),(25,25),(75,25),(125,25)]

    if S == 'terminal':
        interaction = 'Episode %s: total_steps = %s' %(episode+1, step_counter)
        print('\r{}'.format(interaction), end='')
        time.sleep(2)
        print('\r', end='')
    else:
        player_set(coords[S])
        time.sleep(FRESH_TIME)


def rl():
    q_table = build_q_table(N_STATES, ACTIONS)
    for episode in range(MAX_EPISODES):
        step_counter = 0
        S = 0
        is_terminated = False
        update_env(S, episode, step_counter)
        while not is_terminated:
            A = choose_action(S, q_table)
            S_, R = get_env_feedback(S,A)
            q_predict = q_table.loc[S,A]
            if S_ != 'terminal':
                q_target = R + GAMMA * q_table.iloc[S_, :].max() 
            else:
                q_target = R
                is_terminated = True

            q_table.loc[S, A] += ALPHA * (q_target - q_predict)
            S = S_
            update_env(S, episode, step_counter+1)
            step_counter += 1
        return q_table

rl()
</code></pre>

<p>CHANGES: updated the return statement and the algorithm now works so that it goes through 13 episodes!!! HOWEVER, I cannot seem to implement the player token (agent) so that it doesn't leave a trail of all steps taken and I would like it to reset after each episode. This maybe an issue to do with scope:</p>

<p>final solution: </p>

<pre><code>"""""" Basic Reinforcement Learning environment using Turtle Graphics """"""

#imported libraries required for this project
import turtle
import pandas as pd
import numpy as np
import time
#import numpy as np


"""""" Environment """"""

#initialise the screen using a turtle object
wn = turtle.Screen()
wn.bgcolor(""black"")
wn.title(""Basic_Reinforcement_Learning_Environment"")
#wn.bgpic(""game_background.gif"")

#this function initializes the 2D environment
def grid(size): 
    #this function creates one square
    def create_square(size,color=""white""):
        greg.color(color)
        greg.pd()
        for i in range(4):
            greg.fd(size)
            greg.lt(90)
        greg.pu()
        greg.fd(size)
    #this function creates a row of sqaures based on simply one square
    def row(size,color=""white""):
            for i in range(6):
                create_square(size)
            greg.hideturtle()

    row(size)       

greg = turtle.Turtle()
greg.speed(0)
greg.setposition(-150,0)
grid(50)

player = turtle.Turtle()
player.color(""blue"")
player.shape(""circle"")
player.penup()
player.speed(0)
player.setheading(90)

def player_set(S):
    player.setposition(S)



N_STATES = 6   # the length of the 1 dimensional world
ACTIONS = ['left', 'right']     # available actions
EPSILON = 0.9   # greedy police
ALPHA = 0.1     # learning rate
GAMMA = 0.9    # discount factor
MAX_EPISODES = 13   # maximum episodes
FRESH_TIME = 0.3    # fresh time for one move

#this functions builds a Q-table and initializes all values to 0
def build_q_table(n_states, actions):
    table = pd.DataFrame(
        np.zeros((n_states, len(actions))),     # q_table initial values
        columns=actions,    # actions's name
    )
    # print(table)    # show table
    return table

def choose_action(state, q_table):
    # This is how to choose an action
    state_actions = q_table.iloc[state, :]
    # act non-greedy or state-action have no value
    if (np.random.uniform() &gt; EPSILON) or ((state_actions == 0).all()): 
        action_name = np.random.choice(ACTIONS)
    else:   # act greedy
        # replace argmax to idxmax as argmax means a different function 
        action_name = state_actions.idxmax()    
    return action_name


def get_env_feedback(S, A):
    # This is how agent will interact with the environment
    if A == 'right':    # move right
        if S == N_STATES - 2:   # terminate
            S_ = 'terminal'
            R = 1
        else:
            S_ = S + 1
            R = 0
    else:   # move left
        R = 0
        if S == 0:
            S_ = S  # reach the wall
        else:
            S_ = S - 1
    return S_, R

def update_env(S, episode, step_counter):            
    coords = [(-125,25),(-75,25),(-25,25),(25,25),(75,25),(125,25)]

    if S == 'terminal':
        interaction = 'Episode %s: total_steps = %s' %(episode+1, step_counter)
        print('\n{}'.format(interaction), end='')
        time.sleep(2)
        print('\r', end='')
    else:
        player_set(coords[S])
        time.sleep(FRESH_TIME)


def rl():
    q_table = build_q_table(N_STATES, ACTIONS)
    for episode in range(MAX_EPISODES):
        step_counter = 0
        S = 0
        is_terminated = False
        update_env(S, episode, step_counter)
        while not is_terminated:
            A = choose_action(S, q_table)
            S_, R = get_env_feedback(S,A)
            q_predict = q_table.loc[S,A]
            if S_ != 'terminal':
                q_target = R + GAMMA * q_table.iloc[S_, :].max() 
            else:
                q_target = R
                is_terminated = True

            q_table.loc[S, A] += ALPHA * (q_target - q_predict)
            S = S_
            update_env(S, episode, step_counter+1)
            step_counter += 1
    return q_table

rl()
</code></pre>
",8946962,,8946962,,2018-05-22 12:06:21,2018-05-22 12:06:21,Reinforcement algorithm seems to learn but script is getting stuck and agent is not resetting,<python><algorithm><turtle-graphics><reinforcement-learning>,1,2,,,,CC BY-SA 4.0,
2547,44846312,1,44846510,,2017-06-30 12:15:34,,2,5742,"<p>I'm trying to apply reinforcement learning to a problem where the agent interacts with continuous numerical outputs using a recurrent network. Basically, it is a control problem where two outputs control how an agent behave. </p>

<p>I define an policy as epsilon greedy with (1-eps) of the time using the output control values, and eps of the time using the output values +/- a small Gaussian perturbation.
In this sense the agent can explore.
In most of the reinforcement literature I see that policy learning requires discrete actions which can be learned with the REINFORCE (Williams 1992) algorithm, but <strong>I'm unsure what method to use here.</strong></p>

<p>At the moment what I do is use masking to only learn the top choices using an algorithm based on Metropolis Hastings to decide if a transition is goes toward the optimal policy. Pseudo code:</p>

<pre><code>input: rewards, timeIndices
// rewards in (0,1) and optimal is 1 
// relate rewards to likelihood via L(r) = exp(-|r - 1|/std)
// r &lt;= 1 =&gt; |r - 1| = 1 - r
timeMask = zeros(timeIndices.length)
neglogLi =  (1 - mean(rewards)) / std
// Go through random order of reward to approximate Markov process
for r,idx in shuffle(rewards, timeIndices):
    neglogLj = (1 - r)/std 
    if neglogLj &lt; neglogLi || log(random.uniform()) &lt; neglogLi - neglogLj:
        // Accept transition, i.e. learn this action
        targetMask[idx] = 1
        neglogLi = neglogLj
</code></pre>

<p>This provides a <code>targetMask</code> with ones for the actions that will be learned using standard backprop.</p>

<p><strong>Can someone inform me the proper or better way?</strong></p>
",833285,,,,,2017-06-30 12:28:45,How to do reinforcement learning with regression instead of classification,<reinforcement-learning>,1,1,0,,,CC BY-SA 3.0,
2550,44874982,1,44876464,,2017-07-02 20:35:46,,1,597,"<p>I am trying to implement value iteration for the '3x4 windy gridworld' MDP and am having trouble with understanding the Bellman equation and its implementation. </p>

<p>The form of Bellman equation that I am working with is this</p>

<p><a href=""https://i.stack.imgur.com/nPTJX.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/nPTJX.png"" alt=""enter image description here""></a></p>

<p>Suppose this is the gridword I am working with and I want to find the value(<code>U(s)</code>) of the tile marked X. </p>

<p><a href=""https://i.stack.imgur.com/tgq9K.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/tgq9K.png"" alt=""enter image description here""></a></p>

<p>(Image snapshot from <a href=""https://www.youtube.com/watch?v=doxTNCH7oHc&amp;list=PLAwxTw4SYaPnidDwo9e2c7ixIsu_pdSNp&amp;index=26"" rel=""nofollow noreferrer"">this</a> video)</p>

<p>The reward at all the tiles expect the terminal tiles is defined to be zero and it is also assumed that if one tries to make a move in a particular direction, there is a small probability that the the actual move will take place at right angles to the intended move. (If you try to move down from x, you will go down with probability 0.8 but will move either left or right with probability 0.1 each)</p>

<p>Now, when you try to unravel the bellman equation for the position x, there are three neighbors(<code>U(s')</code>) for the action 'UP'. The original location itself(since it can't move up) with a probability 0.8, the +1 state to its right with a probability 0.1 and the tile left to it also with a probability 0.1. These form the <code>s'</code> states. </p>

<p>So a function to find the value of the state X would recursively call all the states <code>s'</code>'s. The +1 state out of this is not a problem since it is a terminal tile and that would constitute for the base case. But one of those state is the original state X itself and I don't understand how that case will ever terminate in the recursive call. Same problem with the third tile as well; will it ever terminate after all the calls to <em>it's</em> neighbors and so on?</p>
",5530553,,5530553,,2017-07-03 05:10:24,2017-07-03 05:10:24,Base cases for value iteration in reinforcement learning,<python><artificial-intelligence><reinforcement-learning>,1,0,,,,CC BY-SA 3.0,
2552,50521650,1,50531389,,2018-05-25 04:38:59,,-3,122,"<p>In the OpenAI paper ""Evolution Strategies as a Scalable Alternative to Reinforcement Learning"", how is the equation in page 3 derived? </p>

<p><img src=""https://i.stack.imgur.com/s4u8n.jpg"" alt=""1]""></p>
",3113320,,9184597,,2018-05-25 20:09:11,2018-05-25 20:09:11,How is the equation in â€œEvolution Strategies as a Scalable Alternative to Reinforcement Learningâ€ derived?,<machine-learning><reinforcement-learning><evolutionary-algorithm>,1,1,,,,CC BY-SA 4.0,
2553,62529802,1,62530356,,2020-06-23 07:45:29,,0,105,"<p>I had come across the following code while reading up about RL. The <em>probs</em> vector contains the probabilities of each action to be taken. And I believe the given loop tries to choose an action randomly from the given distribution. Why/How does this work?</p>
<pre><code>  a = 0
  rand_select = np.random.rand()
  while True:
      rand_select -= probs[a]
      if rand_select &lt; 0 or a + 1 == n_actions:
          break
      a += 1
      actions = a
</code></pre>
<p>After going through similar <a href=""https://stackoverflow.com/questions/4265988/generate-random-numbers-with-a-given-numerical-distribution/4266562#4266562"">code</a>, I realised that &quot;actions&quot; contains the final action to be taken.</p>
",12143665,,10306868,,2020-06-23 08:07:07,2020-06-23 08:25:09,Choosing a random value from a discrete distribution,<python><reinforcement-learning><probability-distribution>,1,1,0,,,CC BY-SA 4.0,
2555,50557601,1,50557604,,2018-05-28 00:03:38,,5,1055,"<p>I understand why machine learning is named as such, and on top of that the nomenclature behind supervised and unsupervised learning. So what is <strong>reinforced</strong> about reinforcement learning?</p>
",,user9856153,,,,2020-10-28 21:09:10,Why is RL called 'reinforcement' learning?,<machine-learning><deep-learning><reinforcement-learning>,3,0,,,,CC BY-SA 4.0,
2557,62629997,1,62630016,,2020-06-29 01:30:09,,2,6596,"<p>I am building a vanilla DQN model to play the OpenAI gym Cartpole game.</p>
<p>However, in the training step where I feed in the state as input and the target Q values as the labels, if I use <code>model.fit(x=states, y=target_q)</code>, it works fine and the agent can eventually play the game well, but if I use <code>model.train_on_batch(x=states, y=target_q)</code>, the loss won't decrease and the model will not play the game anywhere better than a random policy.</p>
<p>I wonder what is the difference between <code>fit</code> and <code>train_on_batch</code>? To my understanding, <code>fit</code> calls <code>train_on_batch</code> with a batch size of 32 under the hood which should make no difference since specifying the batch size to equal the actual data size I feed in makes no difference.</p>
<p>The full code is here if more contextual information is needed to answer this question: <a href=""https://github.com/ultronify/cartpole-tf"" rel=""nofollow noreferrer"">https://github.com/ultronify/cartpole-tf</a></p>
",6750238,,,,,2020-06-29 16:37:51,Difference between TensorFlow model fit and train_on_batch,<python><tensorflow><machine-learning><deep-learning><reinforcement-learning>,1,0,0,,,CC BY-SA 4.0,
2558,63010463,1,65597559,,2020-07-21 08:27:28,,0,108,"<p>Iâ€™m trying to use alpha zero general to apply on Abalone. And here is the original code of alpha zero general: <a href=""https://github.com/suragnair/alpha-zero-general"" rel=""nofollow noreferrer"">https://github.com/suragnair/alpha-zero-general</a></p>
<p>There are implements of some rectangle board games such as connect4, gobang, othello, tictactoe, and tictactoe_3d. However, Abalone, which has a hexagon board, makes me confused about how to describe it board in a 2d array which should be the input of the network.</p>
<p>I've implemented another code before I found alpha zero general. This is the original board list I used:</p>
<pre><code>       [  2, 2, 0, 1, 1  ], 
      [  2, 2, 2, 1, 1, 1  ],
     [  0, 2, 2, 0, 1, 1, 0  ],
   [  0, 0, 0, 0, 0, 0, 0, 0  ],
  [  0, 0, 0, 0, 0, 0, 0, 0, 0 ],
   [  0, 0, 0, 0, 0, 0, 0, 0  ],
    [  0, 1, 1, 0, 2, 2, 0  ],
      [  1, 1, 1, 2, 2, 2  ],
       [  1, 1, 0, 2, 2  ]

</code></pre>
<p>Where 1 stands for blacks, 2 stands for white. However, I found it cannot be inputted to CNN.</p>
<p>I have an idea to map it on an 2d numpy array but don't know whether it's ok to be used.</p>
<pre><code>[  2, 2, -1, -1, 0, 1, 1, 2, 2  ], 
[  2, 2, -1, -1, -1, 1, 1, 1, 2  ],
[  2, 0, -1, -1, 0, 1, 1, 0, 2  ],
[  2, 0, 0, 0, 0, 0, 0, 0, 0  ],
[  0, 0, 0, 0, 0, 0, 0, 0, 0  ],
[  0, 0, 0, 0, 0, 0, 0, 0, 2  ],
[  2, 0, 1, 1, 0, -1, -1, 0, 2  ],
[  2, 1, 1, 1, -1, -1, -1, 2, 2  ],
[  2, 2, 1, 1, 0, -1, -1, 2, 2  ]

</code></pre>
<p>where 1 stands for black, -1 stands for white, and 2 stands for invalid squares.</p>
<p>Is this idea ok?</p>
<p>Is there a recommended way to apply it to a hexagon board game or is there any exists example that I can refer to?</p>
",9543663,,,,,2021-01-06 14:25:25,Apply alpha-zero-general to Abalone (a hexagonal board game),<deep-learning><reinforcement-learning><hexagonal-tiles>,1,0,,,,CC BY-SA 4.0,
2559,45231492,1,45231995,,2017-07-21 07:23:02,,4,1286,"<p>I am currently reading Sutton's <code>Reinforcement Learning: An introduction</code> book. After reading chapter 6.1 I wanted to implement a <code>TD(0)</code> RL algorithm for this setting:</p>

<p><a href=""https://i.stack.imgur.com/ts9va.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/ts9va.png"" alt=""enter image description here""></a>
<a href=""https://i.stack.imgur.com/JHdT2.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/JHdT2.png"" alt=""enter image description here""></a></p>

<p>To do this, I tried to implement the pseudo-code presented here:
<a href=""https://i.stack.imgur.com/tlxPB.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/tlxPB.png"" alt=""enter image description here""></a></p>

<p>Doing this I wondered how to do this step <code>A &lt;- action given by Ï€ for S</code>: I can I choose the optimal action <code>A</code> for my current state <code>S</code>? As the value function <code>V(S)</code> is just depending on the state and not on the action I do not really know, how this can be done.</p>

<p>I found <a href=""https://math.stackexchange.com/questions/1884168/implementing-temporal-difference-learning-for-a-random-walk-in-python"">this</a> question (where I got the images from) which deals with the same exercise - but here the action is just picked randomly and not choosen by an action policy <code>Ï€</code>.</p>

<p>Edit: Or this is pseudo-code not complete, so that I have to approximate the <code>action-value function Q(s, a)</code> in another way, too?</p>
",3157209,,3157209,,2017-07-21 07:30:54,2017-07-21 08:02:42,How to choose action in TD(0) learning,<reinforcement-learning><temporal-difference>,1,0,,,,CC BY-SA 3.0,
2561,44936983,1,44937054,,2017-07-05 22:29:14,,1,280,"<pre><code>decay_rate = 0.99 # decay factor for RMSProp leaky sum of grad^2
</code></pre>

<p>I'm perplexed by the wording of comments like the above where they talk about a ""leaky"" sum of squares for the RMSProp optimizer. So far I've been able to uncover that this particular line is copy-pasta'd from Andrej Karpathy's <a href=""https://karpathy.github.io/2016/05/31/rl/"" rel=""nofollow noreferrer""><em>Deep Reinforcement Learning: Pong from Pixels</em></a>, and that RMSProp is an <a href=""http://sebastianruder.com/optimizing-gradient-descent/index.html#rmsprop"" rel=""nofollow noreferrer"">unpublished optimizer proposed by Hinton in one of his Coursera Classes</a>. Looking at the math for RMSProp from <a href=""http://sebastianruder.com/optimizing-gradient-descent/index.html#rmsprop"" rel=""nofollow noreferrer"">link 2</a>, it's hard to figure out how any of this is ""leaky.""</p>

<p>Would anyone happen to know why RMSProp is described this way?</p>
",1529734,,,,,2017-07-05 22:35:09,"Why is RMSProp considered ""leaky""?",<machine-learning><artificial-intelligence><reinforcement-learning><gradient>,1,0,,,,CC BY-SA 3.0,
2562,50676296,1,50676836,,2018-06-04 08:29:00,,0,1335,"<p>I'm trying to update a tensor's max value with another value, like so:</p>

<pre><code>actions = tf.argmax(output, axis=1)
gen_targets = tf.scatter_nd_update(output, actions, q_value)
</code></pre>

<p>I'm getting an error: <code>AttributeError: 'Tensor' object has no attribute 'handle'</code> on <code>scatter_nd_update</code>.</p>

<p>The <code>output</code> and <code>actions</code> are placeholders declared as:</p>

<pre><code>output = tf.placeholder('float', shape=[None, num_action])
reward = tf.placeholder('float', shape=[None])
</code></pre>

<p>What am I doing wrong and what would be the correct way to achieve this?</p>
",1562773,,,,,2018-06-12 11:37:47,scatter update tensor with index obtained using argmax,<python><tensorflow><reinforcement-learning>,1,0,0,,,CC BY-SA 4.0,
2563,27008226,1,27013761,,2014-11-19 03:04:25,,9,6052,"<p>I'm trying to find optimal policy in environment with continuous states (dim. = 20) and discrete actions (3 possible actions). And there is a specific moment: for optimal policy one action (call it ""action 0"") should be chosen much more frequently than other two (~100 times more often; this two action more risky).</p>

<p>I've tried Q-learning with NN value-function approximation. Results were rather bad: NN learns always choose ""action 0"". I think that policy gradient methods (on NN weights) may help, but don't understand how to use them on discrete actions.</p>

<p>Could you give some advise what to try? (maybe algorithms, papers to read).
What are the state-of-the-art RL algorithms when state space is continuous and action space is discrete?</p>

<p>Thanks.</p>
",2527078,,3782161,,2014-11-19 12:03:16,2018-01-27 11:46:28,"Reinforcement learning algorithms for continuous states, discrete actions",<machine-learning><reinforcement-learning>,1,0,0,,,CC BY-SA 3.0,
2564,45258655,1,45267307,,2017-07-22 20:00:18,,1,658,"<p>A typical skeleton of pytorch neural network has a forward() method, then we compute loss based on outputs of forward pass, and call backward() on that loss to update the gradients. What if my loss is determined externally (e.g. by running simulation in some RL environment). Can I still leverage this typical structure this way?</p>

<ul>
<li>This might be a bit dumb as we no longer know exactly how much each element of output influences the loss, but perhaps there is some trickery I'm not aware of. Otherwise I'm not sure how neural nets can be used in combination with other RL algorithms.</li>
</ul>

<p>Thank you!</p>
",1172195,,,,,2017-07-23 16:21:54,Is there a way to use an external loss function in pytorch?,<neural-network><deep-learning><reinforcement-learning><pytorch>,1,0,,,,CC BY-SA 3.0,
2566,63072770,1,63074078,,2020-07-24 11:37:20,,0,274,"<p>I am trying to train a Actor Critic Model with LSTM in both actor and critic.
I am new to all this and can not understand why <code>&quot;RuntimeError: Dimension out of range (expected to be in range of [-1, 0], but got 1)&quot;</code> is comming.</p>
<p>I am forwardPropagating from actor and getting error</p>
<p>below is my code and error message.I am using pytorch version 0.4.1</p>
<p>Can someone please help to check what is wrong with this code.</p>
<pre><code>import os
import time
import random
import numpy as np
import matplotlib.pyplot as plt
import pandas as pd
from sklearn.preprocessing import StandardScaler
import torch
import torch.nn as nn
import torch.nn.functional as F
from random import random as rndm
from torch.autograd import Variable
from collections import deque
   
torch.set_default_tensor_type('torch.DoubleTensor')


class Actor(nn.Module):
  
  def __init__(self, state_dim, action_dim, max_action):
    super(Actor, self).__init__()
    self.lstm = nn.LSTMCell(state_dim, 256)
    self.layer_1 = nn.Linear(256, 400)
    self.layer_2 = nn.Linear(400, 300)
    self.layer_3 = nn.Linear(300, action_dim)
    self.hx = torch.zeros(1,256)
    self.cx = torch.zeros(1,256)
    self.max_action = max_action

  def forward(self, x):
    self.hx, self.cx = self.lstm(x, (self.hx, self.cx))
    x = F.relu(self.layer_1(self.hx))
    x = F.relu(self.layer_2(x))
    x = self.max_action * torch.tanh(self.layer_3(x))
    return x

state_dim = 3
action_dim = 3
max_action = 1

policy = Actor(state_dim, action_dim, max_action)

s = torch.tensor([20,20,100])
next_action = policy(s)
</code></pre>
<p>and the error message is :</p>
<pre><code>next_action = policy(s)
Traceback (most recent call last):

  File &quot;&lt;ipython-input-20-de717f0ad3d2&gt;&quot;, line 1, in &lt;module&gt;
    next_action = policy(s)

  File &quot;C:\Users\granthjain\anaconda3\lib\site-packages\torch\nn\modules\module.py&quot;, line 477, in __call__
    result = self.forward(*input, **kwargs)

  File &quot;&lt;ipython-input-4-aed4daf511cb&gt;&quot;, line 14, in forward
    self.hx, self.cx = self.lstm(x, (self.hx, self.cx))

  File &quot;C:\Users\granthjain\anaconda3\lib\site-packages\torch\nn\modules\module.py&quot;, line 477, in __call__
    result = self.forward(*input, **kwargs)

  File &quot;C:\Users\granthjain\anaconda3\lib\site-packages\torch\nn\modules\rnn.py&quot;, line 704, in forward
    self.check_forward_input(input)

  File &quot;C:\Users\granthjain\anaconda3\lib\site-packages\torch\nn\modules\rnn.py&quot;, line 523, in check_forward_input
    if input.size(1) != self.input_size:

RuntimeError: Dimension out of range (expected to be in range of [-1, 0], but got 1)
</code></pre>
<p>I am using pytorch version 0.4.1</p>
<p>Can someone please help to check what is wrong with this code.</p>
",2783767,,2783767,,2020-07-24 12:26:57,2020-07-24 12:59:58,"pytoch RuntimeError: Dimension out of range (expected to be in range of [-1, 0], but got 1",<python><pytorch><artificial-intelligence><reinforcement-learning>,1,0,,,,CC BY-SA 4.0,
2570,45298898,1,45311515,,2017-07-25 09:15:25,,4,512,"<p>I am currently reading Sutton's introduction about reinforcement learning. After arriving in chapter 10 (On-Policy prediction with approximation), I am now wondering how to choose the form of the function <code>q</code> for which the optimal weights <code>w</code> shall be approximated.</p>

<p>I am referring to the first line of the pseudo code below from Sutton: How do I choose a good differentiable function <a href=""https://i.stack.imgur.com/xrbyk.gif"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/xrbyk.gif"" alt=""enter image description here""></a>? Are there any standard strategies to choose it?</p>

<p><a href=""https://i.stack.imgur.com/NM3wo.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/NM3wo.png"" alt=""enter image description here""></a></p>
",3157209,,,,,2017-07-25 19:07:07,Choose function for On-Policy prediction with approximation,<reinforcement-learning><approximation>,1,0,,,,CC BY-SA 3.0,
2571,50627861,1,50663731,,2018-05-31 15:27:51,,1,2192,"<p>I read this good <a href=""https://learningai.io/projects/2017/07/28/ai-gym-workout.html"" rel=""nofollow noreferrer"">article</a> about the Proximal Policy Optimization algorithm, and now I want update my VanillaPG agent to a PPO agent to learn more about it. However, I'm still not sure how to implement this in real code, especially since I'm using a simple discrete action space.</p>

<p>So what I do with my VPG Agent is, if there are 3 actions, the network outputs 3 values (out), on which I use softmax (p) and use the result as a distribution to choose one of the actions. For training, I take the states, actions and advantages and use this loss function:</p>

<pre><code>loss = -tf.reduce_sum(advantages * tf.log(ch_action_p_values))
</code></pre>

<p>How can I extend this algorithm to use PPO for discrete actions? All of the implementations I found work with continuous actions spaces. I'm not sure if I have to change my loss function to the first one used in the article. And I'm not even sure of which probabilities I have to calculate KLD. Are prob_s_a_* and D_KL single values for the whole batch, or one value for each sample? How can I calculate them in TF for my agent?</p>
",3592550,,,,,2018-06-03 06:06:01,Implement simple PPO Agent in TensorFlow,<python><tensorflow><reinforcement-learning>,1,0,0,,,CC BY-SA 4.0,
2572,45271441,1,45295777,,2017-07-24 01:31:22,,0,328,"<p>I was testing SARSA with lambda = 1 with Windy Grid World and if the exploration causes the same state-action pair to be visited many times before reaching the goal, the eligibility trace gets incremented each time without any decay, therefore it explodes and causes everything to overflow.
How can this be avoided?</p>
",4309634,,4309634,,2017-07-24 22:52:28,2017-07-25 06:44:55,How to prevent the eligibility trace in SARSA with lambda = 1 from exploding for state-action pairs that are visited a huge number of times?,<reinforcement-learning><temporal-difference><sarsa>,1,2,,,,CC BY-SA 3.0,
2573,50737705,1,50748029,,2018-06-07 09:27:01,,0,240,"<p>I want to modelize the service of selling seats on an airplane as an MDP( markov decision process) to use reinforcement learning for airline revenues optimization, for that I needed to define what would be: states, actions, policy, value and reward. I thought a little a bit about it, but i think there is still something missing.</p>

<p>I modelize my system this way:</p>

<ul>
<li><code>States = (r,c)</code> where r is the number of passengers and c the number of seats bought so <code>r&gt;=c</code>.</li>
<li><code>Actions = (p1,p2,p3)</code> that are the 3 prices. the objective is to decide which one of them give more revenues.</li>
<li>Reward: revenues.</li>
</ul>

<p>Could you please tell me what do u think and help me?</p>

<p>After the modelization, I have to implement all of that wit Reinforcement Learning. Is there a package that do the work ?</p>
",9907816,,1042017,,2018-06-07 20:31:37,2018-06-07 20:31:37,Reinforcement Learning with MDP for revenues optimization,<python><optimization><reinforcement-learning><markov-decision-process>,1,0,,,,CC BY-SA 4.0,
2575,50763983,1,50765510,,2018-06-08 15:32:46,,0,44,"<p>I have a small model used in a reinforcement learning context.</p>

<p>I can input a 2d tensor of states, and I get a 2d tensor of action weigths.</p>

<p>Let say I input two states and I get the following action weights out:</p>

<pre><code>[[0.1, 0.2],
 [0.3, 0.4]]
</code></pre>

<p>Now I have another 2d tensor which have the action number from which I want to get the weights:</p>

<pre><code>[[1],
 [0]]
</code></pre>

<p>How can I use this tensor to get the weight of actions?</p>

<p>In this example I'd like to get:</p>

<pre><code>[[0.2],
 [0.3]]
</code></pre>
",1171446,,,,,2018-06-09 15:56:39,Select weight of action from a tensorflow model,<tensorflow><machine-learning><reinforcement-learning>,2,2,,,,CC BY-SA 4.0,
2576,27059097,1,27061119,,2014-11-21 10:24:46,,1,278,"<p>I have <code>m*n</code> table which each entry have a value .</p>

<p>start position is at <em>top left</em> corner and I can go <em>right</em> or <em>down</em> until I reach <em>lower right</em> corner.</p>

<p>I want a path that if I multiply numbers on that path I get a number that have minimum number of zeros in it's right side .</p>

<p><em>example:</em></p>

<pre><code>   1 2 100
   5 5 4
</code></pre>

<p><em>possible paths :</em></p>

<pre><code>1*2*100*4=800
1*2*5*4= 40
1*5*5*4= 100
</code></pre>

<p>Solution : <code>1*2*5*4= 40</code> because 40 have 1 zero but other paths have 2 zero.</p>

<p>easiest way is using dfs and calculate all paths. but it's not efficient.</p>

<p>I'm looking for an optimal substructure for solving it using dynammic programming.</p>

<p>After thinking for a while I came up to this equation :</p>

<pre><code>T(i,j) = CountZeros(T(i-1,j)*table[i,j]) &lt; CountZeros(T(i,j-1)*table[i,j]) ?
                 T(i-1,j)*table[i,j] : T(i,j-1)*table[i,j]
</code></pre>

<p>Code :</p>

<pre><code>#include &lt;iostream&gt;
#include &lt;vector&gt;
#include &lt;algorithm&gt;
#include &lt;numeric&gt;

using namespace std;
using Table = vector&lt;vector&lt;int&gt;&gt;;
const int rows = 2;
const int cols = 3;
Table memo(rows, vector&lt;int&gt;(cols, -1));

int CountZeros(int number)
{
    if (number &lt; 0)
        return numeric_limits&lt;int&gt;::max();
    int res = 0;
    while (number != 0)
    {
        if (number % 10 == 0)
            res++;
        else break;
        number /= 10;
    }
    return res;
}


int solve(int i, int j, const Table&amp; table)
{
    if (i &lt; 0 || j &lt; 0)
        return -1;

    if (memo[i][j] != -1)
        return memo[i][j];

    int up = solve(i - 1, j, table)*table[i][j];
    int left = solve(i, j - 1, table)*table[i][j];

    memo[i][j] = CountZeros(up) &lt; CountZeros(left) ? up : left;

    return memo[i][j];
}
int main()
{
    Table table =
    {
        { 1, 2, 100 },
        { 5, 5, 4 }
    };

    memo[0][0] = table[0][0];
    cout &lt;&lt; solve(1, 2, table);

}
</code></pre>

<p><a href=""http://coliru.stacked-crooked.com/a/665509132b0d7e7e"" rel=""nofollow"">(Run )</a></p>

<p>But  it is not optimal (for example in above example it give 100 )</p>

<p>Any idea for better optimal sub-structure ? can I solve it with dynammic programming ?!</p>
",2586447,,2586447,,2014-11-21 14:12:09,2014-11-21 18:09:12,multiply numbers on all paths and get a number with minimum number of zeros,<c++><algorithm><dynamic-programming><reinforcement-learning>,1,5,,,,CC BY-SA 3.0,
2579,63097995,1,63098077,,2020-07-26 08:44:03,,0,6150,"<p>I am trying to do batch processing with <code>nn.lstm</code></p>
<p>From the documentation <a href=""https://pytorch.org/docs/master/generated/torch.nn.LSTM.html"" rel=""nofollow noreferrer"">https://pytorch.org/docs/master/generated/torch.nn.LSTM.html</a> I get that h0 and c0 should be of dimension:(num_layers * num_directions, batch, hidden_size).</p>
<p>But when I am trying to give input tensor with batch size&gt;1 and h0 , c0 batch size&gt;1. It is giving me error stating: <code>&quot;RuntimeError: Expected hidden[0] size (1, 1, 256), got (1, 611, 256)&quot;</code></p>
<p>Here is my code:
it contains 1 memory buffer, Actor, Critic, TD3, ENV classes and main training is in TD3 which has actor and critic objects.</p>
<p>Can someone please help a check what am i missing here.</p>
<pre><code>import os
import time
import random
import numpy as np
import matplotlib.pyplot as plt
import pandas as pd
from sklearn.preprocessing import StandardScaler
import torch
import torch.nn as nn
import torch.nn.functional as F
from random import random as rndm
from torch.autograd import Variable
from collections import deque
import pandas_datareader.data as pdr
import datetime

os.chdir('C:\\Users\\granthjain\\Desktop\\startup_code')

torch.set_default_tensor_type('torch.DoubleTensor')

f = open('lstm_with_noise_batch.txt', 'w+')


class ReplayBuffer(object):

    def __init__(self, max_size=1e6):
        self.storage = []
        self.max_size = max_size
        self.ptr = 0

    def add(self, transition):
        if len(self.storage) == self.max_size:
            self.storage[int(self.ptr)] = transition
        else:
            self.storage.append(transition)
        self.ptr = (self.ptr + 1) % self.max_size

    def sample(self, batch_size):

        ind = np.random.randint(0, self.ptr, size=batch_size)
        ind = np.random.randint(self.ptr)
        (batch_states, batch_next_states, batch_actions, batch_rewards,
         batch_dones) = ([], [], [], [], [])
        for i in range(ind - batch_size, ind):
            (state, next_state, action, reward, done) = self.storage[i]

            if state is None:
                continue
            elif next_state is None:
                continue
            elif action is None:
                continue
            elif reward is None:
                continue
            elif done is None:
                continue

            batch_states.append(np.array(state, copy=False))
            batch_next_states.append(np.array(next_state, copy=False))
            batch_actions.append(np.array(action, copy=False))
            batch_rewards.append(np.array(reward, copy=False))
            batch_dones.append(np.array(done, copy=False))

        return (np.array(batch_states, dtype=object).astype(float),
                np.array(batch_next_states,
                dtype=object).astype(float), np.array(batch_actions,
                dtype=object).astype(float), np.array(batch_rewards,
                dtype=object).astype(float), np.array(batch_dones,
                dtype=object).astype(float))


class Actor(nn.Module):

    def __init__(
        self,
        state_dim,
        action_dim,
        max_action,
        ):
        super(Actor, self).__init__()
        self.lstm = nn.LSTM(state_dim, 256)
        self.layer_1 = nn.Linear(256, 400)
        self.layer_2 = nn.Linear(400, 300)
        self.layer_3 = nn.Linear(300, action_dim)
        self.max_action = max_action

    def forward(self, x, hx):
        (hx, cx) = hx
        (output, (hx, cx)) = self.lstm(x, (hx, cx))
        x = F.relu(self.layer_1(output))
        x = F.relu(self.layer_2(x))
        x = self.max_action * torch.tanh(self.layer_3(x))

    # print(&quot;inside forward type cx:&quot;,len(output))

        return (x, hx, cx)


class Critic(nn.Module):

    def __init__(self, state_dim, action_dim):
        super(Critic, self).__init__()

    # Defining the first Critic neural network

        self.lstm1 = nn.LSTM(state_dim + action_dim, 256)
        self.layer_1 = nn.Linear(256, 400)
        self.layer_2 = nn.Linear(400, 300)
        self.layer_3 = nn.Linear(300, 1)

    # Defining the second Critic neural network

        self.lstm2 = nn.LSTM(state_dim + action_dim, 256)
        self.layer_4 = nn.Linear(256, 400)
        self.layer_5 = nn.Linear(400, 300)
        self.layer_6 = nn.Linear(300, 1)

    def forward(
        self,
        x,
        u,
        hx,
        ):
        xu = torch.cat([x, u], 1)

    # Forward-Propagation on the first Critic Neural Network

        xu = torch.reshape(xu, (xu.shape[0], 1, 6))
        (hx1, cx1) = hx
        (hx2, cx2) = hx
        (output, (hx1, cx1)) = self.lstm1(xu, (hx1, hx2))
        x1 = F.relu(self.layer_1(output))
        x1 = F.relu(self.layer_2(x1))
        x1 = self.layer_3(x1)

    # Forward-Propagation on the second Critic Neural Network

        (output, (hx2, cx2)) = self.lstm2(xu, (hx2, cx2))
        x2 = F.relu(self.layer_4(output))
        x2 = F.relu(self.layer_5(x2))
        x2 = self.layer_6(x2)
        return (
            x1,
            x2,
            hx1,
            hx2,
            cx1,
            cx2,
            )

    def Q1(
        self,
        x,
        u,
        hx1,
        ):
        xu = torch.cat([x, u], 1)
        xu = torch.reshape(xu, (xu.shape[0], 1, 6))
        (hx1, cx1) = hx1
        (output, (hx1, cx1)) = self.lstm1(xu, (hx1, cx1))
        x1 = F.relu(self.layer_1(output))
        x1 = F.relu(self.layer_2(x1))
        x1 = self.layer_3(x1)
        return (x1, hx1, cx1)


class TD3(object):

    def __init__(
        self,
        state_dim,
        action_dim,
        max_action,
        ):
        self.actor = Actor(state_dim, action_dim, max_action).to(device)
        self.actor_target = Actor(state_dim, action_dim,
                                  max_action).to(device)
        self.actor_target.load_state_dict(self.actor.state_dict())
        self.actor_optimizer = torch.optim.Adam(self.actor.parameters())
        self.critic = Critic(state_dim, action_dim).to(device)
        self.critic_target = Critic(state_dim, action_dim).to(device)
        self.critic_target.load_state_dict(self.critic.state_dict())
        self.critic_optimizer = \
            torch.optim.Adam(self.critic.parameters())
        self.max_action = max_action

    def select_action(self, state, hx1):
        (hx, cx) = hx1
        x = self.actor(state, hx1)
        return x

    def train(
        self,
        replay_buffer,
        iterations,
        batch_size=50,
        discount=0.99,
        tau=0.005,
        policy_noise=0.2,
        noise_clip=0.5,
        policy_freq=2,
        ):
        
        b_state = torch.Tensor([])
        b_next_state = torch.Tensor([])
        b_done = torch.Tensor([])
        b_reward = torch.Tensor([])
        b_action = torch.Tensor([])

        for it in range(iterations):

            # print ('it: ', it, ' iterations: ', iterations)

      # Step 4: We sample a batch of transitions (s, sâ€™, a, r) from the memory

            (batch_states, batch_next_states, batch_actions,
             batch_rewards, batch_dones) = \
                replay_buffer.sample(batch_size)

            batch_states = batch_states.astype(float)
            batch_next_states = batch_next_states.astype(float)
            batch_actions = batch_actions.astype(float)
            batch_rewards = batch_rewards.astype(float)
            batch_dones = batch_dones.astype(float)

            state = torch.from_numpy(batch_states)
            next_state = torch.from_numpy(batch_next_states)
            action = torch.from_numpy(batch_actions)
            reward = torch.from_numpy(batch_rewards)
            done = torch.from_numpy(batch_dones)

            b_size = 1
            seq_len = state.shape[0]
            batch = b_size
            input_size = state_dim

            state = torch.reshape(state, (seq_len, 1, state_dim))
            next_state = torch.reshape(next_state, (seq_len, 1,
                    state_dim))
            done = torch.reshape(done, (seq_len, 1, 1))
            reward = torch.reshape(reward, (seq_len, 1, 1))
            action = torch.reshape(action, (seq_len, 1, action_dim))
            
            b_state = torch.cat((b_state, state),dim=1)
            b_next_state = torch.cat((b_next_state, next_state),dim=1)
            b_done = torch.cat((b_done, done),dim=1)
            b_reward = torch.cat((b_reward, reward),dim=1)
            b_action = torch.cat((b_action, action),dim=1)
            
        print(&quot;dim state:&quot;,b_state.shape)
        print(&quot;dim next_state:&quot;,b_next_state.shape)
        print(&quot;dim done:&quot;,b_done.shape)
        print(&quot;dim reward:&quot;,b_reward.shape)
        print(&quot;dim action:&quot;,b_action.shape)

      # for h and c shape (num_layers * num_directions, batch, hidden_size)

        h0 = torch.zeros(1, b_state.shape[1], 256)
        c0 = torch.zeros(1, b_state.shape[1], 256)
      # Step 5: From the next state sâ€™, the Actor target plays the next action aâ€™

        next_action = self.actor_target(next_state, (h0, c0))
        next_action = next_action[0]

      # Step 6: We add Gaussian noise to this next action aâ€™ and we clamp it in a range of values supported by the environment

        noise = torch.Tensor(next_action).data.normal_(0,
                policy_noise).to(device)
        noise = noise.clamp(-noise_clip, noise_clip)
        next_action = (next_action + noise).clamp(-self.max_action,
                self.max_action)

      # Step 7: The two Critic targets take each the couple (sâ€™, aâ€™) as input and return two Q-values Qt1(sâ€™,aâ€™) and Qt2(sâ€™,aâ€™) as outputs

        result = self.critic_target(next_state, next_action, (h0,
                c0))
        target_Q1 = result[0]
        target_Q2 = result[1]

      # Step 8: We keep the minimum of these two Q-values: min(Qt1, Qt2)

        target_Q = torch.min(target_Q1, target_Q2).double()

      # Step 9: We get the final target of the two Critic models, which is: Qt = r + Î³ * min(Qt1, Qt2), where Î³ is the discount factor

        target_Q = reward + (1 - done) * discount * target_Q

      # Step 10: The two Critic models take each the couple (s, a) as input and return two Q-values Q1(s,a) and Q2(s,a) as outputs

        action = torch.reshape(action, next_action.shape)
        result = self.critic(state, action, (h0, c0))
        current_Q1 = result[0]
        current_Q2 = result[1]

      # Step 11: We compute the loss coming from the two Critic models: Critic Loss = MSE_Loss(Q1(s,a), Qt) + MSE_Loss(Q2(s,a), Qt)

        critic_loss = F.mse_loss(current_Q1, target_Q) \
            + F.mse_loss(current_Q2, target_Q)

      # Step 12: We backpropagate this Critic loss and update the parameters of the two Critic models with a SGD optimizer

        self.critic_optimizer.zero_grad()
        critic_loss.backward()
        self.critic_optimizer.step()

      # Step 13: Once every two iterations, we update our Actor model by performing gradient ascent on the output of the first Critic model

        out = self.actor(state, (h0, c0))
        out = out[0]
        (actor_loss, hx, cx) = self.critic.Q1(state, out, (h0,
                c0))
        actor_loss = -1 * actor_loss.mean()
        self.actor_optimizer.zero_grad()
        actor_loss.backward()
        self.actor_optimizer.step()

      # Step 14: Still once every two iterations, we update the weights of the Actor target by polyak averaging

        for (param, target_param) in zip(self.actor.parameters(),
                self.actor_target.parameters()):
            target_param.data.copy_(tau * param.data + (1 - tau)
                    * target_param.data)

      # Step 15: Still once every two iterations, we update the weights of the Critic target by polyak averaging

        for (param, target_param) in zip(self.critic.parameters(),
                self.critic_target.parameters()):
            target_param.data.copy_(tau * param.data + (1 - tau)
                    * target_param.data)

  # Making a save method to save a trained model

    def save(self, filename, directory):
        torch.save(self.actor.state_dict(), '%s/%s_actor.pth'
                   % (directory, filename))
        torch.save(self.critic.state_dict(), '%s/%s_critic.pth'
                   % (directory, filename))

  # Making a load method to load a pre-trained model

    def load(self, filename, directory):
        self.actor.load_state_dict(torch.load('%s/%s_actor.pth'
                                   % (directory, filename)))
        self.critic.load_state_dict(torch.load('%s/%s_critic.pth'
                                    % (directory, filename)))


class ENV:

    def __init__(
        self,
        state_dim,
        action_dim,
        data,
        ):
        self.state_dim = state_dim
        self.state = torch.zeros(self.state_dim)
        self.state[state_dim - 1] = 100000.0
        self.next_state = torch.zeros(self.state_dim)
        self.next_state[state_dim - 1] = 100000.0
        self.action_dim = action_dim
        self.data = data
        self.idx = 0
        self._max_episode_steps = 200
        self.state[1] = self.data[self.idx]
        self.next_state[1] = self.data[self.idx]
        self.buy = 0

    def reset(self):
        self.next_state = torch.zeros(self.state_dim)
        self.next_state[state_dim - 1] = 100000.0
        self.state = torch.zeros(self.state_dim)
        self.state[state_dim - 1] = 100000.0
        self.state[1] = self.data[self.idx]
        self.next_state[1] = self.data[self.idx]

        ch = self.state[0]
        cp = self.state[1]
        cc = self.state[2]
        st = torch.tensor([ch, cp, cc])
        self.buy = 0
        return st

    def step(self, action):
        done = False
        act_t = torch.argmax(action)
        self.idx += 1
        if act_t == 0:
            cp = 1.0003 * self.state[1]
            num_s = int(self.state[2] / cp)

            self.next_state[0] += num_s
            self.next_state[2] = self.state[2] % cp

            self.next_state[1] = self.data[self.idx]
            self.buy = 1
        elif act_t == 1:
            self.next_state[1] = self.data[self.idx]
        elif act_t == 2:
            self.next_state[2] = self.state[2] + self.state[1] * (1
                    - 0.0023) * self.state[0]
            self.next_state[0] = 0
            self.next_state[1] = self.data[self.idx]

            if self.buy == 1:
                done = True
                self.buy = 0

        reward = self.next_state[2] - self.state[2] \
            + self.next_state[1] * self.next_state[0] - self.state[1] \
            * self.state[0] - 1

        self.state[0] = self.next_state[0]
        self.state[1] = self.next_state[1]
        self.state[2] = self.next_state[2]

        ch = self.state[0]
        cp = self.state[1]
        cc = self.state[2]

        st = torch.tensor([ch, cp, cc])

        return (st, reward, done)


# Selecting the device (CPU or GPU)

device = torch.device(('cuda' if torch.cuda.is_available() else 'cpu'))

# set the parameters

start_timesteps = 1e3  # Number of iterations/timesteps before which the model randomly chooses an action, and after which it starts to use the policy network
eval_freq = 5e1  # How often the evaluation step is performed (after how many timesteps)
max_timesteps = 5e3  # Total number of iterations/timesteps
save_models = True  # Boolean checker whether or not to save the pre-trained model
expl_noise = 0.1  # Exploration noise - STD value of exploration Gaussian noise
batch_size = 200  # Size of the batch
discount = 0.99  # Discount factor gamma, used in the calculation of the total discounted reward
tau = 0.005  # Target network update rate
policy_noise = 0.2  # STD of Gaussian noise added to the actions for the exploration purposes
noise_clip = 0.5  # Maximum value of the Gaussian noise added to the actions (policy)
policy_freq = 2  # Number of iterations to wait before the policy network (Actor model) is updated

state_dim = 3
action_dim = 3
max_action = 1
idx = 0

# instantiate policy

policy = TD3(state_dim, action_dim, max_action)

indices = pd.read_csv('nifty_test.csv')
indices = indices['0']

indices = pd.read_csv('EQUITY_L.csv')
indices = indices['SYMBOL']


# Create the environment for each ticker
# data = pd.read_csv('PAGEIND.csv')

for ticker in indices:
    print(ticker)

    ohlcv = pd.read_csv(ticker + '.csv')
    data = ohlcv.copy()
    data = data['Close']
    data = np.array(data).reshape(-1, 1)
    count = 0
    max_timesteps = data.shape[0]

    data = torch.DoubleTensor(data)
    env = ENV(state_dim, action_dim, data)

    replay_buffer = ReplayBuffer()

    # init training variables

    total_timesteps = 0
    timesteps_since_eval = 0
    episode_num = 0
    done = True
    t0 = time.time()
    obs = env.reset()
    hx = torch.zeros(1, 1, 256)
    cx = torch.zeros(1, 1, 256)

    # Set rewards and episode timesteps to zero

    episode_reward = 0
    episode_timesteps = 0
    episode_num = 0

    # We start the main loop over max_timesteps

    while total_timesteps &lt; max_timesteps:

      # If the episode is done

        if done | (total_timesteps == max_timesteps - 2) \
            &amp; (episode_timesteps &gt; 200):
            count = count + 1
            if (count % 100 == 0) &amp; (count &gt;= 100) \
                | (total_timesteps == max_timesteps - 2) \
                &amp; (episode_timesteps &gt; 200):

            # If we are not at the very beginning, we start the training process of the model

                if total_timesteps != 0:
                    print('Total Timesteps: {} Episode Num: {} Reward: {}'.format(total_timesteps,
                            episode_num, episode_reward))
                    policy.train(
                        replay_buffer,
                        episode_timesteps,
                        batch_size,
                        discount,
                        tau,
                        policy_noise,
                        noise_clip,
                        policy_freq,
                        )

                    if total_timesteps &gt; 0.6 * max_timesteps + 1:
                        print('model output: Total Timesteps: {} Episode Num: {} Reward: {}'.format(total_timesteps,
                                episode_num, episode_reward))
                        f.write('model output: Total Timesteps: '
                                + str(total_timesteps)
                                + ' episode_num '
                                + str(episode_num)
                                + ' episode_reward '
                                + str(episode_reward))

            # When the training step is done, we reset the state of the environment

                obs = env.reset()

            # Set the Done to False

                done = False

            # Set rewards and episode timesteps to zero

                episode_reward = 0
                episode_timesteps = 0
                episode_num += 1
                hx = torch.zeros(1, 1, 256)
                cx = torch.zeros(1, 1, 256)

      # Before 1000 timesteps, we play random actions

        if total_timesteps &lt; 0.6 * max_timesteps:

    # random action

            actn = torch.randn(action_dim)
            action = torch.zeros(action_dim)
            action[torch.argmax(actn)] = 1
        else:

            # After 1000 timesteps, we switch to the model

    #    input of shape (seq_len, batch, input_size)

            obs1 = torch.reshape(obs, (1, 1, state_dim))
            action = policy.select_action(obs1, (hx, cx))
            
            actn = action[0]
            hx = action[1]
            cx = action[2]
            

        # If the explore_noise parameter is not 0, we add noise to the action and we clip it

            if expl_noise != 0:
                print ('policy action:', actn)
                actn = actn + torch.randn(action_dim)
                action = torch.zeros(action_dim)
                action[torch.argmax(actn)] = 1
        

      # The agent performs the action in the environment, then reaches the next state and receives the reward

        (new_obs, reward, done) = env.step(action)

      # We check if the episode is done

        done_bool = (0 if episode_timesteps + 1
                     == env._max_episode_steps else float(done))

      # We increase the total reward

        episode_reward += reward

      # We store the new transition into the Experience Replay memory (ReplayBuffer)

        replay_buffer.add((obs, new_obs, action, reward, done_bool))

      # We update the state, the episode timestep, the total timesteps, and the timesteps since the evaluation of the policy

        obs = new_obs
        episode_timesteps += 1
        total_timesteps += 1
        timesteps_since_eval += 1

f.close()
</code></pre>
<p>and below is the output:</p>
<pre><code>20MICRONS
Total Timesteps: 611 Episode Num: 0 Reward: -53044.2697380831
dim state: torch.Size([200, 611, 3])
dim next_state: torch.Size([200, 611, 3])
dim done: torch.Size([200, 611, 1])
dim reward: torch.Size([200, 611, 1])
dim action: torch.Size([200, 611, 3])
Traceback (most recent call last):

  File &quot;C:\Users\granthjain\Desktop\try_lstm.py&quot;, line 538, in &lt;module&gt;
    policy_freq,

  File &quot;C:\Users\granthjain\Desktop\try_lstm.py&quot;, line 279, in train
    next_action = self.actor_target(next_state, (h0, c0))

  File &quot;C:\Users\granthjain\anaconda3\lib\site-packages\torch\nn\modules\module.py&quot;, line 550, in __call__
    result = self.forward(*input, **kwargs)

  File &quot;C:\Users\granthjain\Desktop\try_lstm.py&quot;, line 106, in forward
    (output, (hx, cx)) = self.lstm(x, (hx, cx))

  File &quot;C:\Users\granthjain\anaconda3\lib\site-packages\torch\nn\modules\module.py&quot;, line 550, in __call__
    result = self.forward(*input, **kwargs)

  File &quot;C:\Users\granthjain\anaconda3\lib\site-packages\torch\nn\modules\rnn.py&quot;, line 567, in forward
    self.check_forward_args(input, hx, batch_sizes)

  File &quot;C:\Users\granthjain\anaconda3\lib\site-packages\torch\nn\modules\rnn.py&quot;, line 523, in check_forward_args
    'Expected hidden[0] size {}, got {}')

  File &quot;C:\Users\granthjain\anaconda3\lib\site-packages\torch\nn\modules\rnn.py&quot;, line 187, in check_hidden_size
    raise RuntimeError(msg.format(expected_hidden_size, tuple(hx.size())))

RuntimeError: Expected hidden[0] size (1, 1, 256), got (1, 611, 256)
</code></pre>
",2783767,,,,,2020-07-26 08:54:20,"lstm pytorch RuntimeError: Expected hidden[0] size (1, 1, 256), got (1, 611, 256)",<python><pytorch><artificial-intelligence><reinforcement-learning>,1,0,,,,CC BY-SA 4.0,
2581,51626985,1,51635631,,2018-08-01 06:48:23,,1,35,"<p>Below is text snippet from Sequential decision problem in Artifical Intellegence book A modern approach by Stuart Russel and Peter Norvig. Chater 17 section 17.1</p>

<blockquote>
  <p>Stationarity for preferences means the following:</p>
  
  <p>if two state sequences [s0, s1, s2, . . .] and [s0',s1', s2', . . .]
  begin with the same state (i.e., s0 =s01), then the two sequences
  should be preference-ordered the same way as the sequences [s1, s2, .
  . .] and [s1', s2', . . .].</p>
  
  <p>In English, this means that if you prefer one future to another
  starting tomorrow, then you should still prefer that future if it were
  to start today instead.</p>
</blockquote>

<p>I am difficulty in understanding last statement.</p>

<p>In English, this means that if you prefer one future to another starting tomorrow, then you should still prefer that future if it were to start today instead.</p>

<p>Kindly eloboarte and explain.</p>
",519882,,,,,2018-08-01 14:16:52,Stationarity conecpt in Sequential decision in reinforcement learning,<machine-learning><artificial-intelligence><reinforcement-learning>,1,1,,,,CC BY-SA 4.0,
2586,63449098,1,63461290,,2020-08-17 10:33:30,,0,245,"<p>I would like to let my computer learn to play a game in a virtual machine, using reinforcement learning. Unfortunately I cannot read the score, which should be used for positive rewards. The font is kinda strange as well, which is probably the reason. This is my code:</p>
<pre><code>def show(img):
    plt.imshow(img, cmap=&quot;gray&quot;)
    plt.show()

image = cv2.imread('screenshot.png',0)
crop_img = image[100:140, 38:280]


ret, thresh = cv2.threshold(crop_img, 127, 255, cv2.THRESH_BINARY) 
kernel = np.ones((3,3),np.uint8)
img = cv2.erode(thresh,kernel,iterations = 1)

data = pytesseract.image_to_string(img, lang='eng',config='--psm 10 --oem 3 -c tessedit_char_whitelist=0123456789')
show(img)
print(data)
</code></pre>
<p>I tried to extract just the score from the screenshot, which worked out, but it doesn't seem te recognise a single character.</p>
<p><a href=""https://i.stack.imgur.com/TGR8A.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/TGR8A.png"" alt=""the score"" /></a></p>
<p>The amount of lives, which I would like to use for negative rewards do seem to be recognised. Those are kind of strange objects, which tesseract seems to think those are Euro signs, so I could count the amount of Euro signs to determine the amount of lives...</p>
<p>But any tips for the score?</p>
",1843511,,1843511,,2020-08-17 11:33:27,2020-08-18 03:04:12,Tesseract: cannot read digits from pixelated font,<python><python-3.x><opencv><tesseract><reinforcement-learning>,1,0,,,,CC BY-SA 4.0,
2589,51693567,1,51693613,,2018-08-05 10:44:04,,1,898,"<p>I am training a neural network (feedforward, Tanh hidden layers) that receives states as inputs and gives actions as outputs. I am following the REINFORCE algorithm for policy-gradient reinforcement learning.</p>

<p>However, I need my control actions to be bounded (let us say from 0-5). Currently the way I am doing this is by using a sigmoid output function and multiplying the output by 5. Although my algorithm has a moderate performance, I find the following drawback from using this â€œbounding schemeâ€ for the output:</p>

<p>I know for regression (hence I guess for reinforcement learning) a linear output is best, and although the sigmoid has a linear part I am afraid the network has not been able to capture this linear output behaviour correctly, or it captures it way too slowly (as its best performance is for classification, therefore polarizing the output).</p>

<p>I am wondering what other alternatives there are, and maybe some heuristics on the matter.</p>
",2960040,,,,,2018-08-05 10:50:34,Best way to bound outputs from neural networks on reinforcement learning,<tensorflow><machine-learning><artificial-intelligence><pytorch><reinforcement-learning>,1,0,,,,CC BY-SA 4.0,
2597,51774091,1,51794321,,2018-08-09 19:00:53,,1,907,"<p>I'm trying to build an Agent that can play Pocket Tanks using RL. The problem I'm facing now is that how can I train a neural network to output the correct Power and Angle. so instead of actions classification. and I want a regression.  </p>
",5824057,,,,,2018-08-10 21:43:59,Regression through reinforcement learning,<python><neural-network><reinforcement-learning>,1,3,0,,,CC BY-SA 4.0,
2599,51823506,1,51826088,,2018-08-13 13:27:50,,1,397,"<p>I am implementing the bandit problem using various algorithms. The issue that I am facing is that epsilon-greedy is performing better than UCB for 5arms and horizon of 2000 for an epsilon value of 0.95.
I am aware of the fact that epsilon-greedy does indeed perform better when the horizon is comparable with a number of arms.
But since my arms are significantly less than the horizon, UCB should perform better. Any idea why this is happening?
I am enclosing my UCB implementation.</p>

<pre><code>else if(algorithm.compare(""UCB"") == 0){

if(pulls == 0){
  armpullfrequency = new int[numArms];
  armRewards = new float[numArms];
  armmean = new double[numArms];
  UCB = new double[numArms];

  for(int i=0; i&lt;numArms; i++){
    armpullfrequency[i] = 0;
    armRewards[i] = 0.0;
    armmean[i] = (double)0;
    UCB[i] = (double)0;
  }
}

else{

  armpullfrequency[pulled_arm] = armpullfrequency[pulled_arm] + 1;
  armRewards[pulled_arm] = armRewards[pulled_arm] + reward;
}
int selected_arm = 0;
//int randint = (rand() % 100);
if(pulls&lt;=6){
  for(int i=0;i&lt;numArms;i++){
    if(armpullfrequency[i]==0){
      selected_arm = i;
      return selected_arm;
    }
  }
}

for(int i=0;i&lt;numArms;i++){
    int freq = armpullfrequency[i];
    float prize = armRewards[i];
    double mean = eval_mean(freq, prize);
    armmean[i] = mean;
  }

for(int i=0; i&lt;numArms;i++){
  int freq = armpullfrequency[i];
  double mean = armmean[i];
  double UCBval = UCBUpdate(mean, freq, pulls);
  UCB[i] = UCBval;
}

selected_arm = LargestElementIndex(UCB, numArms);
return(selected_arm);
</code></pre>

<p>My UCB and LargestElementIndex function are:-</p>

<pre><code>int LargestElementIndex(double arr[], int size){
  int max = 0;
  for(int i=0;i&lt;size; i++){
    if(arr[i]&gt;max){
     max = arr[i];
    }
  }
  return max;
}

int UCBUpdate(double mean, int freq, int pulls){
  double result = mean + sqrt((double)2.0 *(log(pulls))/(double)freq);
  return result;
}
</code></pre>

<p>The results in case of UCB is:-
maxMean 0.5805 numTotalPulls 2000 cumulativeReward 716.308
Regret = 444.692</p>

<p>The results in the case of Epsilon Greedy is:-
max means 0.5805 numTotalPulls 2000 cumulativeReward 823.948
Regret = 337.052</p>
",5896168,,10156179,,2018-08-13 14:02:27,2018-08-13 15:44:28,Epsilon Greedy Performing better than UCB for small number of arms,<c++><machine-learning><artificial-intelligence><reinforcement-learning>,1,0,,,,CC BY-SA 4.0,
2601,51944199,1,51946609,,2018-08-21 08:01:20,,0,340,"<p>I'm trying to calculate loss in an RL project with 3 discrete actions. I have the output prediction of my model for (from <code>tf.layers.dense()</code>) (e.g. 3 possible actions, batch size 2):</p>

<pre><code>[[10, 20.2, 4.3],
 [5, 3, 8.9]]
</code></pre>

<p>I have a the action that was taken by the agent (e.g.):</p>

<pre><code>[[1],
 [2]]
</code></pre>

<p>And I have the reward for taking that action from the environment (e.g):</p>

<pre><code>[[30.0],
 [15.0]]
</code></pre>

<p>I want to calculate the loss for the taken action, using the action as an index and the reward. I don't have any information for the actions that weren't taken. If it were just calculating the difference I'd expect the loss (from the previous examples) to be:</p>

<pre><code>[[0, 9.8, 0],
 [0, 0, 6.1]]
</code></pre>

<p>I've tried:</p>

<pre><code>updated = tf.scatter_update(logits, action, reward)
loss = tf.nn.softmax_cross_entropy_with_logits_v2(labels=updated, logits=logits)
</code></pre>

<p>But this gives <code>AttributeError: 'Tensor' object has no attribute '_lazy_read'</code>. I believe this is because the inputs are Tensors but not Variables which <code>scatter_update()</code> requires.</p>

<p>How can I calculate loss for this?</p>
",292440,,,,,2018-08-31 02:18:22,Calculating loss from action and reward in Tensorflow,<python><tensorflow><reinforcement-learning>,1,0,,,,CC BY-SA 4.0,
2602,67517535,1,67530717,,2021-05-13 10:18:03,,3,108,"<p>I am trying to create a simultaneous multi agent environment using <a href=""https://juliareinforcementlearning.org/"" rel=""nofollow noreferrer"">reinforcementlearning.jl</a><br />
I have successfully represented the environment and it works with a RandomPolicy for every agent.</p>
<p>But my state space is large (actually it's a 14 tuple with each value in a certain range). So I can not use Tabular Approximators to estimate the Q or V values. That's why I have decided to use a Neural Network Approximator. But the docs do not discuss much about it, nor are there any examples were neural network approximator is used. I am stuck how to figure out how to use such approximator. If anyone can explain how to go about it, or refer to any example, it would be helpful.</p>
<p>Moreover I found from docs that using a Neural Network approximator needs us to use a  <a href=""https://github.com/JuliaReinforcementLearning/ReinforcementLearning.jl/blob/master/src/ReinforcementLearningCore/src/policies/agents/trajectories/trajectory.jl"" rel=""nofollow noreferrer"">CircularArraySARTTrajectory</a>. But defining this trajectory requires a key word argument called capacity. I don't know what it means, nor it is discussed about in the docs and GitHub.</p>
<p>I tried writing the code that uses neural network approximator but I get error.</p>
<pre><code># Create a flux based DNN for q - value estimation
STATE_SIZE = length(myenv.channels) # 14
ACTION_SIZE = length(values)        # 2

model = Chain(
      Dense(STATE_SIZE, 24, tanh),
      Dense(24, 48, tanh),
      Dense(48, ACTION_SIZE)
  ) |&gt; gpu

Î· = 1f-2 # Learning rate
Î·_decay = 1f-3
opt = Flux.Optimiser(ADAM(Î·), InvDecay(Î·_decay))

policies = MultiAgentManager(
   (
       Agent(
           policy = NamedPolicy(
              p =&gt; VBasedPolicy(;
                  learner = BasicDQNLearner(;
                     approximator = NeuralNetworkApproximator(;
                         model = model,
                         optimizer = opt
                    )
                 )
             )
         ),
         trajectory = CircularArraySARTTrajectory(;
             capacity = 14,
             state=Array{Float64, 1},
             action=Int,
             terminal = Bool
         )
     )
     for p in players(myenv)
 )...
)
</code></pre>
<p>Error/StackTrace</p>
<pre><code>MethodError: no method matching iterate(::Type{Array{Float64,1}})
Closest candidates are:
  iterate(::Plots.NaNSegmentsIterator) at 
C:\Users\vchou\.julia\packages\Plots\lzHOt\src\utils.jl:124
  iterate(::Plots.NaNSegmentsIterator, ::Int64) at 
C:\Users\vchou\.julia\packages\Plots\lzHOt\src\utils.jl:124
  iterate(::LibGit2.GitBranchIter) at 
C:\buildbot\worker\package_win64\build\usr\share\julia\stdlib\v1.5\LibGit2\src\reference.jl:343
  ...

Stacktrace:
 [1] first(::Type{T} where T) at .\abstractarray.jl:341
 [2] (::ReinforcementLearningCore.var&quot;#53#54&quot;{Int64})(::Type{T} where T) at C:\Users\vchou\.julia\packages\ReinforcementLearningCore\NWrFY\src\policies\agents\trajectories\trajectory.jl:46
 [3] map(::ReinforcementLearningCore.var&quot;#53#54&quot;{Int64}, ::Tuple{DataType,DataType}) at .\tuple.jl:158
 [4] map(::Function, ::NamedTuple{(:state, :action),Tuple{DataType,DataType}}) at .\namedtuple.jl:187
 [5] CircularArrayTrajectory(; capacity::Int64, kwargs::Base.Iterators.Pairs{Symbol,DataType,Tuple{Symbol,Symbol},NamedTuple{(:state, :action),Tuple{DataType,DataType}}}) at C:\Users\vchou\.julia\packages\ReinforcementLearningCore\NWrFY\src\policies\agents\trajectories\trajectory.jl:45
 [6] Trajectory{var&quot;#s57&quot;} where var&quot;#s57&quot;&lt;:(NamedTuple{(:state, :action, :reward, :terminal),var&quot;#s16&quot;} where var&quot;#s16&quot;&lt;:(Tuple{var&quot;#s15&quot;,var&quot;#s14&quot;,var&quot;#s12&quot;,var&quot;#s84&quot;} where var&quot;#s84&quot;&lt;:CircularArrayBuffers.CircularArrayBuffer where var&quot;#s12&quot; &lt;:CircularArrayBuffers.CircularArrayBuffer where var&quot;#s14&quot;&lt;:CircularArrayBuffers.CircularArrayBuffer where var&quot;#s15&quot;&lt;:CircularArrayBuffers.CircularArrayBuffer))(; capacity::Int64, state::Type{T} where T, action::Type{T} where T, reward::Pair{DataType,Tuple{}}, terminal::Type{T} where T) at C:\Users\vchou\.julia\packages\ReinforcementLearningCore\NWrFY\src\policies\agents\trajectories\trajectory.jl:76
 [7] (::var&quot;#24#25&quot;)(::String) at .\none:0
 [8] iterate(::Base.Generator{Array{String,1},var&quot;#24#25&quot;}) at .\generator.jl:47
 [9] top-level scope at In[18]:15
 [10] include_string(::Function, ::Module, ::String, ::String) at .\loading.jl:1091
</code></pre>
",15915226,,3789665,,2021-05-14 07:33:22,2021-05-14 07:41:41,Using neural network approximator in reinforcementlearning.jl,<julia><reinforcement-learning>,1,0,0,,,CC BY-SA 4.0,
2606,11072538,1,11300589,,2012-06-17 15:34:47,,2,316,"<p>I am working on a power management problem where I control the power management of a computing board based on the occurance of events. I am using Reinforcement learning (the traditional Q-learning) for power management where the computing boards works as a Service Provider (SP) for processing requests (images). The SP is connected to a smart camera and the Power Manager (PM) algorithm runs on the camera where it issues appropriate power commands (sleep, wake-up) to the SP. The smart camera captures images (requests) based on the occurance of an event and maintains a Service Queue (SQ) for the requests (images). I also have an ANN based workload estimator that classifies the current workload as <em>low</em> or <em>high</em>. The state space for the Q-learning algorithm is therefore comprises a composite for Q(s,a) where s=(SR, SQ, SP). SR is the state of the workload. SQ is the state of the service queue and SP is the state of the service provider. 
Based on the current workload, state of the queue and the state of the service provider, the PM issues certain commands to the SP (sleep, wake-up). 
The decision is taken at the following stages:</p>

<ol>
<li>SP is idle </li>
<li>SP just entered the sleep state and SQ>=1</li>
<li>SP is in the sleep state and SQ transits from 0 to 1.</li>
</ol>

<p>For each action, a cost is assigned which consists of a weighted sum of average power consumption and average latency per request caused by the action. In both sleep state and idle state, the action comprises selecting some time-out values from a list of pre-defined time-out values.  My problem is as follows:</p>

<p>When the SP enters sleep state and selects a time-out value, some requests may arrive during the time-out value and hence the state of SQ changes. This also changes the composite state (e.g., S(0,0,0) to S(0,N,0). At the end of time-out value, the PM decides to wake-up the SP (as SQ>0). After waking up, the SP processes the requests and when SQ =0 it has a state (0,0,1) or (1,0,1). It then assigns a cost to the previous state. It also updates the Q-matrix accordingly. <strong>My problem is that, shall the cost be assigned to state (0,0,0) or to (0,N,0)?</strong> In principle, the previous state is (0,N,0) but this request is reached automatically at the arrival of some requests in the queue and hence there is not action taken in this state and no action is available to assign cost. </p>
",846400,,846400,,2012-06-17 15:39:11,2012-07-03 11:59:10,Reinforcement learning for power management,<machine-learning><power-management><reinforcement-learning>,1,2,,,,CC BY-SA 3.0,
2607,46585392,1,46619783,,2017-10-05 12:15:13,,0,52,"<p>What is a good way to handle the first round of off policy training with Deep Deterministic Policy Gradients? </p>

<p>Here is my problem: I initialize all weights with <a href=""https://www.tensorflow.org/api_docs/python/tf/contrib/layers/xavier_initializer"" rel=""nofollow noreferrer"">Xavier Initialization</a> and zeros for bias. However when computing the critic loss I'm getting an infinite MSE since the difference between <code>Q_target</code> and <code>Q_eval</code> is so large. Is it a bad idea to just clip this to a very large value?</p>

<pre><code>Q_target_i = r_i + discount * Q_target(i+1)
critic_loss = MSE(Q_target_i, Q_eval_i)
</code></pre>
",3701294,,2464597,,2017-10-05 15:31:25,2017-10-07 11:47:02,Poorly initialized target critic,<tensorflow><deep-learning><reinforcement-learning>,1,0,,,,CC BY-SA 3.0,
2608,17934171,1,17957003,,2013-07-29 21:04:04,,6,2336,"<p>I'm using Sutton &amp; Barto's ebook <em>Reinforcement Learning: An Introduction</em> to study reinforcement learning. I'm having some issues trying to emulate the results (plots) on the <a href=""http://webdocs.cs.ualberta.ca/~sutton/book/ebook/node16.html"" rel=""nofollow noreferrer"">action-value page</a>.</p>

<p>More specifically, how can I simulate the <code>greedy</code> value for each task? The book says:</p>

<blockquote>
  <p>...we can plot the performance and behavior of various methods as
  they improve with experience over 1000 plays...</p>
</blockquote>

<p>So I guess I have to keep track of the <em>exploratory</em> values as better ones are found. The issue is how to do this using the <em>greedy</em> approach - since there are no exploratory moves, how do I know <em>what is a greedy behavior</em>?</p>

<p>Thanks for all the comments and answers!</p>

<p>UPDATE: See code on my answer.</p>
",814601,,121687,,2017-03-08 21:03:00,2019-03-19 09:32:50,n-armed bandit simulation in R,<r><simulation><reinforcement-learning>,4,11,0,,2013-07-30 20:43:34,CC BY-SA 3.0,
2614,46979986,1,46995992,,2017-10-27 16:51:15,,2,467,"<p>I want to learn how deep reinforcement algorithm works and what time it takes to train itself for any given environment.
I came up with a very simple example of environment:</p>

<p>There is a counter which holds an integer between 0 to 100.
counting to 100 is its goal. </p>

<p>there is one parameter <code>direction</code> whose value can be +1 or -1.
it simply show the direction to move.</p>

<p>out neural network takes this direction as input and 2 possible action as output.</p>

<ol>
<li>Change the direction</li>
<li>Do not change the direction</li>
</ol>

<p>1st action will simply flip the direction (+1 => -1 or -1 =>+1). 2nd action will keep the direction as it is.</p>

<p>I am using python for backend and javascript for frontend.
It seems to take too much time, and still it is pretty random. i have used 4 layer perceptron. training rate of 0.001 . memory learning with batch of 100. Code is of Udemy tutorial of Artificial Intelligence and is working properly. </p>

<p>My question is, What should be the reward for completion and for each state.? and  how much time it is required to train simple example as that.?</p>
",6911973,,6911973,,2017-11-07 17:54:40,2017-11-07 17:54:40,deep reinforcement learning parameters and training time for a simple game,<machine-learning><neural-network><artificial-intelligence><reinforcement-learning><pytorch>,1,0,0,,,CC BY-SA 3.0,
2618,11693500,1,11699077,,2012-07-27 18:21:59,,5,1500,"<p><strong>The problem:</strong></p>
<p>I've trained an agent to perform a simple task in a grid world (go to the top of the grid while not hitting obstacles), but the following situation always seems to occur. It finds itself in a easy part of the state space (no obstacles), and so continually gets a strong positive reinforcement signal. Then when it does find itself is difficult part of the state space (wedged next to two obstacles) it simply chooses same action as before, to no effect (It goes up and hits the obstacle). Eventually the Q value for this value matches the negative reward, but by this time the other actions have even lower Q values from being useless in the easy part of the state space, so the error signal drops to zero and the incorrect action is still always chosen.</p>
<p>How can I prevent this from happening? I've thought of a few solutions, but none seem viable:</p>
<ul>
<li><em>Use a policy that is always exploration heavy.</em> As the obstacles take ~5 actions to get around, a single random action every now and then seems ineffective.</li>
<li><em>Make the reward function such that bad actions are worse when they are repeated.</em> This makes the reward function break the Markov property. Maybe this isn't a bad thing, but I simply don't have a clue.</li>
<li><em>Only reward the agent for completing the task.</em> The task takes over a thousand actions to complete, so the training signal would be way too weak.</li>
</ul>
<p><strong>Some background on the task:</strong></p>
<p>So I've made a little testbed for trying out RL algorithms -- something like a more complex version of the grid-world described in the Sutton book. The world is a large binary grid (300 by 1000) populated by 1's in the form of randomly sized rectangles on a backdrop of 0's. A band of 1's surrounds the edges of the world.</p>
<p>An agent occupies a single space in this world and only a fixed windows around it (41 by 41 window with the agent in the center). The agent's actions consist of moving by 1 space in any of the four cardinal directions. The agent can only move through spaces marked by a 0, 1's are impassible.</p>
<p>The current task to be performed in this environment is to make it to the top of the grid world starting from a random position along the bottom. A reward of +1 is given for successfully moving upwards. A reward of -1 is given for any move that would hit an obstacle or the edge of the world. All other states receive a reward of 0.</p>
<p>The agent uses the basic SARSA algorithm with a neural net value function approximator (as discussed in the Sutton book). For policy decisions I've tried both e-greedy and softmax.</p>
",821806,,4685471,,2021-06-26 21:57:15,2021-06-26 21:57:15,How to get out of 'sticky' states?,<machine-learning><neural-network><reinforcement-learning>,1,4,0,2021-06-26 23:40:54,,CC BY-SA 4.0,
2619,11696990,1,11697030,,2012-07-27 23:47:53,,3,1193,"<p>I am building a model where firms have to set prices and make production decisions. Prices are continuous and so are the decision variables. (inventory, last sales, prices...).</p>

<p>What reinforcement learning method can I use that maps continuous to continuous ?
Which python packages are there? If there are no python packages, I could write a wrapper.</p>
",236830,,96780,,2013-08-27 16:53:51,2018-12-18 16:39:59,Reinforcement learning methodes that map continuous to continuous,<python><machine-learning><reinforcement-learning><economics>,1,0,0,,,CC BY-SA 3.0,
2620,64704137,1,64858376,,2020-11-05 17:56:37,,0,271,"<p>Unity provides two RL algorithms to train agents: PPO and SAC.</p>
<p>I have been searching for weeks now on how to write my own algorithms and only found a mention of a gym-unity wrapper that wraps Unity Environments and I could just write my algorithms using Gym. This wrapper has 0 useful documentation so I don't have anywhere to start.</p>
<p>My questions are:
(1) How can I import custom-written RL models into unity?
(2) Is there a better documentation for the wrapper?</p>
",12514502,OmarAI,,,,2020-11-16 12:51:42,Is there a way to import custom Reinforcement Learning Models into Unity?,<machine-learning><reinforcement-learning><deep-learning>,1,0,,,,CC BY-SA 4.0,
2627,48573243,1,48601831,,2018-02-01 23:08:32,,2,986,"<p>I am trying to implement the deep q learning programs DeepMind used to train an AI to play Atari games. One of the features they use and is mentioned in multiple tutorials is to have two versions of your neural network; one to update as you cycle through mini-batch training data (call this Q), and one to call as your doing this to help construct the training data (Q'). Then periodically (say every 10k data points) the weights in Q' get set to the current values of Q. </p>

<p>My question is what is the best way to do this in TensorFlow? Both to store two identical architecture networks at the same time, and to periodically update ones weights from the other? My current net is shown below and is currently just using the default graph and an interactive session. </p>

<pre><code>sess = tf.InteractiveSession()

x = tf.placeholder(tf.float32, shape=[None, height, width, m])
y_ = tf.placeholder(tf.float32, shape=[None, env.action_space.n])

W_conv1 = weight_variable([8, 8, 4, 32])
b_conv1 = bias_variable([32])
h_conv1 = tf.nn.relu(conv2d(x, W_conv1, 4, 4) + b_conv1)

W_conv2 = weight_variable([4, 4, 32, 64])
b_conv2 = bias_variable([64])
h_conv2 = tf.nn.relu(conv2d(h_conv1, W_conv2, 2, 2) + b_conv2)

W_conv3 = weight_variable([3, 3, 64, 64])
b_conv3 = bias_variable([64])
h_conv3 = tf.nn.relu(conv2d(h_conv2, W_conv3, 1, 1) + b_conv3)

# Flattern conv to dense
flat_input_size = 14*10*64
h_conv3_reshape = tf.reshape(h_conv3, [-1, flat_input_size])

# Dense layers
W_fc1 = weight_variable([flat_input_size, 512])
b_fc1 = bias_variable([512])
h_fc1 = tf.nn.relu(tf.matmul(h_conv3_reshape, W_fc1) + b_fc1)

W_fc2 = weight_variable([512, env.action_space.n])
b_fc2 = bias_variable([env.action_space.n])
y_conv = tf.matmul(h_fc1, W_fc2) + b_fc2

accuracy = tf.squared_difference(y_, y_conv)
loss = tf.reduce_mean(accuracy)
optimizer = tf.train.AdamOptimizer(0.0001).minimize(loss)

tf.global_variables_initializer().run()
</code></pre>
",6857955,,712995,,2018-02-03 21:43:14,2018-02-03 21:43:14,Have 2 versions of the same TensorFlow network with different weights and update one from the other,<python><tensorflow><neural-network><reinforcement-learning><q-learning>,1,0,0,,,CC BY-SA 3.0,
2632,32216216,1,32216668,,2015-08-26 00:32:54,,0,93,"<p>my question might be easy, but I am not sure about time indexes in well known Q-learning equation. </p>

<p>The equation: 
Qt+1(St, At) = Qt(St, At) + alpha * (Rt+1 + gamma * max_A(Qt(St+1, A)) - Qt(St, At))</p>

<p>and I don't understand what Rt+1 stands for. Simple example:</p>

<ol>
<li>We are at state X at time T. </li>
<li>pick new action based on epsilon-greedy</li>
<li>apply action </li>
<li>We are at state Y at time T + 1</li>
<li>(now we want update Q values for state Y) reward is calculated from action X -> Y (?) or is it reward from action Y -> Z after evaluating all next Q-values (max_A(Q(Y, A)))</li>
<li>repeat 1</li>
</ol>
",1815451,,,,,2015-08-26 01:59:34,Qlearning and indexing of reward,<artificial-intelligence><reinforcement-learning>,1,0,0,,,CC BY-SA 3.0,
2633,47980012,1,48025703,,2017-12-26 14:35:56,,7,2473,"<p>I have a question regarding appropriate activation functions with environments that have both positive and negative rewards.</p>

<p>In reinforcement learning, our output, I believe, should be the expected reward for all possible actions. Since some options have a negative reward, we would want an output range that includes negative numbers. </p>

<p>This would lead me to believe that the only appropriate activation functions would either be linear or tanh. However, I see any many RL papers the use of Relu. </p>

<p>So two questions:</p>

<ol>
<li><p>If you do want to have both negative and positive outputs, are you limited to just tanh and linear?</p></li>
<li><p>Is it a better strategy (if possible) to scale rewards up so that they are all in the positive domain (i.e. instead of [-1,0,1], [0, 1, 2]) in order for the model to leverage alternative activation functions?</p></li>
</ol>
",2942295,,,,,2021-06-07 10:19:03,RL Activation Functions with Negative Rewards,<machine-learning><reinforcement-learning><q-learning><activation-function>,2,2,,,,CC BY-SA 3.0,
2635,52878679,1,52881846,,2018-10-18 16:33:33,,-1,194,"<p>I'm trying to use a reinforcement learning algorithm to play a simple mini-golf game. </p>

<ul>
<li>I want to give inputs(angle and force) to a game engine.</li>
<li>Get the final position of the ball.</li>
<li>Based on the final position calculate reward.</li>
<li>Iterate process until success.</li>
</ul>

<p>I think I can achieve this by using the greedy approach or function approximation. I want to know whether this is possible and want to find a similar example.</p>
",5375484,,,,,2018-10-21 04:12:46,reinforcement learning mini-golf game,<machine-learning><reinforcement-learning>,1,1,0,,,CC BY-SA 4.0,
2638,32428237,1,32429142,,2015-09-06 20:57:14,,2,675,"<p>Currently I am trying to get Tesauro's TD gammon to working. However I am a bit confused about how the board is encoded for input into the neural network.</p>

<p>I understand that he used 4 units per point on the board for each player (2 * 96 units), each two additional units for checkers on the bar and borne-off checkers (2 * 2 units) as well as two units indicating whose turn it is. That gives a total of 198 inputs. I also fully understand how to encode different numbers of checkers per point.</p>

<p>What I am not really sure of however, is the sequence of inputs. Is it that the 96 first inputs encode white checkers on the board, followed by two inputs for the white bar and borne-off checkers; and are the remaining inputs dedicated to black checkers, black bar, black off and the two units for indicating current player?</p>

<p>Or is rather that 4 successive input units encode one point of the board for one color, the next 4 input units encode the same point but now for the other player?</p>

<p>IÂ´d be very glad if someone had some knowledge to share, since everything I found on the web is pretty ambiguous in terms of which input sequence Tesauro used to encode a particular backgammon situation.</p>

<p>Cheers,
Stephan</p>
",5178014,,,,,2017-06-18 18:13:07,Board encoding in Tesauro's TD-Gammon,<machine-learning><artificial-intelligence><reinforcement-learning>,2,5,,,,CC BY-SA 3.0,
2641,53540424,1,53540809,,2018-11-29 13:43:18,,1,225,"<p>I am currently diving deeper into tensorflow and I am a bit puzzled with the proper use of <code>tf.nn.Conv2d(input, filter, strides, padding)</code>. Although it looks simple at first glance I cannot get my hear around the following issue:</p>

<p>The use of <code>filter, strides, padding</code> is clear to me. However what is not clear is the correct application of the <code>input</code>. </p>

<p>I am coming from a reinforcement learning Atari (Pong) problem in which I want to use the network for batch training AND (with a certain probability) also for predictions in each step. That means, for training I am feeding the network a full batch of let's say 100 , each single unit consisting of 3 frames with size 160, 128. Using the NHWC format of tensorflow my input to <code>input</code> would be a <code>tf.placeholder</code> of shape <code>(100,160,128,3)</code>. So for training I am feeding 100 160x128x3 packages.</p>

<p>However, when predicting outputs from my network (going up or down with the pong paddle) in a certain situation I am only feeding the network <strong>one package</strong> of 160x128x3 (i.e. one package of three frames). Now this is where tensorflow crashes. It expects <code>(100,160,128,3)</code> but receives <code>(1,160,128,3)</code>. </p>

<p>Now I am puzzled. I obviously do not want to set the batch size to 1 and always feed only one package for training. But how can I proceed here? How shall this be implemented with <code>tf.nn.conv2d</code>? </p>

<p>Very much appreciated if someone cann steer me into the right direction here</p>

<p>Thank you in advance for your help!
Kevin</p>
",10722768,,,,,2018-11-29 14:05:24,How to use Tensorflow tf.nn.Conv2d simultaneously for training and prediction?,<python><tensorflow><neural-network><deep-learning><reinforcement-learning>,1,0,,,,CC BY-SA 4.0,
2642,67762898,1,67762972,,2021-05-30 15:32:22,,0,453,"<p>I made a simple replay buffer that when I sample from it gives me the error <code>TypeError: 'type' object is not iterable</code></p>
<pre><code>import collections
import numpy as np

Experience = collections.namedtuple(&quot;Experience&quot;, field_names=[&quot;state&quot;, &quot;action&quot;, &quot;reward&quot;, &quot;done&quot;, &quot;next_state&quot;])

class ReplayBuffer:
    def __init__(self, capacity):
        self.buffer = collections.deque(maxlen=capacity)

    def __len__(self):
        return len(self.buffer)

    def add_exp(self, exp: Experience):
        self.buffer.append(exp)

    def sample(self, batch_size):
        idxs = np.random.choice(len(self.buffer), batch_size, replace=False)
        states, actions, rewards, dones, next_states = zip(*[self.buffer[idx] for idx in idxs])

        return np.array(states), np.array(actions), \
               np.array(rewards, dtype=np.float32), \
               np.array(dones, dtype=np.uint8), \
               np.array(next_states)
</code></pre>
<p>When I print the type of self.buffer[0] it gives <code>'type'</code> but shouldn't it be <code>ReplayBuffer.Experience</code>?</p>
",16077384,,355230,,2021-05-30 15:40:07,2021-05-30 15:52:33,TypeError: 'type' object is not iterable when iterating over collections.deque that contains collections.namedtuple,<python><reinforcement-learning><python-collections>,1,3,,2021-05-30 20:23:07,,CC BY-SA 4.0,
2643,53613722,1,53638579,,2018-12-04 13:08:23,,4,6056,"<p>This question comes from watching the following video on TensorFlow and Reinforcement Learning from Google I/O 18: <a href=""https://www.youtube.com/watch?v=t1A3NTttvBA"" rel=""nofollow noreferrer"">https://www.youtube.com/watch?v=t1A3NTttvBA</a></p>

<p>Here they train a very simple RL algorithm to play the game of Pong.</p>

<p>In the slides they use, the loss is defined like this ( approx @ 11m 25s ):</p>

<pre><code>loss = -R(sampled_actions * log(action_probabilities))
</code></pre>

<p>Further they show the following code ( approx @ 20m 26s):</p>

<pre><code># loss
cross_entropies = tf.losses.softmax_cross_entropy(
    onehot_labels=tf.one_hot(actions, 3), logits=Ylogits)

loss = tf.reduce_sum(rewards * cross_entropies)

# training operation
optimizer = tf.train.RMSPropOptimizer(learning_rate=0.001, decay=0.99)
train_op = optimizer.minimize(loss)
</code></pre>

<p>Now my question is this; They use the +1 for winning and -1 for losing as rewards. In the code that is provided, any cross entropy loss that's multiplied by a negative reward will be very low? And if the training operation is using the optimizer to minimize the loss, well then the algorithm is trained to lose?</p>

<p>Or is there something fundamental I'm missing ( probably because of my very limited mathematical skills ) </p>
",5413637,,,,,2021-09-23 13:09:47,Loss function for simple Reinforcement Learning algorithm,<python><tensorflow><reinforcement-learning>,1,0,0,,,CC BY-SA 4.0,
2647,53622782,1,54250493,,2018-12-04 23:10:50,,1,123,"<p>If a Q-Learning agent actually performs noticeably better against opponents in a specific card game when intermediate rewards are included, would this show a flaw in the algorithm or a flaw in its implementation?</p>
",9595463,,712995,,2019-10-19 07:58:01,2019-10-19 07:58:01,Q-Learning Intermediate Rewards,<reinforcement-learning><q-learning><reward-system>,1,0,,,,CC BY-SA 4.0,
2649,53727531,1,53765632,,2018-12-11 15:39:35,,0,175,"<p>I want to train an AI using Reinforcement Learning in python. The goal is that AI should be able to shoot moving balls come to the game env. randomly at different speeds and from different positions. The AI (player) position is fixed and it can only specify the angle of the bullet. The bullet speed is also fixed. Actually, I don't know what are the States and Actions in this continuous and stochastic environment. And please let me know if there is any tutorial available for this type of game environment. Mostly game RL tuts are about the optimal moving of AI from point A to Point B, which I think is not applicable to my problem. </p>
",8246788,,,,,2018-12-13 15:56:39,What is the code of shooting bullets to dynamic objects in Python?,<python-3.x><reinforcement-learning><q-learning>,1,1,,,,CC BY-SA 4.0,
2653,68227563,1,68244722,,2021-07-02 15:11:14,,0,216,"<p>I am trying to apply q-learning to my custom reinforcement learning environment that is representing energy storage arbitrage (electricity trading with a battery,charge when prices are low and discharge when prices increase). The environment works but I am not able to apply q-learning to it. Below the environment is a script that is able to run the environment but I am unsure what I should make the state variable. Any ideas on how to apply q-learning to optimize the charge/discharge cycles? the reset function starts the next day from a dataset with hourly prices for electricity. picture of the dataframe is below.</p>
<p>class BatteryEnv(gym.Env):</p>
<pre><code>def __init__(self, df):

    self.dict_actions = {0:'discharge',1:'charge',2:'wait'}
    self.df = df
    self.action_space = spaces.Discrete(3)
    self.observation_space = spaces.Box(low=0, high=100, shape=(1,1))
    
    self.reward_list = []
    self.actual_load_list = []#observations
    self.SOE_list=[] #State of energy 
  
    self.state_idx = 0 #iteration (hour of the day)
    self.SOE = 0 #SOE
    self.MAX_charge = 20 #C-rate kinda
    self.Capacity =100
    

def step(self, action): 
    #mapping integer to action for actual load calculation
    str_action = self.dict_actions[action]
    
    #increase state idx within episode (1= 1 hour)
    self.state_idx+=1  
    
    #calculating our actual load
    if str_action == 'charge' and self.SOE &lt; self.Capacity:
        SOE_charge = np.clip(self.Capacity - self.SOE, 0, self.MAX_charge)
        self.SOE += SOE_charge
        obs = SOE_charge * self.df['prices'][self.state_idx]
        
    elif str_action == 'discharge' and self.SOE &gt; 0:
        SOE_discharge = np.clip(self.SOE, 0, self.MAX_charge)
        self.SOE -= SOE_discharge
        obs = -SOE_discharge * self.df['prices'][self.state_idx]

        
    else:
        self.SOE += 0
        obs = 0 * self.df['prices'][self.state_idx]

    
    # appending actual load to list for monitoring and comparison purposes
    self.actual_load_list.append(obs)
    self.SOE_list.append(self.SOE)
    
    #reward system
    if obs&lt;0: #if observation is positive we spending money. if negative we earning
        reward =1
    else:
        reward =-1
    
    # appending curr reward to list for monitoring and comparison purposes
    self.reward_list.append(reward) 

    #checking whether our episode (day interval) ends
    if self.df.iloc[self.state_idx,:].Daynum != self.df.iloc[self.state_idx-1].Daynum: 
        done = True
    else:
        done = False
        
    return obs, reward, done
    
def reset(self): 
    return df.iloc[self.state_idx,:]

def render():
    pass
</code></pre>
<p>The below codes are able to to show that the environment is working.</p>
<pre><code>for episode in range(7):
observation = env.reset()
for t in range(24): #can't be smaller than 24 as 24 time points equal to 1 episode (1 day)
    #print(observation)
    action = env.action_space.sample() #random actions
    observation, reward, done = env.step(action)
    if done:
        print(&quot;Episode finished after {} timesteps&quot;.format(t+1)), print (observation), print(reward)
        break
</code></pre>
<p><a href=""https://i.stack.imgur.com/Eq5xl.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/Eq5xl.png"" alt=""each timestep is one hour, prices are electricty price for that hour, and daynum is the number of  day out of 365"" /></a></p>
",14299963,,,,,2021-07-04 12:27:41,"issues applying q-learning with custom environment (python, reinforcement learning, openai)",<python><reinforcement-learning><openai>,1,0,,,,CC BY-SA 4.0,
2654,53689861,1,53692878,,2018-12-09 06:08:11,,0,175,"<p>I've seen a lot of posts that visualize a reinforcement agent's improvement as the training continues. They usually track the sum of all rewards or a moving average of winning percentage, but I don't think I ever read how they actually did it. Should I export the results of all sessions to a CSV file and then visualize it? Of course, the visualization part wouldn't be difficult, but I have always wondered how they collected data in the first place.</p>
",4370146,,4370146,,2018-12-09 06:15:07,2018-12-09 22:34:18,Visualizing a Reinforcement Learning Agent's Progress,<visualization><reinforcement-learning>,1,0,,,,CC BY-SA 4.0,
2655,53802453,1,53828462,,2018-12-16 13:09:48,,0,456,"<p>This PPO implementation has a bug somewhere and I can't figure out what's wrong. The network returns a normal distribution and a value estimate from the critic. The last layer of the actor provides four <code>F.tanh</code>ed action values, which are used as mean value for the distribution. <code>nn.Parameter(torch.zeros(action_dim))</code> is the standard deviation.</p>

<p>The trajectories for 20 parallel agents are added to the same memory. Episode length is 1000 and <code>memory.sample()</code> returns a <code>np.random.permutation</code> of the 20k memory entries as tensors with batches of size 64. Before stacking the batch tensors, the values are stored as (1,-1) tensors in <code>collection.deque</code>s. The returned tensors are <code>detach()</code>ed.</p>

<p>environment</p>

<pre><code>brain_name = envs.brain_names[0]
env_info = envs.reset(train_mode=True)[brain_name] 
env_info = envs.step(actions.cpu().detach().numpy())[brain_name]
next_states = env_info.vector_observations
rewards = env_info.rewards                 
dones = env_info.local_done  
</code></pre>

<p>update step</p>

<pre><code>def clipped_surrogate_update(policy, memory, num_epochs=10, clip_param=0.2, gradient_clip=5, beta=0.001, value_loss_coeff=0.5):

    advantages_batch, states_batch, log_probs_old_batch, returns_batch, actions_batch = memory.sample()

    advantages_batch = (advantages_batch - advantages_batch.mean()) / advantages_batch.std()

    for _ in range(num_epochs):
        for i in range(len(advantages_batch)):

            advantages_sample = advantages_batch[i]
            states_sample = states_batch[i]
            log_probs_old_sample = log_probs_old_batch[i]
            returns_sample = returns_batch[i]
            actions_sample = actions_batch[i]

            dist, values = policy(states_sample)

            log_probs_new = dist.log_prob(actions_sample.to(device)).sum(-1).unsqueeze(-1)
            entropy = dist.entropy().sum(-1).unsqueeze(-1).mean()

            ratio = (log_probs_new - log_probs_old_sample).exp()

            clipped_ratio = torch.clamp(ratio, 1-clip_param, 1+clip_param)
            clipped_surrogate_loss = -torch.min(ratio*advantages_sample, clipped_ratio*advantages_sample).mean()
            value_function_loss = (returns_sample - values).pow(2).mean()

            Loss = clipped_surrogate_loss - beta * entropy + value_loss_coeff * value_function_loss

            optimizer_policy.zero_grad()
            Loss.backward()
            torch.nn.utils.clip_grad_norm_(policy.parameters(), gradient_clip)
            optimizer_policy.step()
            del Loss
</code></pre>

<p>data sampling</p>

<pre><code>def collect_trajectories(envs, env_info, policy, memory, tmax=200, nrand=0, gae_tau = 0.95, discount = 0.995):

    next_episode = False   

    states = env_info.vector_observations
    n_agents = len(env_info.agents)

    state_list=[]
    reward_list=[]
    prob_list=[]
    action_list=[]  
    value_list=[]

    if nrand &gt; 0:
        # perform nrand random steps
        for _ in range(nrand):
            actions = np.random.randn(num_agents, action_size) 
            actions = np.clip(actions, -1, 1)             
            env_info = envs.step(actions)[brain_name]           
            states = env_info.vector_observations        


    for t in range(tmax):   

        states = torch.FloatTensor(states).to(device)
        dist, values = policy(states)

        actions = dist.sample()
        probs = dist.log_prob(actions).sum(-1).unsqueeze(-1) 

        env_info = envs.step(actions.cpu().detach().numpy())[brain_name] 
        next_states = env_info.vector_observations
        rewards = env_info.rewards                                          
        dones = env_info.local_done           

        state_list.append(states)
        reward_list.append(rewards)
        prob_list.append(probs)
        action_list.append(actions)
        value_list.append(values)

        states = next_states


        if np.any(dones):
            next_episode = True
            break


    _, next_value = policy(torch.FloatTensor(states).to(device))

    reward_arr = np.array(reward_list)
    undiscounted_rewards = np.sum(reward_arr, axis=0)

    state_arr = torch.stack(state_list)
    prob_arr = torch.stack(prob_list)
    action_arr = torch.stack(action_list)
    value_arr = torch.stack(value_list)

    reward_arr = torch.FloatTensor(reward_arr[:, :, np.newaxis])

    advantage_list = []
    return_list = []

    returns = next_value.detach()
    advantages = torch.FloatTensor(np.zeros((n_agents, 1)))
    for i in reversed(range(state_arr.shape[0])):

        returns = reward_arr[i] + discount * returns

        td_error = reward_arr[i] + discount * next_value - value_arr[i]
        advantages = advantages * gae_tau * discount + td_error
        next_value = value_arr[i]           

        advantage_list.append(advantages.detach())
        return_list.append(returns.detach())


    advantage_arr = torch.stack(advantage_list) 
    return_arr = torch.stack(return_list)

    for i in range(state_arr.shape[0]):
        memory.add({'advantages': advantage_arr[i],
                    'states': state_arr[i],
                    'log_probs_old': prob_arr[i],
                    'returns': return_arr[i],
                    'actions': action_arr[i]})

    return undiscounted_rewards, next_episode
</code></pre>
",4040776,,4040776,,2018-12-18 08:34:50,2018-12-18 08:36:21,Pytorch PPO implementation is not learning,<python><pytorch><reinforcement-learning>,1,0,,,,CC BY-SA 4.0,
2657,49193415,1,49197866,,2018-03-09 12:03:24,,3,560,"<p>I am searching for a method to solve a Markov Decision Process (MDP). I know the transition from one state to another is deterministic, but the evironment is non-stationary. This means the reward the agent earns, can be different, when visiting the same state again. Is there an algorithm, like Q-Learning or SARSA, I can use for my problem?</p>
",9462497,,,,,2018-03-09 16:14:26,How to solve a deterministic MDP in a non-stationary environment,<reinforcement-learning><expert-system><markov-decision-process>,1,0,0,,,CC BY-SA 3.0,
2658,53571945,1,53573022,,2018-12-01 14:50:21,,-1,99,"<p>For a project I am working on, I am aiming to predict market trends and make long or short plays as a result. I am looking to use a reinforcement algorithm for this. In a paper I read recently however, the authors suggested using a two tiered system; an SVM classifier to determine market trend and three algorithms based on positive, negative or sideways market trend. Therefore, each algorithm is trained with data of the same trend so there exists less variability. </p>

<p>My question is, would using three algorithms improve the accuracy of the result, or would one model (with the same amount of data in total) provide the same accuracy? </p>

<p>Apologies if this seems a very basic question, I am new to machine learning and am eager to learn. Cheers</p>
",10731939,,10731939,,2018-12-01 15:39:30,2018-12-01 16:52:53,Use of SVM classifier and multiple algorithms to improve accuracy,<python><machine-learning><reinforcement-learning><algorithmic-trading>,1,2,,,,CC BY-SA 4.0,
2661,53830617,1,53910502,,2018-12-18 10:04:19,,0,360,"<p>I'm trying implement DDPG in Tensorflow. The action space is continuous with upper bound <code>P_max</code> and lower bound <code>P_min</code>. Based on <a href=""https://pdfs.semanticscholar.org/49da/57cf2900cd50d64a6d63d45e1bccd454fcbb.pdf"" rel=""nofollow noreferrer"">this paper</a>, the inverting gradients is a good approach for continuous action space. However, I get stucked when update the actor network. I'll go through my code in the following.</p>

<p>First, I build placeholder for state, next_state, and reward. Where <code>S_DIM</code> is the state dimension.</p>

<pre><code>self.S = tf.placeholder(tf.float32, [None, S_DIM], name='state')
self.S_ = tf.placeholder(tf.float32, [None, S_DIM], name='next_state')
self.R = tf.placeholder(tf.float32, [None, 1], name='reward')
</code></pre>

<p>Build neural network for actor and critic, where <code>A_DIM</code> is the action space:</p>

<pre><code>def build_a(self, s, scope, trainable):
    with tf.variable_scope('actor'):
        with tf.variable_scope(scope):
            l1 = tf.layers.dense(s, 100, tf.nn.relu,trainable=trainable)
            a = tf.layers.dense(l1, A_DIM, trainable=trainable)                
            return a

def build_c(self, s, a, scope, trainable):
    with tf.variable_scope('critic'):
        with tf.variable_scope(scope):
            concat_layer = tf.concat([s, a], axis=1)
            l1 = tf.layers.dense(concat_layer, 100, tf.nn.relu, trainable=trainable)
            q = tf.layers.dense(l1, 1, trainable=trainable)
            return q

self.a = self.build_a(self.S, scope='evaluation', trainable=True)
self.a_ = self.build_a(self.S_, scope='target', trainable=False)
self.q = self.build_c(self.S, self.a, scope='evaluation', trainable=True)
self.q_ = self.build_c(self.S_, a_, scope='target', trainable=False)
</code></pre>

<p>Access parameters in neural network for later use:</p>

<pre><code>self.ae_params = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope='actor/evaluation')
self.at_params = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope='actor/target')
self.ce_params = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope='critic/evaluation')
self.ct_params = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope='critic/target')
</code></pre>

<p>Then, update critic with time-difference Bellman equation by minimizing the difference between <code>q_target</code> and <code>q</code>. <code>GAMMA</code> is the discount factor, for example: 0.99</p>

<pre><code>q_target = self.R + GAMMA * q_
self.c_loss = tf.losses.mean_squared_error(q_target, self.q_)
self.ctrain = tf.train.AdamOptimizer(0.001).minimize(self.c_loss, var_list=self.ce_params)
</code></pre>

<p>Finally, update actor (where I get stuck):</p>

<pre><code>dq_da = tf.gradients(q, self.a)[0]  # partial Q, partial a
upper_method = lambda: dq_da * (upper - self.a) / (upper - lower)
lower_method = lambda: dq_da * (self.a - lower) / (upper - lower)
# if gradient suggests increasing action, apply upper method
# else, lower method
adjust_dq_da = tf.cond(tf.greater(dq_da, 0), upper_method, lower_method)
grad = tf.gradients(self.a, self.ae_params, grad_ys=adjust_dq_da)
# apply gradient to the parameters in actor network
self.atrain = tf.train.AdamOptimizer(-0.0001).apply_gradients(zip(grad, self.ae_params))
</code></pre>

<p>And I got error:</p>

<pre><code>ValueError: Shape must be rank 0 but is rank 2 for 'actor_gradient/cond/Switch' (op: 'Switch') with input shapes: [?,1], [?,1].
</code></pre>

<p>Is there any way to improve this?</p>
",9862891,,1782792,,2018-12-18 10:07:43,2022-01-22 01:48:08,How to implement inverting gradient in Tensorflow?,<python><tensorflow><reinforcement-learning>,1,0,,,,CC BY-SA 4.0,
2663,65815598,1,65815968,,2021-01-20 18:37:14,,1,573,"<p>I have an Actor Critic neural network where the Actor is its own class and the Critic is its own class with its own neural network and .forward() function. I then am creating an object of each of these classes in a larger Model class. My setup is as follows:</p>
<pre><code>self.actor = Actor().to(device)
self.actor_opt = optim.Adam(self.actor.parameters(), lr=lr)
self.critic = Critic().to(device)
self.critic_opt = optim.Adam(self.critic.parameters(), lr=lr)
</code></pre>
<p>I then calculate two different loss functions and want to update each neural network separately. For the critic:</p>
<pre><code>loss_critic = F.smooth_l1_loss(value, expected)
self.critic_opt.zero_grad()
loss_critic.backward()
self.critic_opt.step()
</code></pre>
<p>and for the actor:</p>
<pre><code>loss_actor = -self.critic(state, action)
self.actor_opt.zero_grad()
loss_actor.backward()
self.actor_opt.step()
</code></pre>
<p>However, when doing this, I get the following error:</p>
<pre><code>RuntimeError: Trying to backward through the graph a second time, but the saved 
intermediate results have already been freed. Specify retain_graph=True when
calling backward the first time.
</code></pre>
<p>When reading up on this, I understood that I only need to retain_graph=True when calling backward twice on the same network, and in most cases this is not good to set to True as I will run out of GPU. Moreover, when I comment out one of the .backward() functions, the error goes away, leading me to believe that for some reason the code is thinking that both backward() functions are being called on the same neural network, even though I think I am doing it separately. What could be the reason for this? Is there a way to specify for which neural network I am calling the backward function on?</p>
<p>Edit:
For reference, the optimize() function in this code here <a href=""https://github.com/wudongming97/PyTorch-DDPG/blob/master/train.py"" rel=""nofollow noreferrer"">https://github.com/wudongming97/PyTorch-DDPG/blob/master/train.py</a> uses backward() twice with no issue (I've cloned the repo and tested it). I'd like my code to operate similarly where I backprop through critic and actor separately.</p>
",5020491,,5020491,,2021-01-20 20:00:49,2021-01-20 20:00:49,Calling .backward() function for two different neural networks but getting retain_graph=True error,<python><neural-network><pytorch><reinforcement-learning>,1,0,0,,,CC BY-SA 4.0,
2665,66091558,1,66091868,,2021-02-07 18:24:48,,-1,37,"<p>Most tutorials and RL courses focuses on teaching how to apply a model (e.g. Q-Learning) to an environment (gym environments) one can input a state in order to get some output / reward</p>
<p>How it is possible to use RL for historical data, where you cannot get new data? (for example, from a massive auction dataset, how can I derive the best policy using RL)</p>
",14748312,,,,,2021-02-07 18:55:56,(How) can I use reinforcement learning for already seen data?,<machine-learning><reinforcement-learning>,1,0,,,,CC BY-SA 4.0,
2676,49234736,1,49244770,,2018-03-12 12:01:02,,0,349,"<p>I am well known with that a â€œnormalâ€ neural network should use normalized input data so one variable does not have a bigger influence on the weights in the NN than others.</p>

<p>But what if you have a Qnetwork where your training data and test data can differ a lot and can change over time in a continous problem?</p>

<p>My idea was to just run a normal run without normalization of input data and then see the variance and mean from the input datas of the run and then use the variance and mean to normalize my input data of my next run.
But what is the standard to do in this case?</p>

<p>Best regards SÃ¸ren Koch</p>
",5950877,,,,,2018-03-12 21:35:23,Normalization of input data to Qnetwork,<python><scikit-learn><reinforcement-learning><q-learning>,1,0,,,,CC BY-SA 3.0,
2679,66308538,1,66599611,,2021-02-22 00:12:14,,-2,165,"<p>When looking into AI, I only ever see 1 training period and then your model learns and it is perfect.  But what if the data doesn't have a true pattern like financial prices, or a playing a game for example.  Then your algorithm fails to learn and you are left with nothing.</p>
<p>I did some research into openAI and how they taught ai algos to play Dota 2.  One of the programmers said that over the weekend, he taught the algorithm how to block creeps by giving it rewards.  Did they take the existing model, added some rewards when the character was standing in front of creeps, and then let it rip and it would all of a sudden learn a new skill?</p>
<p>There is no information about how this is done!  It's more of a progressive learning system rather than a 1 time train and done.  Please shed some light on this process and how I can train a financial algorithm &quot;features&quot;.</p>
",1779362,,,,,2021-03-12 12:05:38,Training an AI algorithm to learn new features,<tensorflow><artificial-intelligence><tensorflow2.0><reinforcement-learning>,2,1,0,2021-04-03 08:55:25,,CC BY-SA 4.0,
2680,66306571,1,66306797,,2021-02-21 20:04:21,,0,2560,"<p>I was working on q learning in a maze environment, However, at the initial stage, it was working fine but afterward, I was getting the following
max_future_q = np.max(q_table[new_discrete_state])
IndexError: index 20 is out of bounds for axis 1 with size 20</p>
<p>I am not understanding what is the issue here
Below is the code:</p>
<pre><code>enter code here
import gym
import numpy as np
import gym_maze

env = gym.make(&quot;maze-v0&quot;)

LEARNING_RATE = 0.1

DISCOUNT = 0.95
EPISODES = 25000
SHOW_EVERY = 3000

DISCRETE_OS_SIZE = [20, 20]
discrete_os_win_size = (env.observation_space.high - env.observation_space.low)/DISCRETE_OS_SIZE

# Exploration settings
epsilon = 1  # not a constant, qoing to be decayed
START_EPSILON_DECAYING = 1
END_EPSILON_DECAYING = EPISODES//2
epsilon_decay_value = epsilon/(END_EPSILON_DECAYING - START_EPSILON_DECAYING)


q_table = np.random.uniform(low=-2, high=0, size=(DISCRETE_OS_SIZE + [env.action_space.n]))


def get_discrete_state(state):
    discrete_state = (state - env.observation_space.low)/discrete_os_win_size
    return tuple(discrete_state.astype(np.int))  # we use this tuple to look up the 3 Q values for the available actions in the q-table


for episode in range(EPISODES):
    discrete_state = get_discrete_state(env.reset())
    done = False

    if episode % SHOW_EVERY == 0:
        render = True
        print(episode)
    else:
        render = False

    while not done:

        if np.random.random() &gt; epsilon:
            # Get action from Q table
            action = np.argmax(q_table[discrete_state])
        else:
            # Get random action
            action = np.random.randint(0, env.action_space.n)


        new_state, reward, done, _ = env.step(action)

        new_discrete_state = get_discrete_state(new_state)

        if episode % SHOW_EVERY == 0:
            env.render()
        #new_q = (1 - LEARNING_RATE) * current_q + LEARNING_RATE * (reward + DISCOUNT * max_future_q)

        # If simulation did not end yet after last step - update Q table
        if not done:

            # Maximum possible Q value in next step (for new state)
            max_future_q = np.max(q_table[new_discrete_state])

            # Current Q value (for current state and performed action)
            current_q = q_table[discrete_state + (action,)]

            # And here's our equation for a new Q value for current state and action
            new_q = (1 - LEARNING_RATE) * current_q + LEARNING_RATE * (reward + DISCOUNT * max_future_q)

            # Update Q table with new Q value
            q_table[discrete_state + (action,)] = new_q


        # Simulation ended (for any reson) - if goal position is achived - update Q value with reward directly
        elif new_state[0] &gt;= env.goal_position:
            #q_table[discrete_state + (action,)] = reward
            q_table[discrete_state + (action,)] = 0

        discrete_state = new_discrete_state

    # Decaying is being done every episode if episode number is within decaying range
    if END_EPSILON_DECAYING &gt;= episode &gt;= START_EPSILON_DECAYING:
        epsilon -= epsilon_decay_value


env.close()

    
</code></pre>
",13678699,,,,,2021-03-11 13:56:19,"what does ""IndexError: index 20 is out of bounds for axis 1 with size 20""",<python><reinforcement-learning><maze><q-learning>,2,1,,,,CC BY-SA 4.0,
2683,66335165,1,66640939,,2021-02-23 14:34:46,,1,4533,"<p>My python code for a reinforcement task cannot render when I run it in docker. I tried installing  GL when I got the import error but it still gives me the same error. Is there ay other way to fix this without messing with package managers?</p>
<p>Error message:</p>
<pre><code>/usr/local/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: WARN: Box bound precision lowered by casting to float32
warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))
Episode 0
Traceback (most recent call last):
File &quot;/usr/local/lib/python3.7/site-packages/lbforaging/foraging/rendering.py&quot;, line 33, in &lt;module&gt;
  from pyglet.gl import *
File &quot;/usr/local/lib/python3.7/site-packages/pyglet/gl/__init__.py&quot;, line 95, in &lt;module&gt;
  from pyglet.gl.lib import GLException
File &quot;/usr/local/lib/python3.7/site-packages/pyglet/gl/lib.py&quot;, line 149, in &lt;module&gt;
  from pyglet.gl.lib_glx import link_GL, link_GLU, link_GLX
File &quot;/usr/local/lib/python3.7/site-packages/pyglet/gl/lib_glx.py&quot;, line 45, in &lt;module&gt;
  gl_lib = pyglet.lib.load_library('GL')
File &quot;/usr/local/lib/python3.7/site-packages/pyglet/lib.py&quot;, line 164, in load_library
  raise ImportError('Library &quot;%s&quot; not found.' % names[0])
ImportError: Library &quot;GL&quot; not found.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
File &quot;if.py&quot;, line 106, in &lt;module&gt;
  env.render()
File &quot;/usr/local/lib/python3.7/site-packages/lbforaging/foraging/environment.py&quot;, line 501, in render
  self._init_render()
File &quot;/usr/local/lib/python3.7/site-packages/lbforaging/foraging/environment.py&quot;, line 494, in 
 _init_render
from .rendering import Viewer
File &quot;/usr/local/lib/python3.7/site-packages/lbforaging/foraging/rendering.py&quot;, line 41, in &lt;module&gt;
  &quot;&quot;&quot;
ImportError:
Error occured while running `from pyglet.gl import *`
HINT: make sure you have OpenGL install. On Ubuntu, you can run 'apt-get install python-opengl'.
If you're running on a server, you may need a virtual frame buffer; something like this should work:
'xvfb-run -s &quot;-screen 0 1400x900x24&quot; python &lt;your_script.py&gt;'
</code></pre>
<p><code>pip3 install GL</code> says requirement already satisfied and gives the same error &quot;GL not found&quot;
and <code>pip3 install OpenGL</code> gives:</p>
<pre><code>ERROR: Could not find a version that satisfies the requirement OpenGL
ERROR: No matching distribution found for OpenGL
</code></pre>
",14734613,,14734613,,2021-02-23 14:48:59,2021-03-15 15:30:01,"ImportError: Error occured while running `from pyglet.gl import *` while handling exception :Library ""GL"" not found",<python><opengl><render><reinforcement-learning><pyglet>,1,0,,,,CC BY-SA 4.0,
2684,66318564,1,66328885,,2021-02-22 15:26:44,,-1,38,"<p>I have a question about reward in RL.
is this sentence true? and if it is why?
thank you in advance</p>
<p>&quot;the reward each time (for the same action from the same state) needs not to be the same.&quot;</p>
",15260705,,,,,2021-02-23 07:36:53,question about reward in reinforcement learning (RL),<state><action><reinforcement-learning><reward>,1,0,,,,CC BY-SA 4.0,
2687,37190880,1,37208762,,2016-05-12 15:07:07,,0,1933,"<p>I am aware of keras, block n a few others Python libraries for nn which do RL among others. But is there a library than can make the task of visualizations easy? In terms of 3D model of agents/environment,Seeing the simulations etc... I can see a few RL videos online that show the simulated agent/environment but either they have made visual models from the ground up or used some other language/technology...(or they are very old)</p>
",1475752,,,,,2016-05-13 11:20:55,Simulation and visualization libraries for reinforcement learning in python?,<python><machine-learning><visualization><simulation><reinforcement-learning>,2,0,,,,CC BY-SA 3.0,
2688,49319961,1,49320968,,2018-03-16 11:41:49,,1,240,"<p>I'm looking at Andrej Karpathy's ""Training a Neural Network ATARI Pong agent with Policy Gradients from raw pixels"" <a href=""https://gist.github.com/karpathy/a4166c7fe253700972fcbc77e4ea32c5"" rel=""nofollow noreferrer"">https://gist.github.com/karpathy/a4166c7fe253700972fcbc77e4ea32c5</a> . I'm not a Python person so I'm trying to write this code in Matlab. I have 2 questions.</p>

<p>Question 1: I noticed that <code>xs</code>, <code>hs</code>, <code>dlogps</code>, and <code>drs</code> are initialized to <code>[],[],[],[]</code> (line 67) and reset to <code>[],[],[],[]</code> after each episode (line 103). But <code>epx</code>, <code>eph</code>, <code>epdlogp</code>, and <code>epr</code> are neither initialized nor reset. They seem to keep growing forever (lines 99-102). Am I correct? I'm not familiar with the nuances of <code>np.vstack</code>.</p>

<p>Question 2: If I had a game with player movement options up, down, right, and left, how would I need to modify this code to make it work (beside the obvious modification to 4 nodes in the output layer)?</p>

<p>Thanks.</p>
",9502760,,24447,,2018-03-16 12:24:45,2018-03-16 12:36:58,Karpathy's code training neural net to play Pong using Policy Gradients,<python><matlab><machine-learning><neural-network><reinforcement-learning>,1,0,,,,CC BY-SA 3.0,
2691,37409528,1,37451048,,2016-05-24 09:29:51,,0,261,"<p>I am trying to implement an agent which uses Q-learning to play <a href=""https://en.wikipedia.org/wiki/Ludo_(board_game)"" rel=""nofollow"">Ludo</a>. I've trained it with an e-greedy action selector, with an epsilon of 0.1, and a learning rate of 0.6, and discount factor of 0.8. </p>

<p>I ran the game for around 50K steps, and haven't won a single game. This is puzzling as the Q table seem to be pretty accurate with what I want it to be. Why am I losing so much to random players? Shouldn't the system be able to win if the Q-table isn't changing that much, and in general how many iterations would I have to train my agent?</p>

<p>I am not sure how much information is needed, I will update the post with relevant information if needed. </p>

<p>Possible states, represented as rows in the Q-table:</p>

<ol>
<li>In home</li>
<li>On globe</li>
<li>On a star</li>
<li>In goal</li>
<li>On the Winner Road</li>
<li>In safety with same colored player</li>
<li>On free space</li>
</ol>

<p>Possible actions, represented as columns for each state:</p>

<ol>
<li>Move out from home</li>
<li>Get in goal</li>
<li>Move to globe</li>
<li>Move to star</li>
<li>Move to goal via star</li>
<li>Get into safety with an same colored token</li>
<li>Get into the Winner Road</li>
<li>Suicide if the opponent is on a globe</li>
<li>Kill opponent</li>
<li>Just move</li>
<li>No move possible</li>
</ol>

<p>I start by initializing my Q-table with random values, and end with a table that looks like this after 5000 iteration: </p>

<pre><code>-21.9241  345.35 169.189 462.934 308.445 842.939 256.074  712.23 283.328 137.078   -32.8
398.895   968.8 574.977 488.216 468.481 948.541  904.77 159.578 237.928 29.7712 417.599
1314.25 756.426 333.321  589.25 616.682 583.632  481.84 457.585  683.22 329.132 227.329
1127.58 1457.92 1365.58 1429.26 1482.69 1574.66 1434.77 1195.64 1231.01 1232.07    1068
807.592 1070.17  544.13 1385.63 883.123 1662.97  524.08 966.205 1649.67 509.825 909.006
225.453 1141.34 536.544 242.647 1522.26 1484.47 297.704 993.186 589.984  689.73 1340.89
1295.03 310.461 361.776 399.866 663.152 334.657 497.956  229.94 294.462 311.505 1428.26
</code></pre>

<p>My immediate reward is based on how far each token is in the game multiplied with constant 10, after an action has been performed. Home position has position -1 and goal position has position 99. and all position in-between has positions between 0 - 55.  If a Token is in goal, will a extra reward +100 be added to the immediate reward for each token in goal. </p>

<p>Usually, my player moves always one token to the goal... and thats it. </p>
",5585502,,1497463,,2016-05-26 04:50:42,2016-05-26 04:50:42,AI Player is not performing well? why?,<c++><artificial-intelligence><reinforcement-learning><q-learning>,1,1,0,,,CC BY-SA 3.0,
2694,37524472,1,37561698,,2016-05-30 11:24:35,,4,2815,"<p>I've recently made an attempt to implement a basic Q-Learning algorithm in Golang. Note that I'm new to Reinforcement Learning and AI in general, so the error may very well be mine.</p>

<p>Here's how I implemented the solution to an m,n,k-game environment:
At each given time <code>t</code>, the agent holds the last state-action <code>(s, a)</code> and the acquired reward for it; the agent selects a move <code>a'</code> based on an Epsilon-greedy policy and calculates the reward <code>r</code>, then proceeds to update the value of <code>Q(s, a)</code> for time <code>t-1</code></p>

<pre><code>func (agent *RLAgent) learn(reward float64) {
    var mState = marshallState(agent.prevState, agent.id)
    var oldVal = agent.values[mState]

    agent.values[mState] = oldVal + (agent.LearningRate *
        (agent.prevScore + (agent.DiscountFactor * reward) - oldVal))
}
</code></pre>

<p>Note:</p>

<ul>
<li><code>agent.prevState</code> holds previous state right after taking the action and before the environment responds (i.e. after the agent makes it's move and before the other player makes a move) I use that in place of the state-action tuple, but I'm not quite sure if that's the right approach</li>
<li><code>agent.prevScore</code> holds the reward to previous state-action</li>
<li>The <code>reward</code> argument represents the reward for current step's state-action (<code>Qmax</code>)</li>
</ul>

<p>With <code>agent.LearningRate = 0.2</code> and <code>agent.DiscountFactor = 0.8</code> the agent fails to reach 100K episodes because of state-action value overflow.
I'm using golang's <code>float64</code> (Standard IEEE 754-1985 double precision floating point variable) which overflows at around <code>Â±1.80Ã—10^308</code> and yields <code>Â±Infiniti</code>. That's too big a value I'd say!</p>

<p>Here's the state of a model trained with a learning rate of <code>0.02</code> and a discount factor of <code>0.08</code> which got through 2M episodes (1M games with itself):</p>

<pre><code>Reinforcement learning model report
Iterations: 2000000
Learned states: 4973
Maximum value: 88781786878142287058992045692178302709335321375413536179603017129368394119653322992958428880260210391115335655910912645569618040471973513955473468092393367618971462560382976.000000
Minimum value: 0.000000
</code></pre>

<p>The reward function returns:</p>

<ul>
<li>Agent won: 1</li>
<li>Agent lost: -1</li>
<li>Draw: 0</li>
<li>Game continues: 0.5</li>
</ul>

<p>But you can see that the minimum value is zero, and the maximum value is too high.</p>

<p>It may be worth mentioning that with a simpler learning method I found in a python script works perfectly fine and feels actually more intelligent! When I play with it, most of the time the result is a draw (it even wins if I play carelessly), whereas with the standard Q-Learning method, I can't even let it win!</p>

<pre><code>agent.values[mState] = oldVal + (agent.LearningRate * (reward - agent.prevScore))
</code></pre>

<p>Any ideas on how to fix this?
Is that kind of state-action value normal in Q-Learning?!</p>

<hr>

<p><strong>Update:</strong>
After reading Pablo's answer and the slight but important edit that Nick provided to this question, I realized the problem was <code>prevScore</code> containing the Q-value of previous step (equal to <code>oldVal</code>) instead of the reward of the previous step (in this example, -1, 0, 0.5 or 1).</p>

<p>After that change, the agent now behaves normally and after 2M episodes, the state of the model is as follows:</p>

<pre><code>Reinforcement learning model report
Iterations: 2000000
Learned states: 5477
Maximum value: 1.090465
Minimum value: -0.554718
</code></pre>

<p>and out of 5 games with the agent, there were 2 wins for me (the agent did not recognize that I had two stones in a row) and 3 draws.</p>
",717221,,717221,,2016-06-01 19:50:33,2016-06-01 19:50:33,Q-Learning values get too high,<go><floating-point><reinforcement-learning><q-learning>,2,0,0,,,CC BY-SA 3.0,
2698,37401417,1,37402775,,2016-05-23 22:14:11,,3,3149,"<p>I know this might be a pretty stupid question to ask, but what the hell.. </p>

<p>I at the moment trying to implement soft max action selector, which uses the boltzmann distribution. </p>

<p><a href=""http://i.stack.imgur.com/pmZR0.png"" rel=""nofollow"">Formula</a></p>

<p>What I am bit unsure about, is how how do known if you want to use a specific  action? 
I mean the function provides me with a probability?, but how do I use that to select which action I want to perform?</p>
",6322262,,,,,2016-05-27 00:51:49,Action selection with softmax?,<c++><reinforcement-learning><q-learning><softmax>,1,5,0,,,CC BY-SA 3.0,
2707,70984103,1,71008828,,2022-02-04 09:31:44,,0,163,"<p>My question is I wrote the Q-learning algorithm in c++ with epsilon greedy policy now I have to plot the learning curve for the Q-values. What exactly I should have to plot because I have an 11x5 Q matrix, so should I take one Q value and plot its learning or should I have to take the whole matrix for a learning curve, could you guide me with it.
Thank you</p>
",17356236,,,,,2022-08-01 01:26:40,Learning Curve in Q-learning,<c++><reinforcement-learning><q-learning>,1,0,,,,CC BY-SA 4.0,
2710,71168412,1,71187773,,2022-02-18 04:33:09,,2,257,"<p>I am trying to create a batched environment version of an SAC agent example from the Tensorflow Agents library, the original code can be found <a href=""https://github.com/tensorflow/agents/blob/master/tf_agents/agents/sac/examples/v2/train_eval.py"" rel=""nofollow noreferrer"">here</a>. I am also using a custom environment.</p>
<p>I am pursuing a batched environment setup in order to better leverage GPU resources in order to speed up training. My understanding is that by passing batches of trajectories to the GPU, there will be less overhead incurred when passing data from the host (CPU) to the device (GPU).</p>
<p>My custom environment is called <code>SacEnv</code>, and I attempt to create a batched environment like so:</p>
<pre class=""lang-py prettyprint-override""><code>py_envs = [SacEnv() for _ in range(0, batch_size)]
batched_env = batched_py_environment.BatchedPyEnvironment(envs=py_envs)
tf_env = tf_py_environment.TFPyEnvironment(batched_env)
</code></pre>
<p>My hope is that this will create a batched environment consisting of a 'batch' of non-batched environments. However I am receiving the following error when running the code:</p>
<pre><code>ValueError: Cannot assign value to variable ' Accumulator:0': Shape mismatch.The variable shape (1,), and the assigned value shape (32,) are incompatible.
</code></pre>
<p>with the stack trace:</p>
<pre><code>Traceback (most recent call last):
  File &quot;/home/gary/Desktop/code/sac_test/sac_main2.py&quot;, line 370, in &lt;module&gt;
    app.run(main)
  File &quot;/home/gary/anaconda3/envs/py39/lib/python3.9/site-packages/absl/app.py&quot;, line 312, in run
    _run_main(main, args)
  File &quot;/home/gary/anaconda3/envs/py39/lib/python3.9/site-packages/absl/app.py&quot;, line 258, in _run_main
    sys.exit(main(argv))
  File &quot;/home/gary/Desktop/code/sac_test/sac_main2.py&quot;, line 366, in main
    train_eval(FLAGS.root_dir)
  File &quot;/home/gary/anaconda3/envs/py39/lib/python3.9/site-packages/gin/config.py&quot;, line 1605, in gin_wrapper
    utils.augment_exception_message_and_reraise(e, err_str)
  File &quot;/home/gary/anaconda3/envs/py39/lib/python3.9/site-packages/gin/utils.py&quot;, line 41, in augment_exception_message_and_reraise
    raise proxy.with_traceback(exception.__traceback__) from None
  File &quot;/home/gary/anaconda3/envs/py39/lib/python3.9/site-packages/gin/config.py&quot;, line 1582, in gin_wrapper
    return fn(*new_args, **new_kwargs)
  File &quot;/home/gary/Desktop/code/sac_test/sac_main2.py&quot;, line 274, in train_eval
    results = metric_utils.eager_compute(
  File &quot;/home/gary/anaconda3/envs/py39/lib/python3.9/site-packages/gin/config.py&quot;, line 1605, in gin_wrapper
    utils.augment_exception_message_and_reraise(e, err_str)
  File &quot;/home/gary/anaconda3/envs/py39/lib/python3.9/site-packages/gin/utils.py&quot;, line 41, in augment_exception_message_and_reraise
    raise proxy.with_traceback(exception.__traceback__) from None
  File &quot;/home/gary/anaconda3/envs/py39/lib/python3.9/site-packages/gin/config.py&quot;, line 1582, in gin_wrapper
    return fn(*new_args, **new_kwargs)
  File &quot;/home/gary/anaconda3/envs/py39/lib/python3.9/site-packages/tf_agents/eval/metric_utils.py&quot;, line 163, in eager_compute
    common.function(driver.run)(time_step, policy_state)
  File &quot;/home/gary/anaconda3/envs/py39/lib/python3.9/site-packages/tensorflow/python/util/traceback_utils.py&quot;, line 153, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File &quot;/home/gary/anaconda3/envs/py39/lib/python3.9/site-packages/tf_agents/drivers/dynamic_episode_driver.py&quot;, line 211, in run
    return self._run_fn(
  File &quot;/home/gary/anaconda3/envs/py39/lib/python3.9/site-packages/tf_agents/utils/common.py&quot;, line 188, in with_check_resource_vars
    return fn(*fn_args, **fn_kwargs)
  File &quot;/home/gary/anaconda3/envs/py39/lib/python3.9/site-packages/tf_agents/drivers/dynamic_episode_driver.py&quot;, line 238, in _run
    tf.while_loop(
  File &quot;/home/gary/anaconda3/envs/py39/lib/python3.9/site-packages/tf_agents/drivers/dynamic_episode_driver.py&quot;, line 154, in loop_body
    observer_ops = [observer(traj) for observer in self._observers]
  File &quot;/home/gary/anaconda3/envs/py39/lib/python3.9/site-packages/tf_agents/drivers/dynamic_episode_driver.py&quot;, line 154, in &lt;listcomp&gt;
    observer_ops = [observer(traj) for observer in self._observers]
  File &quot;/home/gary/anaconda3/envs/py39/lib/python3.9/site-packages/tf_agents/metrics/tf_metric.py&quot;, line 93, in __call__
    return self._update_state(*args, **kwargs)
  File &quot;/home/gary/anaconda3/envs/py39/lib/python3.9/site-packages/tf_agents/metrics/tf_metric.py&quot;, line 81, in _update_state
    return self.call(*arg, **kwargs)
ValueError: in user code:

    File &quot;/home/gary/anaconda3/envs/py39/lib/python3.9/site-packages/tf_agents/metrics/tf_metrics.py&quot;, line 176, in call  *
        self._return_accumulator.assign(

    ValueError: Cannot assign value to variable ' Accumulator:0': Shape mismatch.The variable shape (1,), and the assigned value shape (32,) are incompatible.

  In call to configurable 'eager_compute' (&lt;function eager_compute at 0x7fa4d6e5e040&gt;)
  In call to configurable 'train_eval' (&lt;function train_eval at 0x7fa4c8622dc0&gt;)
</code></pre>
<p>I have dug through the <code>tf_metric.py</code> code to try and understand the error, however I have been unsuccessful. A related issue was solved when I added the batch size (32) to the initializer for the <code>AverageReturnMetric</code> instance, and this issue seems related.</p>
<p>The full code is:</p>
<pre class=""lang-py prettyprint-override""><code># coding=utf-8
# Copyright 2020 The TF-Agents Authors.
#
# Licensed under the Apache License, Version 2.0 (the &quot;License&quot;);
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     https://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an &quot;AS IS&quot; BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

# Lint as: python2, python3

r&quot;&quot;&quot;Train and Eval SAC.

All hyperparameters come from the SAC paper
https://arxiv.org/pdf/1812.05905.pdf

To run:

```bash
tensorboard --logdir $HOME/tmp/sac/gym/HalfCheetah-v2/ --port 2223 &amp;

python tf_agents/agents/sac/examples/v2/train_eval.py \
  --root_dir=$HOME/tmp/sac/gym/HalfCheetah-v2/ \
  --alsologtostderr
\```
&quot;&quot;&quot;

from __future__ import absolute_import
from __future__ import division
from __future__ import print_function

from sac_env import SacEnv

import os
import time

from absl import app
from absl import flags
from absl import logging

import gin
from six.moves import range
import tensorflow as tf  # pylint: disable=g-explicit-tensorflow-version-import

from tf_agents.agents.ddpg import critic_network
from tf_agents.agents.sac import sac_agent
from tf_agents.agents.sac import tanh_normal_projection_network
from tf_agents.drivers import dynamic_step_driver
#from tf_agents.environments import suite_mujoco
from tf_agents.environments import tf_py_environment
from tf_agents.environments import batched_py_environment
from tf_agents.eval import metric_utils
from tf_agents.metrics import tf_metrics
from tf_agents.networks import actor_distribution_network
from tf_agents.policies import greedy_policy
from tf_agents.policies import random_tf_policy
from tf_agents.replay_buffers import tf_uniform_replay_buffer
from tf_agents.utils import common
from tf_agents.train.utils import strategy_utils


flags.DEFINE_string('root_dir', os.getenv('TEST_UNDECLARED_OUTPUTS_DIR'),
                    'Root directory for writing logs/summaries/checkpoints.')
flags.DEFINE_multi_string('gin_file', None, 'Path to the trainer config files.')
flags.DEFINE_multi_string('gin_param', None, 'Gin binding to pass through.')

FLAGS = flags.FLAGS

gpus = tf.config.list_physical_devices('GPU')
if gpus:
    try:
        for gpu in gpus:
            tf.config.experimental.set_memory_growth(gpu, True)
        logical_gpus = tf.config.experimental.list_logical_devices('GPU')
        print(len(gpus), &quot;Physical GPUs,&quot;, len(logical_gpus), &quot;Logical GPUs&quot;)
    except RuntimeError as e:
        print(e)

@gin.configurable
def train_eval(
    root_dir,
    env_name='SacEnv',
    # The SAC paper reported:
    # Hopper and Cartpole results up to 1000000 iters,
    # Humanoid results up to 10000000 iters,
    # Other mujoco tasks up to 3000000 iters.
    num_iterations=3000000,
    actor_fc_layers=(256, 256),
    critic_obs_fc_layers=None,
    critic_action_fc_layers=None,
    critic_joint_fc_layers=(256, 256),
    # Params for collect
    # Follow https://github.com/haarnoja/sac/blob/master/examples/variants.py
    # HalfCheetah and Ant take 10000 initial collection steps.
    # Other mujoco tasks take 1000.
    # Different choices roughly keep the initial episodes about the same.
    #initial_collect_steps=10000,
    initial_collect_steps=2000,
    collect_steps_per_iteration=1,
    replay_buffer_capacity=31250, # 1000000 / 32
    # Params for target update
    target_update_tau=0.005,
    target_update_period=1,
    # Params for train
    train_steps_per_iteration=1,
    #batch_size=256,
    batch_size=32,
    actor_learning_rate=3e-4,
    critic_learning_rate=3e-4,
    alpha_learning_rate=3e-4,
    td_errors_loss_fn=tf.math.squared_difference,
    gamma=0.99,
    reward_scale_factor=0.1,
    gradient_clipping=None,
    use_tf_functions=True,
    # Params for eval
    num_eval_episodes=30,
    eval_interval=10000,
    # Params for summaries and logging
    train_checkpoint_interval=50000,
    policy_checkpoint_interval=50000,
    rb_checkpoint_interval=50000,
    log_interval=1000,
    summary_interval=1000,
    summaries_flush_secs=10,
    debug_summaries=False,
    summarize_grads_and_vars=False,
    eval_metrics_callback=None):
  &quot;&quot;&quot;A simple train and eval for SAC.&quot;&quot;&quot;
  root_dir = os.path.expanduser(root_dir)
  train_dir = os.path.join(root_dir, 'train')
  eval_dir = os.path.join(root_dir, 'eval')

  train_summary_writer = tf.compat.v2.summary.create_file_writer(
      train_dir, flush_millis=summaries_flush_secs * 1000)
  train_summary_writer.set_as_default()

  eval_summary_writer = tf.compat.v2.summary.create_file_writer(
      eval_dir, flush_millis=summaries_flush_secs * 1000)
  eval_metrics = [
      tf_metrics.AverageReturnMetric(buffer_size=num_eval_episodes),
      tf_metrics.AverageEpisodeLengthMetric(buffer_size=num_eval_episodes)
  ]

  global_step = tf.compat.v1.train.get_or_create_global_step()
  with tf.compat.v2.summary.record_if(
      lambda: tf.math.equal(global_step % summary_interval, 0)):


    py_envs = [SacEnv() for _ in range(0, batch_size)]
    batched_env = batched_py_environment.BatchedPyEnvironment(envs=py_envs)
    tf_env = tf_py_environment.TFPyEnvironment(batched_env)
    
    eval_py_envs = [SacEnv() for _ in range(0, batch_size)]
    eval_batched_env = batched_py_environment.BatchedPyEnvironment(envs=eval_py_envs)
    eval_tf_env = tf_py_environment.TFPyEnvironment(eval_batched_env)

    time_step_spec = tf_env.time_step_spec()
    observation_spec = time_step_spec.observation
    action_spec = tf_env.action_spec()

    strategy = strategy_utils.get_strategy(tpu=False, use_gpu=True)

    with strategy.scope():
        actor_net = actor_distribution_network.ActorDistributionNetwork(
            observation_spec,
            action_spec,
            fc_layer_params=actor_fc_layers,
            continuous_projection_net=tanh_normal_projection_network
            .TanhNormalProjectionNetwork)
        critic_net = critic_network.CriticNetwork(
            (observation_spec, action_spec),
            observation_fc_layer_params=critic_obs_fc_layers,
            action_fc_layer_params=critic_action_fc_layers,
            joint_fc_layer_params=critic_joint_fc_layers,
            kernel_initializer='glorot_uniform',
            last_kernel_initializer='glorot_uniform')

        tf_agent = sac_agent.SacAgent(
            time_step_spec,
            action_spec,
            actor_network=actor_net,
            critic_network=critic_net,
            actor_optimizer=tf.compat.v1.train.AdamOptimizer(
                learning_rate=actor_learning_rate),
            critic_optimizer=tf.compat.v1.train.AdamOptimizer(
                learning_rate=critic_learning_rate),
            alpha_optimizer=tf.compat.v1.train.AdamOptimizer(
                learning_rate=alpha_learning_rate),
            target_update_tau=target_update_tau,
            target_update_period=target_update_period,
            td_errors_loss_fn=td_errors_loss_fn,
            gamma=gamma,
            reward_scale_factor=reward_scale_factor,
            gradient_clipping=gradient_clipping,
            debug_summaries=debug_summaries,
            summarize_grads_and_vars=summarize_grads_and_vars,
            train_step_counter=global_step)
    tf_agent.initialize()

    # Make the replay buffer.
    replay_buffer = tf_uniform_replay_buffer.TFUniformReplayBuffer(
        data_spec=tf_agent.collect_data_spec,
        batch_size=batch_size,
        max_length=replay_buffer_capacity,
        device=&quot;/device:GPU:0&quot;)
    replay_observer = [replay_buffer.add_batch]

    train_metrics = [
        tf_metrics.NumberOfEpisodes(),
        tf_metrics.EnvironmentSteps(),
        tf_metrics.AverageReturnMetric(
            buffer_size=num_eval_episodes, batch_size=tf_env.batch_size),
        tf_metrics.AverageEpisodeLengthMetric(
            buffer_size=num_eval_episodes, batch_size=tf_env.batch_size),
    ]

    eval_policy = greedy_policy.GreedyPolicy(tf_agent.policy)
    initial_collect_policy = random_tf_policy.RandomTFPolicy(
        tf_env.time_step_spec(), tf_env.action_spec())
    collect_policy = tf_agent.collect_policy

    train_checkpointer = common.Checkpointer(
        ckpt_dir=train_dir,
        agent=tf_agent,
        global_step=global_step,
        metrics=metric_utils.MetricsGroup(train_metrics, 'train_metrics'))
    policy_checkpointer = common.Checkpointer(
        ckpt_dir=os.path.join(train_dir, 'policy'),
        policy=eval_policy,
        global_step=global_step)
    rb_checkpointer = common.Checkpointer(
        ckpt_dir=os.path.join(train_dir, 'replay_buffer'),
        max_to_keep=1,
        replay_buffer=replay_buffer)

    train_checkpointer.initialize_or_restore()
    rb_checkpointer.initialize_or_restore()

    initial_collect_driver = dynamic_step_driver.DynamicStepDriver(
        tf_env,
        initial_collect_policy,
        observers=replay_observer + train_metrics,
        num_steps=initial_collect_steps)

    collect_driver = dynamic_step_driver.DynamicStepDriver(
        tf_env,
        collect_policy,
        observers=replay_observer + train_metrics,
        num_steps=collect_steps_per_iteration)

    if use_tf_functions:
      initial_collect_driver.run = common.function(initial_collect_driver.run)
      collect_driver.run = common.function(collect_driver.run)
      tf_agent.train = common.function(tf_agent.train)

    if replay_buffer.num_frames() == 0:
      # Collect initial replay data.
      logging.info(
          'Initializing replay buffer by collecting experience for %d steps '
          'with a random policy.', initial_collect_steps)
      initial_collect_driver.run()

    results = metric_utils.eager_compute(
        eval_metrics,
        eval_tf_env,
        eval_policy,
        num_episodes=num_eval_episodes,
        train_step=global_step,
        summary_writer=eval_summary_writer,
        summary_prefix='Metrics',
    )
    if eval_metrics_callback is not None:
      eval_metrics_callback(results, global_step.numpy())
    metric_utils.log_metrics(eval_metrics)

    time_step = None
    policy_state = collect_policy.get_initial_state(tf_env.batch_size)

    timed_at_step = global_step.numpy()
    time_acc = 0

    # Prepare replay buffer as dataset with invalid transitions filtered.
    def _filter_invalid_transition(trajectories, unused_arg1):
      return ~trajectories.is_boundary()[0]
    dataset = replay_buffer.as_dataset(
        sample_batch_size=batch_size,
        num_steps=2).unbatch().filter(
            _filter_invalid_transition).batch(batch_size).prefetch(5)
    # Dataset generates trajectories with shape [Bx2x...]
    iterator = iter(dataset)

    def train_step():
      experience, _ = next(iterator)
      return tf_agent.train(experience)

    if use_tf_functions:
      train_step = common.function(train_step)

    global_step_val = global_step.numpy()
    while global_step_val &lt; num_iterations:
      start_time = time.time()
      time_step, policy_state = collect_driver.run(
          time_step=time_step,
          policy_state=policy_state,
      )
      for _ in range(train_steps_per_iteration):
        train_loss = train_step()
      time_acc += time.time() - start_time

      global_step_val = global_step.numpy()

      if global_step_val % log_interval == 0:
        logging.info('step = %d, loss = %f', global_step_val,
                     train_loss.loss)
        steps_per_sec = (global_step_val - timed_at_step) / time_acc
        logging.info('%.3f steps/sec', steps_per_sec)
        tf.compat.v2.summary.scalar(
            name='global_steps_per_sec', data=steps_per_sec, step=global_step)
        timed_at_step = global_step_val
        time_acc = 0

      for train_metric in train_metrics:
        train_metric.tf_summaries(
            train_step=global_step, step_metrics=train_metrics[:2])

      if global_step_val % eval_interval == 0:
        results = metric_utils.eager_compute(
            eval_metrics,
            eval_tf_env,
            eval_policy,
            num_episodes=num_eval_episodes,
            train_step=global_step,
            summary_writer=eval_summary_writer,
            summary_prefix='Metrics',
        )
        if eval_metrics_callback is not None:
          eval_metrics_callback(results, global_step_val)
        metric_utils.log_metrics(eval_metrics)

      if global_step_val % train_checkpoint_interval == 0:
        train_checkpointer.save(global_step=global_step_val)

      if global_step_val % policy_checkpoint_interval == 0:
        policy_checkpointer.save(global_step=global_step_val)

      if global_step_val % rb_checkpoint_interval == 0:
        rb_checkpointer.save(global_step=global_step_val)
    return train_loss


def main(_):
  tf.compat.v1.enable_v2_behavior()
  logging.set_verbosity(logging.INFO)
  gin.parse_config_files_and_bindings(FLAGS.gin_file, FLAGS.gin_param)
  train_eval(FLAGS.root_dir)

if __name__ == '__main__':
  flags.mark_flag_as_required('root_dir')
  app.run(main)
</code></pre>
<p>What is the appropriate way to create a batched environment for a custom, non-batched environment? I can share my custom environment, but I don't believe the issue lies there as the code works fine when using batch sizes of 1.</p>
<p>Also, any tips on increasing GPU utilization in reinforcement learning scenarios would be greatly appreciated. I have examined examples of using tensorboard-profiler to profile GPU utilization, but it seems these require callbacks and a <code>fit</code> function, which doesn't seem to be applicable in RL use-cases.</p>
",2824935,,,,,2022-02-19 18:11:13,Using BatchedPyEnvironment in tf_agents,<tensorflow><gpu><reinforcement-learning>,1,0,,,,CC BY-SA 4.0,
2712,71040388,1,71041627,,2022-02-08 20:21:16,,0,278,"<p>Let's say we have the following neural network in PyTorch</p>
<pre><code>seq_model = nn.Sequential(
nn.Linear(1, 13),
nn.Tanh(),
nn.Linear(13, 1))
</code></pre>
<p>With the following input tensor</p>
<pre><code>input = torch.tensor([1.0, 1.0, 5.0], dtype=torch.float32).unsqueeze(1)
</code></pre>
<p>I can run forward through the net and get</p>
<pre><code>seq_model(input)

tensor([[-0.0165],
        [-0.0165],
        [-0.2289]], grad_fn=&lt;TanhBackward0&gt;)
</code></pre>
<p>Probably I also can get a single scalar value as an output, but I'm not sure how.
Thank you. I'm trying to use such an network for reinforcment learning, and use it
as an value function approximator for game board state evaluation.</p>
",4696815,,,,,2022-02-27 19:34:47,PyTorch - Neural Network - Output single scalar value,<deep-learning><pytorch><reinforcement-learning>,1,0,,,,CC BY-SA 4.0,
2720,71255967,1,71257059,,2022-02-24 17:30:56,,0,107,"<p>I have a quick question which I can't find asked anywhere (yet). Is there a quick and efficient way to perform an operation like this:</p>
<p><img src=""https://latex.codecogs.com/svg.image?%5Csum_%7Bi=0%7D%5E%7Bn-1%7D(%5Cgamma)%5Ei"" alt=""Sum with exponent"" /></p>
<p>Where gamma is a constant. So to put it simply: take the sum of all gamma's raised to the power of i. I have fooled around a bit with <code>np.sum</code> and <code>np.exp</code> but this seems to be my best guess:</p>
<pre><code>n = 4
gamma = 2
np.sum([gamma**i for i in range(n)])
</code></pre>
<p>Let me know what you've come up with :)</p>
",6898146,,,,,2022-03-01 20:17:09,Is there an efficient np.sum with exponent operator possible in Python?,<python><numpy><reinforcement-learning>,2,3,,,,CC BY-SA 4.0,
2731,71383828,1,71387643,,2022-03-07 15:57:07,,0,62,"<p>Here I am trying to update a specific row of my numpy matrix but it is acting a bit weird</p>
<pre><code>tpm1=np.array([[1,0, 0, 0, 0],[1, 0, 0, 0, 0],[1,0, 0, 0, 0],[1,0, 0, 0, 0],[1,0, 0, 0, 0]])
tpm1[1,:]=0.15*tpm1[1,:]
[[1 0 0 0 0]
 [1 0 0 0 0]
 [1 0 0 0 0]
 [1 0 0 0 0]
 [1 0 0 0 0]]

</code></pre>
<p>Output is given as</p>
<pre><code>[[1 0 0 0 0]
 [0 0 0 0 0]
 [1 0 0 0 0]
 [1 0 0 0 0]
 [1 0 0 0 0]]
</code></pre>
<p>But I expect [0.15,0,0,0,0] in the second row. I am not sure what's happening here</p>
",16308628,,,,,2022-03-07 21:47:19,How Can I update a particular row in numpy matrix?,<python><arrays><numpy><multidimensional-array><reinforcement-learning>,1,0,,,,CC BY-SA 4.0,
2734,71459191,1,71459334,,2022-03-13 17:32:10,,1,120,"<p>What is the connection between discount factor gamma and horizon in RL.</p>
<p>What I have learned so far is that the horizon is the agent`s time to live. Intuitively, agents with finite horizon will choose actions differently than if it has to live forever. In the latter case, the agent will try to maximize all the expected rewards it may get far in the future.</p>
<p>But the idea of the discount factor is also the same. Are the values of gamma near zero makes the horizon finite?</p>
",18455719,,,,,2022-03-13 17:50:35,Relationship of Horizon and Discount factor in Reinforcement Learning,<reinforcement-learning>,1,0,,,,CC BY-SA 4.0,
2747,54211391,1,54211628,,2019-01-16 06:17:11,,-1,54,"<p>I am working on stock market analysis and prediction using machine learning methods, especially with reinforcement learning. I am trying to predict short, long and flat. (buy, hold, sell) . (any suggestion or material is appreciated), 
currently, I am giving historical data into my agent and agent predict buy, sell or hold signal. 
my question is how to measure stock quantity. e.g. if my model gives a buy signal, how to measure how much stock I should buy.</p>
",7815796,,,,,2019-01-16 06:37:03,in stock trading how to masure quantity of stock,<artificial-intelligence><reinforcement-learning><stock><policy-gradient-descent>,1,0,,,,CC BY-SA 4.0,
2748,68550901,1,68551308,,2021-07-27 19:41:03,,0,294,"<p>I'm working on a project where I need to collect rollouts to fill a dataset and then train a model on it. I'd like to collect these rollouts in parallel to speed up the process so I'm trying to use the multi-processing provided by the ray library :</p>
<pre><code>import time
import ray

ray.init()

@ray.remote
class MainActor:
    def __init__(self):
        self.data_set = []

    def process_item(self, item):
        # process incoming item
        item+=1
        return item

    def append(self, val):
        # add item to data set after processing it
        item = self.process_item(val)
        self.data_set.append(item)

    def train(self):
        # train on datatset
        return len(self.data_set)

main_actor = MainActor.remote()

@ray.remote
def rollout_collector(main_actor):
    t = time.time()
    for i in range(40000):
        main_actor.append.remote(i)
    print(&quot;time per rollout : &quot;,time.time() - t)


t = time.time()
ray.get([rollout_collector.remote(main_actor) for i in range(3)])
ray.get(main_actor.train.remote())
print(&quot;total time with multi-processing: &quot;,time.time() - t)

# ============================== Single process ================================

class MainActor:
    def __init__(self):
        self.data_set = []

    def process_item(self, item):
        # process incoming item
        item+=1
        return item

    def append(self, val):
        # add item to data set after processing it
        item = self.process_item(val)
        self.data_set.append(item)

    def train(self):
        # train on datatset
        pass

main_actor = MainActor()
t = time.time()
for i in range(120000):
    main_actor.append(i)
main_actor.train()
print(&quot;total time with single process : &quot;,time.time() - t)
</code></pre>
<p>The rollout_collector collect items and then store them, after processing, inside the MainActor to finally train on it. However, this method is extremely slow :</p>
<p><a href=""https://i.stack.imgur.com/tV4f6.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/tV4f6.png"" alt=""enter image description here"" /></a></p>
<p>Each rollout when using ray take as much as 20 sec compare to 0.12 sec without ray and multi-processing. I have one cpu for the main actor and 3 for the rollout_collectors.
I don't think I can store the rollouts inside the rollout_collector workers and then send everything to the main actor as I'm using reservoir sampling for the training. Furthermore, I get a really long execution time for a very simple function when using ray : the training function on MainActor juste return the length of the dataset but it still take 20 sec to execute.</p>
<p>So my question is Am I doing something wrong ? What would be the best way to achieve my goal ?</p>
",10909024,,,,,2021-07-27 20:19:01,How to efficiently fill a list with ray multi-processing?,<python><multiprocessing><reinforcement-learning><ray>,1,0,,,,CC BY-SA 4.0,
2750,69650499,1,69654319,,2021-10-20 17:23:49,,2,93,"<p>I'm using drake for some model-free reinforcement learning and I noticed that Drake uses a non-fixed step integration when simulating an update. This makes sense for the sake of integrating multiple times over a smaller duration when the accelerations of a body is large, but in the case of using reinforcement learning this results in some significant compute overhead and slow rollouts. I was wondering if there is a principled way to allow the simulation environment to operate in a fixed timestep integration mode beyond the method that I'm currently using (code below). I'm using the PyDrake bindings, and PPO as the RL algorithm currently.</p>
<pre><code>integrator = simulator.get_mutable_integrator()
integrator.set_fixed_step_mode(True)
</code></pre>
",2864968,,,,,2021-10-23 21:11:38,Best practice to set Drake's simulator for fixed integration when using with reinforcement learning?,<reinforcement-learning><drake>,2,0,,,,CC BY-SA 4.0,
2754,69593105,1,69600392,,2021-10-16 06:03:57,,1,279,"<p>I successfully followed <a href=""https://www.tensorflow.org/agents/tutorials/1_dqn_tutorial"" rel=""nofollow noreferrer"">this official tensorflow tutorial</a> for training an agent to solve the 'CartPole-v0' gym environment. I only diverged from the tutorial in that I did not use <a href=""https://pypi.org/project/reverb/"" rel=""nofollow noreferrer"">reverb</a>, because it's not supported on Windows. I tried to modify the example to train the agent to solve my own (extremely simple) environment, but it fails to converge on a solution after 10,000 iterations, which I feel should be more than plenty.</p>
<p>I tried adjusting training iterations, learning rates, batch sizes, discounts, and everything else I could think of. Nothing had an effect on the result.</p>
<p>I would like the agent to converge on a policy that always gets +1 reward (ideally in only a few hundred iterations, since this environment is so extremely simple), instead of one that occasionally dips to -1. Instead, here's a graph of the actual outcome:</p>
<p>(The text is small so I will say that orange is episode length in steps, and blue is the average reward. The X axis is the number of training iterations, from 0 to 10,000.)
<a href=""https://i.stack.imgur.com/ripcz.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/ripcz.png"" alt=""graph showing suboptimal rewards"" /></a></p>
<h1>CODE</h1>
<h2>Everything here is run top to bottom, but I put it in separate code blocks to make it easier to read/debug.</h2>
<p>Imports</p>
<pre class=""lang-py prettyprint-override""><code>import numpy as np
import tf_agents as tfa
import tensorflow as tf

# for reproducability
np.random.seed(100)
tf.random.set_seed(100)
</code></pre>
<p>Environment. You can probably skip reading the whole class, it passes <code>validate_py_environment</code> just fine, so I don't believe there is a problem here.</p>
<pre class=""lang-py prettyprint-override""><code># This environment is very simple. You start at position = 0
# and at every step you can either move left, right, or do nothing
# if you move right 3 times (and get to position = 3) then you win.
# otherwise, if you go left to position = -3, the you lose.
# you also lose if it takes more than 10 steps.
# losing gives you -1 reward, winning gives you +1
class SimpleGame(tfa.environments.py_environment.PyEnvironment):
    def __init__(self):
        # 0 - move left
        # 1 - do nothing
        # 2 - move right
        self._action_spec = tfa.specs.array_spec.BoundedArraySpec(
            shape = (),
            dtype = np.int32,
            minimum = 0,
            maximum = 2,
            name = 'action'
        )
        
        self._observation_spec = tfa.specs.array_spec.BoundedArraySpec(
            shape = (1,),
            dtype = np.int32,
            minimum = -3,
            maximum = 3,
            name = 'observation'
        )
        
        self._position = 0
        
        self._step_counter = 0

    def action_spec(self):
        return self._action_spec
    
    def observation_spec(self):
        return self._observation_spec
    
    def _observe(self):
        return np.array([self._position], dtype = np.int32)
    
    def _reset(self):
        self._position = 0
        self._step_counter = 0
        return tfa.trajectories.time_step.restart(self._observe())
    
    def _step(self, action):
        if abs(self._position) &gt;= 3 or self._step_counter &gt;= 10:
            return self.reset()
        
        self._step_counter += 1
        
        if action == 0:
            self._position -= 1
        elif action == 1:
            pass
        elif action == 2:
            self._position += 1
        else:
            raise ValueError('`action` should be 0 (left), 1 (do nothing) or 2 (right). You gave `%s`' % action)
        
        reward = 0
        if self._position &gt;= 3:
            reward = 1
        elif self._position &lt;= -3 or self._step_counter &gt;= 10:
            reward = -1
        
        if reward != 0:
            return tfa.trajectories.time_step.termination(
                self._observe(),
                reward
            )
        else: # this game isn't over yet
            return tfa.trajectories.time_step.transition(
                self._observe(),
                reward = 0, 
                discount = 1.0 
            )

# no issue here:
tfa.environments.utils.validate_py_environment(SimpleGame(), episodes=10)
</code></pre>
<p>Environment Instances</p>
<pre class=""lang-py prettyprint-override""><code>train_py_env = SimpleGame()
test_py_env = SimpleGame()
train_env = tfa.environments.tf_py_environment.TFPyEnvironment(train_py_env)
test_env = tfa.environments.tf_py_environment.TFPyEnvironment(test_py_env)
</code></pre>
<p>Agent Creation</p>
<pre class=""lang-py prettyprint-override""><code>q_network = tfa.networks.sequential.Sequential([
    tf.keras.layers.Dense(16, activation = 'relu'),
    tf.keras.layers.Dense(3, activation = None) 
])

agent = tfa.agents.dqn.dqn_agent.DqnAgent(
    train_env.time_step_spec(),
    train_env.action_spec(),
    q_network = q_network,
    optimizer = tf.keras.optimizers.Adam(),
    td_errors_loss_fn = tfa.utils.common.element_wise_squared_loss,
    n_step_update = 1
)

agent.initialize()

agent.train = tfa.utils.common.function(agent.train)
</code></pre>
<p>Policy Evaluator. You can probably skip reading this. I believe it is correct.</p>
<pre class=""lang-py prettyprint-override""><code># simulate some episodes by following the given policy
# return the average reward and episode length
def evaluate_policy(env, policy, episodes = 10):
    total_reward = 0.0
    total_steps = 0
    
    for ep in range(episodes):
        time_step = env.reset()
        
        # this will always just add 0, but kept it for completion
        total_reward += time_step.reward.numpy()[0]
        
        while not time_step.is_last(): 
            action_step = policy.action(time_step)
            action_tensor = action_step.action
            action = action_tensor.numpy()[0]
            
            time_step = env.step(action)
            
            total_reward += time_step.reward.numpy()[0]
            total_steps += 1
        
    average_reward = total_reward / episodes
    average_ep_length = total_steps / episodes
    return average_reward, average_ep_length

# evaluate policy before any training
avg_reward, avg_length = evaluate_policy(test_env, agent.policy)
print(&quot;initial policy gives average reward of %.2f after an average %d steps&quot; % (avg_reward, avg_length))
#&gt; initial policy gives average reward of -1.00 after an average 10 steps
</code></pre>
<p>Replay Buffer</p>
<pre class=""lang-py prettyprint-override""><code>replay_buffer = tfa.replay_buffers.tf_uniform_replay_buffer.TFUniformReplayBuffer(
    data_spec = agent.collect_data_spec,
    batch_size = train_env.batch_size,
    max_length = 10000
)

replay_dataset = replay_buffer.as_dataset(
    num_parallel_calls = 3, 
    sample_batch_size = 64,
    num_steps = 2
).prefetch(3)

def record_experience(buffer, time_step, action_step, next_time_step):
    buffer.add_batch(
        tfa.trajectories.trajectory.from_transition(time_step, action_step, next_time_step)
    )

replay_dataset_iterator = iter(replay_dataset)
</code></pre>
<p>Training Process</p>
<pre class=""lang-py prettyprint-override""><code>time_step = train_env.reset()

episode_length_history = []
reward_history = []

for step in range(10000 + 1): # +1 just to so 10000 is included in the plot
    
    for _ in range(10):
        action_step = agent.collect_policy.action(time_step)
        
        action_tensor = action_step.action
        action = action_tensor.numpy()[0]
        new_time_step = train_env.step(action)
        
        reward = new_time_step.reward.numpy()[0]
        
        record_experience(replay_buffer, time_step, action_step, new_time_step)
        
        time_step = new_time_step
    
    training_experience, unused_diagnostics_info = next(replay_dataset_iterator)
    
    train_step = agent.train(training_experience)
    loss = train_step.loss
    
    print(&quot;step: %d, loss: %d&quot; % (step, loss))
    
    if step % 100 == 0:
        avg_reward, avg_length = evaluate_policy(test_env, agent.policy)
        print(&quot;average reward: %.2f average steps: %d&quot; % (avg_reward, avg_length))
        
        # record for the plot
        reward_history.append(avg_reward)
        episode_length_history.append(avg_length)
</code></pre>
<p>Plotting (not relevant to the issue, just included for completion)</p>
<pre class=""lang-py prettyprint-override""><code>import matplotlib.pyplot as plt

fig, ax = plt.subplots()
ax.set_xlabel('train iterations')

fig.subplots_adjust(right=0.8)

reward_ax = ax
length_ax = ax.twinx()

length_ax.set_frame_on(True)
length_ax.patch.set_visible(False)
length_ax.set_ylabel('episode length', color = &quot;orange&quot;)
length_ax.tick_params(axis = 'y', colors = &quot;orange&quot;)

reward_ax.set_ylabel('reward', color = &quot;blue&quot;)
reward_ax.tick_params(axis = 'y', colors = &quot;blue&quot;)

train_iterations = [i * 100 for i in range(len(reward_history))]
reward_ax.plot(train_iterations, reward_history, color = &quot;blue&quot;)
length_ax.plot(train_iterations, episode_length_history, color = &quot;orange&quot;)

plt.show()
</code></pre>
",9816919,,,,,2021-10-16 23:56:54,tf_agents doesn't properly learn a simple environment,<python><tensorflow><reinforcement-learning><tensorflow-agents>,1,1,0,,,CC BY-SA 4.0,
2756,55371106,1,55378798,,2019-03-27 06:30:21,,2,129,"<p>I'm wondering why is the Trust Region Policy Optimization a On-policy algorithm? </p>

<p>In my opinion, in TRPO, we samples by the old policy and update the new policy and apply the importance sampling to correct the bias. Thus, it is more like a off-policy algorithm. 
But recently, I read a <a href=""https://arxiv.org/pdf/1706.01905"" rel=""nofollow noreferrer"">paper</a> which said: </p>

<blockquote>
  <p>In contrast to off-policy algorithms, on-policy methods require
  updating function approximatorsaccording to the currently followed
  policy.   In particular, we will consider Trust Region
  PolicyOptimization, an extension of traditional policy gradient
  methods  using the natural gradient direction.</p>
</blockquote>

<p>Does any point I misunderstand?</p>
",11264345,,1071020,,2019-03-27 07:03:26,2019-03-27 13:47:36,Why is the Trust Region Policy Optimization a On-policy algorithm?,<artificial-intelligence><reinforcement-learning>,1,0,0,,,CC BY-SA 4.0,
2757,55410391,1,55421550,,2019-03-29 04:11:00,,0,49,"<p>I am trying to implement proximal policy optimization, and I'm facing a very strange problem.</p>

<p>Here is a minimal demonstration of the problem:</p>

<pre><code>import numpy as np
import tensorflow as tf

raw_probs = tf.get_variable(""raw_probs"",[4])
probs = tf.nn.softmax(raw_probs)

actions = tf.placeholder(dtype=tf.int32, shape=[None], name='actions')
rewards = tf.placeholder(dtype=tf.float32, shape=[None], name='rewards')
old_probs = tf.placeholder(dtype=tf.float32, shape=[None], name='old_probs')
new_probs = tf.reduce_sum(probs * tf.one_hot(indices=actions, depth=4))
ratios = new_probs / old_probs
clipped_ratios = tf.clip_by_value(ratios, clip_value_min=0.8, clip_value_max=1.2)
loss_clip = -tf.reduce_mean(tf.minimum(tf.multiply(rewards, ratios), tf.multiply(rewards, clipped_ratios)))

optimizer = tf.train.AdamOptimizer()
train_pol = optimizer.minimize(loss_clip)

with tf.Session() as sess:
    sess.run(tf.global_variables_initializer())

    for i in range(1000):
        input_actions = []
        input_rewards = []
        input_old_probs = []

        for j in range(20):
            tmp_probs = sess.run(probs)
            if j == 0:
                print(tmp_probs)
            act = np.random.choice(4,p=tmp_probs)
            input_actions.append(act)
            if act == 0:
                input_rewards.append(1)
            else:
                input_rewards.append(-1)
            input_old_probs.append(tmp_probs[act])

        sess.run(train_pol,feed_dict={actions: input_actions,rewards: input_rewards,old_probs: input_old_probs})
</code></pre>

<p>The program draws numbers according to a probability distribution. If it draws 0, it is given a reward of 1. If it draws other numbers, it is given a reward of -1. The program then adjusts the probabilities according to the results.</p>

<p>In theory, the probability of choosing 0 should always increase, eventually converging to 1. In practice, however, it is decreasing.</p>

<p>What am I doing wrong here?</p>
",2249675,,,,,2019-03-29 16:15:34,Policy-based learning does not converge,<python><tensorflow><machine-learning><reinforcement-learning>,1,0,,,,CC BY-SA 4.0,
2758,54171713,1,54181309,,2019-01-13 18:05:54,,0,499,"<p>I have a DQN agent which is trained on a specific network to perform a task. However, when training the agent I noticed that after an initial number of epochs where the agent shows a general growth in the score of the task, there suddenly occurs a drastic decrease in the performance of the agent as if it is starting out afresh. This happens a number of times. </p>

<p><strong>My agent shows fluctuations in performance from bad to good and so on.</strong> Is this normal for DQN agents. What diagnosis should I perform to enable remove such fluctuations? I have used experience replay and exploration-exploitation for the agent. I am relatively new to the field so the question may be pretty trivial.</p>
",8033068,,,,,2019-01-14 12:10:09,Deep Q-Learning Agent performance degrades after a certain number of epochs,<deep-learning><reinforcement-learning>,1,0,0,,,CC BY-SA 4.0,
2761,55381040,1,55387407,,2019-03-27 15:34:21,,2,975,"<p>I am trying to use FlappyBird environment in Google Colab for reinforcement learning task. I have downloaded the PLE and PYGAME and set the video driver as 'dummy'. Though the import is successful, it throws 'couldn't find png' error when I instantiate the FlappyBird() class.</p>

<p>I have installed the PLE and PyGame.</p>

<pre><code>import os
!git clone https://github.com/ntasfi/PyGame-Learning-Environment.git
os.chdir('PyGame-Learning-Environment')
!pip install -e .
!pip install pygame
os.chdir('/content')
</code></pre>

<p>Set videoDriver as 'dummy' because no video device is available in colab.</p>

<pre><code>import os
os.environ['SDL_VIDEODRIVER']='dummy'
</code></pre>

<p>Imported ple and flappyBird</p>

<pre><code>from ple.games.flappybird import FlappyBird
from ple import PLE  
game = FlappyBird()
</code></pre>

<p>It throws an error:</p>

<pre><code>error:  Traceback (most recent call last)
&lt;ipython-input-5-c0174ea47a10&gt; in &lt;module&gt;()
  3 
  4 
----&gt; 5 game = FlappyBird()
  6 p = PLE(game, fps=30, display_screen=True)

/usr/local/lib/python3.6/dist-packages/ple/games/flappybird/__init__.py in __init__(self, width, height, pipe_gap)

/usr/local/lib/python3.6/dist-packages/ple/games/flappybird/__init__.py in _load_images(self)

/usr/local/lib/python3.6/dist-packages/ple/games/flappybird/__init__.py in &lt;listcomp&gt;(.0)

error: Couldn't open /usr/local/lib/python3.6/dist-packages/ple/games/flappybird/assets/redbird-upflap.png
</code></pre>

<p>How should I solve this problem?</p>
",9794639,,,,,2019-03-27 22:34:40,Unable to run FlappyBird PLE in google colab,<pygame><google-colaboratory><reinforcement-learning>,1,0,,,,CC BY-SA 4.0,
2764,55447787,1,55469731,,2019-04-01 03:33:22,,1,955,"<p>In Andrew Ng's fitted value iteration algorithm, which I gave the detail steps as below, it will try to find the best action of one state s(i) in step 3. when the agent was in s(i), we execute the possible action a(1) and we transited to s(i)'. </p>

<p>My question is how we can reverted to s(i) again and executed the 2nd possible action a(2)? Suppose, we use this algorithm to control a helicopter, I think we can not revert state back easily. </p>

<h3>Algorithm</h3>

<pre><code>1. Randomly sample m states s(1), s(2), . . . s(m) âˆˆ S.
2. Initialize Î¸ := 0.
3. Repeat {
    For i = 1, . . . , m {
        For each action a âˆˆ A {
            Sample sâ€² 1, . . . , sâ€² k âˆ¼ Ps(i)a (using a model of the MDP).
            Set q(a) = k1 Pk j=1 R(s(i)) + Î³V (sâ€² j)
            // Hence, q(a) is an estimate of R(s(i))+Î³Esâ€²âˆ¼P
            s(i)a[V (sâ€²)].
        }
        Set y(i) = maxa q(a).
        // Hence, y(i) is an estimate of R(s(i))+Î³ maxa Esâ€²âˆ¼P
        s(i)a[V (sâ€²)].
   }
   // In the original value iteration algorithm (over discrete states)
   // we updated the value function according to V (s(i)) := y(i).
   // In this algorithm, we want V (s(i)) â‰ˆ y(i), which weâ€™ll achieve
   // using supervised learning (linear regression).
   Set Î¸ := arg minÎ¸ 1 2 Pm i=1 Î¸T Ï†(s(i)) âˆ’ y(i)2
}
</code></pre>
",11287964,,6553328,,2019-04-01 08:30:47,2019-04-02 08:03:57,Fitted value iteration algorithm of Markov Reinforcement Learning,<algorithm><machine-learning><reinforcement-learning><model-fitting>,1,2,,,,CC BY-SA 4.0,
2766,55545051,1,55548089,,2019-04-06 01:21:13,,1,116,"<p>I am new to reinforcement learning. I had recently learned about approximate q learning, or feature-based q learning, in which you describe states by features to save space. I have tried to implement this in a simple grid game. Here, the agent is supposed to learn to not go into a firepit(signaled by an f) and to instead eat up as much dots as possible. Here is the grid used:</p>

<p>...A <br/>
.f.f <br/>
.f.f<br/>
...f<br/></p>

<p>Here A signals the agent's starting location. Now, when implementing, I set up two features. One was 1/((distance to closest dot)^2), and the other was (distance to firepit) + 1. When the agent enters a firepit, the program returns with a reward of -100. If it goes to a non firepit position that was already visited(and thus there is no dot to be eaten), the reward is -50. If it goes to an unvisited dot, the reward is +500. In the above grid, no matter what the initial weights are, the program never learns the correct weight values. Specifically, in the output, the first training session gains a score(how many dots it ate) of 3, but for all other training sessions, the score is just 1 and the weights converge to an incorrect value of -125 for weight 1(distance to firepit) and 25 for weight 2(distance to unvisited dot). Is there something specifically wrong with my code or is my understanding of approximate q learning incorrect?</p>

<p>I have tried to play around with the rewards that the environment is giving and also with the initial weights. None of these have fixed the problem. 
Here is the link to the entire program: <a href=""https://repl.it/repls/WrongCheeryInterface"" rel=""nofollow noreferrer"">https://repl.it/repls/WrongCheeryInterface</a></p>

<p>Here is what is going on in the main loop:</p>

<pre><code>while(points != NUMPOINTS){
bool playerDied = false;
if(!start){
  if(!atFirepit()){
    r = 0;
    if(visited[player.x][player.y] == 0){
      points += 1;
      r += 500;
    }else{
      r += -50;
    }
  }else{
    playerDied = true;
    r = -100;
  }
}

//Update visited
visited[player.x][player.y] = 1;

if(!start){
  //This is based off the q learning update formula
  pairPoint qAndA = getMaxQAndAction();
  double maxQValue = qAndA.q;
  double sample = r;
  if(!playerDied &amp;&amp; points != NUMPOINTS)
    sample = r + (gamma2 * maxQValue);
  double diff = sample - qVal;
  updateWeights(player, diff);
}

// checking end game condition
if(playerDied || points == NUMPOINTS) break;

pairPoint qAndA = getMaxQAndAction();
qVal = qAndA.q;
int bestAction = qAndA.a;

//update player and q value
player.x += dx[bestAction];
player.y += dy[bestAction];

start = false;
}
</code></pre>

<p>I would expect that both weights would still be positive, but one of them is negative(the one giving distance to the firepit). </p>

<p>I also expected the program to learn overtime that it is bad to enter a firepit and also bad, but not as bad, to go to an unvisited dot.</p>
",11303221,,,,,2019-04-06 10:18:19,Problems with implementing approximate(feature based) q learning,<c++><machine-learning><reinforcement-learning><q-learning>,1,0,,,,CC BY-SA 4.0,
2769,55513605,1,55521710,,2019-04-04 10:25:56,,2,575,"<p>I have a model, which I would like to build a custom loss function, I have my States which are my X values and then I have my actions which are 7 one-hot categorical values which are my Y values, that I am predicting. </p>

<p>However I'm not sure how to pass the reward to the loss function. I'm also not sure what the actual function should be, but I can experiment with this later. </p>

<pre><code>x = input_data[:, :-2]  # States
y = input_data[:, -2]  # Actions
r = input_data[:, -1]  # Rewards

def custom_loss(y_pred, y_true):
     loss = K.square(y_pred - y_true) * r
     return loss

model.compile(loss=custom_loss, optimizer='adam', metrics=['accuracy'])
model.fit(x, y)
</code></pre>
",4809603,,,,,2019-04-04 17:21:37,Custom Loss Function for Reward using Keras in Python,<python><reinforcement-learning><loss-function>,1,0,,,,CC BY-SA 4.0,
2771,72483775,1,72805349,,2022-06-03 01:12:54,,0,427,"<p>I want to gradually decrease the clip_range (epsilon, exploration vs. exploitation parameter) throughout training in my PPO model.</p>
<p>I have tried to simply run &quot;model.clip_range = new_value&quot;, but this doesn't work.</p>
<p>In the docs <a href=""https://stable-baselines3.readthedocs.io/en/master/modules/ppo.html"" rel=""nofollow noreferrer"">here</a> , it says &quot;clip_range (Union[float, Callable[[float], float]]) â€“ Clipping parameter, it can be a function of the current progress remaining (from 1 to 0).&quot;</p>
<p>Does anyone know how to actually change this parameter during training, or how to input &quot;a function of the current progress remaining&quot;?</p>
",13615502,,,,,2022-06-29 17:09:50,Stable Baselines3 PPO() - how to change clip_range parameter during training?,<reinforcement-learning><stable-baselines>,1,0,,,,CC BY-SA 4.0,
2773,72501444,1,72510305,,2022-06-04 16:36:33,,1,95,"<p>Here is some sample REINFORCE code found in the <a href=""https://pytorch.org/docs/stable/distributions.html"" rel=""nofollow noreferrer"">PyTorch distributions docs</a>:</p>
<pre><code>probs = policy_network(state)
m = Categorical(probs)
action = m.sample()
next_state, reward = env.step(action)
loss = -m.log_prob(action) * reward
loss.backward()
</code></pre>
<p>I don't understand why this loss is differentiable. In particular, how does <code>m.log_prob(action)</code> maintain the computational path of the network output <code>probs</code>? How are <code>m.log_prob(action)</code> and <code>probs</code> are 'connected'?</p>
<p>Edit: I looked at the <a href=""https://pytorch.org/docs/stable/_modules/torch/distributions/categorical.html#Categorical.log_prob"" rel=""nofollow noreferrer"">implementation</a> of <code>log_prob</code>, and it doesn't even seem to reference <code>self.probs</code> anywhere; only <code>self.logits</code>.</p>
",5361456,,5361456,,2022-06-04 16:48:45,2022-06-05 19:02:02,Why is REINFORCE loss differentiable?,<pytorch><reinforcement-learning>,1,2,,,,CC BY-SA 4.0,
2774,72537702,1,72674552,,2022-06-07 21:22:24,,0,44,"<p>When defining state for a specific problem in reinforcement learning, How to decide what to include and what to leave for the definition, and also how to set difference between an observation and a state.
For example assuming that the agent is in the context of human resource and planning where it needs to hire some workers based on the demand of jobs, considering the cost of hiring them (assuming the budget is limited) is a state in the format of (# workers, cost) a good definition of state?
In total I don't know what information is needed to be in state and what should be left as it's rather observation.
Thank you</p>
",17958304,,,,,2022-06-19 05:50:10,State definition in Reinforcement learning,<reinforcement-learning>,1,0,,,,CC BY-SA 4.0,
2775,55000663,1,55042126,,2019-03-05 10:29:28,,3,659,"<p>I have some problems with figuring out why you need to revisit all time steps from an episode on each horizon advance for the On-Line version of the Î»-return algorithm from the book:<br />
<a href=""http://www.incompleteideas.net/book/RLbook2018.pdf#page=309"" rel=""nofollow noreferrer""><code>Reinforcement Learning: An Introduction, 2nd Edition, Chapter 12, Sutton &amp; Barto</code></a></p>
<p><img src=""https://i.stack.imgur.com/VdIJp.png"" alt=""Horizon step-by-step expansion"" /></p>
<p>Here all sequences of weight vectors W1, W2,..., Wh for each horizon h start from W0(the weights from the end of the previous episode). However they do not seem to depend on the returns/weights from the previous horizon and can be calculated independently. This appears to me explained like that just for clarification and you can calculate them only for the final horizon h=T at episode termination. This will be the same what is done for the Off-line version of the algorithm and the actual update rule is:</p>
<p><img src=""https://i.stack.imgur.com/FEtWf.png"" alt=""General weight-vector update formula"" /></p>
<p>Not surprisingly I get exactly the same results for the 2 algorithms on the 19-states Random Walk example:</p>
<p><a href=""https://i.stack.imgur.com/toJ34.jpg"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/toJ34.jpg"" alt=""On-line VS Off-line graph chart"" /></a></p>
<p>In the book it is mentioned that the on-line version should perform a little bit better and for that case it should have the same results as the True Online TD(Î»). When implementing the latter it really outperforms the off-line version but I can't figure it out for the simple and slow on-line version.</p>
<p>Any suggestions will be appreciated.</p>
<p>Thank you</p>
",3648244,,-1,,2020-06-20 09:12:55,2019-04-14 01:46:33,Eligibility Traces: On-line vs Off-line Î»-return algorithm,<lambda><return><offline><reinforcement-learning><online-algorithm>,1,0,0,,,CC BY-SA 4.0,
2783,72606664,1,72662783,,2022-06-13 17:12:43,,3,56,"<p>I'm using Tensorboard to see the progress of the PettingZoo environment that my agents are playing. I can see the reward go up with time, which is good, but I'd like to add other metrics that are specific to my environment. i.e. I'd like TensorBoard to show me more charts with my metrics and how they improve over time.</p>
<p>The only way I could figure out how to do that was by inserting <a href=""https://gist.github.com/cool-RR/8dc91f5e092a7751cf10a9776f04ee1f"" rel=""nofollow noreferrer"">a few lines</a> into the <code>learn</code> method of <code>OnPolicyAlgorithm</code> that's part of SB3. This works and I got the charts I wanted:</p>
<p><a href=""https://i.stack.imgur.com/qhd8H.jpg"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/qhd8H.jpg"" alt="""" /></a></p>
<p>(The two bottom charts are the ones I added.)</p>
<p>But obviously editing library code isn't a good practice. I should make the modifications in my own code, not in the libraries. <strong>Is there currently a more elegant way to add a metric from my PettingZoo environment into TensorBoard?</strong></p>
",76701,,,,,2022-06-17 17:17:32,Add a TensorBoard metric from my PettingZoo environment,<reinforcement-learning><tensorboard><stable-baselines><pettingzoo><multi-agent-reinforcement-learning>,1,0,,,,CC BY-SA 4.0,
2790,55088497,1,55088705,,2019-03-10 14:09:00,,0,237,"<p>I'd like to build <strong>an autonomous ship</strong> in a virtual environment <strong>using DDPG</strong>. <br></p>

<p>However, the problem is that there's an action space of (-180', +180') for steering, and DDPG would be able to choose -180' at (t-1) and +180' at (t+1), which is impossible in the real world. (basically, you can't rotate a steering wheel that fast.)<br><br></p>

<hr>

<p>The possible solution that I thought was this.<br><br></p>

<ol>
<li>Set a maximum steering rate (e.g. 10' per step)<br><br></li>
<li>If the taken action gets out of an available action range of (current_steeringWheel_angle - 10', current_steeringWheel_angle + 10'), change the taken action to the end value in the available action range<br><br></li>
<li>Take a step with the changed action in the virtual environment.<br><br></li>
<li><br>
(1st option) update the DDPG with the changed action.<br>
(2nd option) update the DDPG with the originally taken action.</li>
</ol>
",8762113,,,,,2019-03-10 14:29:54,Limit on Action Change in reinforcement learning,<reinforcement-learning>,1,0,0,,,CC BY-SA 4.0,
2793,72656770,1,72661341,,2022-06-17 08:56:06,,1,277,"<p>For my basic evaulation of learning algorithms
I defined a custom environment.
Now with standard examples for stable baselines the learning
seems always to be initiated by stable baselines automatically
(by stablebaselines choosing random actions itsself and evaluating the rewards).
The standard learning seems to be done like this:</p>
<p>model.learn(total_timesteps=10000)</p>
<p>and this will try out different actions and optimize the action-observation-relations
while learning.</p>
<p>I would like to try out a really basic approach: for my custom environment I would
try to generate lists of examples, which actions should be taken according to some
situations of relevance (so there is a list of predefined observation-action-rewards).</p>
<p>And I would like to train the model with this list.</p>
<p>What would be the most appropriate way to implement this with stablebaselines3
(using pytorch)?</p>
<p>Additional information:
maybe the sense of the question could be compared to the idea in case of an atari game, not to always train a whole game sequence at once (from start to end of game, and then restart again until training ends), but instead to train the agent only with some more specific, representative situations of importance.
Or in chess: it seems to be a huge difference to let an agent
select randomly self choosen or randomly selected moves or
to let him follow moves played by masters in particular interesting
situations.</p>
<p>Maybe one could put the lists as main part of the environment reaction
(so e.g. train the agent with environment 1 for e.g. 1000 steps,
then train with environment 2 for e.g. 1000 steps and so on).
This could be a solution.</p>
<p>But the problem would be, that stable baselines would choose
actions itsself, so that it could not learn a complete sequence
of &quot;correct&quot; or like in chess masterful choosen steps in sequence.</p>
<p>So again the practical question is : is is possible, ot  important to bring stablebaselines to train  predefined actions instead of self choosen ones while training/learning?</p>
",4074395,,4074395,,2022-06-17 11:36:50,2022-06-17 15:01:09,train stable baselines 3 with examples?,<pytorch><reinforcement-learning><stable-baselines>,1,0,0,,,CC BY-SA 4.0,
2797,69439942,1,69447489,,2021-10-04 17:12:26,,2,122,"<p>I am using a simple DDQN-PER (<a href=""https://github.com/higgsfield/RL-Adventure/blob/master/4.prioritized%20dqn.ipynb"" rel=""nofollow noreferrer"">https://github.com/higgsfield/RL-Adventure/blob/master/4.prioritized%20dqn.ipynb</a>) to run my RL experiments. This could be used as a minimal and verifiable code for my question.</p>
<p>Also, here are the seed functions that I have found to be useful -</p>
<pre><code>random.seed(seed)
np.random.seed(seed)
torch.manual_seed(seed)
torch.cuda.manual_seed(seed)
torch.backends.cudnn.deterministic = True
env.seed(seed)
env.action_space.seed(seed)
</code></pre>
<p>Despite this, I cannot reproduce my experiments. I believe I am missing some seed function and will appreciate inputs on the same.</p>
",11628437,,,,,2021-10-05 08:53:45,Missing seed functions to create reproducible RL experiments with Open AI Gym and PyTorch,<pytorch><reinforcement-learning>,1,0,,,,CC BY-SA 4.0,
2820,72995715,1,73168688,,2022-07-15 14:41:14,,1,109,"<p>We have a custom reinforcement learning environment within which we run a PPO agent from stable baselines3 for a multi action selection problem. The agent learns as expected but when we evaluate the learned policy from trained agents the agents achieve worse results (i.e. around 50% lower rewards) when we set  <code>deterministic=True</code> than with <code>deterministic=False</code>. The goal of the study is to find new policies for a real-world problem and so it would be desirable to find a deterministic policy as this is much better understandable for most people... And it seems counterintuitive that more random actions result in better performance.</p>
<p>The <a href=""https://stable-baselines3.readthedocs.io/en/master/"" rel=""nofollow noreferrer"">documentation</a> says only &quot;<strong>deterministic</strong> (bool) â€“ Whether or not to return deterministic actions.&quot;.
I understand this as <code>deterministic=False</code> means that the actions are drawn from a learned distribution with a certain stochasticity (i.e. one specific state can result in several different actions) and <code>deterministic=True</code> means that the actions are fully based on the learned policy (i.e. one specific state always results in one specific action).</p>
<p>The question is what it says about the agent and / or the environment when the performance is better with <code>deterministic=False</code> than with <code>deterministic=True</code>?</p>
",13847693,,,,,2022-07-29 16:56:39,Reinforcement learning deterministic policies worse than non deterministic policies,<reinforcement-learning><policy><deterministic><stable-baselines>,1,0,,,,CC BY-SA 4.0,
2821,72998966,1,72999819,,2022-07-15 19:47:55,,-1,54,"<p>For the sake of the argument, let's say that I am trying to minimize a number of mathematical functions using Reinforcement Learning, where the minimum can essentially lie anywhere between -inf and +inf. (I know that RL would probably not be the most suitable algorithm, but this is just an analogy.)</p>
<p>I want to set up the reward to reflect the &quot;best minimum&quot; found on each step.
The problem is that any specific function can have a {min,max} range of {0,100} for example, or {-1000,+9999999}, or {-99999,-10}, or {-9.000000001,-9.000000002}, or any two conceivable values really - and the ranges are not known beforehand.
I am therefore unsure how I should normalize the reward to lie between {-1,+1}, because such extreme ranges as before of course won't work directly as a reward.</p>
<p>I assume that some kind of relative improvement formula is needed where the new reward is compared to the old, but this creates problems because something like (x_old - x_new) / x_old would see a change of 1 to 0.5 as a 50% improvement, while the true minimum of the function might just as well lie at -1000.</p>
<p>Maybe there are simply too few constraints to sensibly construct a reward function, but I am sure that analogous problems have been encoutered elsewhere?</p>
",13220100,,4685471,,2022-07-18 08:01:06,2022-07-20 01:17:35,RL reward function with unknown range,<machine-learning><mathematical-optimization><reinforcement-learning><reward>,2,2,,,,CC BY-SA 4.0,
2826,73006909,1,73061481,,2022-07-16 18:57:50,,1,78,"<p>I have a problem when importing some dependencies from stable baselines 3 library, I installed it with this command</p>
<pre><code>pip install stable-baselines3[extra]
</code></pre>
<p>But When I import my dependencies</p>
<pre><code>import gym
from stable_baselines3 import A2C
from stable_baselines3.common.vec_env import VecFrameStackFrame
from stable_baselines3.common.evaluation import evaluate_policy
from stable_baselines3.common.env_util import make_atari_env
import os
</code></pre>
<p>I face this error</p>
<pre><code>ImportError: cannot import name 'VecFrameStackFrame' from 'stable_baselines3.common.vec_env' (C:\Users\User\anaconda3\envs\rl_learning\lib\site-packages\stable_baselines3\common\vec_env\__init__.py)
</code></pre>
<p>Any advice?</p>
",9289463,,,,,2022-07-21 06:14:13,Can't import VecFrameStackFrame from Stable-baselines3 - importing problem,<python><deep-learning><pytorch><reinforcement-learning><stable-baselines>,1,0,,,,CC BY-SA 4.0,
2835,73071399,1,73071554,,2022-07-21 18:58:42,,0,196,"<p>I want my model to output a single value, how can I constrain the value to (a, b)?
for example, my code is:</p>
<pre class=""lang-python prettyprint-override""><code>class ActorCritic(nn.Module):
    def __init__(self, num_state_features):
        super(ActorCritic, self).__init__()

        # value
        self.critic_net = nn.Sequential(
            nn.Linear(num_state_features, 64),
            nn.ReLU(),
            nn.Linear(64, 128),
            nn.ReLU(),
            nn.Linear(128, 64),
            nn.ReLU(),
            nn.Linear(64, 1)
        )

        # policy
        self.actor_net = nn.Sequential(
            nn.Linear(num_state_features, 64),
            nn.ReLU(),
            nn.Linear(64, 128),
            nn.ReLU(),
            nn.Linear(128, 64),
            nn.ReLU(),
            nn.Linear(64, 1),
        )

    def forward(self, state):
        value = self.critic_net(state)
        policy_mean = self.actor_net(state)
        return value, policy_mean
</code></pre>
<p>and I want the policy output to be in the range (500, 3000), how can I do this?</p>
<p>(I have tried <code>torch.clamp()</code>, this does not work well since the policy would stay always the same if it is near the limit, for example the output goes to -1000000 and it will then stay 500 forever, or takes really long time to change. The same is true for function like <code>nn.Sigmoid()</code>)</p>
",15783488,,15783488,,2022-07-21 19:04:53,2022-07-21 19:19:32,How to bound the output of a layer in pytorch,<pytorch><reinforcement-learning>,2,0,,,,CC BY-SA 4.0,
2836,73085950,1,73092322,,2022-07-22 20:53:11,,0,108,"<p>Using Stable Baselines 3:</p>
<p>Given that <code>deterministic=True</code> always returns the action with the highest probability, what does that mean for environments where the action space is &quot;box&quot;, &quot;multi-binary&quot; or &quot;multi-discrete&quot; where the agent is supposed to select multiple actions at the same time? How does <code>deterministic=True</code> work in these environments / does it work at all in the way it is supposed to?</p>
<p>The question is partly based on this question about</p>
<p><a href=""https://stackoverflow.com/q/66455636/13847693"">What does &quot;deterministic=True&quot; in stable baselines3 library means?</a></p>
<p>and potentially related to another question from me</p>
<p><a href=""https://stackoverflow.com/q/72995715/13847693"">Reinforcement learning deterministic policies worse than non deterministic policies</a></p>
",13847693,,,,,2022-07-23 16:17:14,"Does ""deterministic = True"" make sense in box, multi-binary or multi-discrete environments?",<reinforcement-learning><policy><deterministic><stable-baselines>,1,0,,,,CC BY-SA 4.0,
2843,73125049,1,73155407,,2022-07-26 14:18:26,,1,117,"<p>I've implemented gridworld example from the book <em>Reinforcement Learning - An Introduction, second edition&quot; from Richard S. Sutton and Andrew G. Barto</em>, Chapter 4, sections 4.1 and 4.2, page 80.</p>
<p>Here is my implementation:
<a href=""https://github.com/ozrentk/dynamic-programming-gridworld-playground"" rel=""nofollow noreferrer"">https://github.com/ozrentk/dynamic-programming-gridworld-playground</a></p>
<p>The original algorithm seems to have a bug since the value function (mapping) is updated one by one value in the source mapping structure. Why is that incorrect? It means that inside the loop for each s (of set S), in the same evaluation loop pass, the next value of the element s (e.g. s_2 of set S) will be evaluated from the newly evaluated element in that pass (e.g. s_1 of set S), instead of s from the current iteration. This problem is avoided here using the double buffering technique. An additional buffer is used for new values of set S. It also means that the program uses more memory because of that buffer.</p>
<p>I must admit that I'm not 100% sure if this is a bug, or if I misinterpreted the algorithm.
Generally, this is the code I'm using:</p>
<pre><code>...

while True:
  delta = 0

  # NOTE: algorithm modified a bit, additional buffer new_values introduced
  # Barto &amp; Sutton seem to have a bug in their algorithm (iterative estimation does not fit figure 4.1)
  # Instead of tracking one state value inside a loop, we track entire state value function mapping
  # outside that loop. Also note that after that change algorithm consumes more memory.
  new_values = [0] * states_n
  for s in non_terminal_states:
    # Evaluate state value under current policy
    next_states = get_next_states(s, policy[s], world_size)
    sum_items = [p * (reward + gamma * values[s_next]) for s_next, p in next_states.items()]
    new_values[s] = sum(sum_items)

    # Track delta
    delta = max(delta, abs(values[s] - new_values[s]))
  
  # (now we switch state value function buffer, instead of switching single state value in the loop)
  values = new_values

  if use_policy_improvement:
    # Policy_improvement is done inside improve_policy(), and if new policy is no better than the 
    # old one, return value of is_policy_stable is True
    is_policy_stable, improved_policy = improve_policy()
    if is_policy_stable:
      print(&quot;Policy is stable.&quot;)
      break
    else:
      print(&quot;- Improving policy... ----------------&quot;)
      policy = improved_policy
      visualize_policy(policy, states, world_size)

  # In case we don't track policy improvement, we need to track delta for the convergence sake
  if delta &lt; theta:
    break

  # Track iteration count
  k += 1

...
</code></pre>
<p>Am I wrong or there is a problem with the policy evaluation part of the algorithm in the book?</p>
<p><a href=""https://i.stack.imgur.com/3E6h1.jpg"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/3E6h1.jpg"" alt=""Policy Evaluation"" /></a></p>
",1632534,,,,,2022-07-28 15:30:53,Python native gridworld implementation (no NumPy),<python><reinforcement-learning><gridworld>,1,0,0,,,CC BY-SA 4.0,
2847,73152465,1,73152609,,2022-07-28 11:53:38,,0,54,"<p>I am implementing multiple RL agents which share a lot of common attributes and methods but differ in only one. Namely the one that calculates the <em>td_error</em>. Out of the top of my head I can think of 3 options to implement this:</p>
<ol>
<li>Use an Agent abstract class (ABCMeta) and let the subclasses each implement all their methods.</li>
<li>Use a normal base class Agent, implement all the common methods and just do <code>pass</code> on the one that is specific to each subclass.</li>
<li>Use just <em>one</em> class Agent and use an attribute to specify the type of <em>td_error</em> to calculate and then just use <code>if else</code> to achieve the specific behavior.</li>
</ol>
<p>Here is what I don't like about each option:</p>
<ol>
<li>It seems I would need to repeat myself when implementing the common methods in each subclass.</li>
<li>It would be possible to create instances of the base class Agent but it wouldn't work since the specific function is not defined.</li>
<li>It is ugly and seems very naive.</li>
</ol>
<p>I have been presented with this situation before and I normally go with option 2. but I am pretty sure there is a more correct way of achieving this.</p>
",9074215,,9074215,,2022-07-28 12:29:31,2022-07-28 12:29:31,Best way to create classes that only differ by one method?,<python><design-patterns><abstract-class><subclass><reinforcement-learning>,1,1,,,,CC BY-SA 4.0,
2862,73224528,1,73227073,,2022-08-03 16:12:19,,-1,140,"<p>I am coding a dqn from scratch and therefore have written my loss function. While calling backward on my loss function, I get the following error - <code>RuntimeError: grad can be implicitly created only for scalar outputs</code></p>
<p>Here's my code -</p>
<pre><code>import numpy as np
import gym
import matplotlib.pyplot as plt
import os
import torch
import random
from torch import nn
from torch.utils.data import DataLoader
from torchvision import datasets, transforms
from collections import deque 
import sys
env = gym.make(&quot;CliffWalking-v0&quot;)


# In[103]:


#Hyperparameters
episodes = 5000
eps = 1.0
learning_rate = 0.1
discount_factor = 0.99
tot_rewards = []
decay_val = 0.001
mem_size = 50000
batch_size = 2
gamma = 0.99


# In[104]:


class NeuralNetwork(nn.Module):
    def __init__(self, state_size, action_size):
        super(NeuralNetwork, self).__init__()
        self.state_size = state_size
        self.action_size = action_size
        self.linear_relu_stack = nn.Sequential(
            nn.Linear(1, 30),
            nn.ReLU(),
            nn.Linear(30, 30),
            nn.ReLU(),
            nn.Linear(30, action_size)
        )
    def forward(self, x):
        x = self.linear_relu_stack(x)
        return x


# In[105]:


model = NeuralNetwork(env.observation_space.n, env.action_space.n)
opt = torch.optim.Adam(params=model.parameters(), lr=learning_rate)
loss = nn.MSELoss()
replay_buffer = deque(maxlen=mem_size)


# In[106]:


state = torch.tensor(env.reset(), dtype=torch.float32)
state = state.unsqueeze(dim=0)
print(state.shape)
out = model(state)


# In[111]:


def compute_td_loss(batch_size):
    state, next_state, reward, done, action = zip(*random.sample(replay_buffer, batch_size))
    state = torch.from_numpy(np.array(state).reshape(-1, 1)).unsqueeze(dim = 0).type(torch.float32)
    next_state = torch.from_numpy(np.array(next_state).reshape(-1, 1)).unsqueeze(dim = 0).type(torch.float32)
    reward = torch.from_numpy(np.array(reward))
    done = torch.from_numpy(np.array(done))
    action = torch.from_numpy(np.array(action)).type(torch.int64)
    q_values = model(state)
    next_q_values = model(next_state)
    q_vals = q_values.squeeze().gather(dim=-1, index=action.reshape(-1,1)).reshape(1, -1)
    max_next_q_values = torch.max(next_q_values,2)[0].detach()
    print(&quot;q_vals = &quot;, q_vals)
    print(&quot;max_next_q_values = &quot;, max_next_q_values)
    loss = 0.5*(reward + gamma*max_next_q_values - q_vals)**2
    print(&quot;reward = &quot;, reward)
    print(&quot;loss = &quot;, loss)
    opt.zero_grad()
    loss.backward()
    opt.step()
    return loss
    


# In[112]:


for i in range(episodes):
    state = env.reset()
    done = False
    steps = 0
    eps_rew = 0 
    while not done and steps&lt;50:
        if np.random.uniform(0,1)&lt;eps:
            action = env.action_space.sample()
        else:
            state = torch.tensor(state, dtype=torch.float32)
            state = state.unsqueeze(dim=0)
            action = np.argmax(model(state).detach().numpy())
        next_state, reward, done, info = env.step(action)
        replay_buffer.append((state, next_state, reward, done, action))
        if len(replay_buffer)&gt;batch_size:
            loss = compute_td_loss(batch_size)
            sys.exit()
        eps = eps/(1 + 0.001)
        eps_rew += reward 
        if done:
            break
        state = next_state
    tot_rewards.append(eps_rew)
</code></pre>
<p>Here's the error that I get -</p>
<pre><code>RuntimeError                              Traceback (most recent call last)
&lt;ipython-input-112-015fd74c95d9&gt; in &lt;module&gt;
     14         replay_buffer.append((state, next_state, reward, done, action))
     15         if len(replay_buffer)&gt;batch_size:
---&gt; 16             loss = compute_td_loss(batch_size)
     17             sys.exit()
     18         eps = eps/(1 + 0.001)

&lt;ipython-input-111-3e1e02c32b4f&gt; in compute_td_loss(batch_size)
     16     print(&quot;loss = &quot;, loss)
     17     opt.zero_grad()
---&gt; 18     loss.backward()
     19     opt.step()
     20     return loss

c:\users\thoma\anaconda3\envs\custom_atari_env\lib\site-packages\torch\_tensor.py in backward(self, gradient, retain_graph, create_graph, inputs)
    253                 create_graph=create_graph,
    254                 inputs=inputs)
--&gt; 255         torch.autograd.backward(self, gradient, retain_graph, create_graph, inputs=inputs)
    256 
    257     def register_hook(self, hook):

c:\users\thoma\anaconda3\envs\custom_atari_env\lib\site-packages\torch\autograd\__init__.py in backward(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)
    141 
    142     grad_tensors_ = _tensor_or_tensors_to_tuple(grad_tensors, len(tensors))
--&gt; 143     grad_tensors_ = _make_grads(tensors, grad_tensors_)
    144     if retain_graph is None:
    145         retain_graph = create_graph

c:\users\thoma\anaconda3\envs\custom_atari_env\lib\site-packages\torch\autograd\__init__.py in _make_grads(outputs, grads)
     48             if out.requires_grad:
     49                 if out.numel() != 1:
---&gt; 50                     raise RuntimeError(&quot;grad can be implicitly created only for scalar outputs&quot;)
     51                 new_grads.append(torch.ones_like(out, memory_format=torch.preserve_format))
     52             else:

RuntimeError: grad can be implicitly created only for scalar outputs
</code></pre>
",11628437,,,,,2022-08-03 19:51:56,How do I make my custom loss function scalar?,<pytorch><reinforcement-learning>,1,2,,,,CC BY-SA 4.0,
2865,73225230,1,73287689,,2022-08-03 17:08:36,,0,166,"<p>I am trying to get to grips with Pytorch and I wanted to try to reproduce this code:</p>
<p><a href=""https://github.com/andy-psai/MountainCar_ActorCritic/blob/master/RL%20Blog%20FINAL%20MEDIUM%20code%2002_12_19.ipynb"" rel=""nofollow noreferrer"">https://github.com/andy-psai/MountainCar_ActorCritic/blob/master/RL%20Blog%20FINAL%20MEDIUM%20code%2002_12_19.ipynb</a></p>
<p>in Pytorch.</p>
<p>I am having a problem in that this error is being returned:</p>
<p>RuntimeError: Trying to backward through the graph a second time (or directly access saved tensors after they have already been freed). Saved intermediate values of the graph are freed when you call .backward() or autograd.grad(). Specify retain_graph=True if you need to backward through the graph a second time or if you need to access saved tensors after calling backward.</p>
<p>A similar question said to use zero_grad() again after the optimizer step, but this hasn't resolved the issue.</p>
<p>I've included the entire code below so hopefully it should be reproduceable.</p>
<p>Any advice would be much appreciated.</p>
<pre><code>import gym
import os
import os.path as osp
import time
import math
import numpy as np
import torch
import torch.nn as nn
import torch.optim as optim
import torch.nn.functional as F
from torch.distributions import Normal

env = gym.envs.make(&quot;MountainCarContinuous-v0&quot;)

# Value function

class Value(nn.Module):
    def __init__(self, dim_states):
        super(Value, self).__init__()
        self.net = nn.Sequential(
        nn.Linear(dim_states, 400),
        nn.ReLU(),
        nn.Linear(400,400),
        nn.ReLU(),
        nn.Linear(400, 1)
    )
        self.optimizer = optim.Adam(self.parameters(), lr = 1e-3)
        self.criterion = nn.MSELoss()

    def forward(self, state):
        return self.net(torch.from_numpy(state).float())

    def compute_return(self, output, target):
        self.optimizer.zero_grad()
        loss = self.criterion(output, target)
        loss.backward()
        self.optimizer.step()
        self.optimizer.zero_grad()

# Policy network

class Policy(nn.Module):
    def __init__(self, dim_states, env):
        super(Policy, self).__init__()

        self.hidden1 = nn.Linear(dim_states, 40)
        self.hidden2 = nn.Linear(40, 40)
        self.mu = nn.Linear(40, 1)
        self.sigma = nn.Linear(40,1)
        self.env = env

        self.optimizer = optim.Adam(self.parameters(), lr = 2e-5)

    def forward(self, state):
        state = torch.from_numpy(state).float()
        x = F.relu(self.hidden1(state))
        x = F.relu(self.hidden2(x))
        mu = self.mu(x)
        sigma = F.softmax(self.sigma(x), dim=-1)
        action_dist = Normal(mu, sigma)
        action_var = action_dist.rsample()
        action_var = torch.clip(action_var,
                        self.env.action_space.low[0],
                        self.env.action_space.high[0])

        return action_var, action_dist

    def compute_return(self, action, dist, td_error):
        self.optimizer.zero_grad()
        loss_actor = -dist.log_prob(action)*td_error
        loss_actor.backward()
        self.optimizer.step()
        self.optimizer.zero_grad()

# Normalise the state space
import sklearn
import sklearn.preprocessing

state_space_samples = np.array(
    [env.observation_space.sample() for x in range(10000)])
scaler = sklearn.preprocessing.StandardScaler()
scaler.fit(state_space_samples)

# Normaliser
def scale_state(state):
    scaled = scaler.transform([state])
    return scaled
##################################

# Parameters

lr_actor = 0.00002
lr_critic = 0.001

actor = Policy(2, env)
critic = Value(2)

# Training loop params
gamma = 0.99
num_episodes = 300

episode_history = []

for episode in range(num_episodes):

    # Receive initial state from E

    state = env.reset()
    reward_total = 0
    steps = 0
    done = False

    while not done:

        action, dist = actor(state)

        # print(np.squeeze(action))
        next_state, reward, done, _ = env.step(
                                        np.array([action.item()]))

        if episode % 50 == 0:
            env.render()

        steps += 1
        reward_total += reward

        # TD Target
        target = reward + gamma * np.squeeze(critic(next_state), axis=0)

        td_error = target - np.squeeze(critic(state), axis=0)

        # Update actor
        actor.compute_return(action, dist, td_error)

        # Update critic
        critic.compute_return(np.squeeze(critic(state), axis=0), target)


    episode_history.append(reward_total)

    print(f&quot;Episode: {episode}, N Steps: {steps}, Cumulative reward {reward_total}&quot;)

    if np.mean(episode_history[-100:]) &gt; 90 and len(episode_history) &gt; 101:
        print(&quot;Solved&quot;)
        print(f&quot;Mean cumulative reward over 100 episodes {np.mean(episode_history[-100:])}&quot;)
</code></pre>
",15243010,,,,,2022-08-09 07:03:16,RuntimeError: Trying to backward through the graph a second time (or directly access saved tensors after they have already been freed),<python><deep-learning><pytorch><reinforcement-learning>,1,0,,,,CC BY-SA 4.0,
2869,55552366,1,55552531,,2019-04-06 18:47:37,,0,1834,"<h2>Short Description of my model</h2>

<p>I am trying to write my own DQN algorithm in Python, using Tensorflow following the paper(<a href=""https://www.nature.com/articles/nature14236"" rel=""nofollow noreferrer"">Mnih et al., 2015</a>). In <code>train_DQN</code> function, I have defined the training procedure, and <code>DQN_CartPole</code> is for defining the function approximation(simple 3-layered Neural Network). For loss function, Huber loss or MSE is implemented followed by the gradient clipping(between -1 and 1). Then, I have implemented soft-update method instead of hard-update of the target network by copying the weights in the main network.</p>

<h2>Question</h2>

<p>I am trying it on the CartPole environment(OpenAI gym), but the rewards does not improve as it does in other people's algorithms, such as <a href=""https://github.com/keras-rl/keras-rl/blob/master/examples/dqn_cartpole.py"" rel=""nofollow noreferrer"">keras-rl</a>. Any help will be appreciated.</p>

<p><a href=""https://i.stack.imgur.com/Hlq3a.png"" rel=""nofollow noreferrer"">reward over timestep</a></p>

<p>If possible, could you have a look at the source code?</p>

<ul>
<li>DQN model: <a href=""https://github.com/Rowing0914/TF_RL/blob/master/agents/DQN_model.py"" rel=""nofollow noreferrer"">https://github.com/Rowing0914/TF_RL/blob/master/agents/DQN_model.py</a></li>
<li>Training Script: <a href=""https://github.com/Rowing0914/TF_RL/blob/master/agents/DQN_train.py"" rel=""nofollow noreferrer"">https://github.com/Rowing0914/TF_RL/blob/master/agents/DQN_train.py</a></li>
<li>Reddit post: <a href=""https://www.reddit.com/r/reinforcementlearning/comments/ba7o55/question_dqn_algorithm_does_not_work_well_on/?utm_source=share&amp;utm_medium=web2x"" rel=""nofollow noreferrer"">https://www.reddit.com/r/reinforcementlearning/comments/ba7o55/question_dqn_algorithm_does_not_work_well_on/?utm_source=share&amp;utm_medium=web2x</a></li>
</ul>

<pre class=""lang-py prettyprint-override""><code>class Parameters:
    def __init__(self, mode=None):
        assert mode != None
        print(""Loading Params for {} Environment"".format(mode))
        if mode == ""Atari"":
            self.state_reshape = (1, 84, 84, 1)
            self.num_frames = 1000000
            self.memory_size = 10000
            self.learning_start = 10000
            self.sync_freq = 1000
            self.batch_size = 32
            self.gamma = 0.99
            self.update_hard_or_soft = ""soft""
            self.soft_update_tau = 1e-2
            self.epsilon_start = 1.0
            self.epsilon_end = 0.01
            self.decay_steps = 1000
            self.prioritized_replay_alpha = 0.6
            self.prioritized_replay_beta_start = 0.4
            self.prioritized_replay_beta_end = 1.0
            self.prioritized_replay_noise = 1e-6
        elif mode == ""CartPole"":
            self.state_reshape = (1, 4)
            self.num_frames = 10000
            self.memory_size = 20000
            self.learning_start = 100
            self.sync_freq = 100
            self.batch_size = 32
            self.gamma = 0.99
            self.update_hard_or_soft = ""soft""
            self.soft_update_tau = 1e-2
            self.epsilon_start = 1.0
            self.epsilon_end = 0.01
            self.decay_steps = 500
            self.prioritized_replay_alpha = 0.6
            self.prioritized_replay_beta_start = 0.4
            self.prioritized_replay_beta_end = 1.0
            self.prioritized_replay_noise = 1e-6


class _DQN:
    """"""
    Boilerplate for DQN Agent
    """"""

    def __init__(self):
        """"""
        define the deep learning model here!

        """"""
        pass

    def predict(self, sess, state):
        """"""
        predict q-values given a state

        :param sess:
        :param state:
        :return:
        """"""
        return sess.run(self.pred, feed_dict={self.state: state})

    def update(self, sess, state, action, Y):
        feed_dict = {self.state: state, self.action: action, self.Y: Y}
        _, loss = sess.run([self.train_op, self.loss], feed_dict=feed_dict)
        # print(action, Y, sess.run(self.idx_flattened, feed_dict=feed_dict))
        return loss


class DQN_CartPole(_DQN):
    """"""
    DQN Agent for CartPole game
    """"""

    def __init__(self, scope, env, loss_fn =""MSE""):
        self.scope = scope
        self.num_action = env.action_space.n
        with tf.variable_scope(scope):
            self.state = tf.placeholder(shape=[None, 4], dtype=tf.float32, name=""X"")
            self.Y = tf.placeholder(shape=[None], dtype=tf.float32, name=""Y"")
            self.action = tf.placeholder(shape=[None], dtype=tf.int32, name=""action"")

            fc1 = tf.keras.layers.Dense(16, activation=tf.nn.relu)(self.state)
            fc2 = tf.keras.layers.Dense(16, activation=tf.nn.relu)(fc1)
            fc3 = tf.keras.layers.Dense(16, activation=tf.nn.relu)(fc2)
            self.pred = tf.keras.layers.Dense(self.num_action, activation=tf.nn.relu)(fc3)

            # indices of the executed actions
            self.idx_flattened = tf.range(0, tf.shape(self.pred)[0]) * tf.shape(self.pred)[1] + self.action

            # passing [-1] to tf.reshape means flatten the array
            # using tf.gather, associate Q-values with the executed actions
            self.action_probs = tf.gather(tf.reshape(self.pred, [-1]), self.idx_flattened)

            if loss_fn == ""huber_loss"":
                # use huber loss
                self.losses = tf.subtract(self.Y, self.action_probs)
                self.loss = huber_loss(self.losses)
            elif loss_fn == ""MSE"":
                # use MSE
                self.losses = tf.squared_difference(self.Y, self.action_probs)
                self.loss = tf.reduce_mean(self.losses)
            else:
                assert False

            # you can choose whatever you want for the optimiser
            # self.optimizer = tf.train.RMSPropOptimizer(0.00025, 0.99, 0.0, 1e-6)
            self.optimizer = tf.train.AdamOptimizer()

            # to apply Gradient Clipping, we have to directly operate on the optimiser
            # check this: https://www.tensorflow.org/api_docs/python/tf/train/Optimizer#processing_gradients_before_applying_them
            self.grads_and_vars = self.optimizer.compute_gradients(self.loss)
            self.clipped_grads_and_vars = [(ClipIfNotNone(grad, -1., 1.), var) for grad, var in self.grads_and_vars]
            self.train_op = self.optimizer.apply_gradients(self.clipped_grads_and_vars)



def train_DQN(main_model, target_model, env, replay_buffer, policy, params):
    """"""
    Train DQN agent which defined above

    :param main_model:
    :param target_model:
    :param env:
    :param params:
    :return:
    """"""

    # log purpose
    losses, all_rewards, cnt_action = [], [], []
    episode_reward, index_episode = 0, 0

    with tf.Session() as sess:
        # initialise all variables used in the model
        sess.run(tf.global_variables_initializer())
        state = env.reset()
        start = time.time()
        for frame_idx in range(1, params.num_frames + 1):
            action = policy.select_action(sess, target_model, state.reshape(params.state_reshape))
            cnt_action.append(action)
            next_state, reward, done, _ = env.step(action)
            replay_buffer.add(state, action, reward, next_state, done)

            state = next_state
            episode_reward += reward

            if done:
                index_episode += 1
                state = env.reset()
                all_rewards.append(episode_reward)

                if frame_idx &gt; params.learning_start and len(replay_buffer) &gt; params.batch_size:
                    states, actions, rewards, next_states, dones = replay_buffer.sample(params.batch_size)
                    next_Q = target_model.predict(sess, next_states)
                    Y = rewards + params.gamma * np.max(next_Q, axis=1) * np.logical_not(dones)
                    loss = main_model.update(sess, states, actions, Y)

                    # Logging and refreshing log purpose values
                    losses.append(np.mean(loss))

                    logging(frame_idx, params.num_frames, index_episode, time.time()-start, episode_reward, np.mean(loss), cnt_action)

                episode_reward = 0
                cnt_action = []
                start = time.time()

            if frame_idx &gt; params.learning_start and frame_idx % params.sync_freq == 0:
                # soft update means we partially add the original weights of target model instead of completely
                # sharing the weights among main and target models
                if params.update_hard_or_soft == ""hard"":
                    sync_main_target(sess, main_model, target_model)
                elif params.update_hard_or_soft == ""soft"":
                    soft_target_model_update(sess, main_model, target_model, tau=params.soft_update_tau)


    return all_rewards, losses

</code></pre>

<h2>Modification</h2>

<ul>
<li>dones -> <code>np.logical_not(dones)</code>  </li>
<li>np.argmax -> np.max  </li>
<li>separating MSE from huber_loss</li>
</ul>
",9246727,,9246727,,2019-04-06 19:56:11,2019-04-06 19:56:11,DQN algorithm does not converge on CartPole-v0,<python><tensorflow><reinforcement-learning>,1,0,,,,CC BY-SA 4.0,
2875,55579331,1,71902207,,2019-04-08 18:07:45,,1,320,"<p>I have a Monte Carlo Tree Search implementation that I need to optimize. So I thought about parallelizing the rollout phase. How to do that? (Is there a code example).
Are there any python modules etc that you would recommend?</p>

<p>I apologize if this isn't the right place to post this.</p>
",10168218,,,,,2022-04-21 13:58:31,Parallelizing Monte Carlo Tree Search,<python><python-3.x><parallel-processing><reinforcement-learning><monte-carlo-tree-search>,1,2,,,,CC BY-SA 4.0,
2877,70015203,1,70027652,,2021-11-18 06:00:14,,2,95,"<p>I am trying to use reinforcement learning in julia to teach a car that is constantly being accelerated backwards (but with a positive initial velocity) to apply brakes so that it gets as close to a target distance as possible before moving backwards.</p>
<p>To do this, I am making use of <code>POMDPs.jl</code> and <code>crux.jl</code> which has many solvers (I'm using DQN). I will list what I believe to be the relevant parts of the script first, and then more of it towards the end.</p>
<p>To define the MDP, I set the initial position, velocity, and force from the brakes as a uniform distribution over some values.</p>
<pre><code>@with_kw struct SliderMDP &lt;: MDP{Array{Float32}, Array{Float32}}
        x0 = Distributions.Uniform(0., 80.)# Distribution to sample initial position
        v0 = Distributions.Uniform(0., 25.) # Distribution to sample initial velocity
        d0 = Distributions.Uniform(0., 2.) # Distribution to sample brake force
        ...
end
</code></pre>
<p>My state holds the values of <code>(position, velocity, brake force)</code>, and the initial state is given as:</p>
<pre><code>function POMDPs.initialstate(mdp::SliderMDP)
    ImplicitDistribution((rng) -&gt; Float32.([rand(rng, mdp.x0), rand(rng, mdp.v0), rand(rng, mdp.d0)]))
end
</code></pre>
<p>Then, I set up my DQN solver using <code>crux.jl</code> and called a function to solve for the policy</p>
<pre><code>solver_dqn = DQN(Ï€=Q_network(), S=s, N=30000)
policy_dqn = solve(solver_dqn, mdp)
</code></pre>
<p>calling <code>solve()</code> gives me the error <code>MethodError: no method matching logpdf(::Distributions.Categorical{Float64, Vector{Float64}}, ::Nothing)</code>. I am quite sure that this comes from the initial state sampling, but I am not sure why or how to fix it. I have only been learning RL from various books and online lectures for a very short time, so any help regarding the error or my the model I set up (or anything else I'm oblivious to) would be appreciated.</p>
<hr />
<p>More comprehensive code:</p>
<p>Packages:</p>
<pre><code>using POMDPs
using POMDPModelTools
using POMDPPolicies
using POMDPSimulators

using Parameters
using Random

using Crux
using Flux

using Distributions
</code></pre>
<p>Rest of it:</p>
<pre><code>@with_kw struct SliderMDP &lt;: MDP{Array{Float32}, Array{Float32}}
    x0 = Distributions.Uniform(0., 80.)# Distribution to sample initial position
    v0 = Distributions.Uniform(0., 25.) # Distribution to sample initial velocity
    d0 = Distributions.Uniform(0., 2.) # Distribution to sample brake force
    
    m::Float64 = 1.
    tension::Float64 = 3.
    dmax::Float64 = 2.
    target::Float64 = 80.
    dt::Float64 = .05
    
    Î³::Float32 = 1.
    actions::Vector{Float64} = [-.1, 0., .1]
end
    
function POMDPs.gen(env::SliderMDP, s, a, rng::AbstractRNG = Random.GLOBAL_RNG)
    x, xÌ‡, d = s

    if x &gt;= env.target
        a = .1
    end
    if d+a &gt;= env.dmax || d+a &lt;= 0
        a = 0.
    end
    
    force = (d + env.tension) * -1
    xÌˆ = force/env.m
    
    # Simulation
    x_ = x + env.dt * xÌ‡
    xÌ‡_ = xÌ‡ + env.dt * xÌˆ
    d_ = d + a

    sp = vcat(x_, xÌ‡_, d_)
    reward = abs(env.target - x) * -1
        
    return (sp=sp, r=reward)
end

    

function POMDPs.initialstate(mdp::SliderMDP)
    ImplicitDistribution((rng) -&gt; Float32.([rand(rng, mdp.x0), rand(rng, mdp.v0), rand(rng, mdp.d0)]))
end
    
POMDPs.isterminal(mdp::SliderMDP, s) = s[2] &lt;= 0
POMDPs.discount(mdp::SliderMDP) = mdp.Î³

mdp = SliderMDP();
s = state_space(mdp); # Using Crux.jl

function Q_network()
    layer1 = Dense(3, 64, relu)
    layer2 = Dense(64, 64, relu)
    layer3 = Dense(64, length(3))
    return DiscreteNetwork(Chain(layer1, layer2, layer3), [-.1, 0, .1])
end

solver_dqn = DQN(Ï€=Q_network(), S=s, N=30000) # Using Crux.jl
policy_dqn = solve(solver_dqn, mdp) # Error comes here

</code></pre>
<p>Stacktrace:</p>
<pre><code>policy_dqn
MethodError: no method matching logpdf(::Distributions.Categorical{Float64, Vector{Float64}}, ::Nothing)

Closest candidates are:

logpdf(::Distributions.DiscreteNonParametric, !Matched::Real) at C:\Users\name\.julia\packages\Distributions\Xrm9e\src\univariate\discrete\discretenonparametric.jl:106

logpdf(::Distributions.UnivariateDistribution{S} where S&lt;:Distributions.ValueSupport, !Matched::AbstractArray) at deprecated.jl:70

logpdf(!Matched::POMDPPolicies.PlaybackPolicy, ::Any) at C:\Users\name\.julia\packages\POMDPPolicies\wMOK3\src\playback.jl:34

...

logpdf(::Crux.ObjectCategorical, ::Float32)@utils.jl:16
logpdf(::Crux.DistributionPolicy, ::Vector{Float64}, ::Float32)@policies.jl:305
var&quot;#exploration#133&quot;(::Base.Iterators.Pairs{Union{}, Union{}, Tuple{}, NamedTuple{(), Tuple{}}}, ::typeof(Crux.exploration), ::Crux.DistributionPolicy, ::Vector{Float64})@policies.jl:302
exploration@policies.jl:297[inlined]
action(::Crux.DistributionPolicy, ::Vector{Float64})@policies.jl:294
var&quot;#exploration#136&quot;(::Crux.DiscreteNetwork, ::Int64, ::typeof(Crux.exploration), ::Crux.MixedPolicy, ::Vector{Float64})@policies.jl:326
var&quot;#step!#173&quot;(::Bool, ::Int64, ::typeof(Crux.step!), ::Dict{Symbol, Array}, ::Int64, ::Crux.Sampler{Main.workspace#2.SliderMDP, Vector{Float32}, Crux.DiscreteNetwork, Crux.ContinuousSpace{Tuple{Int64}}, Crux.DiscreteSpace})@sampler.jl:55
var&quot;#steps!#174&quot;(::Int64, ::Bool, ::Int64, ::Bool, ::Bool, ::Bool, ::typeof(Crux.steps!), ::Crux.Sampler{Main.workspace#2.SliderMDP, Vector{Float32}, Crux.DiscreteNetwork, Crux.ContinuousSpace{Tuple{Int64}}, Crux.DiscreteSpace})@sampler.jl:108
var&quot;#fillto!#177&quot;(::Int64, ::Bool, ::typeof(Crux.fillto!), ::Crux.ExperienceBuffer{Array}, ::Crux.Sampler{Main.workspace#2.SliderMDP, Vector{Float32}, Crux.DiscreteNetwork, Crux.ContinuousSpace{Tuple{Int64}}, Crux.DiscreteSpace}, ::Int64)@sampler.jl:156
solve(::Crux.OffPolicySolver, ::Main.workspace#2.SliderMDP)@off_policy.jl:86
top-level scope@Local: 1[inlined]
</code></pre>
",10382968,,10382968,,2021-11-18 22:42:36,2021-11-18 23:01:39,no method matching logpdf when sampling from uniform distribution,<machine-learning><julia><distribution><reinforcement-learning><markov-decision-process>,1,2,,,,CC BY-SA 4.0,
2879,69927786,1,69933970,,2021-11-11 11:47:51,,-1,36,"<p>I'm studying CS231N, lecture 14, &quot;Reinforcement Learning&quot;. In the lecture, the instructor mentioned the value function, which is shown in the picture:</p>
<p><img src=""https://i.stack.imgur.com/cwEh2.png"" alt=""picture of value function"" /></p>
<p>I am wondering what is that bar between <code>rt</code> and <code>s0</code>? I thought it was something like conditional probability, but I'm not sure about it. Or is it just a division?</p>
",15119732,,3025856,,2021-11-12 00:20:23,2021-11-12 00:22:18,cs231n lec 14 reinforcement learning,<reinforcement-learning><cs231n>,1,1,,,,CC BY-SA 4.0,
2882,55657268,1,55660613,,2019-04-12 17:57:29,,1,54,"<p>I'm reading a book, ""AI for Game Developers"" by Glenn Seemann and David M Bourg,  where they use video game AI as an example of a rule-based system which learns.</p>

<p>Essentially, the player has 3 possible moves, and hits in combos of three strikes. The AI is aiming to predict the player's third strikes. The rules of the system are all the possible 3-move combinations. Each rule has a ""weight"" associated to it. Every time the system guesses incorrectly, the weight of a rule is decreased. When the system has to pick a rule, it picks the rule with the highest weight. </p>

<p>How is this any different from a reinforcement-learning based system? Thanks!  </p>
",9565234,,9565234,,2019-04-12 18:07:59,2019-04-12 23:27:45,Is a rule-based system that learns considered reinforcement learning?,<artificial-intelligence><reinforcement-learning>,1,0,,,,CC BY-SA 4.0,
2885,55747714,1,55747943,,2019-04-18 14:02:51,,0,49,"<p>I am a beginner in the neuronal network field and I want to understand a certain statement. A friend said that a neuronal network gets slower after you fit a lot of data in.</p>

<p>Right now, I just did the coursera ML course from androw ng. There, I implemented backpropagation. I thought it just adaptes the model related to the expected output by using different types of calculations. Nevertheless, it was not like the history was used to adapt the model. Just the current state of the neurons were checked and their weight were adapted backwards in combination with regularisation.</p>

<p>Is my assumption correct or am I wrong? Are there some libraries that use history data that could result in a slowly adapting model after a certain amount of training? </p>

<p>I want to use a simple neuronal network for reinforcement learning and I want to get an idea if I need to reset my model if the target environment changes for some reason. Otherwise my model would be slower and slower in adaption after time.</p>

<p>Thanks for any links and explanations in advanced!</p>
",1387080,,,,,2019-04-18 14:15:17,Do Neuronal networks getting slow in adaption after a lot of training?,<machine-learning><deep-learning><reinforcement-learning>,1,1,,,,CC BY-SA 4.0,
2886,55741358,1,55741687,,2019-04-18 07:46:14,,-3,180,"<p>Requirement: I need to perform a task T, N times, within a game that is played for 500 rounds. </p>

<p>I have a loop that runs certain game related tasks 500 times. Within this, I would like to execute task T randomly, N times. Also, N&lt;500.</p>

<p>How does one achieve this?</p>

<p>I know how to execute T within a loop, N times. But, I would like to randomize and execute it N times within 500 rounds of the game.</p>
",9003184,,,,,2019-04-18 08:08:36,"How to execute a task randomly N times, within a loop that runs M times?",<python><reinforcement-learning>,3,2,,,,CC BY-SA 4.0,
2894,70163823,1,70164895,,2021-11-30 03:08:42,,3,334,"<p>I'm learning about policy gradients and I'm having hard time understanding how does the gradient passes through a random operation. From <a href=""https://pytorch.org/docs/stable/distributions.html"" rel=""nofollow noreferrer"">here</a>: <code>It is not possible to directly backpropagate through random samples. However, there are two main methods for creating surrogate functions that can be backpropagated through</code>.</p>
<p>They have an example of the <code>score function</code>:</p>
<pre><code>probs = policy_network(state)
# Note that this is equivalent to what used to be called multinomial
m = Categorical(probs)
action = m.sample()
next_state, reward = env.step(action)
loss = -m.log_prob(action) * reward
loss.backward()
</code></pre>
<p>Which I tried to create an example of:</p>
<pre><code>import torch
import torch.nn as nn
import torch.optim as optim
from torch.distributions import Normal
import matplotlib.pyplot as plt
from tqdm import tqdm

softplus = torch.nn.Softplus()

class Model_RL(nn.Module):
    def __init__(self):
        super(Model_RL, self).__init__()
        self.fc1 = nn.Linear(1, 20)
        self.fc2 = nn.Linear(20, 30)
        self.fc3 = nn.Linear(30, 2)

    def forward(self, x):
        x1 = self.fc1(x)
        x = torch.relu(x1)
        x2 = self.fc2(x)
        x = torch.relu(x2)
        x3 = softplus(self.fc3(x))
        return x3, x2, x1

# basic 

net_RL = Model_RL()

features = torch.tensor([1.0]) 
x = torch.tensor([1.0]) 
y = torch.tensor(3.0)

baseline = 0
baseline_lr = 0.1

epochs = 3

opt_RL = optim.Adam(net_RL.parameters(), lr=1e-3)
losses = []
xs = []
for _ in tqdm(range(epochs)):
    out_RL = net_RL(x)
    mu, std = out_RL[0]
    dist = Normal(mu, std)
    print(dist)
    a = dist.sample()
    log_p = dist.log_prob(a)
    
    out = features * a
    reward = -torch.square((y - out))
    baseline = (1-baseline_lr)*baseline + baseline_lr*reward
    
    loss = -(reward-baseline)*log_p

    opt_RL.zero_grad()
    loss.backward()
    opt_RL.step()
    losses.append(loss.item())
</code></pre>
<p>This seems to work magically fine which again, I don't understand how the gradient passes through as they mentioned that it can't pass through the random operation (but then somehow it does).</p>
<p>Now since the gradient can't flow through the random operation I tried to replace
<code>mu, std = out_RL[0]</code> with <code>mu, std = out_RL[0].detach()</code> and that caused the error:
<code>RuntimeError: element 0 of tensors does not require grad and does not have a grad_fn</code>. If the gradient doesn't pass through the random operation, I don't understand why would detaching a tensor before the operation matter.</p>
",14735451,,,,,2022-03-25 03:06:40,How does a gradient backpropagates through random samples?,<python><pytorch><reinforcement-learning><backpropagation>,2,1,0,,,CC BY-SA 4.0,
2896,70171503,1,70226361,,2021-11-30 15:02:23,,1,117,"<p>In &lt;Lecture 2: Markov Decision Processes&gt; by David Silver on page 19, it has the following Derived formula:
<a href=""https://i.stack.imgur.com/tBEIu.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/tBEIu.png"" alt=""v(s) equation"" /></a></p>
<p>I found <a href=""https://i.stack.imgur.com/NEnlCt.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/NEnlCt.png"" alt=""enter image description here"" /></a> is equal to <a href=""https://i.stack.imgur.com/6zq32t.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/6zq32t.png"" alt=""enter image description here"" /></a> which means Gt+1 = v(St+1) <strong>so Gt = v(St)</strong>.</p>
<p>According to Return Defination:</p>
<p><a href=""https://i.stack.imgur.com/wAx9Fm.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/wAx9Fm.png"" alt=""enter image description here"" /></a></p>
<p>and according to Gt = v(St):</p>
<p>v(St) = Gt = <a href=""https://i.stack.imgur.com/PctKum.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/PctKum.png"" alt=""enter image description here"" /></a></p>
<p>But the defination of Value Function is</p>
<p><a href=""https://i.stack.imgur.com/ZlqLZt.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/ZlqLZt.png"" alt=""enter image description here"" /></a></p>
<p>which means
v(s) = <a href=""https://i.stack.imgur.com/65hsHt.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/65hsHt.png"" alt=""enter image description here"" /></a> = <a href=""https://i.stack.imgur.com/ATP4xt.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/ATP4xt.png"" alt=""enter image description here"" /></a>
which is absolutly wrong.</p>
<p>My question are:</p>
<ol>
<li>Why Gt+1 = v(St+1)?</li>
<li>Where are my derivation mistakes?</li>
</ol>
",8647945,,17562044,,2022-08-08 00:10:08,2022-08-08 00:10:08,Why Gt+1 = v(St+1) in Bellman Equation for MRPs?,<reinforcement-learning><markov-chains><markov>,1,0,0,,,CC BY-SA 4.0,
2897,70227150,1,70249243,,2021-12-04 15:52:18,,0,192,"<p>I am trying to build a contextual bandit. Since I like to rank the actions, I want to switch to an conditional contextual bandit (as I have read <a href=""https://stackoverflow.com/questions/63635815/how-to-learn-to-rank-using-vowpal-wabbits-contextual-bandit"">here</a>).</p>
<p>But now I have trouble understanding the new vw format.</p>
<p>The example of the <a href=""https://github.com/VowpalWabbit/vowpal_wabbit/wiki/Conditional-Contextual-Bandit"" rel=""nofollow noreferrer"">vowpal wabbit wiki</a> is this:</p>
<pre><code>ccb shared | s_1 s_2
ccb action | a:1 b:1 c:1
ccb action | a:0.5 b:2 c:1
ccb action | a:0.5 
ccb action | c:1
ccb slot  | d:4
ccb slot 1:0.8:0.8,0:0.2 0,1,3 | d:7
</code></pre>
<p>Unfortunatly I do not underhstand the Slot part.
I got that this tells the cost and probability for the chosen action.
<code>ccb slot 1:0.8:0.8,0:0.2 0,1,3</code>
Is it possible to have more than one chosen action?</p>
<p>I also do not understand why it needs features for the slot part? Furthermore i do not fully understand why we have to tell it the action ids to include? What is the purpose of it?
Also which format does it need for the prediction? Why does it need the slot part if I do not have any action costs yet?</p>
<p>Edit: I looked into the <a href=""https://learn.microsoft.com/en-us/azure/cognitive-services/personalizer/concept-multi-slot-personalization#when-to-use-multi-slot-personalization"" rel=""nofollow noreferrer"">azure docs</a> since Vowpal Wabbit has been developed by MS Research. I think I found useful information therr. As soon as I have found the answers, I will post them here.</p>
",17389233,,4685471,,2021-12-08 00:15:43,2021-12-08 00:15:43,How to understand the slots in the vw.format - Vowpal Wabbit Conditional Contextual Bandit,<machine-learning><reinforcement-learning><vowpalwabbit>,1,1,0,,,CC BY-SA 4.0,
2901,70282991,1,70283623,,2021-12-08 23:32:16,,1,325,"<p>I am trying to use pre-released versions of packages on Unity.
I followed some tutorials and guides, and from my understanding, I should check the &quot;Enable pre-release packages&quot; Check-box as I did here:</p>
<p><img src=""https://i.stack.imgur.com/EkCG6.png"" alt=""Here"" /></p>
<p>But still, when I go to the package manager, I don't see any newer version.</p>
<p><img src=""https://i.stack.imgur.com/m33sZ.png"" alt=""Here is what I see after checking the check-box"" /></p>
<p>How do I fix it? I want to use the OnActionReceived(ActionBuffers actions) function, and on my current version, I cant use it.</p>
<p>I'm using the Unity version 2021.1.19f1</p>
<p>Thanks!</p>
",16980627,,361899,,2021-12-09 01:02:30,2021-12-09 01:13:59,Can't find pre-released versions of ML-Agents in Unity,<unity3d><reinforcement-learning><ml-agent>,1,0,,,,CC BY-SA 4.0,
2903,70314837,1,70314954,,2021-12-11 11:42:26,,1,1120,"<p>I am trying to perform a learning model with openai gym, tensorflow and keras.</p>
<p>I build my model with this method:</p>
<pre><code>def build_model(states, actions):
model = Sequential()
model.add(Dense(24, activation='relu', input_shape=states))
model.add(Dense(24, activation='relu'))
model.add(Dense(actions, activation='linear'))
return model


model = build_model(states, actions)
</code></pre>
<p>After, i build my agent:</p>
<pre><code>def build_agent(model, actions):
policy = BoltzmannQPolicy()
memory = SequentialMemory(limit=50000, window_length=1)
dqn = DQNAgent(model=model, memory=memory, policy=policy,
              nb_actions=actions, nb_steps_warmup=10, target_model_update=1e-2)
return dqn
</code></pre>
<p>When i call training:</p>
<pre><code>dqn = build_agent(model, actions)
dqn.compile(Adam(lr=1e-3), metrics=['mae'])
dqn.fit(env, nb_steps=50000, visualize=True, verbose=1)
</code></pre>
<p>I get this error, that is that the session is not initialized or there is no dense_24/bias variable</p>
<pre><code>---------------------------------------------------------------------------

FailedPreconditionError                   Traceback (most recent call last)

 &lt;ipython-input-112-7ebc8b726721&gt; in &lt;module&gt;()
   2 dqn.compile(Adam(lr=1e-3), metrics=['mae'])
   3 
--&gt;4 dqn.fit(env, nb_steps=50000, visualize=True, verbose=1)
   5 

   4 frames

 /usr/local/lib/python3.7/dist-packages/tensorflow/python/client/session.py in __call__(self, *args, **kwargs)
   1483       # called before this destructor, in which case `self._session._session`
   1484       # will be `None`.
-&gt; 1485       if (self._handle is not None and self._session._session is not None and
   1486           not self._session._closed):
   1487         tf_session.TF_SessionReleaseCallable(self._session._session,

   FailedPreconditionError: Could not find variable dense_24/bias. This could mean that the variable has been deleted. In TF1, it can also mean the variable is uninitialized. Debug info: container=localhost, status error message=Resource localhost/dense_24/bias/N10tensorflow3VarE does not exist.
 [[{{node dense_24/BiasAdd/ReadVariableOp}}]]
</code></pre>
<p>Who can help me solve the problem?</p>
",15959884,,,,,2021-12-11 11:59:55,Tensorflow-FailedPreconditionError: Could not find variable dense_24/bias. This could mean that the variable has been deleted,<python><tensorflow><deep-learning><neural-network><reinforcement-learning>,1,0,,,,CC BY-SA 4.0,
2905,70309923,1,70324535,,2021-12-10 20:14:35,,1,36,"<p>I have a deep reinforcement learning agent that interacts with a customized environment and I am displaying the reward value every episode using tensorboard.
The curve looks like this</p>
<p><a href=""https://i.stack.imgur.com/Xn1n0.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/Xn1n0.png"" alt=""Curve"" /></a></p>
<p>For some reason it jumps to step 80 after step 17 every time and I cannot understand why, I don't even know what part of the code I should copy paste here.</p>
<p>Anyone has any idea why it does that ?</p>
",8163377,,,,,2021-12-12 14:38:40,Weird-looking curve in DRL,<tensorflow><deep-learning><tensorflow2.0><reinforcement-learning><tensorboard>,1,0,,,,CC BY-SA 4.0,
2912,70432613,1,70492578,,2021-12-21 08:40:30,,0,41,"<p>I'm trying to do multi-agent reinforcement learning on the grid world navigation task where multiple agents try to collectively reach multiple goals while avoiding collisions with stationary obstacles and each other. As a constraint, each agent can only see within a limited range around itself.</p>
<p>So on a high level, the state of each agent should contain both information to help it avoid collision and information to guide it towards the goals. I'm thinking of implementing the former by including into the agent's state a matrix consisted of the grid cells surrounding the agent, which would show the agent where the obstacles are. However, I'm not sure how to include goal navigation information on top of this matrix. Currently I just flatten the matrix and append all relative goal locations at the end, and use this as the state.</p>
<p>For example, for a grid world as shown below (0 means empty cell, 1 means agents, 2 means obstacles, and 3 represents goals):</p>
<pre><code>[[0 0 0 0 0 0 2 2 0 0]
 [0 0 0 0 0 0 0 0 0 0]
 [0 0 2 2 0 0 0 0 0 0]
 [0 3 2 2 0 0 0 0 0 2]
 [0 0 0 0 0 0 0 0 0 2]
 [0 0 0 0 1 0 0 0 0 2]
 [2 0 0 0 0 2 2 0 3 0]
 [2 0 0 0 0 2 2 0 0 0]
 [0 0 0 0 0 0 0 0 0 0]
 [0 0 2 0 0 1 0 0 0 0]]
</code></pre>
<p>The agent at row5 col4 sees the following cells that are within distance1 around it:</p>
<pre><code>[[0. 0. 0.]
 [0. 1. 0.]
 [0. 0. 2.]]
</code></pre>
<p>flattened, the matrix becomes:</p>
<pre><code>[0,0,0,0,1,0,0,0,2]
</code></pre>
<p>The location of the goal at row3 col1 relative to the aforementioned agent is (5-3=2, 4-1=3)</p>
<p>The location of the goal at row6 col8 relative to the aforementioned agent is (5-6=-1, 4-8=-4)</p>
<p>So after appending the relative locations, the state of the agent becomes:</p>
<pre><code>[0,0,0,0,1,0,0,0,2,2,3,-1,-4]
</code></pre>
<p>(Similar process for the other agent)</p>
<p>Is this a reasonable way of designing the state? My primary concern is that RL might not be able to tell the difference between the flattened matrix and the relative distances. If my concerns are founded, could you give some suggestions on how I should design the state?</p>
<p>Thanks in advance!</p>
<p>Edit: To validate my concern, I trained an agent using PG REINFORCE algorithm. As I feared, the agent learned to avoid obstacles but otherwise just moved randomly without navigating towards the goals.</p>
",17730291,,17730291,,2021-12-27 07:11:04,2021-12-27 07:11:04,How to mix grid matrix and explicit values when designing RL state?,<reinforcement-learning>,1,0,,,,CC BY-SA 4.0,
2913,70458347,1,70461278,,2021-12-23 06:18:55,,2,55,"<p>I followed a PyTorch tutorial to learn reinforcement learning(<a href=""https://pytorch.org/tutorials/intermediate/mario_rl_tutorial.html"" rel=""nofollow noreferrer"">TRAIN A MARIO-PLAYING RL AGENT</a>) but I am confused about the following code:</p>
<pre><code>current_Q = self.net(state, model=&quot;online&quot;)[np.arange(0, self.batch_size), action] # Q_online(s,a)
</code></pre>
<p>What's the purpose of [np.arange(0, self.batch_size), action] after the neural network?(I know that TD_estimate takes in state and action, just confused about this on the programming side) What is this usage(put a list after self.net)?</p>
<p>More related code referenced from the tutorial:</p>
<pre><code>class MarioNet(nn.Module):

def __init__(self, input_dim, output_dim):
    super().__init__()
    c, h, w = input_dim

    if h != 84:
        raise ValueError(f&quot;Expecting input height: 84, got: {h}&quot;)
    if w != 84:
        raise ValueError(f&quot;Expecting input width: 84, got: {w}&quot;)

    self.online = nn.Sequential(
        nn.Conv2d(in_channels=c, out_channels=32, kernel_size=8, stride=4),
        nn.ReLU(),
        nn.Conv2d(in_channels=32, out_channels=64, kernel_size=4, stride=2),
        nn.ReLU(),
        nn.Conv2d(in_channels=64, out_channels=64, kernel_size=3, stride=1),
        nn.ReLU(),
        nn.Flatten(),
        nn.Linear(3136, 512),
        nn.ReLU(),
        nn.Linear(512, output_dim),
    )

    self.target = copy.deepcopy(self.online)

    # Q_target parameters are frozen.
    for p in self.target.parameters():
        p.requires_grad = False

def forward(self, input, model):
    if model == &quot;online&quot;:
        return self.online(input)
    elif model == &quot;target&quot;:
        return self.target(input)
</code></pre>
<p>self.net:</p>
<pre><code>self.net = MarioNet(self.state_dim, self.action_dim).float()
</code></pre>
<p>Thanks for any help!</p>
",,user14895999,,,,2021-12-23 11:07:56,"What is the purpose of [np.arange(0, self.batch_size), action] after the neural network?",<python><pytorch><reinforcement-learning>,1,0,,,,CC BY-SA 4.0,
2920,71549439,1,71615541,,2022-03-20 18:17:52,,3,843,"<p>I'm having a hard time wrapping my head around what and when vectorized environments should be used. If you can provide an example of a use case, that would be great.</p>
<p>Documentation of vectorized environments in SB3:
<a href=""https://stable-baselines3.readthedocs.io/en/master/guide/vec_envs.html"" rel=""nofollow noreferrer"">https://stable-baselines3.readthedocs.io/en/master/guide/vec_envs.html</a></p>
",10664588,,10664588,,2022-03-20 19:15:28,2022-03-25 10:37:01,What are vectorized environments in reinforcement learning?,<reinforcement-learning>,1,0,,,,CC BY-SA 4.0,
2928,71671716,1,71961729,,2022-03-30 04:56:06,,2,209,"<p>I am using tf-agents library to build a contextual bandit.
For this I am building a custom environment.<br />
I am creating a banditpyenvironment and wrapping it in the TFpyenvironment.</p>
<p>The tfpyenvironment automatically adds the batch size dimension (in observation spec). I need to account for this batch size dimension in the _observe and _apply_Action methods. Since depending on  the batch size, I should provide the required (batch size) number of observations (for observe) and also as per batch size, I should take in batch size number of actions and should provide the rewards(for apply action).</p>
<p>I am unable to find a single example on how to tell the tfenvironment what the batch size, without letting automatically add a 1 to the first dimension. Can someone please clarify</p>
<pre><code> def __init__(self, batch_size):

    self.batchsize=batch_size
    observation_spec = BoundedTensorSpec(
    (2,), np.int32, minimum=[1,1], maximum=[5,2], name= 'observation')
    action_spec = BoundedTensorSpec(
        shape=(), dtype=np.int32, minimum=0, maximum=6, name='action')


    super(SampleEnvironment, self).__init__(observation_spec, action_spec)

  def _observe(self):
    batch=[]
    for i in range(self.batchsize):
        each=tf.cast(np.array([np.random.choice([1,2,3,4,5]),np.random.choice([1,2])]), 'int32')
        batch.append(each)
    self.observation=np.array(batch)
    print(&quot;in observe&quot;,self.observation)
    return np.array(self.observation)
</code></pre>
<p>When I try to somehow account for the batchsize in the observe method like above (using a for loop for the batch size), the tfenvironment is again adding 1 to the first dimension as batchsize.
Is there a way to automatically tell the environment that the batch is say 3, instead of it automatically adding 1. At the same time, how would I account for this batch size in replay buffer and agents</p>
",5394072,,5394072,,2022-03-30 05:13:40,2022-04-21 23:00:29,How to pass the batchsize for a custom environment in Tf-agents,<python><tensorflow><reinforcement-learning><tf-agent>,1,0,,,,CC BY-SA 4.0,
2929,71584763,1,71611386,,2022-03-23 09:28:45,,0,546,"<p>I am trainig a reinforcement learning model on google colab using <code>tune</code> and <code>rllib</code>.
At first I was able to show the training results useing tensorboard but it is no longer working and I can't seem to find where it comes from, I didn't change anything so I feel a bit lost here.</p>
<p>What it shows (the directory is the right one) :</p>
<p><a href=""https://i.stack.imgur.com/FVMgI.jpg"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/FVMgI.jpg"" alt="""" /></a>My current directory :</p>
<p><a href=""https://i.stack.imgur.com/rcUBx.jpg"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/rcUBx.jpg"" alt="""" /></a></p>
<p>The training phase:</p>
<pre><code>ray.init(ignore_reinit_error=True)

tune.run(&quot;PPO&quot;,
         config = {&quot;env&quot; : CustomEnv2,
                  #  &quot;evaluation_interval&quot; : 2,
                  #  &quot;evaluation_num_episodes&quot; : 2,
                   &quot;num_workers&quot; :1},
         num_samples=1,
        #  checkpoint_at_end=True,
         stop={&quot;training_iteration&quot;: 10},
         local_dir = './test1')
</code></pre>
<p>Plotting results:</p>
<pre><code>%load_ext tensorboard 

%tensorboard --logdir='/content/test1/PPO/PPO_CustomEnv2_024da_00000_0_2022-03-23_09-02-47'
</code></pre>
",15904492,,15904492,,2022-03-28 09:14:05,2022-03-28 09:14:05,tensorboard not showing results using ray rllib,<python><google-colaboratory><reinforcement-learning><tensorboard><ray>,1,0,,,,CC BY-SA 4.0,
2937,71807485,1,71807866,,2022-04-09 11:04:41,,-1,96,"<p>I am currently training a PPO model for a simulation.
The PPO model fails to understand that certain conditions will lead to no reward.</p>
<p>These conditions that lead to no reward are very simple rules.
I was trying to use these rules to create an 'expert' that the PPO model could use for imitation learning.</p>
<p><strong>Example of Expert-Based Rules:</strong></p>
<p>If resource A is unavailable, then don't select that resource.</p>
<p>If &quot;X&quot; &amp; &quot;Y&quot; don't match, then don't select those.</p>
<p><strong>Example with Imitations Library</strong></p>
<p>I was looking at the &quot;imitations&quot; python library.
The example there shows an expert that is a PPO model with more iterations.</p>
<p><a href=""https://github.com/HumanCompatibleAI/imitation/blob/master/examples/1_train_bc.ipynb"" rel=""nofollow noreferrer"">https://github.com/HumanCompatibleAI/imitation/blob/master/examples/1_train_bc.ipynb</a></p>
<p><a href=""https://i.stack.imgur.com/vRMnV.jpg"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/vRMnV.jpg"" alt=""enter image description here"" /></a></p>
<p><strong>Questions:</strong></p>
<p>Is there a way to convert the simple &quot;rule-based&quot; expert into a PPO model which can be used for imitation learning?</p>
<p>Or is there a different approach to using a &quot;rule-based&quot; expert in imitation learning?</p>
",5405596,,,,,2022-11-17 16:47:17,How to use a rule-based 'expert' for imitation learning?,<python><machine-learning><reinforcement-learning>,1,0,,,,CC BY-SA 4.0,
2946,71901031,1,71902933,,2022-04-17 10:14:37,,0,75,"<p>My goal is to train an agent (ship) that takes two actions for now. 1. Choosing it's heading angle (where to go next) and 2. Choosing it's acceleration (if it will change its speed or not).</p>
<p>However, it seems like that I cannot undestand how to properly construct my action space and state space. I keep getting an error which I do not know how to fix. I have been trying to make it work using the Space wrapper.</p>
<p>I use the following code.</p>
<pre><code>#Packages used
using ReinforcementLearning
using Flux #Needed for all the Neural Networks functionalities
using Plots
using DelimitedFiles #Needed to read all the txt files
using PolygonOps
using Random
using Intervals #not being used

#GeoBoundariesManipulation
include(joinpath(pwd(),&quot;GeoBoundariesManipulation.jl&quot;));
using .GeoBoundariesManipulation

#My problem's parameters
struct ShippingEnvParams
    gridworld_dims::Tuple{Int64,Int64} #Gridworld dimensions
    velocities::Vector{Int64} #available velocities from 6 knots to 20 knots
    acceleration::Vector{Int64} #available acceleration per step: -2, 0, 2
    heading::Vector{CartesianIndex{2}} #all heading manoeuvers
    punishment::Int64 #punishment per ordinary step
    out_of_grid_punishment::Int64 #punishment for going towards an island or out of grid bounds
    StartingPoint::CartesianIndex{2}
    GoalPoint::CartesianIndex{2}
    all_polygons::Vector{Vector{Tuple{Float64,Float64}}} #all the boundaries
end

function ShippingEnvParams(;
    gridworld_dims = (50,50),
    velocities = Vector((6:2:20)), 
    acceleration = Vector((-2:2:2)), 
    heading = [CartesianIndex(0,1);CartesianIndex(0,-1);CartesianIndex(-1,0);CartesianIndex(-1,1);CartesianIndex(-1,-1);CartesianIndex(1,-1);CartesianIndex(1,1);CartesianIndex(1,0)], 
    punishment = -5, 
    out_of_grid_punishment = -100, 
    StartingPoint = GeoBoundariesManipulation.GoalPointToCartesianIndex((-6.733535,61.997345),gridworld_dims[1],gridworld_dims[2]),
    EndingPoint = GeoBoundariesManipulation.GoalPointToCartesianIndex((-6.691500,61.535580),gridworld_dims[1],gridworld_dims[2]),
    AllPolygons = GeoBoundariesManipulation.load_files(&quot;finalboundaries&quot;) 
    )
    ShippingEnvParams(
        gridworld_dims,
        velocities,
        acceleration,
        heading,
        punishment,
        out_of_grid_punishment,
        StartingPoint,
        EndingPoint,
        AllPolygons
    )
end

###ENVIRONMENT CONSTRUCTION
#Instance
mutable struct ShippingEnv &lt;: AbstractEnv
    params::ShippingEnvParams
    action_space::Space{Tuple{UnitRange{Int64},UnitRange{Int64}}}
    observation_space::Space{Tuple{UnitRange{Int64},UnitRange{Int64}}} #state_space
    state::Space{Tuple{Int64,Int64}} #state: (position,velocity)
    action::Space{Tuple{Int64,Int64}} #action: (heading_angle,acceleration)
    done::Bool #checks if agent has reached its goal
    position::CartesianIndex{2}
    time::Float64
    velocity::Int64
    distance::Float64
    reward::Union{Nothing,Float64} 
end

function ShippingEnv()
    params1 = ShippingEnvParams()
    env = ShippingEnv(
        params1,
        #Base.OneTo(length(params.heading)*length(params.velocities)),
        Space((1:length(params1.heading),1:length(params1.acceleration))), #Space: (1-number of heading options, 1-number of acceleration options)
        #Space([1..params.gridworld_dims[1]*params.gridworld_dims[2],minimum(params.velocities)..maximum(params.velocities)]),
        Space((1:(params1.gridworld_dims[1]*params1.gridworld_dims[2]),(1:length(params1.velocities)))), #(1-number of grid tiles, 1-number of velocity options)
        Space((LinearIndices((params1.gridworld_dims[1],params1.gridworld_dims[2]))[params1.StartingPoint],6)),
        Space((1,1)),
        false,
        params1.StartingPoint,
        0.0,
        params1.velocities[1],
        0.0,
        0.0
    )
    reset!(env)
    env
end


#Minimal interfaces implemented
RLBase.action_space(m::ShippingEnv) = m.action_space
RLBase.state_space(m::ShippingEnv) = m.observation_space
RLBase.reward(m::ShippingEnv) = m.done ? 0.0 : -1.0
RLBase.is_terminated(m::ShippingEnv) = m.done
RLBase.state(m::ShippingEnv) = m.state
#Random.seed!(m::ShippingEnv,seed) = Random.seed!(m.rng,seed)

function RLBase.reset!(m::ShippingEnv)
    m.position = m.params.StartingPoint
    m.velocity = m.params.velocities[1]
    m.done = false
    m.time = 0
    m.distance = 0
    nothing
end

#Function defining what happens every time an action is made
function (m::ShippingEnv)(a::Vector{Int64})
    nextstep(m,a[1],a[2])
end

function nextstep(m::ShippingEnv, head_action, acceleration)
    heading = m.params.heading[head_action]
    r = m.params.punishment #initialized punishment if everything's okay
    m.position += heading
    dist_covered = sqrt(heading[1]^2 + heading[2]^2)
    m.distance += dist_covered
    next_state_norm = (m.position[1]/m.params.gridworld_dims[1],m.position[2]/m.params.gridworld_dims[2])
    #Check if next state is out of bounds and assign appropriate punishment
    if m.position[1]&lt;1 || m.position[1]&gt;m.params.gridworld_dims[1] || m.position[2]&lt;1 || m.position[2]&gt;m.params.gridworld_dims[2] || inanypolygon(next_state_norm, m.params.all_polygons)
        r = m.params.out_of_grid_punishment #replace punishment
        m.position -= heading
        m.distance -= dist_covered
    end

    #Checking if velocity+acceleration is out of velocities' bounds
    if (m.velocity + acceleration &gt; minimum(m.params.velocities)) &amp;&amp; (m.velocity + acceleration &lt; maximum(m.params.velocities))
        m.velocity += acceleration
    end
    
    m.time = dist_covered/m.velocity
    m.reward = r -m.time

    m.state[1] = LinearIndices((m.params.gridworld_dims[1],m.params.gridworld_dims[2]))[m.position]
    m.state[2] = m.velocity
end

env = ShippingEnv()
RLBase.test_runnable!(env)
</code></pre>
<p>This is the stacktrace I've been getting after running test_runnable!(env).</p>
<pre><code>Error During Test at C:\Users\kwstas\.julia\packages\ReinforcementLearningBase\E7jI5\src\base.jl:266
  Got exception outside of a @test
  method not implemented
  Stacktrace:
    [1] error(s::String)
      @ Base .\error.jl:33
    [2] (::ShippingEnv)(action::Tuple{Int64, Int64}, player::DefaultPlayer) (repeats 2 times)
      @ ReinforcementLearningBase .\none:0
    [3] macro expansion
      @ C:\Users\kwstas\.julia\packages\ReinforcementLearningBase\E7jI5\src\base.jl:281 [inlined]
    [4] macro expansion
      @ C:\Users\kwstas\AppData\Local\Programs\Julia-1.7.1\share\julia\stdlib\v1.7\Test\src\Test.jl:1283 [inlined]
    [5] test_runnable!(env::ShippingEnv, n::Int64; rng::Random._GLOBAL_RNG)
      @ ReinforcementLearningBase C:\Users\kwstas\.julia\packages\ReinforcementLearningBase\E7jI5\src\base.jl:267
    [6] test_runnable! (repeats 2 times)
      @ C:\Users\kwstas\.julia\packages\ReinforcementLearningBase\E7jI5\src\base.jl:266 [inlined]
    [7] top-level scope
      @ c:\Users\kwstas\Desktop\ThesisDir\RL-New-Env.jl:138
    [8] eval
      @ .\boot.jl:373 [inlined]
    [9] include_string(mapexpr::typeof(REPL.softscope), mod::Module, code::String, filename::String)
      @ Base .\loading.jl:1196
   [10] invokelatest(::Any, ::Any, ::Vararg{Any}; kwargs::Base.Pairs{Symbol, Union{}, Tuple{}, NamedTuple{(), Tuple{}}})
      @ Base .\essentials.jl:716
   [11] invokelatest(::Any, ::Any, ::Vararg{Any})
      @ Base .\essentials.jl:714
   [12] inlineeval(m::Module, code::String, code_line::Int64, code_column::Int64, file::String; softscope::Bool)
      @ VSCodeServer c:\Users\kwstas\.vscode\extensions\julialang.language-julia-1.6.17\scripts\packages\VSCodeServer\src\eval.jl:211
   [13] (::VSCodeServer.var&quot;#65#69&quot;{Bool, Bool, Module, String, Int64, Int64, String, VSCodeServer.ReplRunCodeRequestParams})()
      @ VSCodeServer c:\Users\kwstas\.vscode\extensions\julialang.language-julia-1.6.17\scripts\packages\VSCodeServer\src\eval.jl:155
   [14] withpath(f::VSCodeServer.var&quot;#65#69&quot;{Bool, Bool, Module, String, Int64, Int64, String, VSCodeServer.ReplRunCodeRequestParams}, path::String)
      @ VSCodeServer c:\Users\kwstas\.vscode\extensions\julialang.language-julia-1.6.17\scripts\packages\VSCodeServer\src\repl.jl:184
   [15] (::VSCodeServer.var&quot;#64#68&quot;{Bool, Bool, Bool, Module, String, Int64, Int64, String, VSCodeServer.ReplRunCodeRequestParams})()
      @ VSCodeServer c:\Users\kwstas\.vscode\extensions\julialang.language-julia-1.6.17\scripts\packages\VSCodeServer\src\eval.jl:153
   [16] hideprompt(f::VSCodeServer.var&quot;#64#68&quot;{Bool, Bool, Bool, Module, String, Int64, Int64, String, VSCodeServer.ReplRunCodeRequestParams})
      @ VSCodeServer c:\Users\kwstas\.vscode\extensions\julialang.language-julia-1.6.17\scripts\packages\VSCodeServer\src\repl.jl:36
   [17] (::VSCodeServer.var&quot;#63#67&quot;{Bool, Bool, Bool, Module, String, Int64, Int64, String, VSCodeServer.ReplRunCodeRequestParams})()
      @ VSCodeServer c:\Users\kwstas\.vscode\extensions\julialang.language-julia-1.6.17\scripts\packages\VSCodeServer\src\eval.jl:124
   [18] with_logstate(f::Function, logstate::Any)
      @ Base.CoreLogging .\logging.jl:511
   [19] with_logger
      @ .\logging.jl:623 [inlined]
   [20] (::VSCodeServer.var&quot;#62#66&quot;{VSCodeServer.ReplRunCodeRequestParams})()
      @ VSCodeServer c:\Users\kwstas\.vscode\extensions\julialang.language-julia-1.6.17\scripts\packages\VSCodeServer\src\eval.jl:201
   [21] #invokelatest#2
      @ .\essentials.jl:716 [inlined]
   [22] invokelatest(::Any)
      @ Base .\essentials.jl:714
   [23] macro expansion
      @ c:\Users\kwstas\.vscode\extensions\julialang.language-julia-1.6.17\scripts\packages\VSCodeServer\src\eval.jl:34 [inlined]
   [24] (::VSCodeServer.var&quot;#60#61&quot;)()
      @ VSCodeServer .\task.jl:423
Test Summary:                  | Pass  Error  Total
random policy with ShippingEnv |    2      1      3
ERROR: Some tests did not pass: 2 passed, 0 failed, 1 errored, 0 broken.
</code></pre>
",18835393,,4685471,,2022-04-17 22:05:29,2022-04-17 22:05:29,Multidimensional Action Space in Reinforcement Learning,<machine-learning><julia><reinforcement-learning>,1,0,,,,CC BY-SA 4.0,
2955,71970806,1,71970807,,2022-04-22 14:59:07,,1,112,"<p>I am implementing a policy gradient algorithm with stochastic policies and since &quot;ancillary&quot; non-PyTorch operations are slow in Python, I want to implement the algorithm in C++. Is there a way to implement a normal distribution in the PyTorch C++ API?</p>
",5623016,,,,,2022-04-22 14:59:07,"Is there an equivalent of torch.distributions.Normal in LibTorch, the C++ API for PyTorch?",<c++><machine-learning><pytorch><reinforcement-learning>,1,0,,,,CC BY-SA 4.0,
2959,72007160,1,72007195,,2022-04-26 00:46:50,,0,210,"<p>I have a labeled dataset and  I am going to develop a classifier for a multilabel classification problem (ex: 5 labels). I have already developed BERT, and CNN, but I was wondering if I could use RL for text classification as well.</p>
<p>As I know, using RL we can use a smaller training dataset</p>
<p>I am looking for a python code for RL.</p>
",11849691,,,,,2022-04-26 00:53:24,Are there examples of using reinforcement learning for multi label text classification?,<python><reinforcement-learning><multilabel-classification>,1,0,,,,CC BY-SA 4.0,
2967,72085601,1,72086078,,2022-05-02 10:47:25,,-3,86,"<p>Let's say I have a 5*5 matrix.</p>
<pre class=""lang-py prettyprint-override""><code>[[1. 2. 3. 4. 5.]
 [0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0]]
</code></pre>
<p>For example:</p>
<p>I want to get '4' but '5' is in front of it. I need to relocate '5' which is unnecessary here and place it in any random available location other than the row it was in earlier.</p>
<p>so the matrix should look like this after the relocation</p>
<pre class=""lang-py prettyprint-override""><code>[[0. 1. 2. 3. 4.]
 [0. 0. 0. 0. 0.]
 [0. 5. 0. 0. 0.]
 [0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0.]]
</code></pre>
<p>and if I need '3' it should relocate '4' and '5' to random locations.</p>
<p>please help me with this. Thanks</p>
",18889615,,18554456,,2022-05-02 20:20:53,2022-05-02 20:20:53,How to relocate an element in a matrix to a random empty location in another row?,<python><numpy><random><reinforcement-learning>,1,6,0,,,CC BY-SA 4.0,
2975,72221554,1,72222603,,2022-05-12 20:08:18,,0,62,"<p>I am using PPO stable baselines in Google Colab with Tensorboard activated to track the training progress but after around 100-200K timesteps tensorboard stops updating even with the model still training (learning), does anyone else have this issue and know a fix for it?</p>
",3242388,,3242388,,2022-05-12 22:19:48,2022-05-12 22:19:48,Tensorboard stops updating in Google Colab during learning with stable baselines,<python><reinforcement-learning><tensorboard>,1,0,,,,CC BY-SA 4.0,
2980,68353637,1,68762415,,2021-07-12 20:40:26,,0,301,"<p>I am using Google Colab to run ViZDoom combined with TensorFlow (specifically, the TF-Agents library).
Most of the times when I start the Colab notebook with my code I get the following error:</p>
<pre><code>SystemError: This interpreter version: '3.7.10' doesn't match with version of the interpreter ViZDoom was compiled with: 3.7.11
</code></pre>
<p>This happens when I try to import vizdoom, after all the dependencies and other libraries are installed (<code>from vizdoom import *</code>).</p>
<p>I have managed to make the Colab notebook work by simply running it again from the beginning. Sometimes running it again from the beginning doesn't work, though. It seems to help adding a check for the interpreter version with <code>!python3 --version</code> before all the installs, but that shouldn't set the python version for the installs.</p>
<p>I have also tried installing ViZDoom in two different ways, both displayed in the complete code below. Neither way of installing it works consistently.</p>
<p>Why is the interpreter version changing? Is there a way to make it stay the same, so I don't have to reinstall everything when it randomly happens not to work?</p>
<p>The code I am running until the problem is (every box is one section of the Colab notebook):</p>
<pre><code>from google.colab import drive
drive.mount('/content/drive')
</code></pre>
<pre><code># To check Python version:
# !python3 --version

%%bash
# Install deps from 
# https://github.com/mwydmuch/ViZDoom/blob/master/doc/Building.md#-linux
apt update
apt upgrade

apt install build-essential zlib1g-dev libsdl2-dev libjpeg-dev nasm tar libbz2-dev libgtk2.0-dev \
cmake git libfluidsynth-dev libgme-dev libopenal-dev timidity libwildmidi-dev unzip

# Boost libraries
apt install libboost-all-dev

# Lua binding dependencies
apt install liblua5.1-dev

apt update
apt upgrade
</code></pre>
<pre><code>!pip install tf-agents
</code></pre>
<pre><code>%%bash
apt update
apt upgrade
</code></pre>
<pre><code># Neither way of installing ViZDoom seem to work consistently. I am installing ViZDoom either way at a time, not both ways at the same time.
!pip install git+https://github.com/mwydmuch/ViZDoom
#!pip install vizdoom
</code></pre>
<pre><code>### LINE THAT GOES WRONG ###
from vizdoom import *
### LINE THAT GOES WRONG ###

import numpy as np
import pandas as pd
import seaborn as sbrn

import tensorflow as tf
from tensorflow import keras

from tf_agents.agents.ppo import ppo_agent
from tf_agents.environments import py_environment
from tf_agents.environments import tf_py_environment
from tf_agents.specs import array_spec, BoundedArraySpec, ArraySpec
from tf_agents.networks.actor_distribution_rnn_network import ActorDistributionRnnNetwork
from tf_agents.networks.value_rnn_network import ValueRnnNetwork
from tf_agents.trajectories import time_step

import time
import random
</code></pre>
",6327926,,6327926,,2021-07-14 03:33:35,2021-08-12 18:15:15,Error in Google Colab: SystemError: This interpreter version: '3.7.10' doesn't match with version of the interpreter ViZDoom was compiled with: 3.7.11,<python><python-3.x><google-colaboratory><tensorflow-agents>,1,0,,,,CC BY-SA 4.0,
2981,66733298,1,66755169,,2021-03-21 14:06:37,,0,305,"<p>I'm working on an PPO agent that plays (well, should) Doom using TF-Agents. As input to the agent, I am trying to give it a stack of 4 images. My complete code is in the following link:
<a href=""https://colab.research.google.com/drive/1chrlrLVR_rwAeIZhL01LYkpXsusyFyq_?usp=sharing"" rel=""nofollow noreferrer"">https://colab.research.google.com/drive/1chrlrLVR_rwAeIZhL01LYkpXsusyFyq_?usp=sharing</a></p>
<p>Unhappily, my code does not compile. It returns a TypeError in the line shown below (it is being run in Google Colaboratory).</p>
<pre><code>---------------------------------------------------------------------------
TypeError                                 Traceback (most recent call last)
&lt;ipython-input-10-d1571cbbda6b&gt; in &lt;module&gt;()
      8   t_step = tf_env.reset()
      9   while (episode_steps &lt;= max_steps_per_episode or (not t_step.is_last())):
---&gt; 10     policy_step = agent.policy.action(t_step)
     11     t_step = tf_env.step(policy_step.action)
     12     episode_steps += 1

5 frames
/usr/local/lib/python3.7/dist-packages/tf_agents/utils/nest_utils.py in assert_same_structure(nest1,     nest2, check_types, expand_composites, message)
    112     str2 = tf.nest.map_structure(
    113         lambda _: _DOT, nest2, expand_composites=expand_composites)
--&gt; 114     raise exception('{}:\n  {}\nvs.\n  {}'.format(message, str1, str2))
    115 
    116 

TypeError: policy_state and policy_state_spec structures do not match:
  ()
vs.
  {'actor_network_state': ListWrapper([., .])}
</code></pre>
<p>The thing about this error is, for what I've read in the TF-Agents documentation, the user is not supposed to do anything regarding the policy_state since it is generated automatically based on the agent's networks.</p>
<p>This is a similar error I found, but didn't seem to solve my problem, though it hinted me in one of the tryed solutions:
<a href=""https://stackoverflow.com/questions/57259497/py-environment-time-step-doesnt-match-time-step-spec"">py_environment &#39;time_step&#39; doesn&#39;t match &#39;time_step_spec&#39;</a></p>
<p>After reading the question and the answer above, I realized I was promising an observation_spec like this:</p>
<pre><code>self._observation_spec = array_spec.BoundedArraySpec(shape=(4, 160, 260, 3), dtype=np.float32, minimum=0, maximum=1, name='screen_observation')
</code></pre>
<p>But what I was passing was a list of 4 np.arrays with shape = (160, 260, 3):</p>
<pre><code>self._stacked_frames = []
for _ in range(4):
  new_frame = np.zeros((160, 260, 3), dtype=np.float32)
  self._stacked_frames.append(new_frame)
</code></pre>
<p>I did this because I thought the &quot;shape&quot; of my data wouldn't change, since the list always has the same number of elements as the first dimension of the observation_spec. Lists were easier to delete past frames and add new ones, like this:</p>
<pre><code>def stack_frames(self):
  #This gets the current frame of the game
  new_frame = self.preprocess_frame()

  if self._game.is_new_episode():
    for frame in range(4):
      self._stacked_frames.append(new_frame)
      #This pop was meant to clear an empty frames that was already in the list
      self._stacked_frames.pop(0)
  else:
    self._stacked_frames.append(new_frame)
    self._stacked_frames.pop(0)
  return self._stacked_frames
</code></pre>
<p>I was trying with only np.arrays before, but was not able to delete past frames and add new ones. Probably I was not doing it right, but I felt like the self._stacked_frames was born with the same shape as the observation spec and could not simply delete or add new arrays.</p>
<pre><code>self._stacked_frames = np.zeros((4, 160, 260, 3), dtype=np.float32)

def stack_frames(self):
  new_frame = self.preprocess_frame()
  
  if self._game.is_new_episode():
    for frame in range(4):
      #This delete was meant to clear an empty frames that was already in the list
      self._stacked_frames = np.delete(self._stacked_frames, 0, 0)
      #I tried &quot;np.concatenate((self._stacked_frames, new_frame))&quot; as well
      self._stacked_frames = np.vstack((self._stacked_frames, new_frame))
  else:
    self._stacked_frames = np.delete(self._stacked_frames, 0, 0)
    #I tried &quot;np.concatenate((self._stacked_frames, new_frame))&quot; as well
    self._stacked_frames = np.vstack((self._stacked_frames, new_frame))
  return self._stacked_frames
</code></pre>
<p>This approach up here did not work. Like I said, probably I was doing it wrong. I see three ways of solving this stalemate:</p>
<ol>
<li>I declare the observation_spec as a list of four frames, each declared as np.array(160, 260, 3);</li>
<li>I declared the observation_spec like I did, but delete and add frames from the self._stacked_frames the right way (not sure it is possible, since self._stacked_frames will be declared as np.array(4, 160, 260, 3) and I'm not sure it can become np.array(3, 160, 260, 3) or np.array(5, 160, 260, 3), before going back to being np.array(4, 160, 260, 3);</li>
<li>I still declare the observation_spec like I did, but I do not delete or add frames. I make a loop where I copy the second frame (that enters the stack_frames function in the second slot) into the first slot, the third frame into the second slot, the fourth frame into the third slot, and finally, the new frame into the fourth slot. An illustration follows:</li>
</ol>
<pre><code>             self._stacked_frames Slot: 1 | 2 | 3 | 4
Game image inside self._stacked_frames: A | B | C | D
                        New game image: E
   New game image's positions (step 1): B | B | C | D
   New game image's positions (step 2): B | C | C | D
   New game image's positions (step 3): B | C | D | D
   New game image's positions (step 4): B | C | D | E
              New self._stacked_frames: B | C | D | E
</code></pre>
<p>This last one seemed like the most certain way to work around my problem, considering I'm right about what it is. I tried it, but the TypeError persisted. I tried it like this:</p>
<pre><code>self._stacked_frames = np.zeros((self._frame_stack_size, 160, 260, 3), dtype=np.float32)
</code></pre>
<p>and then:</p>
<pre><code>def stack_frames(self):
  new_frame = self.preprocess_frame()

  if self._game.is_new_episode():
    for frame in range(self._frame_stack_size):
      self._stacked_frames[frame] = new_frame
  else:
    for frame in range((self._frame_stack_size) - 1):
      self._stacked_frames[frame] = self._stacked_frames[frame + 1]
    self._stacked_frames[self._frame_stack_size - 1] = new_frame
  return self._stacked_frames
</code></pre>
<p>Two questions then:</p>
<ol>
<li>Considering I'm right about the TypeError presented, what of the three ways of fixing it is best? Is there anything wrong then with the way I tryed my solution for the 3rd possibility?</li>
<li>Considering I might not be right about the TypeError, what is this error about then?</li>
</ol>
",6327926,,,,,2021-11-30 02:13:28,How to fix a TypeError between policy_state and policy_state_spec in TF-Agents?,<python><numpy><tensorflow><google-colaboratory><tensorflow-agents>,1,0,,,,CC BY-SA 4.0,
2982,57528770,1,57528913,,2019-08-16 17:21:28,,0,2038,"<p>I am trying to create my own PyEnvironment for TF-Agents.
However, this error keeps showing up:</p>

<blockquote>
  <p>AttributeError: module 'tensorflow.python.ops.linalg.linear_operator_util' has no attribute 'matmul_with_broadcast'</p>
</blockquote>

<p>I have found out that this seems to be an issue with <code>tensorflow-probability</code>, but I have installed the version <code>tensorflow-probability=0.7.0</code> recommended at
<a href=""https://github.com/tensorflow/agents/issues/91"" rel=""nofollow noreferrer"">https://github.com/tensorflow/agents/issues/91</a> </p>

<p>I have tried reinstalling and updating</p>

<p><code>tensorflow-gpu=2.0.0-beta1</code>
<code>tf-agents-nightly</code>
<code>tensorflow-probability=0.7.0</code></p>

<p>Here is a minimal code example:</p>

<pre><code>from tf_agents.environments import py_environment


class myEnv(py_environment.PyEnvironment):
    def __init__(self):
        pass

    def _reset(self):
        pass

    def _step(self, action):
        pass
</code></pre>

<p>This is the full error message when running this minimum example:</p>

<pre><code>Traceback (most recent call last):   File "".\env_mwe.py"", line 1, in &lt;module&gt;
    from tf_agents.environments import py_environment   File ""C:\Python37\lib\site-packages\tf_agents\environments\__init__.py"", line 18, in &lt;module&gt;
    from tf_agents.environments import batched_py_environment   File ""C:\Python37\lib\site-packages\tf_agents\environments\batched_py_environment.py"", line 33, in &lt;module&gt;
    from tf_agents.environments import py_environment   File ""C:\Python37\lib\site-packages\tf_agents\environments\py_environment.py"", line 29, in &lt;module&gt;
    from tf_agents.trajectories import time_step as ts   File ""C:\Python37\lib\site-packages\tf_agents\trajectories\__init__.py"", line 19, in &lt;module&gt;
    from tf_agents.trajectories import time_step   File ""C:\Python37\lib\site-packages\tf_agents\trajectories\time_step.py"", line 28, in &lt;module&gt;
    from tf_agents.specs import array_spec   File ""C:\Python37\lib\site-packages\tf_agents\specs\__init__.py"", line 20, in &lt;module&gt;
    from tf_agents.specs.distribution_spec import DistributionSpec   File ""C:\Python37\lib\site-packages\tf_agents\specs\distribution_spec.py"", line 22, in &lt;module&gt;
    import tensorflow_probability as tfp   File ""E:\Users\tmp\AppData\Roaming\Python\Python37\site-packages\tensorflow_probability\__init__.py"", line 78, in &lt;module&gt;
    from tensorflow_probability.python import *  # pylint: disable=wildcard-import   File ""E:\Users\tmp\AppData\Roaming\Python\Python37\site-packages\tensorflow_probability\python\__init__.py"", line 22, in &lt;module&gt;
    from tensorflow_probability.python import distributions   File ""E:\Users\tmp\AppData\Roaming\Python\Python37\site-packages\tensorflow_probability\python\distributions\__init__.py"", line 64, in &lt;module&gt;
    from tensorflow_probability.python.distributions.linear_gaussian_ssm import LinearGaussianStateSpaceModel   File ""E:\Users\tmp\AppData\Roaming\Python\Python37\site-packages\tensorflow_probability\python\distributions\linear_gaussian_ssm.py"", line 41, in &lt;module&gt;
    _matmul = linear_operator_util.matmul_with_broadcast AttributeError: module 'tensorflow.python.ops.linalg.linear_operator_util' has no attribute 'matmul_with_broadcast'
</code></pre>
",3083470,,,,,2019-08-19 14:21:16,AttributeError: module 'tensorflow.python.ops.linalg.linear_operator_util' has no attribute 'matmul_with_broadcast',<python><tensorflow2.0><tensorflow-probability><tensorflow-agents>,2,1,,,,CC BY-SA 4.0,
2984,57565249,1,57661285,,2019-08-20 00:10:25,,8,4240,"<p>I am trying to load a <code>tf-agents</code> policy I saved via</p>

<pre><code>try:
    PolicySaver(collect_policy).save(model_dir + 'collect_policy')
except TypeError:
    tf.saved_model.save(collect_policy, model_dir + 'collect_policy')
</code></pre>

<p>Quick explanation for the try/except block: When originally creating the policy, I can save it via <code>PolicySaver</code>, but when I load it again for another training run, it is a <code>SavedModel</code> and can therefore not be saved by <code>PolicySaver</code>.</p>

<p>This seems to work fine, but now I want to use this policy for self-play, so I load the policy with <code>self.policy = tf.saved_model.load(policy_path)</code> in my AIPlayer class. When I try to use it for prediction, however, it does not work. Here's the (testing) code:</p>

<pre><code>def decide(self, table):
    state = table.getState()
    timestep = ts.restart(np.array([table.getState()], dtype=np.float))
    prediction = self.policy.action(timestep)
    print(prediction)
</code></pre>

<p>the <code>table</code> passed into the function contains the state of the game and the <code>ts.restart()</code> function is copied from my custom pyEnvironment, so the timestep is constructed the exact same way as it would be in the environment. However, I get the following error message for the line <code>prediction=self.policy.action(timestep)</code>:</p>

<pre><code>ValueError: Could not find matching function to call loaded from the SavedModel. Got:
  Positional arguments (2 total):
    * TimeStep(step_type=&lt;tf.Tensor 'time_step:0' shape=() dtype=int32&gt;, reward=&lt;tf.Tensor 'time_step_1:0' shape=() dtype=float32&gt;, discount=&lt;tf.Tensor 'time_step_2:0' shape=() dtype=float32&gt;, observation=&lt;tf.Tensor 'time_step_3:0' shape=(1, 79) dtype=float64&gt;)
    * ()
  Keyword arguments: {}

Expected these arguments to match one of the following 2 option(s):

Option 1:
  Positional arguments (2 total):
    * TimeStep(step_type=TensorSpec(shape=(None,), dtype=tf.int32, name='time_step/step_type'), reward=TensorSpec(shape=(None,), dtype=tf.float32, name='time_step/reward'), discount=TensorSpec(shape=(None,), dtype=tf.float32, name='time_step/discount'), observation=TensorSpec(shape=(None,
79), dtype=tf.float64, name='time_step/observation'))
    * ()
  Keyword arguments: {}

Option 2:
  Positional arguments (2 total):
    * TimeStep(step_type=TensorSpec(shape=(None,), dtype=tf.int32, name='step_type'), reward=TensorSpec(shape=(None,), dtype=tf.float32, name='reward'), discount=TensorSpec(shape=(None,), dtype=tf.float32, name='discount'), observation=TensorSpec(shape=(None, 79), dtype=tf.float64, name='observation'))
    * ()
  Keyword arguments: {}
</code></pre>

<p>What am I doing wrong? Is it really just the tensor names or are the shapes the problem and how can I change that?</p>

<p>Any ideas how to further debug this are appreciated.</p>
",3083470,,3083470,,2019-08-22 14:37:03,2019-08-26 15:59:52,ValueError: Could not find matching function to call loaded from the SavedModel,<python><tensorflow><tensorflow-agents>,1,0,,,,CC BY-SA 4.0,
2987,63865496,1,63866284,,2020-09-12 22:31:19,,-1,106,"<p>Is there an easy native way to implement <a href=""https://www.tensorflow.org/addons/api_docs/python/tfa/optimizers/CyclicalLearningRate"" rel=""nofollow noreferrer"">tfa.optimizers.CyclicalLearningRate</a> w/ <a href=""https://www.tensorflow.org/agents/tutorials/1_dqn_tutorial"" rel=""nofollow noreferrer"">QNetwork on DqnAgent</a>?</p>
<p>Trying to avoid writing my own DqnAgent.</p>
<p>I guess the better question might be, what is a proper way to implement callbacks on DqnAgent?</p>
",4009428,,,,,2020-09-13 01:15:03,"tf-agent, QNetwork => DqnAgent w/ tfa.optimizers.CyclicalLearningRate",<python><tensorflow><machine-learning><tensorflow2.0><tensorflow-agents>,1,0,,2020-09-13 04:23:32,,CC BY-SA 4.0,
2988,68171714,1,68547157,,2021-06-29 01:57:46,,1,197,"<p>I noticed something weird happening when converting a Python environment into a TF environment using tf_agents.environments.TFPyEnvironment and I'd like to ask you what general changes occur.</p>
<p>To clarify the question please find below my code. I want the environment to simulate (in an oversimplied manner) interactions with a customers who want to buy fruits or vegetables. The agent should learn that when a customer asks for fruits, action 0 should be executed for example.</p>
<pre><code>class CustomEnv(py_environment.PyEnvironment):
    
    def __init__(self):
        self._action_spec = array_spec.BoundedArraySpec(
            shape=(), dtype=np.int32, minimum=0, maximum=1)
        self._observation_spec = array_spec.BoundedArraySpec(
        shape=(1,1), dtype=np.int32, minimum=0, maximum=1)
        self._state = [0]
        self._counter = 0
        self._episode_ended = False
        self.dictionary = {0: [&quot;Fruits&quot;], 
                            1: [&quot;Vegetables&quot;]}
    
    def action_spec(self):
        return self._action_spec
    
    def observation_spec(self):
        return self._observation_spec
    
    def _reset(self):
        self._state = [0]
        self._counter = 0
        self._episode_ended = False
        return ts.restart(np.array([self._state], dtype=np.int32))
    
    def preferences(self):
        return np.random.randint(2)
    
    def pickedBasket(self, yes):
        reward = -1.0
        if yes:
            reward = 0.0
        return reward
    
    def _step(self, action):
        if self._episode_ended:
            self._reset()
        
        if self._counter&lt;50:
            self._counter += 1
            
            basket = self.preferences()
            condition = basket in self.dictionary[action]
            reward = self.pickedBasket(condition)
            self._state[0] = basket
            
            if self._counter==50:
                self._episode_ended=True
                return ts.termination(np.array([self._state], 
                                               dtype=np.int32),
                                      reward,
                                      1)
            else:
                return ts.transition(np.array([self._state], 
                                              dtype=np.int32), 
                                     reward, 
                                     discount=1.0)
</code></pre>
<p>When I execute the following to code to check everything is working just fine:</p>
<pre><code>py_env = ContextualMBA()
tf_env = tf_py_environment.TFPyEnvironment(py_env)
time_step = tf_env.reset()
action = 0
next_time_step = tf_env.step(action)
</code></pre>
<p>I get an <strong>unhashable type: 'numpy.ndarray'</strong> for the line <code>condition = basket in self.dictionary[action]</code> so I changed it into <code>condition = basket in self.dictionary[int(action)]</code> and it worked just fine. I'd also like to precise that it worked as a Python environment even without adding the <code>int</code> part. So I'd like to ask what changes the tf_agents.environments.TFPyEnvironment. I don't see how it can influence the type of action <code>action</code> since it isn't related to <code>action_spec</code> or anything (at least directly in the code).</p>
",12300360,,,,,2021-07-27 14:53:24,What changes occur when using tf_agents.environments.TFPyEnvironment to convert a Python RL environment into a TF environment?,<python-3.x><tensorflow><tensorflow-agents>,1,0,,,,CC BY-SA 4.0,
2992,57259497,1,57792036,,2019-07-29 18:24:30,,5,2097,"<p>I have created a custom pyenvironment via tf agents. However I can't validate the environment or take steps within it with py_policy.action
I'm confused as to what is excepted from the time_step_specs</p>

<p>I have tried converting to tf_py_environment via tf_py_environment.TFPyEnvironment and was successful in taking actions with tf_policy but I'm still confused as to the difference.</p>

<pre><code>import abc
import numpy as np
from tf_agents.environments import py_environment
from tf_agents.environments import tf_environment
from tf_agents.environments import tf_py_environment
from tf_agents.environments import utils
from tf_agents.specs import array_spec
from tf_agents.environments import wrappers
from tf_agents.trajectories import time_step as ts
from tf_agents.policies import random_tf_policy
import tensorflow as tf
import tf_agents

class TicTacToe(py_environment.PyEnvironment):
   def __init__(self,n):
    super(TicTacToe,self).__init__()
    self.n = n
    self.winner = None
    self._episode_ended = False
    self.inital_state = np.zeros((n,n))
    self._state = self.inital_state
    self._observation_spec = array_spec.BoundedArraySpec(
        shape = (n,n),dtype='int32',minimum = -1,maximum = 1,name = 
'TicTacToe board state spec')
    self._action_spec = array_spec.BoundedArraySpec(
        shape = (),dtype = 'int32', minimum = 0,maximum = 8, name = 
'TicTacToe action spec')

def observation_spec(self):
    return self._observation_spec

def action_spec(self):
    return self._action_spec

def _reset(self):
    return ts.restart(self.inital_state)

def check_game_over(self):
    for i in range(self.n):
        if (sum(self._state[i,:])==self.n) or 
(sum(self._state[:,i])==self.n):
            self.winner = 1
            return True

        elif (sum(self._state[i,:])==-self.n) or 
    (sum(self._state[:,i])==-self.n):
            self.winner = -1
            return True

    if (self._state.trace()==self.n) or 
(self._state[::-1].trace()==self.n):
        self.winner = 1
        return True
    elif (self._state.trace()==-self.n) or (self._state[::-1].trace()==- 
   self.n):
        self.winner = -1
        return True

    if not (0 in self._state):
        return True

def _step(self,action):
    self._state[action//3,action%3]=1
    self._episode_ended = self.check_game_over

    if self._episode_ended==True:
        if self.winner == 1:
            reward = 1
        elif self.winner == None:
            reward = 0
        else:
            reward = -1
        return ts.termination(self._state,dtype = 'int32',reward=reward)
    else:
        return ts.transition(self._state,dtype = 'int32',reward = 
0.0,discount = 0.9)

env = TicTacToe(3)
utils.validate_py_environment(env, episodes=5)
</code></pre>

<hr>

<p>This is the error I get:</p>

<p>ValueError                                Traceback (most recent call last)
 in 
----> 1 utils.validate_py_environment(env, episodes=5)</p>

<p>C:\Users\bzhang\AppData\Local\Continuum\anaconda3\lib\site-packages\tf_agents\environments\utils.py in validate_py_environment(environment, episodes)
     58       raise ValueError(
     59           'Given <code>time_step</code>: %r does not match expected <code>time_step_spec</code>: %r' %
---> 60           (time_step, time_step_spec))
     61 
     62     action = random_policy.action(time_step).action</p>

<p>ValueError: Given <code>time_step</code>: TimeStep(step_type=array(0), reward=array(0., dtype=float32), discount=array(1., dtype=float32), observation=array([[0., 0., 0.],
       [0., 0., 0.],
       [0., 0., 0.]])) does not match expected <code>time_step_spec</code>: TimeStep(step_type=ArraySpec(shape=(), dtype=dtype('int32'), name='step_type'), reward=ArraySpec(shape=(), dtype=dtype('float32'), name='reward'), discount=BoundedArraySpec(shape=(), dtype=dtype('float32'), name='discount', minimum=0.0, maximum=1.0), observation=BoundedArraySpec(shape=(3, 3), dtype=dtype('int32'), name='TicTacToe board state spec', minimum=-1, maximum=1))</p>
",9145941,,,,,2019-09-04 15:55:26,py_environment 'time_step' doesn't match 'time_step_spec',<tensorflow-agents>,1,2,0,,,CC BY-SA 4.0,
2993,57573540,1,57661499,,2019-08-20 12:20:18,,2,672,"<p>I have written a custom environment so I can play around with reinforcement learning (PPO) and tf-agents.
This works fine if I wrap my env ( which inherits from py_environment.PyEnvironment) in a <code>TfPyEnvironment</code>, but fails if I try to wrap it into a <code>ParallelPyEnvironment</code>. I have tried playing around with all the keyword arguments of <code>ParallelPyEnvironment</code> but the code just runs up to the line and then nothing happens - no Exception, the program does not terminate etc.</p>

<p>Here is my code initialising the environment and showing off the working variant for the <code>eval_env</code>:</p>

<pre><code>train_env = tf_py_environment.TFPyEnvironment(
    ParallelPyEnvironment(
        [CardGameEnv()] * hparams['parallel_environments']
    )
)
# this works perfectly:
eval_env = tf_py_environment.TFPyEnvironment(CardGameEnv(debug=True))
</code></pre>

<p>If I terminate the script via <code>CTRL+C</code>, this is what is being output:</p>

<pre><code>Traceback (most recent call last):
Traceback (most recent call last):
  File ""E:\Users\tmp\Documents\Programming\Neural Nets\Poker_AI\poker_logic\train.py"", line 229, in &lt;module&gt;
  File ""&lt;string&gt;"", line 1, in &lt;module&gt;
    train(model_num=3)
  File ""C:\Python37\lib\multiprocessing\spawn.py"", line 105, in spawn_main
  File ""E:\Users\tmp\Documents\Programming\Neural Nets\Poker_AI\poker_logic\train.py"", line 64, in train
    [CardGameEnv()] * hparams['parallel_environments']
    exitcode = _main(fd)
  File ""E:\Users\tmp\AppData\Roaming\Python\Python37\site-packages\gin\config.py"", line 1009, in wrapper
  File ""C:\Python37\lib\multiprocessing\spawn.py"", line 113, in _main
    preparation_data = reduction.pickle.load(from_parent)
KeyboardInterrupt
    return fn(*new_args, **new_kwargs)
  File ""C:\Python37\lib\site-packages\tf_agents\environments\parallel_py_environment.py"", line 70, in __init__
    self.start()
  File ""C:\Python37\lib\site-packages\tf_agents\environments\parallel_py_environment.py"", line 83, in start
    env.start(wait_to_start=self._start_serially)
  File ""C:\Python37\lib\site-packages\tf_agents\environments\parallel_py_environment.py"", line 223, in start
    self._process.start()
  File ""C:\Python37\lib\multiprocessing\process.py"", line 112, in start
    self._popen = self._Popen(self)
  File ""C:\Python37\lib\multiprocessing\context.py"", line 223, in _Popen
    return _default_context.get_context().Process._Popen(process_obj)
  File ""C:\Python37\lib\multiprocessing\context.py"", line 322, in _Popen
    return Popen(process_obj)
  File ""C:\Python37\lib\multiprocessing\popen_spawn_win32.py"", line 65, in __init__
    reduction.dump(process_obj, to_child)
  File ""C:\Python37\lib\multiprocessing\reduction.py"", line 60, in dump
    ForkingPickler(file, protocol).dump(obj)
  File ""C:\Python37\lib\site-packages\tf_agents\environments\parallel_py_environment.py"", line 264, in __getattr__
    return self._receive()
  File ""C:\Python37\lib\site-packages\tf_agents\environments\parallel_py_environment.py"", line 333, in _receive
    message, payload = self._conn.recv()
  File ""C:\Python37\lib\multiprocessing\connection.py"", line 250, in recv
    buf = self._recv_bytes()
  File ""C:\Python37\lib\multiprocessing\connection.py"", line 306, in _recv_bytes
    [ov.event], False, INFINITE)
KeyboardInterrupt
Error in atexit._run_exitfuncs:
Traceback (most recent call last):
  File ""C:\Python37\lib\site-packages\tf_agents\environments\parallel_py_environment.py"", line 289, in close
    self._process.join(5)
  File ""C:\Python37\lib\multiprocessing\process.py"", line 139, in join
    assert self._popen is not None, 'can only join a started process'
AssertionError: can only join a started process
</code></pre>

<p>From that I conclude that the thread <code>ParallelPyEnvironment</code> is trying to start does not do that, but since I'm not very experienced with threading in Python, I have no idea where to go from here, especially how to fix this.
Current training takes a long time and does not use my PC's capabilities at all (3GB of 32GB RAM used, processor at 3%, GPU barely working at all but VRAM full), so this should speed up training time significantly.</p>
",3083470,,,,,2019-08-26 16:16:19,Tf-Agents ParallelPyEnvironment fails silently,<python><tensorflow><python-multithreading><tensorflow-agents>,1,0,0,,,CC BY-SA 4.0,
3011,70573735,1,70575851,,2022-01-04 03:15:55,,0,361,"<p>I have not seen anything in the <a href=""https://docs.ray.io/en/latest/rllib-models.html"" rel=""nofollow noreferrer"">rllib</a> documentation that would allow me to print a quick summary of the model like <code>print(model.summary())</code> in keras. I tried using tf-slim and</p>
<pre><code>variables = tf.compat.v1.model_variables()
slim.model_analyzer.analyze_vars(variables, print_info=True)
</code></pre>
<p>to get a rough idea for tensorflow models, but this found no variables after the model was initialized (inserted at the end of the <a href=""https://github.com/ray-project/ray/blob/master/rllib/agents/es/es.py"" rel=""nofollow noreferrer"">ESTrainer</a> class _init). Specifically, I have been trying to get a summary of an Evolutionary Strategy (ES) policy to verify that the changes to the model config are being updated as expected, but I have not been able to get a summary print working.</p>
<p>Is there an existing method for this? Is slim expected to work here?</p>
",14345989,,,,,2022-08-01 22:46:54,Printing model summaries for rllib models,<python><tensorflow><rllib>,1,0,,,,CC BY-SA 4.0,
3016,59272939,1,59794351,,2019-12-10 17:56:21,,5,723,"<p>I'm using RLLib's PPOTrainer with a custom environment, I execute <code>trainer.train()</code> two times, the first one completes successfully, but when I execute it for the second time it crashed with an error: </p>

<blockquote>
  <p>lib/python3.7/site-packages/tensorflow_core/python/client/session.py"",
  line 1384, in _do_call (pid=15248)     raise type(e)(node_def, op,
  message) (pid=15248)</p>
  
  <p>tensorflow.python.framework.errors_impl.InvalidArgumentError: </p>
  
  <p>Received a label value of 5 which is outside the valid range of [0, 5). >Label values: 5 5</p>
  
  <p>(pid=15248)      [[node
  default_policy/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits
  (defined at /tensorflow_core/python/framework/ops.py:1751) ]]</p>
</blockquote>

<p>Here's my code:</p>

<p><em>main.py</em></p>

<pre><code>ModelCatalog.register_custom_preprocessor(""tree_obs_prep"", TreeObsPreprocessor)
ray.init()

trainer = PPOTrainer(env=MyEnv, config={
    ""train_batch_size"": 4000,
    ""model"": {
        ""custom_preprocessor"": ""tree_obs_prep""
    }
})

for i in range(2):
    print(trainer.train())
</code></pre>

<p><em>MyEnv.py</em></p>

<pre><code>class MyEnv(rllib.env.MultiAgentEnv):
    def __init__(self, env_config):
        self.n_agents = 2

        self.env = *CREATES ENV*
        self.action_space = gym.spaces.Discrete(5)
        self.observation_space = np.zeros((1, 12))

    def reset(self):
        self.agents_done = []
        obs = self.env.reset()
        return obs[0]

    def step(self, action_dict):
        obs, rewards, dones, infos = self.env.step(action_dict)

        d = dict()
        r = dict()
        o = dict()
        i = dict()
        for i_agent in range(len(self.env.agents)):
            if i_agent not in self.agents_done:
                o[i_agent] = obs[i_agent]
                r[i_agent] = rewards[i_agent]
                d[i_agent] = dones[i_agent]
                i[i_agent] = infos[i)agent]
        d['__all__'] = dones['__all__']

        for agent, done in dones.items():
            if done and agent != '__all__':
                self.agents_done.append(agent)

        return  o, r, d, i
</code></pre>

<p>I have no idea about what's the problem, any suggestion?
What does this error mean?</p>
",4695325,,4695325,,2019-12-10 18:37:03,2020-01-17 20:21:00,"RLLib - Tensorflow - InvalidArgumentError: Received a label value of N which is outside the valid range of [0, N)",<python><tensorflow><ray><rllib>,1,0,0,,,CC BY-SA 4.0,
3031,71937877,1,71945353,,2022-04-20 10:14:22,,0,118,"<p>I'm using RAY, and created a custom env.
However, the custom env needs to open a file, and ray creates workers in a different location.
Therefore, I can't access the file.</p>
<p>when printing the worker location I'm getting : <code>city_v1/DQN/DQN_CityFlows_074cc_00000_0_2022-04-20_13-09-56</code></p>
<p>file exists in:</p>
<pre><code>examples/1x1/file.json
</code></pre>
",17983042,,,,,2022-04-20 19:41:40,open file inside Ray,<ray><rllib>,1,0,,,,CC BY-SA 4.0,
3038,63656778,1,63737268,,2020-08-30 11:06:20,,11,3885,"<p><em><strong>Update:</strong> This is a bug in tensorflow. Track progress <a href=""https://github.com/tensorflow/tensorflow/issues/42980"" rel=""noreferrer"">here</a>.</em></p>
<p>I have created and trained a model using stable-baselines, which uses Tensorflow 1.
Now I need to use this trained model in an environment where I only have access to Tensorflow 2 or PyTorch.
I figured I would go with Tensorflow 2 as the <a href=""https://www.tensorflow.org/api_docs/python/tf/saved_model/load"" rel=""noreferrer"">documentation says</a> I should be able to load models created with Tensorflow 1.</p>
<p><em>I can load the pb file without a problem in Tensorflow 1</em>:</p>
<pre><code>global_session = tf.Session()

with global_session.as_default():
    model_loaded = tf.saved_model.load_v2('tensorflow_model')
    model_loaded = model_loaded.signatures['serving_default']

init = tf.global_variables_initializer()
global_session.run(init)
</code></pre>
<p><em>However in Tensorflow 2 I get the following error</em>:</p>
<pre><code>can_be_imported = tf.saved_model.contains_saved_model('tensorflow_model')
assert(can_be_imported)
model_loaded = tf.saved_model.load('tensorflow_model/')

ValueError: Node 'loss/gradients/model/batch_normalization_3/FusedBatchNormV3_1_grad/FusedBatchNormGradV3' has an _output_shapes attribute inconsistent with the GraphDef for output #3: Dimension 0 in both shapes must be equal, but are 0 and 64. Shapes are [0] and [64].
</code></pre>
<p><em>Model definition</em>:</p>
<pre><code>NUM_CHANNELS = 64

BN1 = BatchNormalization()
BN2 = BatchNormalization()
BN3 = BatchNormalization()
BN4 = BatchNormalization()
BN5 = BatchNormalization()
BN6 = BatchNormalization()
CONV1 = Conv2D(NUM_CHANNELS, kernel_size=3, strides=1, padding='same')
CONV2 = Conv2D(NUM_CHANNELS, kernel_size=3, strides=1, padding='same')
CONV3 = Conv2D(NUM_CHANNELS, kernel_size=3, strides=1)
CONV4 = Conv2D(NUM_CHANNELS, kernel_size=3, strides=1)
FC1 = Dense(128)
FC2 = Dense(64)
FC3 = Dense(7)

def modified_cnn(inputs, **kwargs):
    relu = tf.nn.relu
    log_softmax = tf.nn.log_softmax
    
    layer_1_out = relu(BN1(CONV1(inputs)))
    layer_2_out = relu(BN2(CONV2(layer_1_out)))
    layer_3_out = relu(BN3(CONV3(layer_2_out)))
    layer_4_out = relu(BN4(CONV4(layer_3_out)))
    
    flattened = tf.reshape(layer_4_out, [-1, NUM_CHANNELS * 3 * 2]) 
    
    layer_5_out = relu(BN5(FC1(flattened)))
    layer_6_out = relu(BN6(FC2(layer_5_out)))
    
    return log_softmax(FC3(layer_6_out))

class CustomCnnPolicy(CnnPolicy):
    def __init__(self, *args, **kwargs):
        super(CustomCnnPolicy, self).__init__(*args, **kwargs, cnn_extractor=modified_cnn)

model = PPO2(CustomCnnPolicy, env, verbose=1)
</code></pre>
<p><em>Model saving in TF1:</em></p>
<pre><code>with model.graph.as_default():
    tf.saved_model.simple_save(model.sess, 'tensorflow_model', inputs={&quot;obs&quot;: model.act_model.obs_ph},
                                   outputs={&quot;action&quot;: model.act_model._policy_proba})
</code></pre>
<p>Fully reproducible code can be found in the following 2 google colab notebooks:
<a href=""https://colab.research.google.com/drive/1ftm9vgiJA8LGwLuUFtaG0aYCS9lUrzET?usp=sharing"" rel=""noreferrer"">Tensorflow 1 saving and loading</a>
<a href=""https://colab.research.google.com/drive/18IWxx-eppX2Sjoo2h1hGe0TzCHNiykF1?usp=sharing"" rel=""noreferrer"">Tensorflow 2 loading</a></p>
<p>Direct link to the saved model:
<a href=""https://drive.google.com/drive/folders/1KmfLDIlAqX80Ha1F7MmC314loI2ayiFY?usp=sharing"" rel=""noreferrer"">model</a></p>
",1564252,,1564252,,2020-09-10 20:33:23,2020-09-10 20:33:23,How to load a trained TF1 protobuf model into TF2?,<tensorflow><tensorflow2.0><stable-baselines>,1,0,0,,,CC BY-SA 4.0,
3046,70696295,1,70696407,,2022-01-13 12:03:34,,0,2205,"<p>I am trying to install stable-baselines and run the first <a href=""https://stable-baselines.readthedocs.io/en/master/guide/quickstart.html"" rel=""nofollow noreferrer"">two lines</a> from Getting Started section of the online manual but no option is working. I started with</p>
<pre><code>pip install stable-baselines
</code></pre>
<p>Now when I run:</p>
<pre><code>import gym
from stable_baselines.common.policies import MlpPolicy
</code></pre>
<p>I get</p>
<pre><code>No module named 'tensorflow.contrib'
</code></pre>
<p>This apparently is because tensorflow version 2 doesn't have tensorflow.contrib. But version 2 was released in Sept 2019.  Do I really have to use only tensorflow version 1?</p>
<p>What is the right way to install stable-baselines and run that simple example?</p>
<hr />
<p>I tried</p>
<pre><code>pip install stable-baselines3 
</code></pre>
<p>in a virtual environment. This gives a different error:</p>
<pre><code>In [2]: from stable_baselines.common.policies import MlpPolicy
---------------------------------------------------------------------------
ModuleNotFoundError                       Traceback (most recent call last)
Input In [2], in &lt;module&gt;
----&gt; 1 from stable_baselines.common.policies import MlpPolicy

ModuleNotFoundError: No module named 'stable_baselines'

In [3]: from stable_baselines3.common.policies import MlpPolicy
---------------------------------------------------------------------------
ImportError                               Traceback (most recent call last)
Input In [3], in &lt;module&gt;
----&gt; 1 from stable_baselines3.common.policies import MlpPolicy

ImportError: cannot import name 'MlpPolicy' from 'stable_baselines3.common.policies' (/home/raph/RL/stable-baselines/venv/lib/python3.8/site-packages/stable_baselines3/common/policies.py)
</code></pre>
",1473517,,1473517,,2022-01-13 12:22:10,2022-01-13 12:56:06,The right way to install stable-baselines?,<python><stable-baselines>,2,0,,,,CC BY-SA 4.0,
3073,1656636,1,1660930,,2009-11-01 08:04:22,,5,3610,"<p>I would like to learn about games (strategy) algorithms especially about how do 
enemies algorithms works ? </p>

<p>Is there any good place for beginners?</p>
",63898,,5763590,,2018-11-30 07:18:30,2018-11-30 07:18:30,Where to learn about enemy game algorithms (like Starcraft/Warcraft)?,<algorithm><artificial-intelligence><starcraftgym>,5,0,0,2018-10-08 20:20:30,,CC BY-SA 2.5,
3075,52752976,1,53642526,,2018-10-11 05:41:22,,7,806,"<p>In games like StarCraft you can have up to 200 units (for player) in a map.</p>

<p>There are small but also big maps.</p>

<p>When you for example grab 50 units and tell them to go to the other side of the map some algorithm kicks in and they find path through the obsticles (river, hills, rocks and other).</p>

<p>My question is do you know how the game doesnt slow down because you have 50 paths to calculate. In the meantime other things happens like drones collecting minerals buildinds are made and so on. And if the map is big it should be harder and slower.</p>

<p>So even if the algorithm is good it will take some time for 100 units.</p>

<p>Do you know how this works maybe the algorithm is similar to other games.</p>

<p>As i said when you tell units to move you did not see any delay for calculating the path - they start to run to the destination immediately.</p>

<p>The question is how they make the units go through the shortest path but fast. </p>

<p>There is no delay in most of the games (StarCraft, WarCraft and so on)</p>

<p>Thank you.</p>
",,user2693928,5763590,,2018-11-30 05:49:55,2018-12-06 21:04:54,Shortest path in games (StarCraft example),<path><starcraftgym>,2,3,0,,,CC BY-SA 4.0,
3076,51351598,1,51351856,,2018-07-15 19:36:28,,2,705,"<p>I am trying to run various codes I found in the internet with <code>pysc2</code> Starcraft DeepMind AI agents. I often run into <code>KeyError: 'SOME-VALUE-HERE'</code> invoked by <code>obs.observation[""SOME-VALUE-HERE""]</code>.</p>

<p>For example <a href=""https://github.com/skjb/pysc2-tutorial/blob/master/Building%20a%20Basic%20Agent/simple_agent_step2.py"" rel=""nofollow noreferrer"">this agent</a> and <a href=""https://github.com/deepmind/pysc2/issues/55"" rel=""nofollow noreferrer"">this agent</a> crash on <code>KeyError: 'minimap'</code> invoked by <code>obs.observation['minimap']</code>.</p>

<p>Other example is <code>KeyError: 'screen'</code> invoked by <code>observation[""screen""]</code> when running <a href=""https://github.com/llSourcell/A-Guide-to-DeepMinds-StarCraft-AI-Environment/blob/master/enjoy_mineral_shards.py"" rel=""nofollow noreferrer"">Siraj Raval's enjoy_mineral_shards script</a>.</p>

<p>It is very frustrating as I have not found anybody else running to this error and I really struggle with debugging in <code>pysc2</code> environment. Any help?</p>
",2072457,,5763590,,2018-10-08 01:23:47,2018-10-08 01:23:47,pysc2 Starcraft - obs.observation returns KeyError,<python><starcraftgym>,1,0,,2018-10-08 13:53:40,,CC BY-SA 4.0,
3078,4766084,1,4772643,,2011-01-22 03:57:33,,6,2133,"<p>Does anyone know which programming language the Berkeley Overmind submission to the Starcraft AI competition this past year was?</p>
",102023,,5763590,,2018-10-07 21:32:39,2018-10-07 21:32:39,Programming Language for Berkeley Overmind Starcraft AI competition,<python><prolog><scheme><lisp><starcraftgym>,3,1,0,2018-10-08 10:20:50,,CC BY-SA 2.5,
3079,51224752,1,51225121,,2018-07-07 15:57:20,,0,1102,"<p>I am trying to run the Deepmind Environment for Starcraft II, following this <a href=""https://github.com/deepmind/pysc2"" rel=""nofollow noreferrer"">tutorial</a></p>
<p>After running:</p>
<pre><code>$ python -m pysc2.bin.play --map Simple64
</code></pre>
<p>I get this error:</p>
<pre><code>raise ConnectError(&quot;Failed to connect to the SC2 websocket. Is it up?&quot;)
</code></pre>
<p>within the <code>pysc2.lib.remote_controller</code></p>
<p>Any idea how to fix this?</p>
",5763590,,466862,,2021-04-05 12:27:10,2021-04-09 06:00:32,"""Failed to connect to the SC2 websocket. Is it up?""",<python><connection><starcraftgym>,1,0,,2018-10-08 15:17:15,,CC BY-SA 4.0,
3080,55836416,1,55920210,,2019-04-24 18:26:45,,3,580,"<p>Here is my code that I want to run</p>

<pre><code>import sc2
from sc2 import run_game, maps, Race, Difficulty
from sc2. player import Bot, Computer

class pxk(sc2.BotAI):
    async def on_step(self, iteration):
        await self.distribute_workers()

run_game(maps.get(""AbyssalReefLE""), [
    Bot(Race.Terran, pxk()),
    Computer(Race.Terran, Difficulty.Easy)
    ], realtime=True)
</code></pre>

<p>at first it works, it opens a StarCraft II game window and loads the game, then when the loading is done it shuts done throwing the following error</p>

<pre><code>Traceback (most recent call last):
  File ""starcraft-2.py"", line 12, in &lt;module&gt;
    ], realtime=False)
  File             ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-    packages/sc2/main.py"", line 306, in run_game
    _host_game(map_settings, players, **kwargs)
  File     ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/asynci    o/base_events.py"", line 468, in run_until_complete
    return future.result()
  File     ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-    packages/sc2/main.py"", line 233, in _host_game
    result = await _play_game(players[0], client, realtime,     portconfig, step_time_limit, game_time_limit, rgb_render_config)
  File     ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-    packages/sc2/main.py"", line 204, in _play_game
    result = await _play_game_ai(client, player_id, player.ai,     realtime, step_time_limit, game_time_limit)
  File     ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-    packages/sc2/main.py"", line 92, in _play_game_ai
    game_info = await client.get_game_info()
  File     ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-    packages/sc2/client.py"", line 147, in get_game_info
    return GameInfo(result.game_info)
  File     ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-    packages/sc2/game_info.py"", line 143, in __init__
    self.pathing_grid: PixelMap =     PixelMap(self._proto.start_raw.pathing_grid)
  File     ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-    packages/sc2/pixel_map.py"", line 9, in __init__
    assert self.bits_per_pixel % 8 == 0, ""Unsupported pixel     density""
AssertionError: Unsupported pixel density
ERROR:asyncio:Unclosed client session
client_session: &lt;aiohttp.client.ClientSession object at 0x10e443d30&gt;
</code></pre>
",6904605,,,,,2019-04-30 11:48:35,"sc2 python, AssertionError: Unsupported pixel density",<python><starcraftgym>,1,0,0,,,CC BY-SA 4.0,
